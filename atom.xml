<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>白菜君の技术库</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://blog.crazylaw.cn/"/>
  <updated>2024-06-18T13:51:14.857Z</updated>
  <id>http://blog.crazylaw.cn/</id>
  
  <author>
    <name>白菜(whiteCcinn)</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>mock服务架构设计(二)</title>
    <link href="http://blog.crazylaw.cn/2024/06/18/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/mock%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1(%E4%BA%8C)/"/>
    <id>http://blog.crazylaw.cn/2024/06/18/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/mock%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1(%E4%BA%8C)/</id>
    <published>2024-06-18T14:52:40.000Z</published>
    <updated>2024-06-18T13:51:14.857Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>上一篇《mock服务架构设计（一）》中，我们发现</p><a id="more"></a><p>以下会以一个常规的adx架构来进行说明。</p><p><img src="/images/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/mock%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/adx-%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84.png" alt="adx-服务架构"></p><p>在这个架构中，我们简单的把整体的架构，分为了<code>三部分</code>，分别是：</p><ul><li>中间件: <code>redis集群</code>，<code>mongo服务</code></li><li>业务程序: <code>adx</code></li><li>上游角色: <code>dsp</code></li></ul><p>在实际业务中我们的<code>redis</code>可能会丢服务进行一些例如<code>qps</code>，<code>临时缓存</code>的动作，所以是必不可少的一部分。<code>mongo</code>服务则是充当我们的<code>数据存储层</code>，<code>dsp</code>就是我们<code>adx</code>服务的上游，我们的广告竞价的玩家就是来自于<code>dsp</code>。</p><p>这三个角色大致了解和理解之后，我们会发现，一般的设计中，由于要符合adx的<code>低延时，高并发</code>的行为，我们一般会把服务至于同一个内网中（虽然实际的架构设计可能是多个网关，相互行为NAT行为的内部链路），用以减少我们内部之间的网络耗时。对外的请求，只有向<code>上游DSP</code>进行<code>询价</code>的时候。</p><p>所以既然我们要mock的话，那很显然，我们需要mock我们的dsp服务，但是我们并不希望去调整所谓的<code>上游EP</code>，或者在内部编写<code>hard code代码</code>，虽然在github开源库上我们找到了一些类似<code>滴滴开发</code>的<code>https://github.com/didi/sharingan</code>的<code>流量录制回放</code>服务。但是由于其使用上的繁琐和不便等因素，以及需求的本质性，我认为，暂时不需呀用到这种<code>大规模</code>的<code>流量录制回放</code>服务。</p><p>所以对于这个问题，只需要采用我们新的架构设计，就可以实现这一点。</p><p><img src="/images/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/mock%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/adx-dsp-mock%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84.png" alt="adx-dsp-mock服务架构"></p><p>在上图中，我们可以发现，我们引入了一个常见的角色<code>iptables</code>，这是一个网络防火墙工具，也是内核<code>netfiler</code>的<code>hook</code>工具，他能让我们的流量在经过系统内部的时候，对其进行额外的行为。</p><p><img src="/images/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/mock%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/Netfilter-packet-flow.svg" alt="Netfilter-packet-flow"></p><blockquote><p>Netfilter-packet-flow，这是netfilter系统的工作流程，感兴趣的需要额外去了解哈</p></blockquote><h2 id="理论"><a href="#理论" class="headerlink" title="理论"></a>理论</h2><p>我们借助iptables去实现每个hook节点的行为。用以实现</p><ol><li><code>adx服务器可以与Redis集群</code>正常通信</li><li><code>adx服务器可以与mongo</code>正常通信</li><li><code>adx对dsp的请求</code>需要<code>转发</code>到<code>mock-dsp服务器</code></li></ol><p>配置adx服务器的iptables规则如下：</p><ul><li>ADX服务器的IP为 <code>172.18.0.100/16</code></li><li>目标<code>mock-dsp</code>的IP为 <code>172.18.0.200/16</code>, 端口为<code>8080</code></li><li>Redis集群的IP范围为 <code>172.18.0.0/16</code>(具体ip不用理会), Redis集群的端口范围为 <code>7001-7006</code>, 总共6台机器</li><li>MongoDB服务IP范围为 <code>172.18.0.0/16</code>(具体ip不用理会), 的端口为 <code>27017</code></li></ul><h4 id="配置NAT表"><a href="#配置NAT表" class="headerlink" title="配置NAT表"></a>配置NAT表</h4><p>将<code>非Redis</code>和<code>非MongoDB</code>流量<code>转发</code>到<code>目标内网(MOCK-DSP机器</code>: </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 确保 Redis 和 MongoDB 的流量直接通过</span></span><br><span class="line">iptables -t nat -A OUTPUT -p tcp -d 172.18.0.0/16 --dport 7001:7006 -j ACCEPT</span><br><span class="line">iptables -t nat -A OUTPUT -p tcp -d 172.18.0.0/16 --dport 27017 -j ACCEPT</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 将非 Redis 和非 MongoDB 的流量转发到 MOCK-DSP机器</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 也就是外部的DSP请求全部转发到mock-dsp的机器上</span></span><br><span class="line">iptables -t nat -A OUTPUT -p tcp ! -d 172.18.0.0/16 -m multiport ! --dports 7001:7006,27017 -j DNAT --to-destination 172.18.0.200:8080</span><br></pre></td></tr></table></figure><h4 id="配置SNAT-MASQUERADE规则-确保返回路径正确"><a href="#配置SNAT-MASQUERADE规则-确保返回路径正确" class="headerlink" title="配置SNAT/MASQUERADE规则 (确保返回路径正确)"></a>配置SNAT/MASQUERADE规则 (确保返回路径正确)</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 确保 Redis 和 MongoDB 的流量直接通过</span></span><br><span class="line">iptables -t nat -A POSTROUTING -p tcp -d 172.18.0.0/16 --dport 7001:7006 -j ACCEPT</span><br><span class="line">iptables -t nat -A POSTROUTING -p tcp -d 172.18.0.0/16 --dport 27017 -j ACCEPT</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 确保转发到mock-dsp器的流量可以正确返回</span></span><br><span class="line">iptables -t nat -A POSTROUTING -d 172.18.0.200 -j MASQUERADE</span><br></pre></td></tr></table></figure><h3 id="配置filter表规则（可选，但推荐）"><a href="#配置filter表规则（可选，但推荐）" class="headerlink" title="配置filter表规则（可选，但推荐）"></a>配置filter表规则（可选，但推荐）</h3><blockquote><p>但是实际上我感觉没必要，只是按照规范来说，是需要加上</p></blockquote><p>配置INPUT规则（允许从Redis集群和MongoDB服务来的连接）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 允许来自 Redis 集群的连接</span></span><br><span class="line">iptables -A INPUT -p tcp -s 172.18.0.0/16 --dport 7001:7006 -m state --state NEW,ESTABLISHED -j ACCEPT</span><br><span class="line"><span class="meta">#</span><span class="bash"> 允许来自 MongoDB 服务的连接</span></span><br><span class="line">iptables -A INPUT -p tcp -s 172.18.0.0/16 --dport 27017 -m state --state NEW,ESTABLISHED -j ACCEPT</span><br></pre></td></tr></table></figure><p>配置OUTPUT规则（允许到Redis集群和MongoDB服务的连接）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 允许到 Redis 集群的连接</span></span><br><span class="line">iptables -A OUTPUT -p tcp -d 172.18.0.0/16 --dport 7001:7006 -m state --state NEW,ESTABLISHED -j ACCEPT</span><br><span class="line"><span class="meta">#</span><span class="bash"> 允许到 MongoDB 服务的连接</span></span><br><span class="line">iptables -A OUTPUT -p tcp -d 172.18.0.0/16 --dport 27017 -m state --state NEW,ESTABLISHED -j ACCEPT</span><br><span class="line"><span class="meta">#</span><span class="bash"> 允许到mock-dsp的转发流量（非 Redis 和非 MongoDB 流量）</span></span><br><span class="line">iptables -A OUTPUT -p tcp -d 172.18.0.200 -m state --state NEW,ESTABLISHED -j ACCEPT</span><br></pre></td></tr></table></figure><h2 id="完整的例子如下："><a href="#完整的例子如下：" class="headerlink" title="完整的例子如下："></a>完整的例子如下：</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> DNAT规则：将非 Redis 和非 MongoDB 的流量转发到MOCK-DSP机器</span></span><br><span class="line">iptables -t nat -A OUTPUT -p tcp -d 172.18.0.0/16 --dport 7001:7006 -j ACCEPT</span><br><span class="line">iptables -t nat -A OUTPUT -p tcp -d 172.18.0.0/16 --dport 27017 -j ACCEPT</span><br><span class="line">iptables -t nat -A OUTPUT -p tcp ! -d 172.18.0.0/16 -m multiport ! --dports 7001:7006,27017 -j DNAT --to-destination 172.18.0.200:8080</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> SNAT/MASQUERADE规则：确保返回路径正确</span></span><br><span class="line">iptables -t nat -A POSTROUTING -p tcp -d 172.18.0.0/16 --dport 7001:7006 -j ACCEPT</span><br><span class="line">iptables -t nat -A POSTROUTING -p tcp -d 172.18.0.0/16 --dport 27017 -j ACCEPT</span><br><span class="line">iptables -t nat -A POSTROUTING -d 172.18.0.200 -j MASQUERADE</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 允许来自 Redis 集群的连接</span></span><br><span class="line">iptables -A INPUT -p tcp -s 172.18.0.0/16 --dport 7001:7006 -m state --state NEW,ESTABLISHED -j ACCEPT</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 允许来自 MongoDB 服务的连接</span></span><br><span class="line">iptables -A INPUT -p tcp -s 172.18.0.0/16 --dport 27017 -m state --state NEW,ESTABLISHED -j ACCEPT</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 允许到 Redis 集群的连接</span></span><br><span class="line">iptables -A OUTPUT -p tcp -d 172.18.0.0/16 --dport 7001:7006 -m state --state NEW,ESTABLISHED -j ACCEPT</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 允许到 MongoDB 服务的连接</span></span><br><span class="line">iptables -A OUTPUT -p tcp -d 172.18.0.0/16 --dport 27017 -m state --state NEW,ESTABLISHED -j ACCEPT</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 允许到mock-dsp的转发流量（非 Redis 和非 MongoDB 流量）</span></span><br><span class="line">iptables -A OUTPUT -p tcp -d 172.18.0.200 -m state --state NEW,ESTABLISHED -j ACCEPT</span><br></pre></td></tr></table></figure><p>理论看明白之后，我们再回过头来看这个问题，是不是发现<code>**清晰明了**</code></p><p><img src="/images/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/mock%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/adx-dsp-mock%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84.png" alt="adx-dsp-mock服务架构"></p><h2 id="实战"><a href="#实战" class="headerlink" title="实战"></a>实战</h2><h3 id="本地docker模拟整体架构"><a href="#本地docker模拟整体架构" class="headerlink" title="本地docker模拟整体架构"></a>本地docker模拟整体架构</h3><p><img src="/images/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/mock%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/docker-c.jpg" alt="docker容器服务"></p><p><img src="/images/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/mock%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/network-ip.jpg" alt="容器的network信息"></p><p>这里，我们直接上2张图，分别代表了<code>docker容器服务</code>, <code>容器的network信息</code>。可以看到基本信息都不变, 重点关注一下这里的<code>&lt;dsp&gt; = &lt;mock dsp&gt;</code>， 并且实际ip从<code>172.18.0.200/16 =&gt; 172.168.0.10/16</code>。</p><p>获取这些信息之后，对我们<code>iptables-rule文件</code>调整了一下，代码如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 文件名：iptables-rules</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> filter表的规则</span></span><br><span class="line">*filter</span><br><span class="line">:INPUT ACCEPT [0:0]</span><br><span class="line">:FORWARD ACCEPT [0:0]</span><br><span class="line">:OUTPUT ACCEPT [0:0]</span><br><span class="line"><span class="meta">#</span><span class="bash"> 允许宿主机的请求请求可以在宿主机直接打到到我们的adx服务</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 其实不设置也可以，因为上面已经设置了默认协议为`ACCEPT`</span></span><br><span class="line">-A INPUT -p tcp -m state --state NEW -m tcp --dport 1372 -j ACCEPT</span><br><span class="line">COMMIT</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> nat表的规则</span></span><br><span class="line">*nat</span><br><span class="line">:PREROUTING ACCEPT [0:0]</span><br><span class="line">:INPUT ACCEPT [0:0]</span><br><span class="line">:POSTROUTING ACCEPT [0:0]</span><br><span class="line">:OUTPUT ACCEPT [0:0]</span><br><span class="line"></span><br><span class="line">-A OUTPUT -p tcp -d 172.18.0.0/16 --dport 7001:7006 -j ACCEPT</span><br><span class="line">-A OUTPUT -p tcp -d 172.18.0.0/16 --dport 27017 -j ACCEPT</span><br><span class="line">-A OUTPUT -p tcp ! -d 172.18.0.0/16 -m multiport ! --dports 7001:7006,27017 -j DNAT --to-destination 172.18.0.10:8080</span><br><span class="line"></span><br><span class="line">-A POSTROUTING -p tcp -d 172.18.0.0/16 --dport 7001:7006 -j ACCEPT</span><br><span class="line">-A POSTROUTING -p tcp -d 172.18.0.0/16 --dport 27017 -j ACCEPT</span><br><span class="line">-A POSTROUTING -d 172.18.0.10 -j MASQUERADE</span><br><span class="line"></span><br><span class="line">COMMIT</span><br></pre></td></tr></table></figure><p>在我们的<code>adx服务器上设置iptables规则</code>，执行命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iptables-restore &lt; iptables-rules</span><br></pre></td></tr></table></figure><p>查看我们服务器上的iptables规则：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">root@87cb4c5933af:# iptables -nvL -t nat</span><br><span class="line">Chain PREROUTING (policy ACCEPT 2 packets, 120 bytes)</span><br><span class="line"> pkts bytes target     prot opt in     out     source               destination</span><br><span class="line"></span><br><span class="line">Chain INPUT (policy ACCEPT 2 packets, 120 bytes)</span><br><span class="line"> pkts bytes target     prot opt in     out     source               destination</span><br><span class="line"></span><br><span class="line">Chain POSTROUTING (policy ACCEPT 367 packets, 21822 bytes)</span><br><span class="line"> pkts bytes target     prot opt in     out     source               destination</span><br><span class="line">   11   660 ACCEPT     tcp  --  *      *       0.0.0.0/0            172.18.0.0/16        tcp dpts:7001:7006</span><br><span class="line">    2   120 ACCEPT     tcp  --  *      *       0.0.0.0/0            172.18.0.0/16        tcp dpt:27017</span><br><span class="line">    5   300 MASQUERADE  all  --  *      *       0.0.0.0/0            172.18.0.10</span><br><span class="line"></span><br><span class="line">Chain OUTPUT (policy ACCEPT 367 packets, 21822 bytes)</span><br><span class="line"> pkts bytes target     prot opt in     out     source               destination</span><br><span class="line">   11   660 ACCEPT     tcp  --  *      *       0.0.0.0/0            172.18.0.0/16        tcp dpts:7001:7006</span><br><span class="line">    2   120 ACCEPT     tcp  --  *      *       0.0.0.0/0            172.18.0.0/16        tcp dpt:27017</span><br><span class="line">    5   300 DNAT       tcp  --  *      *       0.0.0.0/0           !172.18.0.0/16        multiport dports  !7001:7006,27017 to:172.18.0.10:8080</span><br><span class="line"><span class="meta">#</span><span class="bash"> Warning: iptables-legacy tables present, use iptables-legacy to see them</span></span><br></pre></td></tr></table></figure><p>这里可以看到，我们的规则已经按要求<code>生效</code></p><p>接下来就是接着我们的启动<code>adx服务</code>和 <code>mock-dsp服务</code></p><p><img src="/images/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/mock%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/adx-%E6%9C%8D%E5%8A%A1%E5%90%AF%E5%8A%A8.jpg" alt="adx-服务启动"></p><p><img src="/images/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/mock%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/dsp-%E6%9C%8D%E5%8A%A1%E5%90%AF%E5%8A%A8.jpg" alt="dsp-服务启动"></p><p>启动完毕之后，我们在宿主机把入参请求发到容器中的adx服务中</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">req=`cat filter.json| jq '.req'`;curl -X POST 'http://127.0.0.1:1372/adx_api?pubid=xxxxx' -d "$req"</span><br></pre></td></tr></table></figure><blockquote><p>这里的filter.json，是从线上bid日志进行match下来的信息，也是后续我们要放到mock-dsp中的数据源<br>当然，更好的方式是从kafaka中自动更新信息</p></blockquote><p><img src="/images/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/mock%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B.jpg" alt="三次握手.jpg"></p><p>通过 <code>tcpdump -n</code>，我们可以看到，这里<code>三次握手的SYN</code>第一步都是向着<code>mock-dsp</code>的机器发出的请求，也就是<code>172.18.0.10/16</code> 这个地址发出，所以我们的<code>nat表</code>的对<code>流量转发</code><strong>完美</strong>的处理了所有流量的流向。</p><p>并且不一会儿我们也得到了一段<code>正常的广告offer填充！</code></p><p><img src="/images/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/mock%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/req.jpg" alt="req.jpg"></p><h2 id="mock-dsp的简易代码："><a href="#mock-dsp的简易代码：" class="headerlink" title="mock-dsp的简易代码："></a>mock-dsp的简易代码：</h2><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line"><span class="string">"fmt"</span></span><br><span class="line"><span class="string">"github.com/gin-gonic/gin"</span></span><br><span class="line"><span class="string">"github.com/goccy/go-json"</span></span><br><span class="line"><span class="string">"io/ioutil"</span></span><br><span class="line"><span class="string">"net/http"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> FileT <span class="keyword">struct</span> &#123;</span><br><span class="line">Req  <span class="keyword">interface</span>&#123;&#125; <span class="string">`json:"req"`</span></span><br><span class="line">Resp <span class="keyword">interface</span>&#123;&#125; <span class="string">`json:"resp"`</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line"><span class="comment">// 启动gin框架，采用默认配置</span></span><br><span class="line">router := gin.Default()</span><br><span class="line">b, err := ioutil.ReadFile(<span class="string">"filter.json"</span>)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="built_in">panic</span>(err)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> f FileT</span><br><span class="line">err = json.Unmarshal(b, &amp;f)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="built_in">panic</span>(err)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 编写匿名的handler函数</span></span><br><span class="line">router.POST(<span class="string">"/request/:dsp_id"</span>, <span class="function"><span class="keyword">func</span><span class="params">(c *gin.Context)</span></span> &#123;</span><br><span class="line">dspId := c.Param(<span class="string">"dsp_id"</span>)</span><br><span class="line">c.JSON(http.StatusOK, f.Resp)</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line">router.GET(<span class="string">"/request/:dsp_id"</span>, <span class="function"><span class="keyword">func</span><span class="params">(c *gin.Context)</span></span> &#123;</span><br><span class="line">dspId := c.Param(<span class="string">"dsp_id"</span>)</span><br><span class="line">c.JSON(http.StatusOK, f.Resp)</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line">router.Run() <span class="comment">//:8080</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>后续我们可以对这个mock-dsp进行更加丰富的针对性处理</p><ul><li>通过kafka代替文件的方式定时更新offer</li><li>针对<code>adformat</code>的和<code>dsp_id</code>进行匹配，得到不同offer的返回</li></ul><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>这一次，我们通过这种<code>架构设计</code>，可以做到后续我们对我们的一些需求得到更好的支持。</p><ul><li>代码调优</li><li>开发同学自测逻辑</li><li>测试同学测试逻辑</li><li>对服务进行压测</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;上一篇《mock服务架构设计（一）》中，我们发现&lt;/p&gt;
    
    </summary>
    
    
      <category term="架构设计" scheme="http://blog.crazylaw.cn/categories/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/"/>
    
    
      <category term="mock" scheme="http://blog.crazylaw.cn/tags/mock/"/>
    
      <category term="机器学习" scheme="http://blog.crazylaw.cn/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="随机森林" scheme="http://blog.crazylaw.cn/tags/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/"/>
    
  </entry>
  
  <entry>
    <title>mock服务架构设计(一)</title>
    <link href="http://blog.crazylaw.cn/2024/05/25/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/mock%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1(%E4%B8%80)/"/>
    <id>http://blog.crazylaw.cn/2024/05/25/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/mock%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1(%E4%B8%80)/</id>
    <published>2024-05-25T03:52:40.000Z</published>
    <updated>2024-06-18T13:47:47.003Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在公司的日常adx开发流程中，我发现其中一个很费心，很费时的行为，那就是要拿到上游的广告offer。</p><p>要拿到一个正常的offer，不单单需要匹配内部的逻辑，还要看上游是否需要对你的这次展示机会进行竞价。</p><p>由于在adx的业务逻辑中，<code>一次请求</code>会同时<code>请求N次</code>上游，所以必然少不了对上游的<code>询价(请求)</code>行为，那我们在日常开发，以及做代码调试的过程中，有没有办法可以更好的做到能拿到offer来对我们的竞价行为进行调试和开发呢？</p><p>还有一种场景就是测试同学会面临的，就是如何在尽量不影响线上数据的情况下，能安全无痛的进行测试adx呢，如果因为测试的行为，导致线上业务数据异常，或者出现了结算问题，对于大规模流量的广告自动化交易程序来说，这都是致命的，严重点来说，可能随时导致破产。</p><p>又换句话说，有没有办法能在我们想要做模拟压测的时候，也能进行正常的内部逻辑呢？其中一个难点就是我们在压测的环节，并<strong>不想</strong>正式去对<code>上游询价</code>，但是我又想要拿到上游的offer，除了 <code>hard code offer</code>的动作外，有没有一些更理想的架构方案呢？</p><p>答案是：有的，接下来就是我们的主题</p><a id="more"></a><p>以下会以一个常规的adx架构来进行说明。</p><p><img src="/images/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/mock%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/adx-%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84.png" alt="adx-服务架构"></p><p>在这个架构中，我们简单的把整体的架构，分为了<code>三部分</code>，分别是：</p><ul><li>中间件: <code>redis集群</code>，<code>mongo服务</code></li><li>业务程序: <code>adx</code></li><li>上游角色: <code>dsp</code></li></ul><p>在实际业务中我们的<code>redis</code>可能会丢服务进行一些例如<code>qps</code>，<code>临时缓存</code>的动作，所以是必不可少的一部分。<code>mongo</code>服务则是充当我们的<code>数据存储层</code>，<code>dsp</code>就是我们<code>adx</code>服务的上游，我们的广告竞价的玩家就是来自于<code>dsp</code>。</p><p>这三个角色大致了解和理解之后，我们会发现，一般的设计中，由于要符合adx的<code>低延时，高并发</code>的行为，我们一般会把服务至于同一个内网中（虽然实际的架构设计可能是多个网关，相互行为NAT行为的内部链路），用以减少我们内部之间的网络耗时。对外的请求，只有向<code>上游DSP</code>进行<code>询价</code>的时候。</p><p>所以既然我们要mock的话，那很显然，我们需要mock我们的dsp服务，但是我们并不希望去调整所谓的<code>上游EP</code>，或者在内部编写<code>hard code代码</code>，虽然在github开源库上我们找到了一些类似<code>滴滴开发</code>的<code>https://github.com/didi/sharingan</code>的<code>流量录制回放</code>服务。但是由于其使用上的繁琐和不便等因素，以及需求的本质性，我认为，暂时不需呀用到这种<code>大规模</code>的<code>流量录制回放</code>服务。</p><p>所以对于这个问题，只需要采用我们新的架构设计，就可以实现这一点。</p><p><img src="/images/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/mock%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/adx-dsp-mock%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84.png" alt="adx-dsp-mock服务架构"></p><p>在上图中，我们可以发现，我们引入了一个常见的角色<code>iptables</code>，这是一个网络防火墙工具，也是内核<code>netfiler</code>的<code>hook</code>工具，他能让我们的流量在经过系统内部的时候，对其进行额外的行为。</p><p><img src="/images/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/mock%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/Netfilter-packet-flow.svg" alt="Netfilter-packet-flow"></p><blockquote><p>Netfilter-packet-flow，这是netfilter系统的工作流程，感兴趣的需要额外去了解哈</p></blockquote><h2 id="理论"><a href="#理论" class="headerlink" title="理论"></a>理论</h2><p>我们借助iptables去实现每个hook节点的行为。用以实现</p><ol><li><code>adx服务器可以与Redis集群</code>正常通信</li><li><code>adx服务器可以与mongo</code>正常通信</li><li><code>adx对dsp的请求</code>需要<code>转发</code>到<code>mock-dsp服务器</code></li></ol><p>配置adx服务器的iptables规则如下：</p><ul><li>ADX服务器的IP为 <code>172.18.0.100/16</code></li><li>目标<code>mock-dsp</code>的IP为 <code>172.18.0.200/16</code>, 端口为<code>8080</code></li><li>Redis集群的IP范围为 <code>172.18.0.0/16</code>(具体ip不用理会), Redis集群的端口范围为 <code>7001-7006</code>, 总共6台机器</li><li>MongoDB服务IP范围为 <code>172.18.0.0/16</code>(具体ip不用理会), 的端口为 <code>27017</code></li></ul><h4 id="配置NAT表"><a href="#配置NAT表" class="headerlink" title="配置NAT表"></a>配置NAT表</h4><p>将<code>非Redis</code>和<code>非MongoDB</code>流量<code>转发</code>到<code>目标内网(MOCK-DSP机器</code>: </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 确保 Redis 和 MongoDB 的流量直接通过</span></span><br><span class="line">iptables -t nat -A OUTPUT -p tcp -d 172.18.0.0/16 --dport 7001:7006 -j ACCEPT</span><br><span class="line">iptables -t nat -A OUTPUT -p tcp -d 172.18.0.0/16 --dport 27017 -j ACCEPT</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 将非 Redis 和非 MongoDB 的流量转发到 MOCK-DSP机器</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 也就是外部的DSP请求全部转发到mock-dsp的机器上</span></span><br><span class="line">iptables -t nat -A OUTPUT -p tcp ! -d 172.18.0.0/16 -m multiport ! --dports 7001:7006,27017 -j DNAT --to-destination 172.18.0.200:8080</span><br></pre></td></tr></table></figure><h4 id="配置SNAT-MASQUERADE规则-确保返回路径正确"><a href="#配置SNAT-MASQUERADE规则-确保返回路径正确" class="headerlink" title="配置SNAT/MASQUERADE规则 (确保返回路径正确)"></a>配置SNAT/MASQUERADE规则 (确保返回路径正确)</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 确保 Redis 和 MongoDB 的流量直接通过</span></span><br><span class="line">iptables -t nat -A POSTROUTING -p tcp -d 172.18.0.0/16 --dport 7001:7006 -j ACCEPT</span><br><span class="line">iptables -t nat -A POSTROUTING -p tcp -d 172.18.0.0/16 --dport 27017 -j ACCEPT</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 确保转发到mock-dsp器的流量可以正确返回</span></span><br><span class="line">iptables -t nat -A POSTROUTING -d 172.18.0.200 -j MASQUERADE</span><br></pre></td></tr></table></figure><h3 id="配置filter表规则（可选，但推荐）"><a href="#配置filter表规则（可选，但推荐）" class="headerlink" title="配置filter表规则（可选，但推荐）"></a>配置filter表规则（可选，但推荐）</h3><blockquote><p>但是实际上我感觉没必要，只是按照规范来说，是需要加上</p></blockquote><p>配置INPUT规则（允许从Redis集群和MongoDB服务来的连接）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 允许来自 Redis 集群的连接</span></span><br><span class="line">iptables -A INPUT -p tcp -s 172.18.0.0/16 --dport 7001:7006 -m state --state NEW,ESTABLISHED -j ACCEPT</span><br><span class="line"><span class="meta">#</span><span class="bash"> 允许来自 MongoDB 服务的连接</span></span><br><span class="line">iptables -A INPUT -p tcp -s 172.18.0.0/16 --dport 27017 -m state --state NEW,ESTABLISHED -j ACCEPT</span><br></pre></td></tr></table></figure><p>配置OUTPUT规则（允许到Redis集群和MongoDB服务的连接）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 允许到 Redis 集群的连接</span></span><br><span class="line">iptables -A OUTPUT -p tcp -d 172.18.0.0/16 --dport 7001:7006 -m state --state NEW,ESTABLISHED -j ACCEPT</span><br><span class="line"><span class="meta">#</span><span class="bash"> 允许到 MongoDB 服务的连接</span></span><br><span class="line">iptables -A OUTPUT -p tcp -d 172.18.0.0/16 --dport 27017 -m state --state NEW,ESTABLISHED -j ACCEPT</span><br><span class="line"><span class="meta">#</span><span class="bash"> 允许到mock-dsp的转发流量（非 Redis 和非 MongoDB 流量）</span></span><br><span class="line">iptables -A OUTPUT -p tcp -d 172.18.0.200 -m state --state NEW,ESTABLISHED -j ACCEPT</span><br></pre></td></tr></table></figure><h2 id="完整的例子如下："><a href="#完整的例子如下：" class="headerlink" title="完整的例子如下："></a>完整的例子如下：</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> DNAT规则：将非 Redis 和非 MongoDB 的流量转发到MOCK-DSP机器</span></span><br><span class="line">iptables -t nat -A OUTPUT -p tcp -d 172.18.0.0/16 --dport 7001:7006 -j ACCEPT</span><br><span class="line">iptables -t nat -A OUTPUT -p tcp -d 172.18.0.0/16 --dport 27017 -j ACCEPT</span><br><span class="line">iptables -t nat -A OUTPUT -p tcp ! -d 172.18.0.0/16 -m multiport ! --dports 7001:7006,27017 -j DNAT --to-destination 172.18.0.200:8080</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> SNAT/MASQUERADE规则：确保返回路径正确</span></span><br><span class="line">iptables -t nat -A POSTROUTING -p tcp -d 172.18.0.0/16 --dport 7001:7006 -j ACCEPT</span><br><span class="line">iptables -t nat -A POSTROUTING -p tcp -d 172.18.0.0/16 --dport 27017 -j ACCEPT</span><br><span class="line">iptables -t nat -A POSTROUTING -d 172.18.0.200 -j MASQUERADE</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 允许来自 Redis 集群的连接</span></span><br><span class="line">iptables -A INPUT -p tcp -s 172.18.0.0/16 --dport 7001:7006 -m state --state NEW,ESTABLISHED -j ACCEPT</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 允许来自 MongoDB 服务的连接</span></span><br><span class="line">iptables -A INPUT -p tcp -s 172.18.0.0/16 --dport 27017 -m state --state NEW,ESTABLISHED -j ACCEPT</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 允许到 Redis 集群的连接</span></span><br><span class="line">iptables -A OUTPUT -p tcp -d 172.18.0.0/16 --dport 7001:7006 -m state --state NEW,ESTABLISHED -j ACCEPT</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 允许到 MongoDB 服务的连接</span></span><br><span class="line">iptables -A OUTPUT -p tcp -d 172.18.0.0/16 --dport 27017 -m state --state NEW,ESTABLISHED -j ACCEPT</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 允许到mock-dsp的转发流量（非 Redis 和非 MongoDB 流量）</span></span><br><span class="line">iptables -A OUTPUT -p tcp -d 172.18.0.200 -m state --state NEW,ESTABLISHED -j ACCEPT</span><br></pre></td></tr></table></figure><p>理论看明白之后，我们再回过头来看这个问题，是不是发现<code>**清晰明了**</code></p><p><img src="/images/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/mock%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/adx-dsp-mock%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84.png" alt="adx-dsp-mock服务架构"></p><h2 id="实战"><a href="#实战" class="headerlink" title="实战"></a>实战</h2><h3 id="本地docker模拟整体架构"><a href="#本地docker模拟整体架构" class="headerlink" title="本地docker模拟整体架构"></a>本地docker模拟整体架构</h3><p><img src="/images/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/mock%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/docker-c.jpg" alt="docker容器服务"></p><p><img src="/images/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/mock%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/network-ip.jpg" alt="容器的network信息"></p><p>这里，我们直接上2张图，分别代表了<code>docker容器服务</code>, <code>容器的network信息</code>。可以看到基本信息都不变, 重点关注一下这里的<code>&lt;dsp&gt; = &lt;mock dsp&gt;</code>， 并且实际ip从<code>172.18.0.200/16 =&gt; 172.168.0.10/16</code>。</p><p>获取这些信息之后，对我们<code>iptables-rule文件</code>调整了一下，代码如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 文件名：iptables-rules</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> filter表的规则</span></span><br><span class="line">*filter</span><br><span class="line">:INPUT ACCEPT [0:0]</span><br><span class="line">:FORWARD ACCEPT [0:0]</span><br><span class="line">:OUTPUT ACCEPT [0:0]</span><br><span class="line"><span class="meta">#</span><span class="bash"> 允许宿主机的请求请求可以在宿主机直接打到到我们的adx服务</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 其实不设置也可以，因为上面已经设置了默认协议为`ACCEPT`</span></span><br><span class="line">-A INPUT -p tcp -m state --state NEW -m tcp --dport 1372 -j ACCEPT</span><br><span class="line">COMMIT</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> nat表的规则</span></span><br><span class="line">*nat</span><br><span class="line">:PREROUTING ACCEPT [0:0]</span><br><span class="line">:INPUT ACCEPT [0:0]</span><br><span class="line">:POSTROUTING ACCEPT [0:0]</span><br><span class="line">:OUTPUT ACCEPT [0:0]</span><br><span class="line"></span><br><span class="line">-A OUTPUT -p tcp -d 172.18.0.0/16 --dport 7001:7006 -j ACCEPT</span><br><span class="line">-A OUTPUT -p tcp -d 172.18.0.0/16 --dport 27017 -j ACCEPT</span><br><span class="line">-A OUTPUT -p tcp ! -d 172.18.0.0/16 -m multiport ! --dports 7001:7006,27017 -j DNAT --to-destination 172.18.0.10:8080</span><br><span class="line"></span><br><span class="line">-A POSTROUTING -p tcp -d 172.18.0.0/16 --dport 7001:7006 -j ACCEPT</span><br><span class="line">-A POSTROUTING -p tcp -d 172.18.0.0/16 --dport 27017 -j ACCEPT</span><br><span class="line">-A POSTROUTING -d 172.18.0.10 -j MASQUERADE</span><br><span class="line"></span><br><span class="line">COMMIT</span><br></pre></td></tr></table></figure><p>在我们的<code>adx服务器上设置iptables规则</code>，执行命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iptables-restore &lt; iptables-rules</span><br></pre></td></tr></table></figure><p>查看我们服务器上的iptables规则：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">root@87cb4c5933af:# iptables -nvL -t nat</span><br><span class="line">Chain PREROUTING (policy ACCEPT 2 packets, 120 bytes)</span><br><span class="line"> pkts bytes target     prot opt in     out     source               destination</span><br><span class="line"></span><br><span class="line">Chain INPUT (policy ACCEPT 2 packets, 120 bytes)</span><br><span class="line"> pkts bytes target     prot opt in     out     source               destination</span><br><span class="line"></span><br><span class="line">Chain POSTROUTING (policy ACCEPT 367 packets, 21822 bytes)</span><br><span class="line"> pkts bytes target     prot opt in     out     source               destination</span><br><span class="line">   11   660 ACCEPT     tcp  --  *      *       0.0.0.0/0            172.18.0.0/16        tcp dpts:7001:7006</span><br><span class="line">    2   120 ACCEPT     tcp  --  *      *       0.0.0.0/0            172.18.0.0/16        tcp dpt:27017</span><br><span class="line">    5   300 MASQUERADE  all  --  *      *       0.0.0.0/0            172.18.0.10</span><br><span class="line"></span><br><span class="line">Chain OUTPUT (policy ACCEPT 367 packets, 21822 bytes)</span><br><span class="line"> pkts bytes target     prot opt in     out     source               destination</span><br><span class="line">   11   660 ACCEPT     tcp  --  *      *       0.0.0.0/0            172.18.0.0/16        tcp dpts:7001:7006</span><br><span class="line">    2   120 ACCEPT     tcp  --  *      *       0.0.0.0/0            172.18.0.0/16        tcp dpt:27017</span><br><span class="line">    5   300 DNAT       tcp  --  *      *       0.0.0.0/0           !172.18.0.0/16        multiport dports  !7001:7006,27017 to:172.18.0.10:8080</span><br><span class="line"><span class="meta">#</span><span class="bash"> Warning: iptables-legacy tables present, use iptables-legacy to see them</span></span><br></pre></td></tr></table></figure><p>这里可以看到，我们的规则已经按要求<code>生效</code></p><p>接下来就是接着我们的启动<code>adx服务</code>和 <code>mock-dsp服务</code></p><p><img src="/images/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/mock%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/adx-%E6%9C%8D%E5%8A%A1%E5%90%AF%E5%8A%A8.jpg" alt="adx-服务启动"></p><p><img src="/images/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/mock%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/dsp-%E6%9C%8D%E5%8A%A1%E5%90%AF%E5%8A%A8.jpg" alt="dsp-服务启动"></p><p>启动完毕之后，我们在宿主机把入参请求发到容器中的adx服务中</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">req=`cat filter.json| jq '.req'`;curl -X POST 'http://127.0.0.1:1372/adx_api?pubid=xxxxx' -d "$req"</span><br></pre></td></tr></table></figure><blockquote><p>这里的filter.json，是从线上bid日志进行match下来的信息，也是后续我们要放到mock-dsp中的数据源<br>当然，更好的方式是从kafaka中自动更新信息</p></blockquote><p><img src="/images/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/mock%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B.jpg" alt="三次握手.jpg"></p><p>通过 <code>tcpdump -n</code>，我们可以看到，这里<code>三次握手的SYN</code>第一步都是向着<code>mock-dsp</code>的机器发出的请求，也就是<code>172.18.0.10/16</code> 这个地址发出，所以我们的<code>nat表</code>的对<code>流量转发</code><strong>完美</strong>的处理了所有流量的流向。</p><p>并且不一会儿我们也得到了一段<code>正常的广告offer填充！</code></p><p><img src="/images/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/mock%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/req.jpg" alt="req.jpg"></p><h2 id="mock-dsp的简易代码："><a href="#mock-dsp的简易代码：" class="headerlink" title="mock-dsp的简易代码："></a>mock-dsp的简易代码：</h2><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line"><span class="string">"fmt"</span></span><br><span class="line"><span class="string">"github.com/gin-gonic/gin"</span></span><br><span class="line"><span class="string">"github.com/goccy/go-json"</span></span><br><span class="line"><span class="string">"io/ioutil"</span></span><br><span class="line"><span class="string">"net/http"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> FileT <span class="keyword">struct</span> &#123;</span><br><span class="line">Req  <span class="keyword">interface</span>&#123;&#125; <span class="string">`json:"req"`</span></span><br><span class="line">Resp <span class="keyword">interface</span>&#123;&#125; <span class="string">`json:"resp"`</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line"><span class="comment">// 启动gin框架，采用默认配置</span></span><br><span class="line">router := gin.Default()</span><br><span class="line">b, err := ioutil.ReadFile(<span class="string">"filter.json"</span>)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="built_in">panic</span>(err)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> f FileT</span><br><span class="line">err = json.Unmarshal(b, &amp;f)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="built_in">panic</span>(err)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 编写匿名的handler函数</span></span><br><span class="line">router.POST(<span class="string">"/request/:dsp_id"</span>, <span class="function"><span class="keyword">func</span><span class="params">(c *gin.Context)</span></span> &#123;</span><br><span class="line">dspId := c.Param(<span class="string">"dsp_id"</span>)</span><br><span class="line">c.JSON(http.StatusOK, f.Resp)</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line">router.GET(<span class="string">"/request/:dsp_id"</span>, <span class="function"><span class="keyword">func</span><span class="params">(c *gin.Context)</span></span> &#123;</span><br><span class="line">dspId := c.Param(<span class="string">"dsp_id"</span>)</span><br><span class="line">c.JSON(http.StatusOK, f.Resp)</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line">router.Run() <span class="comment">//:8080</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>后续我们可以对这个mock-dsp进行更加丰富的针对性处理</p><ul><li>通过kafka代替文件的方式定时更新offer</li><li>针对<code>adformat</code>的和<code>dsp_id</code>进行匹配，得到不同offer的返回</li></ul><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>这一次，我们通过这种<code>架构设计</code>，可以做到后续我们对我们的一些需求得到更好的支持。</p><ul><li>代码调优</li><li>开发同学自测逻辑</li><li>测试同学测试逻辑</li><li>对服务进行压测</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;在公司的日常adx开发流程中，我发现其中一个很费心，很费时的行为，那就是要拿到上游的广告offer。&lt;/p&gt;
&lt;p&gt;要拿到一个正常的offer，不单单需要匹配内部的逻辑，还要看上游是否需要对你的这次展示机会进行竞价。&lt;/p&gt;
&lt;p&gt;由于在adx的业务逻辑中，&lt;code&gt;一次请求&lt;/code&gt;会同时&lt;code&gt;请求N次&lt;/code&gt;上游，所以必然少不了对上游的&lt;code&gt;询价(请求)&lt;/code&gt;行为，那我们在日常开发，以及做代码调试的过程中，有没有办法可以更好的做到能拿到offer来对我们的竞价行为进行调试和开发呢？&lt;/p&gt;
&lt;p&gt;还有一种场景就是测试同学会面临的，就是如何在尽量不影响线上数据的情况下，能安全无痛的进行测试adx呢，如果因为测试的行为，导致线上业务数据异常，或者出现了结算问题，对于大规模流量的广告自动化交易程序来说，这都是致命的，严重点来说，可能随时导致破产。&lt;/p&gt;
&lt;p&gt;又换句话说，有没有办法能在我们想要做模拟压测的时候，也能进行正常的内部逻辑呢？其中一个难点就是我们在压测的环节，并&lt;strong&gt;不想&lt;/strong&gt;正式去对&lt;code&gt;上游询价&lt;/code&gt;，但是我又想要拿到上游的offer，除了 &lt;code&gt;hard code offer&lt;/code&gt;的动作外，有没有一些更理想的架构方案呢？&lt;/p&gt;
&lt;p&gt;答案是：有的，接下来就是我们的主题&lt;/p&gt;
    
    </summary>
    
    
      <category term="架构设计" scheme="http://blog.crazylaw.cn/categories/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/"/>
    
    
      <category term="mock" scheme="http://blog.crazylaw.cn/tags/mock/"/>
    
      <category term="iptables" scheme="http://blog.crazylaw.cn/tags/iptables/"/>
    
  </entry>
  
  <entry>
    <title>树莓派4b Home Assistant</title>
    <link href="http://blog.crazylaw.cn/2024/01/05/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/Home-Assistant-pi/"/>
    <id>http://blog.crazylaw.cn/2024/01/05/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/Home-Assistant-pi/</id>
    <published>2024-01-04T16:00:19.000Z</published>
    <updated>2024-01-10T10:55:23.444Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>前面做了几期关于<code>Home assistant</code>的教程，在前面我是完全使用docker手动部署，并且采用基本都是通过安装<code>home-assistant-core</code>去处理。后面我发现很多东西的使用方式都特别别扭，而且确实很多东西都比较麻烦，所以后面再反过来看，发现了一个叫<code>hass</code>的生态，这个其实就是<code>home-assistant</code>的周边生态，而这些周边，几乎都是必备的，于是我妥协了使用了<code>hass</code>的生态，但是有一点不同的是，我这里并没有使用<code>hassOS</code>，而是采用<code>hassIO</code>，也就是<code>home-assistant-supervisor</code>。</p><p><img src="/images/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/ha-install-compare.jpg" alt="ha-install-compare.jpg"></p><p>下面开始，我就会我的正式体现记录我后面的智能家居做法。</p><a id="more"></a><h2 id="硬件"><a href="#硬件" class="headerlink" title="硬件"></a>硬件</h2><ul><li>树莓派4b+一个（我在写这篇文章的时候，已经出了树莓派5，但是由于ha还没公布稳定支持树莓派5，所以我还是购入了4b+，后面稳定后再替换）</li></ul><h2 id="效果展示"><a href="#效果展示" class="headerlink" title="效果展示"></a>效果展示</h2><p><img src="/images/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/pi-ha-docker.jpg" alt="pi-ha-docker"></p><h2 id="Home-Assistant-Supervisor-安装"><a href="#Home-Assistant-Supervisor-安装" class="headerlink" title="Home Assistant Supervisor 安装"></a>Home Assistant Supervisor 安装</h2><h3 id="切换为root用户"><a href="#切换为root用户" class="headerlink" title="切换为root用户"></a>切换为root用户</h3><p><code>ccinn@raspberrypi:~ $ sudo su -</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt update -y</span><br><span class="line">sudo apt upgrade -y</span><br></pre></td></tr></table></figure><p>查看树莓派版本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ lsb_release -a</span><br><span class="line">No LSB modules are available.</span><br><span class="line">Distributor ID:Debian</span><br><span class="line">Description:Debian GNU/Linux 12 (bookworm)</span><br><span class="line">Release:12</span><br><span class="line">Codename:bookworm</span><br></pre></td></tr></table></figure><p>如果要完整支持<code>Home Assistant Supervised</code>，最好根据 <img src="https://www.home-assistant.io/more-info/unsupported/os" alt="Home Assistant Supervised官方支持系统说明查"> 看本地的系统是否满足</p><p><img src="/images/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/ha-install-condition.jpg" alt="ha-install-condition"></p><p>可以看到，截止到文章发布，官方文档说明是需要Debian12，我的树莓派系统是满足的。</p><h3 id="NetworkManager"><a href="#NetworkManager" class="headerlink" title="NetworkManager"></a>NetworkManager</h3><p>HomeAssistantSupervised需要NetworkManager的支持，树莓派官方使用的是ModemManager、openresolv和dhcpcd5。</p><p>原本的配置可以不动，但是需要固定Mac地址并禁用ModemManger。</p><p>因为NetworkManager一旦安装就会开始工作。所以我们先创建一个NetworkManager的配置文件来固定Mac地址，防止我们后续操作重启时候，树莓派的Mac地址频繁变更。</p><h4 id="固定Mac地址"><a href="#固定Mac地址" class="headerlink" title="固定Mac地址"></a>固定Mac地址</h4><p>预编写NetworkManager的配置：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建配置目录和文件</span></span><br><span class="line">sudo mkdir -p /etc/NetworkManager/conf.d/</span><br><span class="line"><span class="comment"># 对文件追加内容</span></span><br><span class="line">sudo vim /etc/NetworkManager/conf.d/100-disable-wifi-mac-randomization.conf</span><br></pre></td></tr></table></figure><p>追加的内容：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[connection]</span><br><span class="line">wifi.mac-address-randomization=1</span><br><span class="line"></span><br><span class="line">[device]</span><br><span class="line">wifi.scan-rand-mac-address=no</span><br></pre></td></tr></table></figure><p><img src="/images/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/ha-network-manager.jpg" alt="ha-network-manager"></p><p>之后就可以安装network-manager了。</p><blockquote><p>安装完Network-Manager后，网络可能会出现短暂的丢包。这个时候多等一下就好，并且在完成ModemManager的禁用前，请勿重启树莓派系统！！！</p></blockquote><p>安装：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install -y network-manager</span><br></pre></td></tr></table></figure><h3 id="禁用ModemManager"><a href="#禁用ModemManager" class="headerlink" title="禁用ModemManager"></a>禁用ModemManager</h3><p>有些教程会让你卸载<code>dhcpcd5</code>，但是这样重启后需要重新配置网络，并且不能用树莓派的方法配置，这让我这没有显示器的用户很苦恼……所以这里我们就不卸载<code>dhcpcd5</code>，直接禁用<code>ModemManager</code>即可。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 停止ModemManager</span></span><br><span class="line">sudo systemctl stop ModemManager</span><br><span class="line"><span class="comment"># 禁止ModemManager开机自启</span></span><br><span class="line">sudo systemctl <span class="built_in">disable</span> ModemManager</span><br></pre></td></tr></table></figure><p><img src="/images/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/ha-modem-manager.jpg" alt="ha-modem-manager"></p><p>到此，NetworkManager部分就准备好了。</p><h3 id="Apparmor"><a href="#Apparmor" class="headerlink" title="Apparmor"></a>Apparmor</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install -y apparmor-utils jq software-properties-common apt-transport-https avahi-daemon ca-certificates curl dbus socat</span><br></pre></td></tr></table></figure><p>但是需要注意，需要把Apparmor的启动配置参数加到树莓派的启动参数内（参考自：<a href="https://github.com/Kanga-Who/home-assistant/issues/25）：" target="_blank" rel="noopener">https://github.com/Kanga-Who/home-assistant/issues/25）：</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用vim打开/boot/cmdline.txt</span></span><br><span class="line">sudo vim /boot/cmdline.txt</span><br></pre></td></tr></table></figure><p>末尾添加：<code>apparmor=1 security=apparmor</code></p><h3 id="OS-Agent"><a href="#OS-Agent" class="headerlink" title="OS-Agent"></a>OS-Agent</h3><p>还需要安装OS Agent。这个并没有在Debian的软件源内，所以我们需要使用<code>dpkg安装</code>。最新OS Agent的下载地址：<a href="https://github.com/home-assistant/os-agent/releases/latest" target="_blank" rel="noopener">https://github.com/home-assistant/os-agent/releases/latest</a></p><blockquote><p>记得找最新的aarch版</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下载OS Agent 1.2.2</span></span><br><span class="line">wget https://github.com/home-assistant/os-agent/releases/download/1.2.2/os-agent_1.2.2_linux_aarch64.deb</span><br><span class="line"><span class="comment"># 使用dpkg安装</span></span><br><span class="line">sudo dpkg -i os-agent_1.2.2_linux_aarch64.deb</span><br></pre></td></tr></table></figure><h3 id="其他依赖"><a href="#其他依赖" class="headerlink" title="其他依赖"></a>其他依赖</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install \</span><br><span class="line">vim \</span><br><span class="line">jq \</span><br><span class="line">wget \</span><br><span class="line">curl \</span><br><span class="line">udisks2 \</span><br><span class="line">libglib2.0-bin \</span><br><span class="line">dbus -y</span><br></pre></td></tr></table></figure><p>在2022.11.27后Homeassistant正式需要<code>Systemd Journal</code>的支持；我们同样可以使用软件包管理器进行安装：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install systemd-journal-remote -y</span><br></pre></td></tr></table></figure><h3 id="Docker"><a href="#Docker" class="headerlink" title="Docker"></a>Docker</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下载Docker安装脚本</span></span><br><span class="line">sudo curl -fsSL https://get.docker.com -o get-docker.sh</span><br><span class="line"><span class="comment"># 使用阿里镜像源下载并安装Docker</span></span><br><span class="line">sudo sh get-docker.sh --mirror Aliyun</span><br></pre></td></tr></table></figure><p>把我们自带的<code>ccinn用户</code>添加到<code>docker用户组</code>内：</p><blockquote><p>这个用户根据自己的实际用户来<br>有的人默认就是pi用户</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo usermod -aG docker ccinn</span><br></pre></td></tr></table></figure><h3 id="重启设备"><a href="#重启设备" class="headerlink" title="重启设备"></a>重启设备</h3><p>上诉操作，我们已经<code>重新配置了网络</code>、<code>安装了依赖</code>和<code>添加了启动参数</code>，所以在正式安装<code>Home Assisistant Supervised</code>前，我们需要重启设备。</p><h3 id="Supervised"><a href="#Supervised" class="headerlink" title="Supervised"></a>Supervised</h3><p>现在开始安装<code>Home Assisistant Supervised</code>啦。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 下载deb安装包</span></span><br><span class="line">wget https://github.com/home-assistant/supervised-installer/releases/latest/download/homeassistant-supervised.deb</span><br><span class="line"><span class="meta">#</span><span class="bash"> 安装</span></span><br><span class="line">sudo dpkg -i homeassistant-supervised.deb</span><br></pre></td></tr></table></figure><p>之后，没有问题就会出现选项卡，我们选择树莓派4B：</p><p><img src="/images/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/ha-install.jpg" alt="ha-install"></p><p>安装过程……根据自己的网络，这一步可能会卡很久，如果还是不行，记得科学一下再重新安装。</p><h2 id="成果"><a href="#成果" class="headerlink" title="成果"></a>成果</h2><p>使用docker命令，查看<code>Supervised</code>的容器状态（如果并没有Homeassistant容器；那么等10min～20min再试试，期间保持树莓派运行，Homeassistant会组建初始化完成）：</p><p><img src="/images/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/ha-docker-ps.jpg" alt="ha-docker-ps"></p><p>进入<code>IP:4357</code>，可以查看<code>Supervised</code>的状态：</p><p><img src="/images/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/ha-health.jpg" alt="ha-health"></p><p>等待5分钟左右（Home Assisistant Supervised第一次启动比较慢），就可以通过<code>IP:8123</code>在浏览器访问了</p><p><img src="/images/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/ha-ok.jpg" alt="ha-ok"></p><p>至此ha的安装就ok了，接下来会继续一些辅助性的东西。</p><h2 id="安装zsh"><a href="#安装zsh" class="headerlink" title="安装zsh"></a>安装zsh</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">apt install zsh</span><br><span class="line">sh -c <span class="string">"<span class="variable">$(curl -fsSL https://gitee.com/shmhlsy/oh-my-zsh-install.sh/raw/master/install.sh)</span>"</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.zshrc</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在最后一行加入</span></span><br><span class="line"><span class="built_in">export</span> PROMPT=<span class="string">'%n@%m:%F&#123;13&#125;%~ %F&#123;50&#125;%B%# %f%b'</span></span><br></pre></td></tr></table></figure><p>如果想要复制ha容器中的配置出来，可以使用</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker cp $(docker ps | grep -v NAMES | grep homeassistant | awk <span class="string">'&#123;print $1&#125;'</span>):/config /home/ccinn/.homeassistant/</span><br></pre></td></tr></table></figure><p>这样子，<code>/home/ccinn/.homeassistant</code> 目录就是当前的了。</p><p>如果我们想要找到hassio中的ha容器挂载的目录是那里的话，可以用以下命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">cat /var/lib/docker/containers/$(docker inspect homeassistant | jq -r <span class="string">'.[] | .Id'</span>)/config.v2.json | jq <span class="string">'.MountPoints."/config"'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面为标准的路径</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"Source"</span>: <span class="string">"/usr/share/hassio/homeassistant"</span>,</span><br><span class="line">  <span class="string">"Destination"</span>: <span class="string">"/config"</span>,</span><br><span class="line">  <span class="string">"RW"</span>: <span class="literal">true</span>,</span><br><span class="line">  <span class="string">"Name"</span>: <span class="string">""</span>,</span><br><span class="line">  <span class="string">"Driver"</span>: <span class="string">""</span>,</span><br><span class="line">  <span class="string">"Type"</span>: <span class="string">"bind"</span>,</span><br><span class="line">  <span class="string">"Propagation"</span>: <span class="string">"rprivate"</span>,</span><br><span class="line">  <span class="string">"Spec"</span>: &#123;</span><br><span class="line">    <span class="string">"Type"</span>: <span class="string">"bind"</span>,</span><br><span class="line">    <span class="string">"Source"</span>: <span class="string">"/usr/share/hassio/homeassistant"</span>,</span><br><span class="line">    <span class="string">"Target"</span>: <span class="string">"/config"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"SkipMountpointCreation"</span>: <span class="literal">true</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到，容器和宿主机的目录地址是<code>/usr/share/hassio/homeassistant</code></p><p>所以，当我们需要手动安装插件，修改配置等等，直接操作这个目录即可。</p><h2 id="安装-Samba"><a href="#安装-Samba" class="headerlink" title="安装 Samba"></a>安装 Samba</h2><p>我们知道，树莓派是一个小型主机，我们日常使用，肯定有我们自己的主机。我们可以通过Samba服务来进行文件的二次管理。把文件备份到我们的日常使用的主机上，或者直接在我们日常使用的主机上进行操作文件配置。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install samba samba-common-bin</span><br></pre></td></tr></table></figure><p>修改配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sudo vim /etc/samba/smb.conf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在文件底部添加</span></span><br><span class="line">[pi]</span><br><span class="line">path = /home/ccinn/.homeassistant</span><br><span class="line">writeable=Yes </span><br><span class="line">create mask=0777 </span><br><span class="line">directory mask=0777 </span><br><span class="line">public=no</span><br></pre></td></tr></table></figure><blockquote><p>因为我上面用的路径和宿主机下的路径不一致，所以需要做个软连接 <code>ln -s /usr/share/hassio/homeassistant /home/ccinn/.homeassistant</code>, 这样子就能实现，samba和树莓派和ha容器的文件实时同步了。</p></blockquote><p>保存文件后，添加samba用户</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo smbpasswd -a ccinn</span><br></pre></td></tr></table></figure><blockquote><p>注意：使用 sudo smbpasswd -a 命令创建用户时，创建的用户必须为 Linux 系统账户，如我这里的ccinn</p></blockquote><p>输入自己的samba用户的密码，比如 <code>a123456</code></p><p>重启服务让其生效</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl restart smbd</span><br></pre></td></tr></table></figure><p>Samba 安装完成后，树莓派的 Hass 开发环境搭建完毕。</p><h2 id="通过-Samba-访问-Home-Assistant-文件夹"><a href="#通过-Samba-访问-Home-Assistant-文件夹" class="headerlink" title="通过 Samba 访问 Home Assistant 文件夹"></a>通过 Samba 访问 Home Assistant 文件夹</h2><p>因为我的是macOS,所以这里用mac为例子。</p><p>在电脑桌面上打开 <code>访达。</code></p><p>选择桌面左上角菜单中的 <code>前往 &gt; 连接服务器</code>。</p><p>在弹出的窗口中，输入以下地址后单击 <code>连接</code>。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">smb://192.168.8.189/pi</span><br></pre></td></tr></table></figure><p><img src="/images/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/ha-sambd.jpg" alt="ha-sambd"></p><h2 id="安装hacs"><a href="#安装hacs" class="headerlink" title="安装hacs"></a>安装hacs</h2><p>在树莓派中执行如下命令。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget -U - nttps://get.nacs.xyz | bash -</span><br></pre></td></tr></table></figure><p><img src="/images/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/ha-hacs.jpg" alt="ha-hacs"></p><p>可以看到，hacs会自动找到对应的目录进行安装。</p><p><img src="/images/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/ha-hacs-1.jpg" alt="ha-hacs-1"></p><p>然后外面通过samba，也可以看到在<code>custom_components</code>中多了个hacs的<code>自定义组件</code>。</p><p>回到<code>ha-web(IP:8123)</code>中，在用户-集成，添加集成，然后搜索HACS，点击一下hacs，就可以安装hacs了，那么接下来就可以用第三方内容了</p><p><img src="/images/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/ha-hacs-2.jpg" alt="ha-hacs-2"></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;前面做了几期关于&lt;code&gt;Home assistant&lt;/code&gt;的教程，在前面我是完全使用docker手动部署，并且采用基本都是通过安装&lt;code&gt;home-assistant-core&lt;/code&gt;去处理。后面我发现很多东西的使用方式都特别别扭，而且确实很多东西都比较麻烦，所以后面再反过来看，发现了一个叫&lt;code&gt;hass&lt;/code&gt;的生态，这个其实就是&lt;code&gt;home-assistant&lt;/code&gt;的周边生态，而这些周边，几乎都是必备的，于是我妥协了使用了&lt;code&gt;hass&lt;/code&gt;的生态，但是有一点不同的是，我这里并没有使用&lt;code&gt;hassOS&lt;/code&gt;，而是采用&lt;code&gt;hassIO&lt;/code&gt;，也就是&lt;code&gt;home-assistant-supervisor&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/ha-install-compare.jpg&quot; alt=&quot;ha-install-compare.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;下面开始，我就会我的正式体现记录我后面的智能家居做法。&lt;/p&gt;
    
    </summary>
    
    
      <category term="智能家居" scheme="http://blog.crazylaw.cn/categories/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/"/>
    
      <category term="树莓派" scheme="http://blog.crazylaw.cn/categories/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/%E6%A0%91%E8%8E%93%E6%B4%BE/"/>
    
    
      <category term="智能家居" scheme="http://blog.crazylaw.cn/tags/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/"/>
    
      <category term="Home Assistant" scheme="http://blog.crazylaw.cn/tags/Home-Assistant/"/>
    
      <category term="HA" scheme="http://blog.crazylaw.cn/tags/HA/"/>
    
  </entry>
  
  <entry>
    <title>ssp-adx-localcache优化</title>
    <link href="http://blog.crazylaw.cn/2023/08/27/%E5%85%AC%E5%8F%B8/localcache/"/>
    <id>http://blog.crazylaw.cn/2023/08/27/%E5%85%AC%E5%8F%B8/localcache/</id>
    <published>2023-08-27T01:53:00.000Z</published>
    <updated>2023-08-28T09:40:43.131Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近在处理<code>ssp-adx-rtb</code>的服务的性能优化，做了好多方面的优化，其中一个就是我们的本地的<code>localcache</code>的问题。</p><p>经过<code>pprof</code>的性能分析，发现<code>cache2go</code>，在 <code>CPU Flame Graph</code> 中，占比十分严重，基本大于<code>1/3</code>，既然是<code>localcache</code>，那么，我们的目的本意就是为了提速，所以占比那么大，是十分不合理的。</p><p>所以需要找到原因，并且解决它。降低cpu使用率，从而提高服务的QPS，减少服务器成本。</p><p><img src="/images/%E5%85%AC%E5%8F%B8/cache2go1.jpg" alt="cache2go1"></p><a id="more"></a><h2 id="cache2go旧版"><a href="#cache2go旧版" class="headerlink" title="cache2go旧版"></a>cache2go旧版</h2><ul><li><a href="https://github.com/muesli/cache2go" target="_blank" rel="noopener">cache2go</a></li><li><a href="https://github.com/muesli/cache2go" target="_blank" rel="noopener">https://github.com/muesli/cache2go</a></li></ul><p>项目的描述为：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Concurrency-safe Go caching library with expiration capabilities and access counters</span><br></pre></td></tr></table></figure><p>并发安全，并且带有效期的和访问计数器的一个类库组件</p><p>我们需要用他来解决我们的3大核心问题</p><ul><li>本地缓存</li><li>并发安全</li><li>带ttl功能</li></ul><p>对于开源版本第一版本，我们已经做为处理了。</p><p>就是他的淘汰策略，是<code>ttl+lru</code>，当一个缓存在一定时间内被连续访问，或者在一个key，准备过期的时候，如果被访问，那么他的过期时间将继续延长到下一个周期。</p><p>这一特点，并不是我们需求，所以我们需要对这一点进行了调整，过期时间，只需要判断为 <code>ttl</code> 过期即可，不需要加上 <code>lru</code> 的方式。</p><p>这里就不展开细说。</p><h2 id="cache2go新版"><a href="#cache2go新版" class="headerlink" title="cache2go新版"></a>cache2go新版</h2><ul><li><a href="https://github.com/whiteCcinn/cache2go" target="_blank" rel="noopener">cache2go-new</a></li><li><a href="https://github.com/whiteCcinn/cache2go" target="_blank" rel="noopener">https://github.com/whiteCcinn/cache2go</a></li></ul><p><img src="/images/%E5%85%AC%E5%8F%B8/localcache.jpg" alt="localcache"></p><p><img src="/images/%E5%85%AC%E5%8F%B8/cache2go2.jpg" alt="cache2go2"></p><p><img src="/images/%E5%85%AC%E5%8F%B8/cache2go3.jpg" alt="cache2go3"></p><p>在这一个版本，基本把整个库都按需重构了。主要是以下几个方面。</p><ul><li>加入<code>hash分片</code>机制，把key打散到不同的<code>bucket</code>中，让<code>bucket-lock</code>的争抢降低</li><li>同一个<code>cache-table</code>，有且仅有一个<code>goroutine</code>，来处理 <code>ttl</code> 数据，并不会因为分片的个数调整带来更多的无效<code>goroutine</code></li><li>没有采用渐进式的方式来删除key, 在 <code>add</code>, <code>get</code> 的阶段，尽量保持服务的高效性能，方式由于锁带来的性能衰减</li><li>采用<code>双写机制</code>，实现<code>L1</code>和<code>L2</code>的二级包装级别，从而做到 <code>读写分离</code>, 尽可能的避免在必要的场景下由于<code>整个写锁</code>导致<code>读锁阻塞</code>的问题，让后台在处理 <code>ttl</code> 和 <code>重建map</code>的过程中，服务依然高效提供服务</li><li>定期重建底层<code>map</code>属性，来释放map申请的内存，让整个服务相对处于一个内存稳定的状态</li></ul><p><code>需求+机制</code>，就可以在<code>读写较多</code>或者<code>后台需要处理map</code>的情况下，性能依旧保持有一个较好的性能体现。</p><p>为了实现这几点：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> CacheTable <span class="keyword">struct</span> &#123;</span><br><span class="line">sync.RWMutex</span><br><span class="line"></span><br><span class="line">hash *fnv64a          <span class="comment">// 用于hash</span></span><br><span class="line">shardMask <span class="keyword">uint64</span>      <span class="comment">// 用于hash的mask，在做按位与操作的时候，实现求余一样的行为，由于是位运算，效率一般都偏高</span></span><br><span class="line"></span><br><span class="line">name <span class="keyword">string</span>           <span class="comment">// cache的命名</span></span><br><span class="line"></span><br><span class="line">L1Shards  shardItems  <span class="comment">// L1的分片组</span></span><br><span class="line">L2Shards  shardItems  <span class="comment">// L2的分片组</span></span><br><span class="line"></span><br><span class="line">cleanupInterval time.Duration  <span class="comment">// 定时处理ttl的数据</span></span><br><span class="line"></span><br><span class="line">l1BlockChan []*CacheItem <span class="comment">// 用于在L1分片组被后台处理过程中，暂时把数据缓存起来</span></span><br><span class="line">l2BlockChan []*CacheItem <span class="comment">// 用于在L2分片组被后台处理过程中，暂时把数据缓存起来</span></span><br><span class="line"></span><br><span class="line">l1Mask <span class="keyword">int32</span> <span class="comment">// L1原子计数器，用来代替lock，防止lock的堵塞现象，导致服务被影响</span></span><br><span class="line">l2Mask <span class="keyword">int32</span> <span class="comment">// L2原子计数器，用来代替lock，防止lock的堵塞现象，导致服务被影响</span></span><br><span class="line"></span><br><span class="line">switchMask <span class="keyword">uint8</span> <span class="comment">// 记录当前 cache-table是否在处理L1,L2分片组</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Add"><a href="#Add" class="headerlink" title="Add"></a>Add</h3><p>这是一个写入过程，实现起来也不算太复杂</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(table *CacheTable)</span> <span class="title">Add</span><span class="params">(key <span class="keyword">interface</span>&#123;&#125;, lifeSpan time.Duration, data <span class="keyword">interface</span>&#123;&#125;)</span> *<span class="title">CacheItem</span></span> &#123;</span><br><span class="line">item := NewCacheItem(key, lifeSpan, data)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 判断当前表是否在处理L1</span></span><br><span class="line"><span class="keyword">if</span> table.switchMask != <span class="number">1</span>&lt;&lt;<span class="number">1</span> &#123;</span><br><span class="line">        <span class="comment">// 记录L1正在处理写入行为，+1操作</span></span><br><span class="line">atomic.AddInt32(&amp;table.l1Mask, <span class="number">1</span>)</span><br><span class="line">        <span class="comment">// 结束的时候L1写入的时候，-1操作</span></span><br><span class="line"><span class="keyword">defer</span> atomic.AddInt32(&amp;table.l1Mask, <span class="number">-1</span>)</span><br><span class="line">        <span class="comment">// L1内部分片片级写锁开发</span></span><br><span class="line">table.L1Shards[item.hashedKey&amp;table.shardMask].lock.Lock()</span><br><span class="line">        <span class="comment">// L1内部分片写入item</span></span><br><span class="line">table.L1Shards[item.hashedKey&amp;table.shardMask].m[item.key] = item</span><br><span class="line">        <span class="comment">// L1内部分片片级写锁结束</span></span><br><span class="line">table.L1Shards[item.hashedKey&amp;table.shardMask].lock.Unlock()</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// 如果当前后台在处理L1的话，那么先缓存起来</span></span><br><span class="line">table.l1BlockChan = <span class="built_in">append</span>(table.l1BlockChan, item)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 判断当前表是否在处理L2</span></span><br><span class="line"><span class="keyword">if</span> table.switchMask != <span class="number">1</span>&lt;&lt;<span class="number">2</span> &#123;</span><br><span class="line">        <span class="comment">// 记录L2正在处理写入行为，+1操作</span></span><br><span class="line">atomic.AddInt32(&amp;table.l2Mask, <span class="number">1</span>)</span><br><span class="line">        <span class="comment">// 结束的时候L2写入的时候，-1操作</span></span><br><span class="line"><span class="keyword">defer</span> atomic.AddInt32(&amp;table.l2Mask, <span class="number">-1</span>)</span><br><span class="line">        <span class="comment">// L2内部分片片级写锁开发</span></span><br><span class="line">table.L2Shards[item.hashedKey&amp;table.shardMask].lock.Lock()</span><br><span class="line">        <span class="comment">// L2内部分片写入item</span></span><br><span class="line">table.L2Shards[item.hashedKey&amp;table.shardMask].m[item.key] = item</span><br><span class="line">        <span class="comment">// L2内部分片片级写锁结束</span></span><br><span class="line">table.L2Shards[item.hashedKey&amp;table.shardMask].lock.Unlock()</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// 如果当前后台在处理L2的话，那么先缓存起来</span></span><br><span class="line">table.l2BlockChan = <span class="built_in">append</span>(table.l2BlockChan, item)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> item</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>通过双写的方式，实现<code>L1</code>和<code>L2</code>的同时写入,以此达到空间换时间的做法。</p><p>其中 <code>(&amp; 2^(n-1))</code> 做到 <code>(%m)</code>的效果，并且由于是位运算，所以按理说效率会更高</p><h3 id="Value"><a href="#Value" class="headerlink" title="Value"></a>Value</h3><blockquote><p>Value 和 就是Get方法，获取key的item</p></blockquote><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(table *CacheTable)</span> <span class="title">Value</span><span class="params">(key <span class="keyword">interface</span>&#123;&#125;, args ...<span class="keyword">interface</span>&#123;&#125;)</span> <span class="params">(*CacheItem, error)</span></span> &#123;</span><br><span class="line">keyBytes, _ := json.Marshal(key)</span><br><span class="line">    <span class="comment">// 哈希的key</span></span><br><span class="line">hashedKey := table.hash.Sum64(<span class="keyword">string</span>(keyBytes))</span><br><span class="line"><span class="keyword">var</span> sm *shardItem</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> table.switchMask == <span class="number">1</span>&gt;&gt;<span class="number">1</span> &#123;</span><br><span class="line"><span class="comment">// 先查l1</span></span><br><span class="line">sm = table.L1Shards[hashedKey&amp;table.shardMask]</span><br><span class="line">sm.lock.RLock()</span><br><span class="line">r, ok := sm.m[key]</span><br><span class="line">sm.lock.RUnlock()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> ok &#123;</span><br><span class="line"><span class="comment">// 正常返回结果</span></span><br><span class="line"><span class="keyword">return</span> r, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 再查l2</span></span><br><span class="line">sm = table.L2Shards[hashedKey&amp;table.shardMask]</span><br><span class="line">sm.lock.RLock()</span><br><span class="line">r, ok = sm.m[key]</span><br><span class="line">sm.lock.RUnlock()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> ok &#123;</span><br><span class="line"><span class="comment">// 正常返回结果</span></span><br><span class="line"><span class="keyword">return</span> r, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 找不到key</span></span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, ErrKeyNotFound</span><br><span class="line"></span><br><span class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span> table.switchMask == <span class="number">1</span>&lt;&lt;<span class="number">1</span> &#123;</span><br><span class="line"><span class="comment">// 正在处理l1，需要从l2读</span></span><br><span class="line">sm = table.L2Shards[hashedKey&amp;table.shardMask]</span><br><span class="line">sm.lock.RLock()</span><br><span class="line">r, ok := sm.m[key]</span><br><span class="line">sm.lock.RUnlock()</span><br><span class="line"><span class="keyword">if</span> ok &#123;</span><br><span class="line"><span class="comment">// 正常返回结果</span></span><br><span class="line"><span class="keyword">return</span> r, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 找不到key</span></span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, ErrKeyNotFound</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="comment">// 正在处理l2，需要从l1读</span></span><br><span class="line">sm = table.L1Shards[hashedKey&amp;table.shardMask]</span><br><span class="line">sm.lock.RLock()</span><br><span class="line">r, ok := sm.m[key]</span><br><span class="line">sm.lock.RUnlock()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> ok &#123;</span><br><span class="line"><span class="comment">// 正常返回结果</span></span><br><span class="line"><span class="keyword">return</span> r, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 找不到key</span></span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, ErrKeyNotFound</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>可以看到这里，如果后台没有在操作<code>L1</code>, <code>L2</code> 的话，那么先从<code>L1</code>拿数据，然后再从<code>L2</code>拿数据</li><li>如果后台在<code>操作L1</code>, 那么只能从 <code>L2</code> 读取</li><li>如果后台在<code>操作L2</code>, 那么只能从 <code>L1</code> 读取</li></ul><p>所以通过<code>L1</code>和<code>L2</code>，我们实现了一个读写分离的策略，并且在最大的程度上减少<code>分片锁</code>的读写锁冲突，从而提高服务的效率</p><h2 id="后台任务"><a href="#后台任务" class="headerlink" title="后台任务"></a>后台任务</h2><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 定时清理过期缓存</span></span><br><span class="line"><span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">(t *CacheTable, ctx context.Context)</span></span> &#123;</span><br><span class="line">    <span class="comment">// 定时监测ttl数据</span></span><br><span class="line">    ticker := time.NewTicker(cleanInterval)</span><br><span class="line">    <span class="comment">// 定期重建map，以此来释放map申请的空间</span></span><br><span class="line">    reBuildTicker := time.NewTicker(<span class="number">30</span> * time.Minute)</span><br><span class="line">    <span class="keyword">for</span> &#123;</span><br><span class="line">        <span class="keyword">select</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> &lt;-ctx.Done():</span><br><span class="line">            ticker.Stop()</span><br><span class="line">            reBuildTicker.Stop()</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        <span class="keyword">case</span> &lt;-ticker.C:</span><br><span class="line">            <span class="comment">// 表锁</span></span><br><span class="line">            t.Lock()</span><br><span class="line">            <span class="comment">// 扫描需要删除的key</span></span><br><span class="line">            <span class="keyword">var</span> deleteList []*CacheItem</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 先处理l1，再处理l2</span></span><br><span class="line">            t.switchMask = <span class="number">1</span> &lt;&lt; <span class="number">1</span></span><br><span class="line">            now := time.Now()</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 处理l1</span></span><br><span class="line">            <span class="comment">// 不允许l1读写入，读写通过l2</span></span><br><span class="line">            <span class="keyword">for</span> &#123;</span><br><span class="line">                <span class="keyword">if</span> atomic.LoadInt32(&amp;t.l1Mask) == <span class="number">0</span> &#123;</span><br><span class="line">                    <span class="comment">// 当L1已经操作完Add操作的时候继续往下走</span></span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">for</span> i, sad := <span class="keyword">range</span> t.L1Shards &#123;</span><br><span class="line">                <span class="comment">// 分片片级别读锁</span></span><br><span class="line">                sad.lock.RLock()</span><br><span class="line">                <span class="keyword">for</span> _, r := <span class="keyword">range</span> sad.m &#123;</span><br><span class="line">                    <span class="comment">// ttl数据校验处理</span></span><br><span class="line">                    <span class="keyword">if</span> now.Sub(r.createdOn).Seconds() &gt; r.lifeSpan.Seconds() &#123;</span><br><span class="line">                        deleteList = <span class="built_in">append</span>(deleteList, r)</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">                sad.lock.RUnlock()</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 开始删除</span></span><br><span class="line">            <span class="keyword">for</span> _, item := <span class="keyword">range</span> deleteList &#123;</span><br><span class="line">                <span class="comment">// 分片片级别写锁，防止在 Value操作的时候，并行读写异常</span></span><br><span class="line">                t.L1Shards[item.hashedKey&amp;t.shardMask].lock.Lock()</span><br><span class="line">                <span class="built_in">delete</span>(t.L1Shards[item.hashedKey&amp;t.shardMask].m, item.key)</span><br><span class="line">                t.L1Shards[item.hashedKey&amp;t.shardMask].lock.Unlock()</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 重置deleteList</span></span><br><span class="line">            deleteList = <span class="built_in">make</span>([]*CacheItem, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 处理完l1,处理l2</span></span><br><span class="line">            t.switchMask = <span class="number">1</span> &lt;&lt; <span class="number">2</span></span><br><span class="line"></span><br><span class="line">            <span class="comment">// 堵塞的item加回来到l1</span></span><br><span class="line">            l1Length := <span class="built_in">len</span>(t.l1BlockChan)</span><br><span class="line">            <span class="keyword">for</span> _, item := <span class="keyword">range</span> t.l1BlockChan &#123;</span><br><span class="line">                <span class="keyword">if</span> item != <span class="literal">nil</span> &#123;</span><br><span class="line">                    t.L1Shards[item.hashedKey&amp;t.shardMask].lock.Lock()</span><br><span class="line">                    t.L1Shards[item.hashedKey&amp;t.shardMask].m[item.key] = item</span><br><span class="line">                    t.L1Shards[item.hashedKey&amp;t.shardMask].lock.Unlock()</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 重置l1BlockChan, 预先申请大小为原来到一半</span></span><br><span class="line">            t.l1BlockChan = <span class="built_in">make</span>([]*CacheItem, <span class="number">0</span>, l1Length/<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 不允许l2读写入，读写通过l1</span></span><br><span class="line">            <span class="keyword">for</span> &#123;</span><br><span class="line">                <span class="keyword">if</span> atomic.LoadInt32(&amp;t.l2Mask) == <span class="number">0</span> &#123;</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> i, sad := <span class="keyword">range</span> t.L2Shards &#123;</span><br><span class="line">                sad.lock.RLock()</span><br><span class="line">                <span class="keyword">for</span> _, r := <span class="keyword">range</span> sad.m &#123;</span><br><span class="line">                    <span class="keyword">if</span> now.Sub(r.createdOn).Seconds() &gt; r.lifeSpan.Seconds() &#123;</span><br><span class="line">                        deleteList = <span class="built_in">append</span>(deleteList, r)</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">                sad.lock.RUnlock()</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 开始删除</span></span><br><span class="line">            <span class="keyword">for</span> _, item := <span class="keyword">range</span> deleteList &#123;</span><br><span class="line">                t.L2Shards[item.hashedKey&amp;t.shardMask].lock.Lock()</span><br><span class="line">                <span class="built_in">delete</span>(t.L2Shards[item.hashedKey&amp;t.shardMask].m, item.key)</span><br><span class="line">                t.L2Shards[item.hashedKey&amp;t.shardMask].lock.Unlock()</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 恢复正常</span></span><br><span class="line">            t.switchMask = <span class="number">1</span> &gt;&gt; <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> _, item := <span class="keyword">range</span> t.l2BlockChan &#123;</span><br><span class="line">                <span class="comment">//fmt.Println(t.name, t.L1Shards[item.hashedKey&amp;t.shardMask])</span></span><br><span class="line">                <span class="keyword">if</span> item != <span class="literal">nil</span> &#123;</span><br><span class="line">                    t.L2Shards[item.hashedKey&amp;t.shardMask].lock.Lock()</span><br><span class="line">                    t.L2Shards[item.hashedKey&amp;t.shardMask].m[item.key] = item</span><br><span class="line">                    t.L2Shards[item.hashedKey&amp;t.shardMask].lock.Unlock()</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 重置l2BlockChan</span></span><br><span class="line">            t.l2BlockChan = <span class="built_in">make</span>([]*CacheItem, <span class="number">0</span>, l2Length/<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">            t.Unlock()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">case</span> &lt;-reBuildTicker.C:</span><br><span class="line">            t.Lock()</span><br><span class="line">            <span class="comment">// 为了释放map内存</span></span><br><span class="line"></span><br><span class="line">            <span class="comment">// 先处理l1，再处理l2</span></span><br><span class="line">            t.switchMask = <span class="number">1</span> &lt;&lt; <span class="number">1</span></span><br><span class="line">            now := time.Now()</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 处理l1</span></span><br><span class="line">            <span class="comment">// 不允许l1读写入，读写通过l2</span></span><br><span class="line">            <span class="keyword">for</span> &#123;</span><br><span class="line">                <span class="keyword">if</span> atomic.LoadInt32(&amp;t.l1Mask) == <span class="number">0</span> &#123;</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> _, sad := <span class="keyword">range</span> t.L1Shards &#123;</span><br><span class="line">                sad.lock.Lock()</span><br><span class="line">                nm := <span class="built_in">make</span>(shard, <span class="built_in">len</span>(sad.m))</span><br><span class="line">                <span class="keyword">for</span> key, r := <span class="keyword">range</span> sad.m &#123;</span><br><span class="line">                    <span class="keyword">if</span> now.Sub(r.createdOn).Seconds() &lt; r.lifeSpan.Seconds() &#123;</span><br><span class="line">                        nm[key] = r</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">                sad.m = <span class="literal">nil</span></span><br><span class="line">                sad.m = nm</span><br><span class="line">                sad.lock.Unlock()</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 先处理l1，再处理l2</span></span><br><span class="line">            t.switchMask = <span class="number">1</span> &lt;&lt; <span class="number">2</span></span><br><span class="line">            <span class="keyword">for</span> &#123;</span><br><span class="line">                <span class="keyword">if</span> atomic.LoadInt32(&amp;t.l2Mask) == <span class="number">0</span> &#123;</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> _, sad := <span class="keyword">range</span> t.L2Shards &#123;</span><br><span class="line">                sad.lock.Lock()</span><br><span class="line">                nm := <span class="built_in">make</span>(shard, <span class="built_in">len</span>(sad.m))</span><br><span class="line">                <span class="keyword">for</span> key, r := <span class="keyword">range</span> sad.m &#123;</span><br><span class="line">                    <span class="keyword">if</span> now.Sub(r.createdOn).Seconds() &lt; r.lifeSpan.Seconds() &#123;</span><br><span class="line">                        nm[key] = r</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">                sad.m = <span class="literal">nil</span></span><br><span class="line">                sad.m = nm</span><br><span class="line">                sad.lock.Unlock()</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 恢复正常</span></span><br><span class="line">            t.switchMask = <span class="number">1</span> &gt;&gt; <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            runtime.GC()</span><br><span class="line">            debug.FreeOSMemory()</span><br><span class="line">            t.Unlock()</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;(t, ctx)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;最近在处理&lt;code&gt;ssp-adx-rtb&lt;/code&gt;的服务的性能优化，做了好多方面的优化，其中一个就是我们的本地的&lt;code&gt;localcache&lt;/code&gt;的问题。&lt;/p&gt;
&lt;p&gt;经过&lt;code&gt;pprof&lt;/code&gt;的性能分析，发现&lt;code&gt;cache2go&lt;/code&gt;，在 &lt;code&gt;CPU Flame Graph&lt;/code&gt; 中，占比十分严重，基本大于&lt;code&gt;1/3&lt;/code&gt;，既然是&lt;code&gt;localcache&lt;/code&gt;，那么，我们的目的本意就是为了提速，所以占比那么大，是十分不合理的。&lt;/p&gt;
&lt;p&gt;所以需要找到原因，并且解决它。降低cpu使用率，从而提高服务的QPS，减少服务器成本。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/%E5%85%AC%E5%8F%B8/cache2go1.jpg&quot; alt=&quot;cache2go1&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="组件优化" scheme="http://blog.crazylaw.cn/categories/%E7%BB%84%E4%BB%B6%E4%BC%98%E5%8C%96/"/>
    
    
      <category term="cache" scheme="http://blog.crazylaw.cn/tags/cache/"/>
    
  </entry>
  
  <entry>
    <title>Home Assistant （二）</title>
    <link href="http://blog.crazylaw.cn/2023/08/01/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/Home-Assistant2/"/>
    <id>http://blog.crazylaw.cn/2023/08/01/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/Home-Assistant2/</id>
    <published>2023-07-31T16:00:19.000Z</published>
    <updated>2023-08-02T16:45:58.961Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本次主要是讲解一下进阶版的<code>HA</code>，为什么说是进阶版本呢，因为我也是花了几天的时候，去了解各种知识才了解得到的内容。</p><p>内容包括：什么叫<code>hassio</code>，<code>hassos</code>，并且他们和<code>Home Assistant</code> 是什么关系。</p><p>并且我会重点讲解<code>docker版本</code>的<code>HA</code>都是需要怎么玩（<code>不借助hassio</code>），你问我为什么，我就告诉你因为过于繁琐，并且多了很多无用的容器占用系统资源。</p><p>为什么我偏执于<code>docker版本的ha</code>？因为我不想一台机器，只做一件事，这对高配置和高性能的机器是一种绝对的浪费。</p><p>在这个时候，<code>环境隔离</code>就成了<code>重要的因素</code>, 而<code>docker</code>就很好的做到了这一点，并且不像<code>hassio</code>的底层依赖。</p><a id="more"></a><h2 id="MQTT"><a href="#MQTT" class="headerlink" title="MQTT"></a>MQTT</h2><blockquote><p>什么是MQTT协议，这个得大家去了解了，这里我只和大家说，这是一个开源的发布订阅的简易协议<br>它可以做到服务发现的功能，也可以做到事件通知等等<br>它是传统的物联网大家都会选择的一个协议</p></blockquote><p>这里，我们需要安装一个<code>MQTT的服务器</code>，这个项目就是如下</p><ul><li><code>eclipse-mosquitto</code> (<a href="https://github.com/eclipse/mosquitto" target="_blank" rel="noopener">https://github.com/eclipse/mosquitto</a>) (<a href="https://hub.docker.com/r/amd64/eclipse-mosquitto" target="_blank" rel="noopener">https://hub.docker.com/r/amd64/eclipse-mosquitto</a>)</li></ul><p>这是一个一个由于C语言实现的MQTT协议的服务器，但是我们不直接用他。我们借助<code>docker</code>的特性，用docker来安装。</p><p>插一个题外话，其实<code>Home Assistant</code>的 <code>ADD-ON</code> 功能，也是创建一些服务的容器，所以我们手动操作，也是一样的。这个时候就是解答有一些小伙伴经常会纠结的一个问题，为什么我用docker安装的<code>HA</code>，它不存在 <code>Add-on</code> 这个加载项，我要怎么使用这个功能。答案就是：手动处理。当然如果你有编程能力的话，也可以做成一个自动化脚本，这也是我接下来准备做的。</p><p>好的，话不多说，我们继续我们的操作，基于 <code>docker</code> 安装 <code>eclipse-mosquitto</code> 服务。</p><p>找到一个目录，接着执行如下命令创建目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p ~/ha/mqtt/config</span><br><span class="line">mkdir -p ~/ha/mqtt/data</span><br><span class="line">mkdir -p ~/ha/mqtt/log</span><br></pre></td></tr></table></figure><p>在 <code>~/ha/mqtt/config</code> 目录下创建配置文件 <code>mosquitto.conf</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim mosquitto.conf</span><br></pre></td></tr></table></figure><p>内容如下，主要分别是需要持久化数据，存储的路径是容器内部的路径</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 是否允许未提供用户名的客户端进行连接</span></span><br><span class="line">allow_anonymous true</span><br><span class="line"><span class="meta">#</span><span class="bash"> 0.0.0.0很重要，否则你的宿主机转发的端口，也无法被mqtt接收到</span></span><br><span class="line">listener 1883 0.0.0.0</span><br><span class="line"><span class="meta">#</span><span class="bash"> 是否持久化数据</span></span><br><span class="line">persistence true</span><br><span class="line"><span class="meta">#</span><span class="bash"> 持久化数据的路径</span></span><br><span class="line">persistence_location /mosquitto/data</span><br><span class="line"><span class="meta">#</span><span class="bash"> 落盘日志</span></span><br><span class="line">log_dest file /mosquitto/log/mosquitto.log</span><br></pre></td></tr></table></figure><p>具体配置文档 <code>https://mosquitto.org/man/mosquitto-conf-5.html</code></p><p>最后执行如下命令即可启动 <code>mosquitto 容器</code>：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --name=mosquitto --privileged</span><br><span class="line">-p 1883:1883 \</span><br><span class="line">-v $(PWD)/config/mosquitto.conf:/mosquitto/config/mosquitto.conf \</span><br><span class="line">-v $(PWD)/data:/mosquitto/data \</span><br><span class="line">-v $(PWD)/log:/mosquitto/log \</span><br><span class="line">eclipse-mosquitto</span><br></pre></td></tr></table></figure><blockquote><p>我的 <code>$(PWD)</code>代表当前目录，你可以改成上面的 <code>~/ha/mqtt/</code>目录</p></blockquote><p>查看容器是否启动：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">CONTAINER ID   IMAGE                          COMMAND                  CREATED          STATUS          PORTS                                                                      NAMES</span><br><span class="line">b4a8f33dc8b2   eclipse-mosquitto              "/docker-entrypoint.…"   17 seconds ago   Up 17 seconds   0.0.0.0:1883-&gt;1883/tcp      mosquitto</span><br></pre></td></tr></table></figure><p>查看日志信息:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tail log/mosquitto.log</span><br><span class="line">1690908079: mosquitto version 2.0.15 starting</span><br><span class="line">1690908079: Config loaded from /mosquitto/config/mosquitto.conf.</span><br><span class="line">1690908079: Starting in local only mode. Connections will only be possible from clients running on this machine.</span><br><span class="line">1690908079: Create a configuration file which defines a listener to allow remote access.</span><br><span class="line">1690908079: For more details see https://mosquitto.org/documentation/authentication-methods/</span><br><span class="line">1690908079: Opening ipv4 listen socket on port 1883.</span><br><span class="line">1690908079: Opening ipv6 listen socket on port 1883.</span><br><span class="line">1690908079: Error: Address not available</span><br><span class="line">1690908079: mosquitto version 2.0.15 running</span><br></pre></td></tr></table></figure><p>OK，我们看到一切正常。</p><p>让我们来测试一下是不是真的ok了。</p><p>我们同样，用2个容器，分别创建<code>发布者</code>和<code>订阅者</code></p><blockquote><p>由于我的是macbook，所以我的容器可以通过<code>host.docker.internal</code>来访问宿主机，如果是linux环境下的，可以使用 <code>--net host</code> 模式</p></blockquote><p>订阅者（窗口1）:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --rm -it eclipse-mosquitto sh -c 'mosquitto_sub -h host.docker.internal -p 1883 -t "#" -v'</span><br></pre></td></tr></table></figure><blockquote><p>对应的参数就不一一解释了</p></blockquote><p>发布者（窗口2）:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --rm -it eclipse-mosquitto sh -c 'mosquitto_pub -h host.docker.internal -p 1883 -t "test/testdevice"  -m caiwenhui-hello'</span><br></pre></td></tr></table></figure><p>回到<code>窗口1</code>查看订阅者的情况</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker run --rm -it eclipse-mosquitto sh -c 'mosquitto_sub -h host.docker.internal -p 1883 -t "#" -v'                                                                         </span><br><span class="line">test/testdevice caiwenhui-hello</span><br></pre></td></tr></table></figure><p>可以看见，我们的消息和对应的topic都正常接受到了。非常好！</p><p>接下来，我们接入我们的<code>HA</code>！！</p><h2 id="让HA和MQTT连接"><a href="#让HA和MQTT连接" class="headerlink" title="让HA和MQTT连接"></a>让HA和MQTT连接</h2><p>我们打开回到我们的HA, <code>http://127.0.0.1:8123</code></p><p><img src="/images/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/mqtt1.jpg" alt="mqtt1"></p><p><img src="/images/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/mqtt2.jpg" alt="mqtt2"></p><p><img src="/images/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/mqtt3.jpg" alt="mqtt3"></p><p><img src="/images/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/mqtt4.jpg" alt="mqtt4"></p><p>由于我这里没有账号密码，并且允许匿名发送，所以这里可以先忽视这些，正常的情况下，我们需要设置，<code>否则一旦mqtt被破解</code>，被发送<code>恶意指令</code>，可能会让你的设备<code>“发疯”</code></p><p><img src="/images/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/mqtt5.jpg" alt="mqtt5"></p><p><img src="/images/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/mqtt6.jpg" alt="mqtt6"></p><p><img src="/images/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/mqtt7.jpg" alt="mqtt7"></p><p>看到这里，我们配置完成了。接下里，就是在HA里面测试是否真的配置正确并且可通的了。</p><p><img src="/images/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/mqtt8.jpg" alt="mqtt8"></p><p><img src="/images/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/mqtt9.jpg" alt="mqtt9"></p><p><img src="/images/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/mqtt10.jpg" alt="mqtt10"></p><p>可以看到关键信息: <code>i am ha&#39;s caiwenhui</code> , 是因为我在我的电脑发送的消息<code>被HA接收到了</code></p><p><img src="/images/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/mqtt11.jpg" alt="mqtt11"></p><p>可以看到，这个图，代表，我接收到了从ha系统发出的消息。ok，我们ha和mqtt的连接在不借助 <code>hassio</code> 和 <code>add-on</code>的情况下已经打通了。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;本次主要是讲解一下进阶版的&lt;code&gt;HA&lt;/code&gt;，为什么说是进阶版本呢，因为我也是花了几天的时候，去了解各种知识才了解得到的内容。&lt;/p&gt;
&lt;p&gt;内容包括：什么叫&lt;code&gt;hassio&lt;/code&gt;，&lt;code&gt;hassos&lt;/code&gt;，并且他们和&lt;code&gt;Home Assistant&lt;/code&gt; 是什么关系。&lt;/p&gt;
&lt;p&gt;并且我会重点讲解&lt;code&gt;docker版本&lt;/code&gt;的&lt;code&gt;HA&lt;/code&gt;都是需要怎么玩（&lt;code&gt;不借助hassio&lt;/code&gt;），你问我为什么，我就告诉你因为过于繁琐，并且多了很多无用的容器占用系统资源。&lt;/p&gt;
&lt;p&gt;为什么我偏执于&lt;code&gt;docker版本的ha&lt;/code&gt;？因为我不想一台机器，只做一件事，这对高配置和高性能的机器是一种绝对的浪费。&lt;/p&gt;
&lt;p&gt;在这个时候，&lt;code&gt;环境隔离&lt;/code&gt;就成了&lt;code&gt;重要的因素&lt;/code&gt;, 而&lt;code&gt;docker&lt;/code&gt;就很好的做到了这一点，并且不像&lt;code&gt;hassio&lt;/code&gt;的底层依赖。&lt;/p&gt;
    
    </summary>
    
    
      <category term="智能家居" scheme="http://blog.crazylaw.cn/categories/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/"/>
    
    
      <category term="智能家居" scheme="http://blog.crazylaw.cn/tags/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/"/>
    
      <category term="Home Assistant" scheme="http://blog.crazylaw.cn/tags/Home-Assistant/"/>
    
      <category term="HA" scheme="http://blog.crazylaw.cn/tags/HA/"/>
    
  </entry>
  
  <entry>
    <title>Home Assistant （一）</title>
    <link href="http://blog.crazylaw.cn/2023/07/30/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/Home-Assistant/"/>
    <id>http://blog.crazylaw.cn/2023/07/30/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/Home-Assistant/</id>
    <published>2023-07-29T16:00:19.000Z</published>
    <updated>2023-08-01T16:22:01.707Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>既然是技术博客，那么这一期，我会开始做一系列的关于智能家居的玩法。</p><p><code>Home Assistant</code> 也叫 <code>HA</code> + <code>Zigbee协议</code>(硬件之间的通信协议)</p><p><code>HACS</code> 是 <code>HA</code> 的一个第三方插件，这个插件包括了许多开发者所贡献的插件，如果你是一名开发人员，那么你一定了解什么叫<code>依赖库</code>。所以可以理解为 <code>HACS</code> 就是 <code>HA</code> 的第三方插件依赖库，你可以在连找到需要对你有用的插件，而你不必重新开发。</p><p>为什么选择 <code>Zigbee协议</code> ？ 在对比过<code>蓝牙mesh</code>,<code>wifi协议</code>之后，我最终选择了<code>zigbee协议</code>，在于硬件之间的控制信号量传输本身就少，所以它自身的稳定性和分布式的特点，可以很好的支持到家庭中的各个角落，不会出现由于信号差导致失灵的情况，并且<code>zigbee协议</code>的特点，电量的消耗也是十分的低。</p><a id="more"></a><h2 id="硬件"><a href="#硬件" class="headerlink" title="硬件"></a>硬件</h2><p>由于我现在没有<code>树莓派</code>或者<code>nas</code>等<code>本地服务器</code>，所以我以我的<code>macbook pro</code>来例。但是实际情况下的智能家居，我们需要有一台连接在本地局域网的服务，它需要要求的特点包括如下：</p><ul><li>低功耗</li><li>便携，不占用家庭空间</li><li>支持更多的其他功能，例如用于<code>nas</code>，<code>time machine</code>, <code>软路由(科学上学)</code> 等等（非必要，但是可以做在一起，所以机器的性能越出色越好）</li></ul><p>由于是测试用的，所以我只买了2个 <code>涂鸦</code> 品牌的 <code>e27</code> 的<code>智能灯泡</code> 和 <code>zigbee2mqtt</code> 的 <code>zigbee协议信号网关</code></p><p>** 需要去淘宝买 **</p><ul><li><code>zigbee信号网关</code></li><li><code>2个智能灯泡</code></li></ul><h3 id="zigbee信号网关"><a href="#zigbee信号网关" class="headerlink" title="zigbee信号网关"></a>zigbee信号网关</h3><p><img src="/images/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/y5.jpg" alt="y-5"></p><p><img src="/images/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/y6.jpg" alt="y-6"></p><h3 id="支持zigbee协议的智能灯泡"><a href="#支持zigbee协议的智能灯泡" class="headerlink" title="支持zigbee协议的智能灯泡"></a>支持zigbee协议的智能灯泡</h3><p><img src="/images/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/y-1.jpg" alt="y-1"></p><p><img src="/images/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/y2.jpg" alt="y-2"></p><p><img src="/images/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/y3.jpg" alt="y-3"></p><p><img src="/images/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/y4.jpg" alt="y-4"></p><h2 id="Home-Assistant-安装"><a href="#Home-Assistant-安装" class="headerlink" title="Home Assistant 安装"></a>Home Assistant 安装</h2><p>由于目前我在尝试阶段，并且是<code>macbook</code>，并且我更倾向于用<code>docker安装</code>各种服务，在容器化的时代，物理环境的隔离是十分重要的一个环节。</p><h3 id="docker-安装"><a href="#docker-安装" class="headerlink" title="docker 安装"></a>docker 安装</h3><blockquote><p>这里不介绍docker怎么安装了，如果是完全没编程的基础知识的话，折腾起来确实有点麻烦</p></blockquote><p>创建数据存储目录，<code>mkdir -p ~/ha;cd ~/ha</code>，然后执行如下 docker命令 进行安装</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">docker run -d \</span><br><span class="line">  --name homeassistant \</span><br><span class="line">  --privileged \</span><br><span class="line">  -e TZ=Asia/Shanghai \</span><br><span class="line">  -v $(PWD):/config \</span><br><span class="line">  -p 8123:8123 \</span><br><span class="line">  homeassistant/home-assistant</span><br></pre></td></tr></table></figure><blockquote><p>网上很多都说要用–net=host模式，但是其实没必要的，指定端口暴露就好<br>ha的默认端口就是8123</p></blockquote><p><code>docker ps</code> 查看容器情况</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">CONTAINER ID   IMAGE                          COMMAND                  CREATED         STATUS        PORTS                                                                      NAMES</span><br><span class="line">b2f9de1af25b   homeassistant/home-assistant   "/init"                  2 seconds ago   Up 1 second   0.0.0.0:8123-&gt;8123/tcp                                                     homeassistant</span><br></pre></td></tr></table></figure><p>可以看到我们的容器启动了。并且正常运行了。如果你还不放心，可以看一下日志也可以 <code>docker log homeassistant</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">s6-rc: info: service s6rc-oneshot-runner: starting</span><br><span class="line">s6-rc: info: service s6rc-oneshot-runner successfully started</span><br><span class="line">s6-rc: info: service fix-attrs: starting</span><br><span class="line">s6-rc: info: service fix-attrs successfully started</span><br><span class="line">s6-rc: info: service legacy-cont-init: starting</span><br><span class="line">s6-rc: info: service legacy-cont-init successfully started</span><br><span class="line">s6-rc: info: service legacy-services: starting</span><br><span class="line">services-up: info: copying legacy longrun home-assistant (no readiness notification)</span><br><span class="line">s6-rc: info: service legacy-services successfully started</span><br></pre></td></tr></table></figure><p>打开浏览器，在浏览器中输入 <code>127.0.0.1:8123</code>，就会看到如下界面</p><p><img src="/images/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/ha-1.jpg" alt="ha-1"></p><p>接下来，我们创建好账号密码，切勿忘记<code>密码</code>，好记性不如烂笔头，记得记录下来</p><p><img src="/images/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/ha-2.jpg" alt="ha-2"></p><p>这里我们，不怎么需要理会，正常填写即可。</p><p><img src="/images/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/ha-3.jpg" alt="ha-3"></p><p>这里默认的都是关闭的，也推荐关闭。然后点击下一步。</p><p><img src="/images/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/ha-4.jpg" alt="ha-4"></p><p>这里其实就可以选择你要连接的品牌了，但是这里我先跳过，后面再设置。直接点击 <code>完成</code> 即可</p><p><img src="/images/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/ha-5.jpg" alt="ha-5"></p><p>ok，现在我们可以看到了，完成之后，进到 <code>HA系统</code> 了，可以看到，内置了一个谷歌的<code>TTS</code>的功能，可以不用理会（这个是谷歌推出的一个文本转语音的机器学习的功能）。</p><h3 id="HACS-安装"><a href="#HACS-安装" class="headerlink" title="HACS 安装"></a>HACS 安装</h3><p><code>Home Assistant Community Store</code> 为社区建设的<code>HomeAssistant商店</code>，可以安装<code>第三方集成</code>、<code>主题</code>、<code>表盘</code>以及<code>自动化</code>等。</p><p>GitHub 地址如下:</p><ul><li><a href="https://github.com/hacs" target="_blank" rel="noopener">https://github.com/hacs</a></li><li><a href="https://github.com/hacs/integration" target="_blank" rel="noopener">https://github.com/hacs/integration</a></li></ul><p><img src="/images/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/hacs1.jpg" alt="hacs1"></p><p>安装之前，让我们先把资源目录整理好.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 存放等会要安装的 HACS文件</span></span><br><span class="line">mkdir -p ~/ha/custom_components/hacs</span><br><span class="line"><span class="meta">#</span><span class="bash"> 存放未来 HACS 安装的各种首页磁贴啥的（官方叫Lovelace ）</span></span><br><span class="line">mkdir -p ~/ha/www</span><br></pre></td></tr></table></figure><p>打开浏览器，输入<code>github</code> 的地址 <code>https://github.com/hacs/integration/releases</code> ，在这里下载最新的hacs的releases包。</p><p><img src="/images/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/hacs2.jpg" alt="hacs2"></p><p>这里看见，我当前看到的目前的版本是去到了 <code>1.32.1</code>，所以本篇文章将以 <code>1.32.1</code> 为例子.</p><p><img src="/images/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/hacs3.jpg" alt="hacs3"></p><p>下载这个 <code>zip</code> 的压缩包，然后解压到 <code>~/ha/custom_components/hacs</code> 目录</p><p>终端命令如下： <code>unzip ~/Downloads/hacs.zip -d ~/ha/custom_components/hacs</code></p><p><img src="/images/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/hacs4.jpg" alt="hacs4"></p><p>解压后的情况如上图。</p><p><img src="/images/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/hacs5.jpg" alt="hacs5"></p><p>在这里点击<code>开发者工具页面</code>，然后点击<code>检测配置</code>。</p><p><img src="/images/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/hacs6.jpg" alt="hacs6"></p><p><img src="/images/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/hacs7.jpg" alt="hacs7"></p><p>接下来就可以看到<code>检测通过</code>，然后我们在同一个页面中找到<code>重启按钮</code>, 就可以<code>重启我们的HA系统</code>，让<code>插件加载生效</code>.</p><p><img src="/images/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/hacs8.jpg" alt="hacs8"></p><p>点击<code>重启按钮</code>之后，可以看到<code>系统正在重启，目前失去了连接</code>.</p><p><img src="/images/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/hacs9.jpg" alt="hacs9"></p><p>接着，我们手动刷新一下页面，用户登陆失效了，代表重启成功了，所以我们需要重新登陆到我们的<code>HA系统</code>，这个时候，需要输入我们记下来的<code>账号密码</code></p><p>** 换另外一种 ** 方式去安装HACS也可以的，<code>直接在docker容器内部安装</code>，通过<code>hacs脚本</code>自动安装。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker exec -it homeassistant bash -c 'wget -q -O - https://install.hacs.xyz | bash -'</span><br></pre></td></tr></table></figure><p><img src="/images/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/hacs10.jpg" alt="hacs10"></p><p>可以看到，现在安装完毕了，我需要重启然后让插件加载成功。</p><p><img src="/images/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/hacs11.jpg" alt="hacs11"></p><p>接下来，我们点击 <code>配置</code> ，找到 <code>设备与服务</code> 项</p><p><img src="/images/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/hacs12.jpg" alt="hacs12"></p><p>点击 <code>添加集成</code></p><p><img src="/images/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/hacs13.jpg" alt="hacs13"></p><p>搜索一下hacs，然后我们就能识别到的刚才安装的hacs了。</p><p><img src="/images/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/hacs14.jpg" alt="hacs14"></p><p>这里把<code>全部选中</code>，因为这个是第三方插件，所以<code>HA</code>需要确保你会排查问题。所以需要看你是否懂得这些，你只需要全选了即可.</p><p><img src="/images/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/hacs15.jpg" alt="hacs15"></p><p>这是设备注册。需要先拥有Github账号，没有的话注册一个并在浏览器上登录,<code>github.com</code></p><p><img src="/images/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/hacs16.jpg" alt="hacs16"></p><p>然后打开上面的提供的连接: <code>https://github.com/login/device</code>, 输入提供的 <code>设备码</code></p><p><img src="/images/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/hacs17.jpg" alt="hacs17"></p><p>点击 账号授权给 <code>hacs</code> 服务，即可。</p><p><img src="/images/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/hacs18.jpg" alt="hacs18"></p><p>回到 <code>HA系统</code>，会发现已经识别到了，所以完成安装了 <code>HACS</code> 。</p><p><img src="/images/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/hacs19.jpg" alt="hacs19"></p><h2 id="homeassistant-App"><a href="#homeassistant-App" class="headerlink" title="homeassistant App"></a>homeassistant App</h2><p>HA官方提供了APP，iOS和Android都有，可自行下载~</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ios: https://www.github.com/home-assistant/iOS</span><br><span class="line">android: https://github.com/home-assistant/android</span><br></pre></td></tr></table></figure><p>当然，也可以自行去各大应用商店下载，例如 <code>appstore</code></p><p>温馨提示： App需要填入自己的HA地址，所以如果服务跑在家里的话，需要内网穿透或者公网才能在外面使用噢~ </p><p>如果HA内网穿透，configuration.yaml需要加上下面内容，同时内网穿透服务器(如ngrok、frp等)的nginx需要开启websocket支持，否则会出现外网无法访问、能访问但是无法登录等问题。</p><p>HA配置文件<code>configuration.yaml</code></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">http:</span></span><br><span class="line">  <span class="attr">use_x_forwarded_for:</span> <span class="literal">True</span></span><br><span class="line">  <span class="attr">trusted_proxies:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span><span class="string">/24</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">::1/128</span></span><br></pre></td></tr></table></figure><p>nginx配置参考(用frp内网穿透)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">     listen  80;</span><br><span class="line">     server_name  *.frp.yourdomain.cn frp.yourdomain.cn;</span><br><span class="line">     location / &#123;</span><br><span class="line">             proxy_redirect off;</span><br><span class="line">             proxy_set_header Host $http_host;</span><br><span class="line">             proxy_set_header X-Real-IP $remote_addr;</span><br><span class="line">             proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;</span><br><span class="line">             # 下面两行提供websocket支持,homeassistant需要</span><br><span class="line">             proxy_set_header Upgrade $http_upgrade;</span><br><span class="line">             proxy_set_header Connection "upgrade";</span><br><span class="line">             proxy_pass http://127.0.0.1:8123;</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>最后附上同一个局域网下的访问效果图</p><p><img src="/images/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/haapp1.jpg" alt="haapp1"></p><p><img src="/images/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/haapp2.jpg" alt="haapp2"></p><p><img src="/images/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/haapp3.jpg" alt="haapp3"></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;既然是技术博客，那么这一期，我会开始做一系列的关于智能家居的玩法。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Home Assistant&lt;/code&gt; 也叫 &lt;code&gt;HA&lt;/code&gt; + &lt;code&gt;Zigbee协议&lt;/code&gt;(硬件之间的通信协议)&lt;/p&gt;
&lt;p&gt;&lt;code&gt;HACS&lt;/code&gt; 是 &lt;code&gt;HA&lt;/code&gt; 的一个第三方插件，这个插件包括了许多开发者所贡献的插件，如果你是一名开发人员，那么你一定了解什么叫&lt;code&gt;依赖库&lt;/code&gt;。所以可以理解为 &lt;code&gt;HACS&lt;/code&gt; 就是 &lt;code&gt;HA&lt;/code&gt; 的第三方插件依赖库，你可以在连找到需要对你有用的插件，而你不必重新开发。&lt;/p&gt;
&lt;p&gt;为什么选择 &lt;code&gt;Zigbee协议&lt;/code&gt; ？ 在对比过&lt;code&gt;蓝牙mesh&lt;/code&gt;,&lt;code&gt;wifi协议&lt;/code&gt;之后，我最终选择了&lt;code&gt;zigbee协议&lt;/code&gt;，在于硬件之间的控制信号量传输本身就少，所以它自身的稳定性和分布式的特点，可以很好的支持到家庭中的各个角落，不会出现由于信号差导致失灵的情况，并且&lt;code&gt;zigbee协议&lt;/code&gt;的特点，电量的消耗也是十分的低。&lt;/p&gt;
    
    </summary>
    
    
      <category term="智能家居" scheme="http://blog.crazylaw.cn/categories/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/"/>
    
    
      <category term="智能家居" scheme="http://blog.crazylaw.cn/tags/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/"/>
    
      <category term="Home Assistant" scheme="http://blog.crazylaw.cn/tags/Home-Assistant/"/>
    
      <category term="HA" scheme="http://blog.crazylaw.cn/tags/HA/"/>
    
  </entry>
  
  <entry>
    <title>【多媒体】- AAC音频</title>
    <link href="http://blog.crazylaw.cn/2022/08/20/%E5%A4%9A%E5%AA%92%E4%BD%93/aac/"/>
    <id>http://blog.crazylaw.cn/2022/08/20/%E5%A4%9A%E5%AA%92%E4%BD%93/aac/</id>
    <published>2022-08-20T03:10:40.000Z</published>
    <updated>2022-08-20T16:00:19.452Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>音频中的audio有很多格式，分别有<code>aac</code>,<code>mp3</code>等等</p><p>在海外市场中，<code>aac</code>格式比较普遍，所以最近对其进行了一些研究。</p><a id="more"></a><h2 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h2><h2 id="格式协议"><a href="#格式协议" class="headerlink" title="格式协议"></a>格式协议</h2><h2 id="解码"><a href="#解码" class="headerlink" title="解码"></a>解码</h2><h3 id="id3v2"><a href="#id3v2" class="headerlink" title="id3v2"></a>id3v2</h3><h3 id="adts"><a href="#adts" class="headerlink" title="adts"></a>adts</h3>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;音频中的audio有很多格式，分别有&lt;code&gt;aac&lt;/code&gt;,&lt;code&gt;mp3&lt;/code&gt;等等&lt;/p&gt;
&lt;p&gt;在海外市场中，&lt;code&gt;aac&lt;/code&gt;格式比较普遍，所以最近对其进行了一些研究。&lt;/p&gt;
    
    </summary>
    
    
      <category term="多媒体" scheme="http://blog.crazylaw.cn/categories/%E5%A4%9A%E5%AA%92%E4%BD%93/"/>
    
    
      <category term="aac" scheme="http://blog.crazylaw.cn/tags/aac/"/>
    
  </entry>
  
  <entry>
    <title>【广告行业】知识点</title>
    <link href="http://blog.crazylaw.cn/2022/04/12/%E5%85%AC%E5%8F%B8/%E5%B9%BF%E5%91%8A%E8%A1%8C%E4%B8%9A%E7%9F%A5%E8%AF%86%E7%82%B9/"/>
    <id>http://blog.crazylaw.cn/2022/04/12/%E5%85%AC%E5%8F%B8/%E5%B9%BF%E5%91%8A%E8%A1%8C%E4%B8%9A%E7%9F%A5%E8%AF%86%E7%82%B9/</id>
    <published>2022-04-12T01:53:00.000Z</published>
    <updated>2022-04-12T12:20:15.427Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>由于广告行业有众多的知识点，所以记录一篇文章用以了解相关的专业名词和术语，并且加深了解的印象。</p><a id="more"></a><h2 id="头部竞价"><a href="#头部竞价" class="headerlink" title="头部竞价"></a>头部竞价</h2><p><code>Header Bidding</code>，顾名思义，就是<code>头部竞价</code>，跟其相对应的就是<code>Waterfall</code>，瀑布流。在<code>Header Bidding</code>风靡<code>之前</code>，<code>Waterfall</code>才是各家广告平台的主流竞价方式。想要了解<code>Header Bidding</code>，那我们就需要先弄清楚什么是<code>Waterfall</code>了。</p><p>这里涉及到的角色有<code>开发者（publisher）</code>，<code>广告调解平台（mediation platform）</code>，<code>需求方（demand partner）</code>。首先，<code>广告调解平台</code>扮演一个（中立）的第三方角色，<code>接入了多家广告平台的广告源</code>，比如<code>Facebook</code>，<code>Vungle</code>，<code>Fyber</code>，<code>Applovin</code>等，即将各家的广告放到一个SDK中，这时候开发者如果想要变现自家的流量，只需接入广告调解平台的SDK即可，再通过一个简单的配置即可自主决定接通哪几家的广告源了，而不需要每家的广告SDK都接一遍，省时省力，是不是很棒？</p><p>那么，问题来了。开发者通过调解平台的SDK介入了多家的广告源后，流量改怎么分配呢？同一个<code>广告请求（ad request）</code>到底是该发给Facebook，还是google还是vungle还是其他人呢？这时候就涉及到调解平台的算法逻辑了，通常，这个调解平台会对给新介入的广告平台分配一定的流量，测试下其表现，并得到一个大概的<code>eCPM</code>的值；然后，调解平台会根据不同的广告平台在<code>过去24小时内的eCPM的高低来排序</code>，广告请求<code>优先</code>发给<code>eCPM最高的第一位选手</code>，若没有<code>填充（fill）</code>，就<code>下一个</code>，<code>没有填充</code>就<code>再下一个</code>，如此<code>循环往复</code>。<code>Waterfall</code>刚出来的时候真的是一个超级棒的概念，很好地解决了开发者流量变现最大化的问题。</p><p>但是，问题又来了。<strong>昨天排在第一的广告平台，谁能保证它今天给的eCPM也是足够高而排在第一位呢？</strong>又或者，昨天排在第三位的广告平台，今天有个爆款的单子推广，可以给到很高的价格，但是由于调解平台的算法机制，它不具有优先选择广告位的权利，而导致它并不能买到多少量。</p><p>这时候，聪明的移动互联网人就想到了借鉴桌面端广告的做法，没错，就是<code>Header Bidding</code>。头部竞价技术源于网页端，开发者通过在网页头部嵌入代码，从而似的广告请求在公开竞价之前可以发给开发者优选的合作伙伴，拿到优选合作伙伴的返回后将其价格与公开竞价价格做对比，价高者得。而这个模型也得以运用到移动端了。</p><p>调解平台一改往日的瀑布流的形式，将<strong>一个接一个地发广告请求的方式</strong> <code>改为</code> <strong>同时像所有的广告平台发请求</strong>，在一定的时间内将收到的返回最比较，<code>最高出价</code>即赢得广告的展示权。</p><h2 id="DSP（Demand-Side-Platform）需求方平台"><a href="#DSP（Demand-Side-Platform）需求方平台" class="headerlink" title="DSP（Demand-Side Platform）需求方平台"></a>DSP（Demand-Side Platform）需求方平台</h2><p>互联网广告DSP（Demand-Side Platform），就是需求方平台</p><p>DSP为需求方（即广告主或代理商）提供实时竞价投放平台。广告需求方可以在平台上管理广告活动及其投放策略，包括设置目标受众的定向条件、预算、出价、创意等。</p><h2 id="SSP（Supply-Side-Platform）供应方平台"><a href="#SSP（Supply-Side-Platform）供应方平台" class="headerlink" title="SSP（Supply-Side Platform）供应方平台"></a>SSP（Supply-Side Platform）供应方平台</h2><p>SSP服务于媒体方，可以作为分散的流量入口、大小媒体的聚合平台。</p><h2 id="ADN（Ad-Network）广告网盟"><a href="#ADN（Ad-Network）广告网盟" class="headerlink" title="ADN（Ad Network）广告网盟"></a>ADN（Ad Network）广告网盟</h2><p>ADN可以被理解为媒体代理公司，通过为广告主采购媒体方流量，赚取中间差价，其代表有百度网盟等。</p><h2 id="ADX（Ad-Exchange）广告交易平台"><a href="#ADX（Ad-Exchange）广告交易平台" class="headerlink" title="ADX（Ad Exchange）广告交易平台"></a>ADX（Ad Exchange）广告交易平台</h2><p>ADX提供的功能是交换，实现实时竞价、广告库存和广告需求的匹配。广告需求方可以随时改变自己的出价策略和所选择的资源。</p><h2 id="程序化广告-交易模式术语"><a href="#程序化广告-交易模式术语" class="headerlink" title="程序化广告-交易模式术语"></a>程序化广告-交易模式术语</h2><h3 id="RTB-real-time-bidding-实时竞价"><a href="#RTB-real-time-bidding-实时竞价" class="headerlink" title="RTB (real time bidding) 实时竞价"></a>RTB (real time bidding) 实时竞价</h3><p>Real Time Bidding(实时竞价)，也叫Open Auction（公开竞价），简称RTB</p><p>流量需求方在广告交易平台中，设定广告流量底价的情况下，当有流量过来时，与其他程序化广告买家一起对流量出价，广告交易平台收到各个程序化买家的出价后，进行比价，价高者获得流量并同步竞价成功的结果。整个过程都是通过程序化的方式在 100 毫秒内完成的。</p><p>品牌能够对单个展示广告位置进行竞价购买，而不是以预先固定的价格进行购买，从而使购买决策更加划算，避免广告主预算浪费</p><h3 id="PDB（Private-Direct-Buy）程序化直接购买"><a href="#PDB（Private-Direct-Buy）程序化直接购买" class="headerlink" title="PDB（Private Direct Buy）程序化直接购买"></a>PDB（Private Direct Buy）程序化直接购买</h3><blockquote><p>这个概念应该和直投是同一个概念</p></blockquote><p>是目前国内市场最为常见和主流应用的一种私有交易模式。</p><p>指流量需求方用确定的价格买断固定、优质的媒体资源，然后进行程序化广告的精准定向投放。常说的“保价保量”。</p><h3 id="PD（Preferred-Deals）优先交易"><a href="#PD（Preferred-Deals）优先交易" class="headerlink" title="PD（Preferred Deals）优先交易"></a>PD（Preferred Deals）优先交易</h3><p>与 PDB 区别在于，这种私有交易方式在广告资源上具有一定的不确定性。即流量需求方可以购买某一优质广告位，但其能获得多少曝光展示量却不能预先保证。常说的“保价不保量”。</p><h3 id="PA（Private-Auction）私有竞价"><a href="#PA（Private-Auction）私有竞价" class="headerlink" title="PA（Private Auction）私有竞价"></a>PA（Private Auction）私有竞价</h3><p>供应方平台将较优质的固定广告位资源专门拿出来，放在一个半公开市场中，仅由进入白名单的买方（VIP）进行竞价，价高者得。因此，广告位可以锁定，但采买价格和是否最终获得曝光都不能预先保证。常说的“不保价不保量”。</p><h2 id="程序化广告-效果术语"><a href="#程序化广告-效果术语" class="headerlink" title="程序化广告-效果术语"></a>程序化广告-效果术语</h2><p>广告传播影响受众的认知、心理、行为和态度，由此带来的直接和间接广告效益，对广告效果的评估的也有着多方面要素和维度。</p><h3 id="ROI"><a href="#ROI" class="headerlink" title="ROI"></a>ROI</h3><p><code>Return On Investment（投资回报率）</code>，简称<code>ROI</code>。即营销者通过广告投放得到的经济回报占广告投入（花费）的比例。</p><h3 id="Impression"><a href="#Impression" class="headerlink" title="Impression"></a>Impression</h3><p><code>Impression</code>，即<code>曝光量</code>，也被称为“展示量”、“展现量”。即投放期广告被展示的总次数。一般用户每浏览一次页面，同时页面中广告位的广告被展示一次，就是一个曝光。</p><h3 id="Click"><a href="#Click" class="headerlink" title="Click"></a>Click</h3><p>Click，即<code>点击量</code>，为投放期用户点击某个广告的总次数。</p><h3 id="CTR-Click-Through-Rate-点击率"><a href="#CTR-Click-Through-Rate-点击率" class="headerlink" title="CTR(Click-Through-Rate) 点击率"></a>CTR(Click-Through-Rate) 点击率</h3><p>广告被点击的次数与广告曝光次数的比例</p><p>计算公式：Click/Impression*100%</p><p>反映了广告的受关注程度，或用来衡量广告的吸引程度。</p><h3 id="RR（Reach-Rate）到达率"><a href="#RR（Reach-Rate）到达率" class="headerlink" title="RR（Reach Rate）到达率"></a>RR（Reach Rate）到达率</h3><p>到达量与点击量的比例（到达量/点击量*100%）</p><p>到达量：即有多少用户点击广告后进入落地页。</p><h3 id="CR（Conversion-Rate）转化率"><a href="#CR（Conversion-Rate）转化率" class="headerlink" title="CR（Conversion Rate）转化率"></a>CR（Conversion Rate）转化率</h3><p>转化量与点击量的比例（转化量/点击量*100%）</p><p>转化量：即有多少用户点击广告并进入落地页（活动页）后，继续发生咨询、注册、下载、加入购物车、下单等行为。</p><h3 id="留存率"><a href="#留存率" class="headerlink" title="留存率"></a>留存率</h3><p>特定周期内（如次日留存、七日留存等），留存用户数量（有多少用户留下来）占广告（当时）导入的新增用户数量的比例。留存率=留存用户数/新增用户数量*100%</p><h3 id="LT（Life-Time）生命周期"><a href="#LT（Life-Time）生命周期" class="headerlink" title="LT（Life Time）生命周期"></a>LT（Life Time）生命周期</h3><p>一个用户从第1次到最后1次参与游戏之间的时间段，一般按月计算平均值</p><h3 id="LTV-Life-Time-Value-用户终生价值"><a href="#LTV-Life-Time-Value-用户终生价值" class="headerlink" title="LTV(Life Time Value) 用户终生价值"></a>LTV(Life Time Value) 用户终生价值</h3><p>用户在生命周期内为该游戏创造的收入总计，可以看成是一个ARPU 值的长期累计。</p><p>计算公式：LTV = ARPUxLT。</p><h3 id="DAU-Daily-Active-Users-日活跃用户数量"><a href="#DAU-Daily-Active-Users-日活跃用户数量" class="headerlink" title="DAU(Daily Active Users) 日活跃用户数量"></a>DAU(Daily Active Users) 日活跃用户数量</h3><p>DAU 指的是某产品（网站、软件或游戏等）在一日之内登录或使用过的用户总数（不包括重复的用户）。</p><p>DAU 是一个比较基本的指标，能够相对片面地展现产品短时间内的热度。</p><p>一般来说只看产品的 DAU 其实意义不大，单纯通过 DAU 无法判断产品的真实质量，且 DAU 很容易伪造。</p><h3 id="MAU-Monthly-Active-Users-月活跃用户数量"><a href="#MAU-Monthly-Active-Users-月活跃用户数量" class="headerlink" title="MAU(Monthly Active Users) 月活跃用户数量"></a>MAU(Monthly Active Users) 月活跃用户数量</h3><p>MAU 指的是某产品（网站、软件或游戏等）在一个月（统计月）之内登录或使用过的用户总数（不包括重复的用户）。</p><p>MAU 同样也是一个比较基本的指标，能够相对片面的展现产品一段时间内的热度。</p><p>通过 DAU 和 MAU 虽然能够看出产品在一段时间内的热度，但是无法精确地判断产品的留存率，因为你无法得知用户的属性（是否为老用户）。此时 DNU 和 DOU 是时候出来救场了。</p><h3 id="DNU-amp-DOU"><a href="#DNU-amp-DOU" class="headerlink" title="DNU &amp; DOU"></a>DNU &amp; DOU</h3><p>「Daily New Users（日新增用户数量 ）&amp; Daily Old Users（日非新增用户数量）」</p><p>这两个词的意义非常明确，就是直接展现了产品短时间内的新老用户情况。</p><p>通过 DNU 和 DOU 加上 DAU 和 MAU 这几个指标能够比较直接地判断产品的留存率（即用户粘性）。</p><h3 id="ARPU-Average-Revenue-Per-User-每用户平均收益"><a href="#ARPU-Average-Revenue-Per-User-每用户平均收益" class="headerlink" title="ARPU(Average Revenue Per User) 每用户平均收益"></a>ARPU(Average Revenue Per User) 每用户平均收益</h3><p>目前 ARPU 这个概念在许多行业都有着广泛的应用，特别是在互联网游戏产业里面，随着免费游戏的兴起，ARPU 日渐成为游戏运营商着重关注的元素。</p><p>一般来说，不同的行业乃至不同的企业都有自己专属的 ARPU 计算方式。</p><p>常用计算公式：每用户平均收益 = 总收益 ÷ 总用户数</p><h2 id="广告相关"><a href="#广告相关" class="headerlink" title="广告相关"></a>广告相关</h2><h3 id="eCPM（Effective-Cost-Per-Mille）-每千次展示收益"><a href="#eCPM（Effective-Cost-Per-Mille）-每千次展示收益" class="headerlink" title="eCPM（Effective Cost Per Mille） 每千次展示收益"></a>eCPM（Effective Cost Per Mille） 每千次展示收益</h3><p>eCPM 是一个主要面向产品方的指标。</p><p>指的是在产品（网页、应用等等）中展示某广告 1000 次所带来的收益。</p><p>计算公式：每千次展示收益 = 总收益 ÷ 广告展示总次数 × 1000</p><blockquote><p>只要是有效展示就行，可以说是很简单粗暴的变现方式了…</p></blockquote><h3 id="CPM"><a href="#CPM" class="headerlink" title="CPM"></a>CPM</h3><p>「Cost Per Mille / Cost Per Thousand Impressions（每千次印象成本）」</p><p>CPM 是一种主要面向广告主的广告计费模式。</p><p>指的是广告投放过程中，平均每向 1000 人展示某广告 1 次需要的成本。一般同一 IP 在 24 小时内最多只有一次有效展示。</p><p>计算公式：每千次印象成本 = 总成本 ÷ 广告达到人数 × 1000</p><blockquote><p>你可以不看，但是我这个广告一定要播！</p></blockquote><h3 id="CPC（Cost-Per-Click）-每点击成本"><a href="#CPC（Cost-Per-Click）-每点击成本" class="headerlink" title="CPC（Cost Per Click） 每点击成本"></a>CPC（Cost Per Click） 每点击成本</h3><p>CPC 是一种主要面向广告主的广告计费模式。</p><p>指的是在广告投放中，广告主仅为用户的有效点击行为付费，而不再为广告的展示次数付费。</p><p>计算公式：每点击成本 = 总成本 ÷ 点击数</p><blockquote><p>不点不给钱！</p></blockquote><h3 id="CPA（Cost-Per-Action）-每行动成本"><a href="#CPA（Cost-Per-Action）-每行动成本" class="headerlink" title="CPA（Cost Per Action） 每行动成本"></a>CPA（Cost Per Action） 每行动成本</h3><p>CPA 是一种主要面向广告主的广告计费模式。</p><p>CPA 顾名思义是按照用户的行为（Action）作为指标来计费，这个行为可以是注册、咨询或加入购物车等等。</p><p>意思就是用户点击了广告还不算，需要用户有注册成功之类的行为才行，不过这种模式下广告费也相对较高。</p><blockquote><p>这是一个不受产品方待见的模式，条件太苛刻了…</p></blockquote><h3 id="CPS-Cost-Per-Sale-每销售成本"><a href="#CPS-Cost-Per-Sale-每销售成本" class="headerlink" title="CPS(Cost Per Sale) 每销售成本"></a>CPS(Cost Per Sale) 每销售成本</h3><p>CPS 是一种面向广告主的广告计费模式。</p><p>在该模式下，广告主的商品成功销售出去之后，产品方才可以获取到一定比例的佣金（提成）。</p><blockquote><p>这么说来似乎有点销售的意思（吃提成）</p></blockquote><h3 id="CPP（Cost-Per-Purchase）每购买成本"><a href="#CPP（Cost-Per-Purchase）每购买成本" class="headerlink" title="CPP（Cost Per Purchase）每购买成本"></a>CPP（Cost Per Purchase）每购买成本</h3><p>CPP 是一种面向广告主的广告计费模式。</p><p>在该模式下，用户点击广告并成功进行交易后，广告主按照销售笔数付给产品方广告费用。</p><blockquote><p>要注意了，CPP 是按照销售笔数来算钱的</p></blockquote><h3 id="CPR（Cost-Per-Response）每回应成本"><a href="#CPR（Cost-Per-Response）每回应成本" class="headerlink" title="CPR（Cost Per Response）每回应成本"></a>CPR（Cost Per Response）每回应成本</h3><p>CPR 是一种面向广告主的广告计费模式，这种模式的特点为：及时反应、直接互动、准确记录。</p><p>在 CPR 模式下，广告展示后，还需要用户给予广告主回应才算有效。所谓回应，一般是拨打电话之类的形式。例如电视购物广告，一般在固定时段播出，当用户拨打了广告中的电话之后才算作有效传播。</p><p>这种模式要求相对较高，也挺不受待见的，这广告费太难赚了…</p><blockquote><p>又想起了被电视购物广告支配的日子..</p></blockquote><h3 id="PPC（Pay-Per-Click）点击付费广告"><a href="#PPC（Pay-Per-Click）点击付费广告" class="headerlink" title="PPC（Pay Per Click）点击付费广告"></a>PPC（Pay Per Click）点击付费广告</h3><p>PPC 是大公司最常用的网络广告形式，这种方法费用很高，但效果也很好，比如百度竞价、搜狐和新浪首页上的 Banner 广告。</p><p>计价公式：起价 + (点击数 × 每次点击的价格)</p><p>越是著名的搜索引擎，起价越高，最高可达数万甚至数十万，而每次点击的价格在 0.30 元左右。</p><p>提供点击付费的网站非常多，主要有各大门户网站(如搜狐、新浪)和搜索引擎（Google 和百度），以及其他浏览量较大的网站，比如提供软件下载的华军等等。</p><blockquote><p>就是那种在搜索引擎搜素关键词，然后在展示在搜索结果前面的那种广告…</p></blockquote><h3 id="PPS（Pay-Per-Sale）-按销售付费"><a href="#PPS（Pay-Per-Sale）-按销售付费" class="headerlink" title="PPS（Pay Per Sale） 按销售付费"></a>PPS（Pay Per Sale） 按销售付费</h3><p>PPS 广告是根据网络广告所产生的直接销售数量而付费的一种定价模式。</p><p>PPS 和 CPS 基本一个意思，类似于淘宝客这类的服务。广义上不仅仅是指互联网广告范畴，应包括所有形式的基于成功销售而收取一定比例佣金的商业合作方式。</p><h2 id="网站相关"><a href="#网站相关" class="headerlink" title="网站相关"></a>网站相关</h2><h3 id="UV（Unique-Visitors）独立访客数"><a href="#UV（Unique-Visitors）独立访客数" class="headerlink" title="UV（Unique Visitors）独立访客数"></a>UV（Unique Visitors）独立访客数</h3><p>UV 表示某网站 1 天内（00:00 - 24:00）的独立访客总数。</p><p>所谓的“独立”访客，是以浏览器的 Cookie 为依据的，只要 Cookie 相同，那么就算更换了 IP 也都将被视为同一个访客，且当天无论访问多少次 UV 都只会加 1 个。</p><blockquote><p>通常是一部手机（电脑）一个坑~</p></blockquote><h3 id="PV（Page-Views）页面浏览量"><a href="#PV（Page-Views）页面浏览量" class="headerlink" title="PV（Page Views）页面浏览量"></a>PV（Page Views）页面浏览量</h3><p>PV 表示某页面在一定统计周期内的总浏览量。</p><p>在一定周期内，用户每次打开或刷新该页面都将被视为 1 次浏览。</p><blockquote><p>刷新一下多一个真好玩~</p></blockquote><h3 id="IP（Internet-Protocol）独立-IP-数"><a href="#IP（Internet-Protocol）独立-IP-数" class="headerlink" title="IP（Internet Protocol）独立 IP 数"></a>IP（Internet Protocol）独立 IP 数</h3><p>IP 表示 1 天内（00:00 - 24:00）访问某网站的 IP 总数。</p><p>该指标以广域网 IP 为依据，同一 IP 的设备无论访问多少次都只算一个计数，也就是说，连接同一路由器的不同设备在同一天内多次访问该网站，在正常情况下都只会增加一个 IP 计数。</p><blockquote><p>一条网线一个坑~</p></blockquote><h3 id="RV（Repeat-Visitors）重复访客数"><a href="#RV（Repeat-Visitors）重复访客数" class="headerlink" title="RV（Repeat Visitors）重复访客数"></a>RV（Repeat Visitors）重复访客数</h3><p>RV 指的是在一定统计周期内，访问某网站两次或两次以上的访客总数。</p><blockquote><p>俗称：回头客</p></blockquote><h3 id="TP（Time-On-Page）页面停留时间"><a href="#TP（Time-On-Page）页面停留时间" class="headerlink" title="TP（Time On Page）页面停留时间"></a>TP（Time On Page）页面停留时间</h3><p>TP 指的是（总的）用户在某个页面的平均停留时长。</p><p>TP 时长可以反映出某个页面对用户的吸引力，帮助判断用户的喜好。</p><blockquote><p>能看完这篇文章的话 TP 应该能有 5 分钟吧</p></blockquote><h3 id="TS（Time-On-Site）-网站停留时间"><a href="#TS（Time-On-Site）-网站停留时间" class="headerlink" title="TS（Time On Site） 网站停留时间"></a>TS（Time On Site） 网站停留时间</h3><p>TS 指的是（总的）用户在某网站（包括了该网站下的所有页面）的平均停留时长。</p><blockquote><p>反正视频网站的 TS 肯定都挺长的</p></blockquote><h3 id="SD（Session-Duration-）平均会话时长"><a href="#SD（Session-Duration-）平均会话时长" class="headerlink" title="SD（Session Duration ）平均会话时长"></a>SD（Session Duration ）平均会话时长</h3><p>SD 是在 Google Analytics 中使用的一个指标，用来统计网站的平均停留时长。</p><p>SD 的作用类似于 Time On Site，但是这两者的计算方式不一样。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;由于广告行业有众多的知识点，所以记录一篇文章用以了解相关的专业名词和术语，并且加深了解的印象。&lt;/p&gt;
    
    </summary>
    
    
      <category term="广告行业" scheme="http://blog.crazylaw.cn/categories/%E5%B9%BF%E5%91%8A%E8%A1%8C%E4%B8%9A/"/>
    
    
      <category term="广告行业" scheme="http://blog.crazylaw.cn/tags/%E5%B9%BF%E5%91%8A%E8%A1%8C%E4%B8%9A/"/>
    
  </entry>
  
  <entry>
    <title>【Mac】Mac安装软件流程</title>
    <link href="http://blog.crazylaw.cn/2022/03/29/Mac/Mac%E5%AE%89%E8%A3%85%E8%BD%AF%E4%BB%B6%E6%B5%81%E7%A8%8B/"/>
    <id>http://blog.crazylaw.cn/2022/03/29/Mac/Mac%E5%AE%89%E8%A3%85%E8%BD%AF%E4%BB%B6%E6%B5%81%E7%A8%8B/</id>
    <published>2022-03-29T08:35:30.000Z</published>
    <updated>2022-04-11T10:31:25.337Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近，准备重新打造我的mac电脑。</p><a id="more"></a><h2 id="iterm2"><a href="#iterm2" class="headerlink" title="iterm2"></a>iterm2</h2><p><img src="/images/%E6%9D%82/iterm2.png" alt="iterm2"></p><p><img src="/images/%E6%9D%82/iterm2-2.png" alt="iterm2"></p><p><img src="/images/%E6%9D%82/iterm2-fonts.png" alt="iterm2"></p><ul><li>安装zsh</li><li>安装 <a href="https://github.com/romkatv/powerlevel10k" target="_blank" rel="noopener">powerlevel10k</a> (安装方式参考官网文档)</li><li>替换 <code>.vimrc</code> 中的 <code>zsh主题</code>, <code>ZSH_THEME=&quot;powerlevel10k/powerlevel10k&quot;</code></li></ul><h2 id="vim相关"><a href="#vim相关" class="headerlink" title="vim相关"></a>vim相关</h2><p>我的个人vim配置 <a href="https://github.com/whiteCcinn/ccinn-vim" target="_blank" rel="noopener">ccinn-vim</a></p><p><img src="/images/%E6%9D%82/vim.png" alt="vim"></p><p>vim的插件管理选择有 <code>Vundle</code>, <code>vim-plug</code></p><h3 id="vim的目录插件"><a href="#vim的目录插件" class="headerlink" title="vim的目录插件"></a>vim的目录插件</h3><ul><li><a href="https://github.com/preservim/nerdtree" target="_blank" rel="noopener">scrooloose/nerdtree</a></li></ul><h3 id="vim的目录增强插件-字体和icon的下载和安装"><a href="#vim的目录增强插件-字体和icon的下载和安装" class="headerlink" title="vim的目录增强插件,字体和icon的下载和安装"></a>vim的目录增强插件,字体和icon的下载和安装</h3><p>可以在nerd目录下显示图标，其他字体等</p><ul><li><p><a href="https://github.com/ryanoasis/nerd-fonts" target="_blank" rel="noopener">ryanoasis/nerd-fonts</a></p></li><li><p><a href="https://github.com/ryanoasis/vim-devicons" target="_blank" rel="noopener">ryanoasis/vim-devicons</a></p></li></ul><blockquote><p>主体iterm2 的字体需要和nerd-fonts一致</p></blockquote><h3 id="vim-窗口状态栏"><a href="#vim-窗口状态栏" class="headerlink" title="vim 窗口状态栏"></a>vim 窗口状态栏</h3><ul><li><a href="https://github.com/vim-airline/vim-airline" target="_blank" rel="noopener">vim-airline/vim-airline</a></li><li><a href="https://github.com/vim-airline/vim-airline-themes" target="_blank" rel="noopener">vim-airline/vim-airline-themes</a></li></ul><h2 id="Clean-my-mac"><a href="#Clean-my-mac" class="headerlink" title="Clean my mac"></a>Clean my mac</h2><p>付费软件，清理电脑</p><h2 id="ntfs-for-mac"><a href="#ntfs-for-mac" class="headerlink" title="ntfs for mac"></a>ntfs for mac</h2><p>外置移动硬盘专用</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;最近，准备重新打造我的mac电脑。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Mac" scheme="http://blog.crazylaw.cn/categories/Mac/"/>
    
    
      <category term="Mac" scheme="http://blog.crazylaw.cn/tags/Mac/"/>
    
  </entry>
  
  <entry>
    <title>【DevOps】git命令场景用法</title>
    <link href="http://blog.crazylaw.cn/2022/03/19/DevOps/git%E5%91%BD%E4%BB%A4%E5%9C%BA%E6%99%AF%E7%94%A8%E6%B3%95/"/>
    <id>http://blog.crazylaw.cn/2022/03/19/DevOps/git%E5%91%BD%E4%BB%A4%E5%9C%BA%E6%99%AF%E7%94%A8%E6%B3%95/</id>
    <published>2022-03-19T06:35:30.000Z</published>
    <updated>2022-04-15T07:40:12.764Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>现在，git已经成为了大家的代码仓库管理的一个工具了。在日常工作，我们会遇到各种个样的git问题，因此，用一篇文章来累计记录，日常生活中，我们会遇到，但是不常用的命令。</p><a id="more"></a><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p>其实 <code>git</code> 的概念，我们开发者应该很多有会了，存在以下几个区域：</p><ul><li>工作区   <code>（你的任何改动）</code></li><li>暂存区   <code>（git add）</code></li><li>本地仓库 <code>（git commit）</code></li><li>远端代码仓库 <code>（git push）</code></li></ul><h2 id="合并"><a href="#合并" class="headerlink" title="合并"></a>合并</h2><p>我们一般说合并其实是有</p><h3 id=""><a href="#" class="headerlink" title=""></a></h3>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;现在，git已经成为了大家的代码仓库管理的一个工具了。在日常工作，我们会遇到各种个样的git问题，因此，用一篇文章来累计记录，日常生活中，我们会遇到，但是不常用的命令。&lt;/p&gt;
    
    </summary>
    
    
      <category term="DevOps" scheme="http://blog.crazylaw.cn/categories/DevOps/"/>
    
    
      <category term="DevOps" scheme="http://blog.crazylaw.cn/tags/DevOps/"/>
    
  </entry>
  
  <entry>
    <title>【Golang】- go map源码阅读</title>
    <link href="http://blog.crazylaw.cn/2022/03/10/Golang/go%20map%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/"/>
    <id>http://blog.crazylaw.cn/2022/03/10/Golang/go%20map%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/</id>
    <published>2022-03-10T07:55:51.000Z</published>
    <updated>2022-03-22T08:54:11.241Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近发现同事去面试，发现很多时候会被问到<code>go map</code>的底层结构。今天我们来记录一下map的底层实现。</p><a id="more"></a><p>当下的源码阅读基于1.17</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// A header for a Go map.</span></span><br><span class="line"><span class="keyword">type</span> hmap <span class="keyword">struct</span> &#123;</span><br><span class="line"><span class="comment">// Note: the format of the hmap is also encoded in cmd/compile/internal/gc/reflect.go.</span></span><br><span class="line"><span class="comment">// Make sure this stays in sync with the compiler's definition.</span></span><br><span class="line">count     <span class="keyword">int</span> <span class="comment">// # live cells == size of map.  Must be first (used by len() builtin)</span></span><br><span class="line">flags     <span class="keyword">uint8</span></span><br><span class="line">B         <span class="keyword">uint8</span>  <span class="comment">// log_2 of # of buckets (can hold up to loadFactor * 2^B items)</span></span><br><span class="line">noverflow <span class="keyword">uint16</span> <span class="comment">// approximate number of overflow buckets; see incrnoverflow for details</span></span><br><span class="line">hash0     <span class="keyword">uint32</span> <span class="comment">// hash seed</span></span><br><span class="line"></span><br><span class="line">buckets    unsafe.Pointer <span class="comment">// array of 2^B Buckets. may be nil if count==0.</span></span><br><span class="line">oldbuckets unsafe.Pointer <span class="comment">// previous bucket array of half the size, non-nil only when growing</span></span><br><span class="line">nevacuate  <span class="keyword">uintptr</span>        <span class="comment">// progress counter for evacuation (buckets less than this have been evacuated)</span></span><br><span class="line"></span><br><span class="line">extra *mapextra <span class="comment">// optional fields</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到这是一个map的头部结构，其中有几个关键结构，分别是</p><ul><li><code>count</code> 当前map的元素个数</li><li><code>buckets</code> 桶的数量，一般是2^B个</li><li><code>oldbuckets</code> 扩容前的buckets</li></ul><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// mapextra holds fields that are not present on all maps.</span></span><br><span class="line"><span class="keyword">type</span> mapextra <span class="keyword">struct</span> &#123;</span><br><span class="line"><span class="comment">// If both key and elem do not contain pointers and are inline, then we mark bucket</span></span><br><span class="line"><span class="comment">// type as containing no pointers. This avoids scanning such maps.</span></span><br><span class="line"><span class="comment">// However, bmap.overflow is a pointer. In order to keep overflow buckets</span></span><br><span class="line"><span class="comment">// alive, we store pointers to all overflow buckets in hmap.extra.overflow and hmap.extra.oldoverflow.</span></span><br><span class="line"><span class="comment">// overflow and oldoverflow are only used if key and elem do not contain pointers.</span></span><br><span class="line"><span class="comment">// overflow contains overflow buckets for hmap.buckets.</span></span><br><span class="line"><span class="comment">// oldoverflow contains overflow buckets for hmap.oldbuckets.</span></span><br><span class="line"><span class="comment">// The indirection allows to store a pointer to the slice in hiter.</span></span><br><span class="line">overflow    *[]*bmap</span><br><span class="line">oldoverflow *[]*bmap</span><br><span class="line"></span><br><span class="line"><span class="comment">// nextOverflow holds a pointer to a free overflow bucket.</span></span><br><span class="line">nextOverflow *bmap</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>简单来说，这个可以忽略。</p></blockquote><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// A bucket for a Go map.</span></span><br><span class="line"><span class="keyword">type</span> bmap <span class="keyword">struct</span> &#123;</span><br><span class="line"><span class="comment">// tophash generally contains the top byte of the hash value</span></span><br><span class="line"><span class="comment">// for each key in this bucket. If tophash[0] &lt; minTopHash,</span></span><br><span class="line"><span class="comment">// tophash[0] is a bucket evacuation state instead.</span></span><br><span class="line">tophash [bucketCnt]<span class="keyword">uint8</span></span><br><span class="line"><span class="comment">// Followed by bucketCnt keys and then bucketCnt elems.</span></span><br><span class="line"><span class="comment">// <span class="doctag">NOTE:</span> packing all the keys together and then all the elems together makes the</span></span><br><span class="line"><span class="comment">// code a bit more complicated than alternating key/elem/key/elem/... but it allows</span></span><br><span class="line"><span class="comment">// us to eliminate padding which would be needed for, e.g., map[int64]int8.</span></span><br><span class="line"><span class="comment">// Followed by an overflow pointer.</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>bmap有2个模块的属性是在编译注入的，在源码上没办法浏览。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">                                                    ┌─────────────────────────────────────┐</span><br><span class="line">                                                    │bmap                                 │</span><br><span class="line">┌─────────────────────────────────────┐             │                                     │</span><br><span class="line">│bmap                                 │             │                                     │</span><br><span class="line">│                                     │             │                                     │</span><br><span class="line">│                                     │             │    ┌──────────────────────────────┐ │</span><br><span class="line">│                                     │             │    │tohash[bucketCnt]uint8        │ │</span><br><span class="line">│    ┌──────────────────────────────┐ │             │    │                              │ │</span><br><span class="line">│    │tohash[bucketCnt]uint8        │ │             │    │                              │ │</span><br><span class="line">│    │                              │ │             │    │                              │ │</span><br><span class="line">│    │                              │ │             │    └──────────────────────────────┘ │</span><br><span class="line">│    │                              │ │             │                                     │</span><br><span class="line">│    └──────────────────────────────┘ │             │    ++++++++++++++++++++++++++++++++ │</span><br><span class="line">│                                     │             │    +  byte-array                  + │</span><br><span class="line">│    ++++++++++++++++++++++++++++++++ │             │    +        (save key-value)      + │</span><br><span class="line">│    +  byte-array                  + │     ┌──────►│    +                              + │</span><br><span class="line">│    +        (save key-value)      + │     │       │    ++++++++++++++++++++++++++++++++ │</span><br><span class="line">│    +                              + │     │       │                                     │</span><br><span class="line">│    ++++++++++++++++++++++++++++++++ │     │       │    ++++++++++++++++++++++++++++++++ │</span><br><span class="line">│                                     │     │       │    +   point to growed bucket     + │</span><br><span class="line">│    ++++++++++++++++++++++++++++++++ │     │       │    +                              + │</span><br><span class="line">│    +   point to growed bucket     + │     │       │    +                              + │</span><br><span class="line">│    +                              + ├─────┘       │    +++++++++++++─┐+++++++++++++++++ │</span><br><span class="line">│    +                              + │             │                  │                  │</span><br><span class="line">│    ++++++++++++++++++++++++++++++++ │             │                  │                  │</span><br><span class="line">│                                     │             └──────────────────┼──────────────────┘</span><br><span class="line">│                                     │                                │</span><br><span class="line">└─────────────────────────────────────┘                                │</span><br><span class="line">                                                                       │</span><br><span class="line">                                                                       ▼</span><br><span class="line">                                                     ┌─────────────────────────────────────┐</span><br><span class="line">                                                     │bmap                                 │</span><br><span class="line">                                                     │                                     │</span><br><span class="line">                                                     │                                     │</span><br><span class="line">                                                     │                                     │</span><br><span class="line">                                                     │    ┌──────────────────────────────┐ │</span><br><span class="line">                                                     │    │tohash[bucketCnt]uint8        │ │</span><br><span class="line">                                                     │    │                              │ │</span><br><span class="line">                                                     │    │                              │ │</span><br><span class="line">                                                     │    │                              │ │</span><br><span class="line">                                                     │    └──────────────────────────────┘ │</span><br><span class="line">                                                     │                                     │</span><br><span class="line">                                                     │    ++++++++++++++++++++++++++++++++ │</span><br><span class="line">                                                     │    +  byte-array                  + │</span><br><span class="line">                                                     │    +        (save key-value)      + │</span><br><span class="line">                                                     │    +                              + │</span><br><span class="line">                                                     │    ++++++++++++++++++++++++++++++++ │</span><br><span class="line">                                                     │                                     │</span><br><span class="line">                                                     │    ++++++++++++++++++++++++++++++++ │</span><br><span class="line">                                                     │    +   point to growed bucket     + │</span><br><span class="line">                                                     │    +                              + │</span><br><span class="line">                                                     │    +                              + │</span><br><span class="line">                                                     │    ++++++++++++++++++++++++++++++++ │</span><br><span class="line">                                                     │                                     │</span><br><span class="line">                                                     │                                     │</span><br><span class="line">                                                     └─────────────────────────────────────┘s</span><br></pre></td></tr></table></figure><p>相比于hmap，bucket的结构显得简单一些，<code>byte-array</code>是我们使用的map中的key和value就存储在这里。<code>高位哈希值</code>数组记录的是当前bucket中key相关的<code>索引</code></p><h2 id="mapassign-赋值过程"><a href="#mapassign-赋值过程" class="headerlink" title="mapassign 赋值过程"></a>mapassign 赋值过程</h2><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1. 判断会否当前已经进行了写保护</span></span><br><span class="line"><span class="keyword">if</span> h.flags&amp;hashWriting != <span class="number">0</span> &#123;</span><br><span class="line">throw(<span class="string">"concurrent map writes"</span>)</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 2. 根据key计算哈希值</span></span><br><span class="line">hash := t.hasher(key, <span class="keyword">uintptr</span>(h.hash0))</span><br><span class="line"><span class="comment">// 3. 进行写保护</span></span><br><span class="line">h.flags ^= hashWriting</span><br><span class="line"><span class="comment">// 4. 计算hash的低位部分</span></span><br><span class="line">bucket := hash &amp; bucketMask(h.B)</span><br><span class="line"><span class="comment">// 5. 判断是否正在扩容，如果是，则数据迁移</span></span><br><span class="line"><span class="keyword">if</span> h.growing() &#123;</span><br><span class="line">growWork(t, h, bucket)</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 6. 根据低位hash找到对应的bucket</span></span><br><span class="line">b := (*bmap)(add(h.buckets, bucket*<span class="keyword">uintptr</span>(t.bucketsize)))</span><br><span class="line"><span class="comment">// 7. 计算高位hash</span></span><br><span class="line">top := tophash(hash)</span><br><span class="line"><span class="comment">// 8. 从对应的bucket以及overflow buckets中找到对应的key的位置</span></span><br><span class="line"><span class="comment">// 9. 判断是否需要扩容，如果需要，则重新找到key的位置</span></span><br><span class="line"><span class="keyword">if</span> !h.growing() &amp;&amp; (overLoadFactor(h.count+<span class="number">1</span>, h.B) || tooManyOverflowBuckets(h.noverflow, h.B)) &#123;</span><br><span class="line">hashGrow(t, h)</span><br><span class="line"><span class="keyword">goto</span> again <span class="comment">// Growing the table invalidates everything, so try again</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 10. 拿着可以插入kv的内存地址进行赋值</span></span><br><span class="line"><span class="keyword">if</span> t.indirectkey() &#123;</span><br><span class="line">kmem := newobject(t.key)</span><br><span class="line">*(*unsafe.Pointer)(insertk) = kmem</span><br><span class="line">insertk = kmem</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> t.indirectelem() &#123;</span><br><span class="line">vmem := newobject(t.elem)</span><br><span class="line">*(*unsafe.Pointer)(elem) = vmem</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 11. 写保护检查，并且解除写保护</span></span><br><span class="line"><span class="keyword">if</span> h.flags&amp;hashWriting == <span class="number">0</span> &#123;</span><br><span class="line">throw(<span class="string">"concurrent map writes"</span>)</span><br><span class="line">&#125;</span><br><span class="line">h.flags &amp;^= hashWriting</span><br></pre></td></tr></table></figure><p>赋值过程就是：</p><ol><li>进行写保护</li><li>根据key计算哈希值</li><li>在低位哈希中找到bucket</li><li>计算高位hash</li><li>在bucket和overflow bucket桶中找到能插入key/value的位置</li><li>找到了就赋值</li><li>解除锁保护</li></ol><blockquote><p>其中有多次判断bucket是否需要扩容和是否正在扩容</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;最近发现同事去面试，发现很多时候会被问到&lt;code&gt;go map&lt;/code&gt;的底层结构。今天我们来记录一下map的底层实现。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Go源码剖析系列" scheme="http://blog.crazylaw.cn/categories/Go%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90%E7%B3%BB%E5%88%97/"/>
    
    
      <category term="Golang" scheme="http://blog.crazylaw.cn/tags/Golang/"/>
    
      <category term="Go源码剖析" scheme="http://blog.crazylaw.cn/tags/Go%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>【Golang】- go time.Sleep源码阅读</title>
    <link href="http://blog.crazylaw.cn/2022/03/10/Golang/go%20time.Sleep%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/"/>
    <id>http://blog.crazylaw.cn/2022/03/10/Golang/go%20time.Sleep%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/</id>
    <published>2022-03-10T07:55:51.000Z</published>
    <updated>2022-03-10T14:09:34.592Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>由于time.Sleep()会挂起我们的协程，我们来看一下它的底层原理。</p><a id="more"></a><h2 id="sleep-的实现"><a href="#sleep-的实现" class="headerlink" title="sleep 的实现"></a>sleep 的实现</h2><p>我们通常使用 <code>time.Sleep(1 * time.Second)</code> 来将 goroutine 暂时休眠一段时间。sleep 操作在底层实现也是基于 timer 实现的。</p><p>有一些比较有意思的地方，单独拿出来讲下。</p><p>我们固然也可以这么做来实现 goroutine 的休眠:</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">timer := time.NewTimer(<span class="number">2</span> * time.Seconds)</span><br><span class="line">&lt;-timer.C</span><br></pre></td></tr></table></figure><p>这么做当然可以。但 golang 底层显然不是这么做的，因为这样有两个明显的额外性能损耗。</p><ul><li>每次调用 sleep 的时候，都要创建一个 timer 对象</li><li>需要一个 channel 来传递事件</li></ul><p>既然都可以放在 runtime 里面做。golang 里面做的更加干净：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// timeSleep puts the current goroutine to sleep for at least ns nanoseconds.</span></span><br><span class="line"><span class="comment">//go:linkname timeSleep time.Sleep</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">timeSleep</span><span class="params">(ns <span class="keyword">int64</span>)</span></span> &#123;</span><br><span class="line"><span class="keyword">if</span> ns &lt;= <span class="number">0</span> &#123;</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">gp := getg()</span><br><span class="line">t := gp.timer</span><br><span class="line"><span class="keyword">if</span> t == <span class="literal">nil</span> &#123;</span><br><span class="line">t = <span class="built_in">new</span>(timer)</span><br><span class="line">gp.timer = t</span><br><span class="line">&#125;</span><br><span class="line">t.f = goroutineReady</span><br><span class="line">t.arg = gp</span><br><span class="line">t.nextwhen = nanotime() + ns</span><br><span class="line"><span class="keyword">if</span> t.nextwhen &lt; <span class="number">0</span> &#123; <span class="comment">// check for overflow.</span></span><br><span class="line">t.nextwhen = maxWhen</span><br><span class="line">&#125;</span><br><span class="line">gopark(resetForSleep, unsafe.Pointer(t), waitReasonSleep, traceEvGoSleep, <span class="number">1</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>在G对象上存在一个timer属性，在G的生命周期里timer都是唯一存在，解决了重复新建对象的问题</li><li>如果不存在timer，则在第一次的时候创建timer</li></ul><p>并且把<code>t.f</code>设置成<code>goroutineReay</code>(这个意思是time到了时间之后设置一个触发函数，这个触发函数就是唤醒我们当前G任务)。</p><p>然后通过<code>gopark</code>来挂起当前的G任务</p><h2 id="定时器的触发机制"><a href="#定时器的触发机制" class="headerlink" title="定时器的触发机制"></a>定时器的触发机制</h2><p>共分两种方式，分别为 <code>调度器触发</code> 和 <code>监控线程sysmon</code> 触发，两者主要是通过调用函数 <code>checkTimers()</code> 来实现的。</p><p>主要有两个地方会检查计时器，一个是 <code>runtime.schedule</code>，另一个是 <code>findrunnable</code>。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// runtime/proc.go</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">schedule</span><span class="params">()</span></span> &#123; </span><br><span class="line"> _g_ := getg() </span><br><span class="line"> </span><br><span class="line">top: </span><br><span class="line"> pp := _g_.m.p.ptr() </span><br><span class="line"> pp.preempt = <span class="literal">false</span> </span><br><span class="line"> </span><br><span class="line"> <span class="comment">// 处理调度时的计时器触发 </span></span><br><span class="line"> checkTimers(pp, <span class="number">0</span>) </span><br><span class="line"> ... </span><br><span class="line"> </span><br><span class="line"> execute(gp, inheritTime) </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>另外一种是当前处理器 P 没有可执行的 Timer，且没有可执行的 G。那么按照调度模型，就会去<code>窃取其他计时器</code>和 <code>G</code>：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// runtime/proc.go</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">findrunnable</span><span class="params">()</span> <span class="params">(gp *g, inheritTime <span class="keyword">bool</span>)</span></span> &#123; </span><br><span class="line"> _g_ := getg() </span><br><span class="line"> </span><br><span class="line">top: </span><br><span class="line"> _p_ := _g_.m.p.ptr() </span><br><span class="line"> ... </span><br><span class="line"> now, pollUntil, _ := checkTimers(_p_, <span class="number">0</span>) </span><br><span class="line"> ... </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;由于time.Sleep()会挂起我们的协程，我们来看一下它的底层原理。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Go源码剖析系列" scheme="http://blog.crazylaw.cn/categories/Go%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90%E7%B3%BB%E5%88%97/"/>
    
    
      <category term="Golang" scheme="http://blog.crazylaw.cn/tags/Golang/"/>
    
      <category term="Go源码剖析" scheme="http://blog.crazylaw.cn/tags/Go%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>【Golang】- go channel源码阅读</title>
    <link href="http://blog.crazylaw.cn/2022/03/04/Golang/go%20channel%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/"/>
    <id>http://blog.crazylaw.cn/2022/03/04/Golang/go%20channel%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/</id>
    <published>2022-03-03T16:43:51.000Z</published>
    <updated>2022-03-29T02:24:03.596Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>channel 是 Golang 中一个非常重要的特性，也是 <code>Golang CSP</code> 并发模型的一个重要体现。简单来说就是，goroutine 之间可以通过 channel 进行通信。</p><p>channel 在 Golang 如此重要，在代码中使用频率非常高，以至于不得不好奇其内部实现。本文将基于 <code>go 1.17</code> 的源码，分析 channel 的内部实现原理。</p><a id="more"></a><h2 id="channel-的基本使用"><a href="#channel-的基本使用" class="headerlink" title="channel 的基本使用"></a>channel 的基本使用</h2><p>在正式分析 channel 的实现之前，我们先看下 channel 的最基本用法，代码如下：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"><span class="keyword">import</span> <span class="string">"fmt"</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    c := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">int</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">        c &lt;- <span class="number">1</span> <span class="comment">// send to channel</span></span><br><span class="line">    &#125;()</span><br><span class="line"></span><br><span class="line">    x := &lt;-c <span class="comment">// recv from channel</span></span><br><span class="line"></span><br><span class="line">    fmt.Println(x)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在以上代码中，我们通过 <code>make(chan int)</code> 来创建了一个类型为 int 的 channel。<br>在一个 goroutine 中使用 <code>c &lt;- 1</code> 将数据发送到 channel 中。在主 goroutine 中通过 <code>x := &lt;- c</code> 从 channel 中读取数据并赋值给 x。</p><p>以上代码对应了 channel 的两种基本操作：</p><ul><li>send 操作 <code>c &lt;- 1</code> 表示发送数据到 channel</li><li>recv 操作 <code>x := &lt;- c</code> 表示从 channel 中接收数据。</li></ul><p>此外，channel 还分为<code>有缓存 channel</code> 和<code>无缓存 channel</code>。上述代码中，我们使用的是无缓冲的 channel。对于无缓冲的 channel，如果当前没有其他 goroutine 正在接收 channel 数据，则发送方会阻塞在发送语句处。</p><p>我们可以在 channel 初始化时指定缓冲区大小。例如，<code>make(chan int, 2)</code> 则指定缓冲区大小为 2。在缓冲区未满之前，发送方无阻塞地可以往 channel 发送数据，无需等待接收方准备好。而如果缓冲区已满，则发送方依然会阻塞。</p><h2 id="channel-对应的底层实现函数"><a href="#channel-对应的底层实现函数" class="headerlink" title="channel 对应的底层实现函数"></a>channel 对应的底层实现函数</h2><p>在探究 channel 源码之前，我们肯定首先需要先找到 channel 在 Golang 的具体实现在哪。因为我们在使用 channel 时，用的是 <code>&lt;- 符号</code>，并不能直接在 go 源码中找到其实现。但是 Golang 编译器必然会将 <code>&lt;-</code> 符号翻译成底层对应的实现。</p><p>我们可以使用 Go 自带的命令: <code>go tool compile -N -l -S hello.go</code>, 将代码翻译成对应的汇编指令。</p><p>或者，直接可以使用 <code>Compiler Explorer</code> 这个在线工具。对于上述示例代码可以直接在这个链接看其汇编结果: <a href="go.godbolt.org/z/3xw5Cj">go.godbolt.org/z/3xw5Cj</a>。如下图：</p><p><img src="/images/Go/%E6%BA%90%E7%A0%81/chansend1.png" alt="chansend1"></p><blockquote><p>chansend1</p></blockquote><p><img src="/images/Go/%E6%BA%90%E7%A0%81/chanrevc1.png" alt="chanrevc1"></p><blockquote><p>chanrevc1</p></blockquote><p>通过仔细查看以上示例代码对应的汇编指令，可以发现以下的对应关系：</p><p>channel 的构造语句 <code>make(chan int)</code>, 对应的是 <code>runtime.makechan</code> 函数<br>发送语句 <code>c &lt;- 1</code>, 对应的是 <code>runtime.chansend1</code> 函数<br>接收语句 <code>x := &lt;- c</code>, 对应的是 <code>runtime.chanrecv1</code> 函数<br>以上几个函数的实现都位于 go 源码中的 <code>runtime/chan.go</code> 代码文件中。我们接下来针对这几个函数，探究下 channel 的实现。</p><h2 id="channel-的构造"><a href="#channel-的构造" class="headerlink" title="channel 的构造"></a>channel 的构造</h2><p>channel 的构造语句 <code>make(chan int)</code>，将会被 golang 编译器翻译为 <code>runtime.makechan</code> 函数, 其函数签名如下：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">makechan</span><span class="params">(t *chantype, size <span class="keyword">int</span>)</span> *<span class="title">hchan</span></span></span><br></pre></td></tr></table></figure><p>其中，<code>t *chantype</code> 即构造 channel 时传入的元素类型。<code>size int</code> 即用户指定的 channel 缓冲区大小，不指定则为 0。该函数的返回值是 <code>*hchan</code>。hchan 则是 channel 在 golang 中的内部实现。其定义如下：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> hchan <span class="keyword">struct</span> &#123;</span><br><span class="line">qcount   <span class="keyword">uint</span>           <span class="comment">// buffer 中已放入的元素个数</span></span><br><span class="line">dataqsiz <span class="keyword">uint</span>           <span class="comment">// 用户构造 channel 时指定的 buf 大小</span></span><br><span class="line">buf      unsafe.Pointer <span class="comment">// buffer</span></span><br><span class="line">elemsize <span class="keyword">uint16</span>         <span class="comment">// buffer 中每个元素的大小</span></span><br><span class="line">closed   <span class="keyword">uint32</span>         <span class="comment">// channel 是否关闭，== 0 代表未 closed</span></span><br><span class="line">elemtype *_type         <span class="comment">// channel 元素的类型信息</span></span><br><span class="line">sendx    <span class="keyword">uint</span>           <span class="comment">// buffer 中已发送的索引位置 send index</span></span><br><span class="line">recvx    <span class="keyword">uint</span>           <span class="comment">// buffer 中已接收的索引位置 receive index</span></span><br><span class="line">recvq    waitq          <span class="comment">// 等待接收的 goroutine  list of recv waiters</span></span><br><span class="line">sendq    waitq          <span class="comment">// 等待发送的 goroutine list of send waiters</span></span><br><span class="line"></span><br><span class="line">lock mutex</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>hchan 中的所有属性大致可以分为三类：</p><ul><li>buffer 相关的属性。例如 <code>buf</code>、<code>dataqsiz</code>、<code>qcount</code> 等。 当 channel 的缓冲区大小不为 0 时，buffer 中存放了待接收的数据。使用 <code>ring buffer</code> 实现。</li><li>waitq 相关的属性，可以理解为是一个 FIFO 的标准队列。其中 <code>recvq</code> 中是正在等待接收数据的 goroutine，<code>sendq</code> 中是等待发送数据的 goroutine。waitq 使用<code>双向链表</code>实现。</li><li>其他属性，例如 lock、elemtype、closed。</li></ul><p>通过简单分析 hchan 的属性，我们可以知道其中有两个重要的组件，<code>buffer</code> 和 <code>waitq</code>。hchan 所有行为和实现都是围绕这两个组件进行的。</p><h2 id="向-channel-中发送数据"><a href="#向-channel-中发送数据" class="headerlink" title="向 channel 中发送数据"></a>向 channel 中发送数据</h2><p>channel 的发送和接收流程很相似，我们先分析下 channel 的发送过程 (如 <code>c &lt;- 1</code>), 对应于 <code>runtime.chansend</code> 函数的实现。</p><p>在尝试向 channel 中发送数据时，如果 <code>recvq</code> 队列不为空，则首先会从 <code>recvq</code> 中头部取出一个等待接收数据的 goroutine 出来。并将数据直接发送给该 goroutine。代码如下：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">lock(&amp;c.lock)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> c.closed != <span class="number">0</span> &#123;</span><br><span class="line">unlock(&amp;c.lock)</span><br><span class="line"><span class="built_in">panic</span>(plainError(<span class="string">"send on closed channel"</span>))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> sg := c.recvq.dequeue(); sg != <span class="literal">nil</span> &#123;</span><br><span class="line">send(c, sg, ep, <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123; unlock(&amp;c.lock) &#125;, <span class="number">3</span>)</span><br><span class="line"><span class="keyword">return</span> <span class="literal">true</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>我们看到当我们整个send的过程是需要加锁处理的，并且也可以看到我们老生常谈的一个问题，当向cloesd的channel数据的时候，会导致panic产生</p></blockquote><p>recvq 中是正在等待接收数据的 goroutine。当某个 goroutine 使用 recv 操作 (例如，<code>x := &lt;- c</code>)，如果此时 channel 的缓存中没有数据，且没有其他 goroutine 正在等待发送数据 (即 <code>sendq</code> 为空)，会将该 goroutine 以及要接收的数据地址打包成 <code>sudog</code> 对象，并放入到 recvq 中。</p><p>继续接着讲上面的代码，如果此时 <code>recvq</code> 不为空，则调用 <code>send 函数</code>将数据拷贝到对应的 goroutine 的堆栈上。</p><p>这个时候<code>不经过</code>我们的<code>环形缓存！！！</code></p><p>send 函数的实现主要包含两点：</p><ol><li><code>memmove(dst, src, t.size)</code> 进行数据的转移，本质上就是一个内存拷贝。</li><li><code>goready(gp, skip+1)</code> goready 的作用是唤醒对应的 goroutine。</li></ol><p>而如果 <code>recvq</code> 队列为空，则说明此时<code>没有等待接收</code>数据的 goroutine，那么此时 channel 会尝试把数据放到缓存中。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> c.qcount &lt; c.dataqsiz &#123;</span><br><span class="line"><span class="comment">// Space is available in the channel buffer. Enqueue the element to send.</span></span><br><span class="line">qp := chanbuf(c, c.sendx)</span><br><span class="line"><span class="keyword">if</span> raceenabled &#123;</span><br><span class="line">racenotify(c, c.sendx, <span class="literal">nil</span>)</span><br><span class="line">&#125;</span><br><span class="line">typedmemmove(c.elemtype, qp, ep)</span><br><span class="line">c.sendx++</span><br><span class="line"><span class="keyword">if</span> c.sendx == c.dataqsiz &#123;</span><br><span class="line">c.sendx = <span class="number">0</span></span><br><span class="line">&#125;</span><br><span class="line">c.qcount++</span><br><span class="line">unlock(&amp;c.lock)</span><br><span class="line"><span class="keyword">return</span> <span class="literal">true</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>以上代码的作用其实非常简单，就是把数据放到 buffer 中而已。此过程涉及了 <code>ring buffer</code> 的操作，其中 <code>dataqsiz</code> 代表用户指定的 channel 的 buffer 大小，如果不指定则默认为 0。</p><p>如果用户使用的是无缓冲 channel 或者此时 buffer 已满，则 <code>c.qcount &lt; c.dataqsiz</code> 条件不会满足, 以上流程也并不会执行到。此时会将当前的 goroutine 以及要发送的数据放入到 <code>sendq</code> 队列中，同时会切出该 goroutine</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Block on the channel. Some receiver will complete our operation for us.</span></span><br><span class="line">gp := getg()</span><br><span class="line">mysg := acquireSudog()</span><br><span class="line">mysg.releasetime = <span class="number">0</span></span><br><span class="line"><span class="keyword">if</span> t0 != <span class="number">0</span> &#123;</span><br><span class="line">mysg.releasetime = <span class="number">-1</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// No stack splits between assigning elem and enqueuing mysg</span></span><br><span class="line"><span class="comment">// on gp.waiting where copystack can find it.</span></span><br><span class="line">mysg.elem = ep</span><br><span class="line">mysg.waitlink = <span class="literal">nil</span></span><br><span class="line">mysg.g = gp</span><br><span class="line">mysg.isSelect = <span class="literal">false</span></span><br><span class="line">mysg.c = c</span><br><span class="line">gp.waiting = mysg</span><br><span class="line">gp.param = <span class="literal">nil</span></span><br><span class="line">c.sendq.enqueue(mysg)</span><br><span class="line"><span class="comment">// Signal to anyone trying to shrink our stack that we're about</span></span><br><span class="line"><span class="comment">// to park on a channel. The window between when this G's status</span></span><br><span class="line"><span class="comment">// changes and when we set gp.activeStackChans is not safe for</span></span><br><span class="line"><span class="comment">// stack shrinking.</span></span><br><span class="line">atomic.Store8(&amp;gp.parkingOnChan, <span class="number">1</span>)</span><br><span class="line"><span class="comment">// 将 goroutine 转入 waiting 状态</span></span><br><span class="line">gopark(chanparkcommit, unsafe.Pointer(&amp;c.lock), waitReasonChanSend, traceEvGoBlockSend, <span class="number">2</span>)</span><br><span class="line"><span class="comment">// Ensure the value being sent is kept alive until the</span></span><br><span class="line"><span class="comment">// receiver copies it out. The sudog has a pointer to the</span></span><br><span class="line"><span class="comment">// stack object, but sudogs aren't considered as roots of the</span></span><br><span class="line"><span class="comment">// stack tracer.</span></span><br><span class="line">KeepAlive(ep)</span><br><span class="line"><span class="comment">// 确保正在发送的值保持活动状态，直到接收者将其复制出来。sudog有一个指向堆栈对象的指针，但是sudog不被认为是堆栈跟踪程序的根。</span></span><br><span class="line"><span class="comment">// 总而言之：防止被GC</span></span><br></pre></td></tr></table></figure><p>调用 gopark 后，对于用户侧来看，该向 channel 发送数据的代码语句会进行阻塞。</p><p>以上过程就是 channel 的发送语句 (如，<code>c &lt;- 1</code>) 的内部工作流程，同时整个发送过程都使用 <code>c.lock</code> 进行加锁，保证并发安全。</p><p>简单来说，整个流程如下：</p><ol><li>检查 recvq 是否为空，如果不为空，则从 recvq 头部<code>取一个 goroutine</code>，将数据发送过去，并<code>唤醒对应的 goroutine</code> 即可</li><li>如果 recvq 为空，则将数据放入到 buffer 中</li><li>如果 buffer 已满，则将要发送的数据和当前 goroutine 打包成 <code>sudog</code> 对象放入到 <code>sendq</code> 中。并将当前 goroutine 置为 waiting 状态。</li></ol><p>从 channel 中接收数据的过程基本与发送过程类似，此处不再赘述了。</p><p>这里需要注意的是，channel 的<code>整个发送过程</code>和<code>接收过程</code>都使用 <code>runtime.mutex</code> 进行加锁。<code>runtime.mutex</code> 是 runtime 相关源码中常用到的一个<code>轻量级锁</code>。整个过程并不是最高效的 <code>lockfree</code> 的做法。</p><p>golang 在这里有个 <a href="https://github.com/golang/go/issues/8899" target="_blank" rel="noopener">issue:go/issues#8899</a>，给出了 <code>lockfree</code> 的 <code>channel</code> 的方案。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;channel 是 Golang 中一个非常重要的特性，也是 &lt;code&gt;Golang CSP&lt;/code&gt; 并发模型的一个重要体现。简单来说就是，goroutine 之间可以通过 channel 进行通信。&lt;/p&gt;
&lt;p&gt;channel 在 Golang 如此重要，在代码中使用频率非常高，以至于不得不好奇其内部实现。本文将基于 &lt;code&gt;go 1.17&lt;/code&gt; 的源码，分析 channel 的内部实现原理。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Go源码剖析系列" scheme="http://blog.crazylaw.cn/categories/Go%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90%E7%B3%BB%E5%88%97/"/>
    
    
      <category term="Golang" scheme="http://blog.crazylaw.cn/tags/Golang/"/>
    
      <category term="Go源码剖析" scheme="http://blog.crazylaw.cn/tags/Go%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>【Golang】- Sync包详解</title>
    <link href="http://blog.crazylaw.cn/2022/03/04/Golang/sync%E5%8C%85%E8%AF%A6%E8%A7%A3/"/>
    <id>http://blog.crazylaw.cn/2022/03/04/Golang/sync%E5%8C%85%E8%AF%A6%E8%A7%A3/</id>
    <published>2022-03-03T16:43:51.000Z</published>
    <updated>2022-04-15T07:40:12.764Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>我们直到sync包给我们提供了一系列并发安全的数据结构。之前有见过一次sync-map，但是这一次刚好复习整理一下sync包的知识点。</p><a id="more"></a><h2 id="Sync"><a href="#Sync" class="headerlink" title="Sync"></a>Sync</h2><ul><li>Sync.Map</li><li>Sync.Once</li><li>Sync.Pool</li><li>Sync.Cond</li><li>Sync.WaitGroup</li></ul><h2 id="sync-Map"><a href="#sync-Map" class="headerlink" title="sync.Map"></a>sync.Map</h2><ul><li>sync.Map主要针对于Map对于并发读写不支持的场景下提出实现的，其原理是通过对map的写操作进行加锁：Sync.RWMutex</li><li>同时sync.Map实现了读写分离，当对map进行读操作时，通过读read Map, 当read Map中不存在是去dirty map中读取</li></ul><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> Map <span class="keyword">struct</span> &#123;</span><br><span class="line">me Mutex</span><br><span class="line">read atomic.Value  <span class="comment">// readOnly,读数据</span></span><br><span class="line">dirty <span class="keyword">map</span>[<span class="keyword">interface</span>&#123;&#125;]*entry <span class="comment">// 包含最新的写入数据，当missed达到一定的值时，将值赋给read</span></span><br><span class="line">misses <span class="keyword">int</span>  <span class="comment">// 计数作用，每次从read中读失败，则missed加一</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// readOnly的数据结构</span></span><br><span class="line"><span class="keyword">type</span> readOnly <span class="keyword">struct</span>&#123;</span><br><span class="line">m <span class="keyword">map</span>[<span class="keyword">interface</span>&#123;&#125;]*entry</span><br><span class="line">amended <span class="keyword">bool</span>  <span class="comment">// Map.dirty中的数据和这里的m中的数据不同时值为true</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// entry的数据结构：</span></span><br><span class="line"><span class="keyword">type</span> entry <span class="keyword">struct</span> &#123;</span><br><span class="line">p unsafe.Pointer <span class="comment">// *interface&#123;&#125;</span></span><br><span class="line"><span class="comment">// 可见value是一个指针值，虽然read和dirty存在冗余情况，但由于是指针类型，存储空间不会太多</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>sync.Map相关问题</p><ul><li>sync.Map的核心实现：两个map,一个用于写，一个用于读，这样的设计思想可以类比于缓存与数据库</li><li>sync.Map的局限性：如果写远高于读，dirty -&gt; readOnly这个类似于刷新数据的频率较高，不如直接使用mutex + map的效率高</li><li>sync.Map的设计思想：保证高频率读的无锁结构，空间换时间的思想</li></ul><h2 id="sync-WaitGroup"><a href="#sync-WaitGroup" class="headerlink" title="sync.WaitGroup"></a>sync.WaitGroup</h2><ul><li>sync.WaitGroup常用于针对goroutine的并发执行，通过WaitGroup可以等待所有的go程序执行结束之后再执行之后的逻辑</li><li>WaitGroup对象内部有一个计数器，最初重0开始，提供了三个方法：Add(),Done(),Wait()用来控制计数器的数量。Add(n)把计数器设置为n,Done()每次把计数器减一，Wait()会阻塞代码的执行，直到计数器的值减到0为止。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;我们直到sync包给我们提供了一系列并发安全的数据结构。之前有见过一次sync-map，但是这一次刚好复习整理一下sync包的知识点。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Golang" scheme="http://blog.crazylaw.cn/categories/Golang/"/>
    
    
      <category term="Golang" scheme="http://blog.crazylaw.cn/tags/Golang/"/>
    
  </entry>
  
  <entry>
    <title>【大数据】- 在公司从0到1落地flink流计算任务</title>
    <link href="http://blog.crazylaw.cn/2022/02/15/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%9C%A8%E5%85%AC%E5%8F%B8%E4%BB%8E0%E5%88%B01%E8%90%BD%E5%9C%B0flink%E6%B5%81%E8%AE%A1%E7%AE%97%E4%BB%BB%E5%8A%A1/"/>
    <id>http://blog.crazylaw.cn/2022/02/15/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%9C%A8%E5%85%AC%E5%8F%B8%E4%BB%8E0%E5%88%B01%E8%90%BD%E5%9C%B0flink%E6%B5%81%E8%AE%A1%E7%AE%97%E4%BB%BB%E5%8A%A1/</id>
    <published>2022-02-15T03:10:40.000Z</published>
    <updated>2022-02-17T01:14:41.637Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在公司落地一套flink，总结到目前为止做了的事情。</p><a id="more"></a><h2 id="开发环境的部署"><a href="#开发环境的部署" class="headerlink" title="开发环境的部署"></a>开发环境的部署</h2><p>我们默认场景下，<code>flink</code>使用<code>hive-catalog</code>，所以<code>hive</code>安装在这里。</p><p>Hive使用<code>mysql</code>作为<code>外部数据存储</code>，所以这里使用<code>mysql</code></p><p>对于flink的开发，如果我想要一整套的本地的docker开发环境。</p><p>需要集成如下服务：</p><ul><li>hadoop</li><li>hive</li><li>flink</li><li>kafka</li><li>mysql</li></ul><p>所以做了一个<a href="https://github.com/whiteCcinn/flink-docker-compose" target="_blank" rel="noopener">flink-docker-compose</a></p><p>在该项目中，由于不是采用<code>CDH</code>来集成的，都是一个个源码包手动安装的。所以需要下载源码包。</p><p>目前的版本为：</p><ul><li>flink: 1.12.0_2.11</li><li>mysql: 5.6 （8.0-jdbc）</li><li>kafka: 2.12_2.11</li><li>maven: 3.6.3</li><li>jdk: 8/11 (默认jdk8)</li></ul><blockquote><p>本地环境的话，jdk需要自行处理好</p></blockquote><ul><li>hadoop: 3.1.1</li><li>hive: 3.1.0</li></ul><h3 id="一键下载源码包"><a href="#一键下载源码包" class="headerlink" title="一键下载源码包"></a>一键下载源码包</h3><p>为了方便方便大家下载，对应的镜像链接，也都集成在了<code>download.sh</code>中，如果需要利用<code>迅雷</code>等p2p加速下载软件，可以通过从中提取出来 <code>url</code> 进行下载。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./download.sh all</span><br></pre></td></tr></table></figure><h3 id="可设置的-env"><a href="#可设置的-env" class="headerlink" title="可设置的.env"></a>可设置的<code>.env</code></h3><p>利用<code>docker-compose</code>对 <code>.env</code>的支持，可以在当中设置<code>build image</code>的一些环境变量和参数</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Hadoop</span></span><br><span class="line">HADOOP_VERSION=3.1.1</span><br><span class="line"><span class="meta">#</span><span class="bash"> Hive</span></span><br><span class="line">HIVE_VERSION=3.1.0</span><br><span class="line"><span class="meta">#</span><span class="bash"> Scala</span></span><br><span class="line">SCALA_VERSION=2.11</span><br><span class="line"><span class="meta">#</span><span class="bash"> Flink</span></span><br><span class="line">FLINK_VERSION=1.12.0</span><br><span class="line"><span class="meta">#</span><span class="bash"> Kafka</span></span><br><span class="line">KAFKA_VERSION=2.4.0</span><br><span class="line"><span class="meta">#</span><span class="bash"> Zookeeper</span></span><br><span class="line">ZOOKEEPER_VERSION=3.5.6</span><br><span class="line"><span class="meta">#</span><span class="bash"> Mysql</span></span><br><span class="line">MYSQL_VERSION=5.6</span><br><span class="line">MYSQL_DATABASE=default</span><br><span class="line">MYSQL_PORT=3306</span><br><span class="line">MYSQL_ROOT_PASSWORD=lnhzjm/B4qrSc</span><br><span class="line">MYSQL_ENTRYPOINT_INITDB=./deploy/mysql/docker-entrypoint-initdb.d</span><br><span class="line">MYSQL_TIMEZONE=UTC</span><br></pre></td></tr></table></figure><h3 id="kafka的网络"><a href="#kafka的网络" class="headerlink" title="kafka的网络"></a>kafka的网络</h3><p>我们知道kafka的网络协议是<code>支持多端口</code>的，由于我们有时候flink是在本地，有时候是在容器中，所以我们希望我们的kafka集群，支持容器内的网络，也支持和我们物理机的网络。</p><p>这个时候，我们需要设置kafka的2套端口协议。所以你可以看到</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kafka1:</span></span><br><span class="line">   <span class="attr">build:</span></span><br><span class="line">     <span class="attr">context:</span> <span class="string">./deploy/kafka</span></span><br><span class="line">     <span class="attr">args:</span></span><br><span class="line">       <span class="attr">scala_version:</span> <span class="string">$&#123;SCALA_VERSION&#125;</span></span><br><span class="line">       <span class="attr">kafka_version:</span> <span class="string">$&#123;KAFKA_VERSION&#125;</span></span><br><span class="line">   <span class="attr">container_name:</span> <span class="string">flink-kafka1</span></span><br><span class="line">   <span class="attr">ports:</span></span><br><span class="line">     <span class="bullet">-</span> <span class="string">'19092:19092'</span></span><br><span class="line">   <span class="attr">environment:</span></span><br><span class="line">     <span class="attr">KAFKA_PORT:</span> <span class="number">19092</span></span><br><span class="line">     <span class="attr">KAFKA_ADVERTISED_LISTENERS:</span> <span class="string">PLAINTEXT://:9092,EXTERNAL_PLAINTEXT://kafka1:19092</span></span><br><span class="line">     <span class="attr">KAFKA_LISTENER_SECURITY_PROTOCOL_MAP:</span> <span class="string">PLAINTEXT:PLAINTEXT,EXTERNAL_PLAINTEXT:PLAINTEXT</span></span><br><span class="line">     <span class="attr">KAFKA_LISTENERS:</span> <span class="string">PLAINTEXT://:9092,EXTERNAL_PLAINTEXT://:19092</span></span><br><span class="line">     <span class="attr">KAFKA_ZOOKEEPER_CONNECT:</span> <span class="string">zookeeper:2181</span></span><br><span class="line">     <span class="attr">KAFKA_DEFAULT_REPLICATION_FACTOR:</span> <span class="number">3</span></span><br><span class="line">   <span class="attr">networks:</span></span><br><span class="line">     <span class="attr">flink-networks:</span></span><br><span class="line">       <span class="attr">ipv4_address:</span> <span class="number">192.168</span><span class="number">.6</span><span class="number">.211</span></span><br><span class="line">   <span class="attr">extra_hosts:</span></span><br><span class="line">     <span class="bullet">-</span> <span class="string">'zookeeper:192.168.6.215'</span></span><br><span class="line">     <span class="bullet">-</span> <span class="string">'kafka1:192.168.6.211'</span></span><br><span class="line">     <span class="bullet">-</span> <span class="string">'kafka2:192.168.6.212'</span></span><br><span class="line">     <span class="bullet">-</span> <span class="string">'kafka3:192.168.6.213'</span></span><br><span class="line">     <span class="bullet">-</span> <span class="string">'kafka4:192.168.6.214'</span></span><br><span class="line">   <span class="attr">depends_on:</span></span><br><span class="line">     <span class="bullet">-</span> <span class="string">zookeeper</span></span><br></pre></td></tr></table></figure><p>看到这里的</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">KAFKA_ADVERTISED_LISTENERS:</span> <span class="string">PLAINTEXT://:9092,EXTERNAL_PLAINTEXT://kafka1:19092</span></span><br><span class="line"><span class="attr">KAFKA_LISTENER_SECURITY_PROTOCOL_MAP:</span> <span class="string">PLAINTEXT:PLAINTEXT,EXTERNAL_PLAINTEXT:PLAINTEXT</span></span><br><span class="line"><span class="attr">KAFKA_LISTENERS:</span> <span class="string">PLAINTEXT://:9092,EXTERNAL_PLAINTEXT://:19092</span></span><br></pre></td></tr></table></figure><p>这个就是决定我们的<code>2套协议</code>的关键所在，分别是对<code>9092（容器内）</code>和<code>19092(和物理机)</code>端口的支持。</p><p>但是设置完了这个，由于一般kafka-client会从可本机的可访问的<code>dns服务器</code>上寻找<code>host映射</code>，在连接的时候必备的流程。</p><p>在本地连接的时候，会通过<code>kafka1/kafka2</code>等hostname返回到client，client需要在本机找到所有的ip映射，所以我们需要设置一下<code>etc/hosts</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo "127.0.0.1 kafka1 kafka2 kafka3 kafka4" &gt;&gt; /etc/hosts</span><br></pre></td></tr></table></figure><p>目前为止，我们所需要的环境变量已经处理完了。</p><h2 id="基于datastream-api的flink开发"><a href="#基于datastream-api的flink开发" class="headerlink" title="基于datastream-api的flink开发"></a>基于datastream-api的flink开发</h2><p>我们知道flink提供了3种API，分别是<code>datastream-api</code>,<code>table-api</code>,<code>sql-api</code></p><p><code>datastream</code>，也是flink的最原始的api，和flink集成一体，通过<code>datastream-api</code>，我们可以实现各种灵活的数据流处理。</p><p>按照我们以往对流计算数据的处理，在游戏公司中，一个游戏项目部署一个流计算的任务即为合理。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── README.md</span><br><span class="line">├── pom.xml</span><br><span class="line">└── src</span><br><span class="line">    └── main</span><br><span class="line">        ├── java</span><br><span class="line">        │   ├── deps</span><br><span class="line">        │   │   ├── oaYdSdk</span><br><span class="line">        │   │   │   ├── Youdu.java</span><br><span class="line">        │   │   │   └── test</span><br><span class="line">        │   │   │       └── YouduTest.java</span><br><span class="line">        │   │   └── util</span><br><span class="line">        │   │       ├── ParameterToolEnvironmentUtils.java</span><br><span class="line">        │   │       └── Util.java</span><br><span class="line">        │   └── org</span><br><span class="line">        │       └── cp</span><br><span class="line">        │           └── flink</span><br><span class="line">        │               ├── Bootstrap.java</span><br><span class="line">        │               ├── async</span><br><span class="line">        │               │   └── AsyncOaYdHttpClient.java</span><br><span class="line">        │               ├── events</span><br><span class="line">        │               │   ├── CommonEvent.java</span><br><span class="line">        │               │   ├── CommonEventHeader.java</span><br><span class="line">        │               │   ├── app_error</span><br><span class="line">        │               │   │   ├── Event.java</span><br><span class="line">        │               │   │   ├── EventHeader.java</span><br><span class="line">        │               │   │   └── EventLog.java</span><br><span class="line">        │               │   ├── log_ban</span><br><span class="line">        │               │   │   ├── Event.java</span><br><span class="line">        │               │   │   ├── EventHeader.java</span><br><span class="line">        │               │   │   └── EventLog.java</span><br><span class="line">        │               │   ├── log_client_loss</span><br><span class="line">        │               │   │   ├── Event.java</span><br><span class="line">        │               │   │   ├── EventHeader.java</span><br><span class="line">        │               │   │   └── EventLog.java</span><br><span class="line">        │               │   ├── log_consume_gold</span><br><span class="line">        │               │   │   ├── Event.java</span><br><span class="line">        │               │   │   ├── EventHeader.java</span><br><span class="line">        │               │   │   └── EventLog.java</span><br><span class="line">        │               │   ├── log_fcm_error</span><br><span class="line">        │               │   │   ├── Event.java</span><br><span class="line">        │               │   │   ├── EventHeader.java</span><br><span class="line">        │               │   │   └── EventLog.java</span><br><span class="line">        │               │   ├── log_index_record</span><br><span class="line">        │               │   │   ├── Event.java</span><br><span class="line">        │               │   │   ├── EventHeader.java</span><br><span class="line">        │               │   │   └── EventLog.java</span><br><span class="line">        │               │   ├── log_index_record_data</span><br><span class="line">        │               │   │   ├── Event.java</span><br><span class="line">        │               │   │   ├── EventHeader.java</span><br><span class="line">        │               │   │   └── EventLog.java</span><br><span class="line">        │               │   ├── log_role_create</span><br><span class="line">        │               │   │   ├── Event.java</span><br><span class="line">        │               │   │   ├── EventHeader.java</span><br><span class="line">        │               │   │   └── EventLog.java</span><br><span class="line">        │               │   └── t_log_market</span><br><span class="line">        │               │       ├── Event.java</span><br><span class="line">        │               │       ├── EventHeader.java</span><br><span class="line">        │               │       └── EventLog.java</span><br><span class="line">        │               ├── jobs</span><br><span class="line">        │               │   ├── alarm</span><br><span class="line">        │               │   │   ├── ErrorReport_10008.java</span><br><span class="line">        │               │   │   ├── Job_10002.java</span><br><span class="line">        │               │   │   ├── Job_10008.java</span><br><span class="line">        │               │   │   ├── Job_19.java</span><br><span class="line">        │               │   │   ├── README.md</span><br><span class="line">        │               │   │   └── handler</span><br><span class="line">        │               │   │       ├── AbstractHandler.java</span><br><span class="line">        │               │   │       ├── errorReport_10008</span><br><span class="line">        │               │   │       │   ├── Logic.java</span><br><span class="line">        │               │   │       │   ├── Logic_10012.java</span><br><span class="line">        │               │   │       │   ├── Logic_19.java</span><br><span class="line">        │               │   │       │   └── Logic_20.java</span><br><span class="line">        │               │   │       ├── job_10002</span><br><span class="line">        │               │   │       │   ├── LogIndexRecordDataHandler.java</span><br><span class="line">        │               │   │       │   ├── LogIndexRecordHandler.java</span><br><span class="line">        │               │   │       │   └── model</span><br><span class="line">        │               │   │       │       ├── log_index_record</span><br><span class="line">        │               │   │       │       │   └── StatisticsMcfx2Model.java</span><br><span class="line">        │               │   │       │       └── log_index_record_data</span><br><span class="line">        │               │   │       │           └── StatisticsMcfx1Model.java</span><br><span class="line">        │               │   │       ├── job_10008</span><br><span class="line">        │               │   │       │   ├── AppErrorHandler.java</span><br><span class="line">        │               │   │       │   ├── LogFcmErrorHandler.java</span><br><span class="line">        │               │   │       │   └── model</span><br><span class="line">        │               │   │       │       ├── app_error</span><br><span class="line">        │               │   │       │       │   └── StatisticsAppErrorModel.java</span><br><span class="line">        │               │   │       │       └── log_fcm_error</span><br><span class="line">        │               │   │       │           └── StatisticsFcmErrorModel.java</span><br><span class="line">        │               │   │       └── job_19</span><br><span class="line">        │               │   │           ├── LogBanHandler.java</span><br><span class="line">        │               │   │           ├── LogClientLossHandler.java</span><br><span class="line">        │               │   │           ├── LogConsumeGoldHandler.java</span><br><span class="line">        │               │   │           ├── LogRoleCreateHandler.java</span><br><span class="line">        │               │   │           ├── TLogMarketHandler.java</span><br><span class="line">        │               │   │           └── model</span><br><span class="line">        │               │   │               ├── log_ban</span><br><span class="line">        │               │   │               │   └── StatisticsModel.java</span><br><span class="line">        │               │   │               ├── log_client_loss</span><br><span class="line">        │               │   │               │   └── IpMonitorModel.java</span><br><span class="line">        │               │   │               ├── log_consume_gold</span><br><span class="line">        │               │   │               │   ├── StatisticsBindGoldModel.java</span><br><span class="line">        │               │   │               │   └── StatisticsUnBindGoldModel.java</span><br><span class="line">        │               │   │               ├── log_role_create</span><br><span class="line">        │               │   │               │   └── SingleServerRoleCreateModel.java</span><br><span class="line">        │               │   │               └── t_log_market</span><br><span class="line">        │               │   │                   ├── MarketTransactionLogByBuyerModel.java</span><br><span class="line">        │               │   │                   └── MarketTransactionLogBySellerModel.java</span><br><span class="line">        │               │   └── stream</span><br><span class="line">        │               │       └── README.md</span><br><span class="line">        │               ├── mock</span><br><span class="line">        │               │   ├── MockAppError.java</span><br><span class="line">        │               │   ├── MockLogFcmError.java</span><br><span class="line">        │               │   └── README.md</span><br><span class="line">        │               ├── serializer</span><br><span class="line">        │               │   ├── AbstractSerializer.java</span><br><span class="line">        │               │   └── log_role_create</span><br><span class="line">        │               │       └── LogRoleCreateDeSerializer.java</span><br><span class="line">        │               └── sinks</span><br><span class="line">        │                   ├── AsyncOaYdSdkHttpSink.java</span><br><span class="line">        │                   ├── MysqlItem.java</span><br><span class="line">        │                   └── MysqlSink.java</span><br><span class="line">        └── resources</span><br><span class="line">            ├── application-dev.properties</span><br><span class="line">            ├── application-local.properties</span><br><span class="line">            ├── application-pro.properties</span><br><span class="line">            ├── application.properties</span><br><span class="line">            ├── jobs</span><br><span class="line">            │   ├── org.cp.flink.jobs.alarm.ErrorReport_10008</span><br><span class="line">            │   │   ├── application-dev.properties</span><br><span class="line">            │   │   ├── application-local.properties</span><br><span class="line">            │   │   ├── application-pro.properties</span><br><span class="line">            │   │   └── application.properties</span><br><span class="line">            │   ├── org.cp.flink.jobs.alarm.Job_10002</span><br><span class="line">            │   │   ├── application-dev.properties</span><br><span class="line">            │   │   ├── application-local.properties</span><br><span class="line">            │   │   ├── application-pro.properties</span><br><span class="line">            │   │   └── application.properties</span><br><span class="line">            │   ├── org.cp.flink.jobs.alarm.Job_10008</span><br><span class="line">            │   │   ├── application-dev.properties</span><br><span class="line">            │   │   ├── application-local.properties</span><br><span class="line">            │   │   ├── application-pro.properties</span><br><span class="line">            │   │   └── application.properties</span><br><span class="line">            │   └── org.cp.flink.jobs.alarm.Job_19</span><br><span class="line">            │       ├── application-dev.properties</span><br><span class="line">            │       ├── application-local.properties</span><br><span class="line">            │       ├── application-pro.properties</span><br><span class="line">            │       └── application.properties</span><br><span class="line">            └── log4j2.properties</span><br></pre></td></tr></table></figure><p>这是我们早期的一个<code>代码层级结构</code>，所有的流计算任务基于一个flink项目下，<code>resources</code>下的配置根据当前需要提交的项目和环境来进行区分加载具体的配置，可以做到支持<code>多环境</code>,<code>多项目</code>下配置灵活配置。</p><p>我们看到 <code>org.cp.flink</code>目下，就是我们的所有flink代码。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">➜  flinkjob git:(master) ✗ tree -d src/main/java/org</span><br><span class="line">src/main/java/org</span><br><span class="line">└── cp</span><br><span class="line">    └── flink</span><br><span class="line">        ├── async</span><br><span class="line">        ├── events</span><br><span class="line">        │   ├── app_error</span><br><span class="line">        │   ├── log_ban</span><br><span class="line">        │   ├── log_client_loss</span><br><span class="line">        │   ├── log_consume_gold</span><br><span class="line">        │   ├── log_fcm_error</span><br><span class="line">        │   ├── log_index_record</span><br><span class="line">        │   ├── log_index_record_data</span><br><span class="line">        │   ├── log_role_create</span><br><span class="line">        │   └── t_log_market</span><br><span class="line">        ├── jobs</span><br><span class="line">        │   ├── alarm</span><br><span class="line">        │   │   └── handler</span><br><span class="line">        │   │       ├── job_10002</span><br><span class="line">        │   │       │   └── model</span><br><span class="line">        │   │       │       ├── log_index_record</span><br><span class="line">        │   │       │       └── log_index_record_data</span><br><span class="line">        │   │       ├── job_10008</span><br><span class="line">        │   │       │   └── model</span><br><span class="line">        │   │       │       ├── app_error</span><br><span class="line">        │   │       │       └── log_fcm_error</span><br><span class="line">        │   │       └── job_19</span><br><span class="line">        │   │           └── model</span><br><span class="line">        │   │               ├── log_ban</span><br><span class="line">        │   │               ├── log_client_loss</span><br><span class="line">        │   │               ├── log_consume_gold</span><br><span class="line">        │   │               ├── log_role_create</span><br><span class="line">        │   │               └── t_log_market</span><br><span class="line">        │   └── stream</span><br><span class="line">        ├── mock</span><br><span class="line">        ├── serializer</span><br><span class="line">        │   └── log_role_create</span><br><span class="line">        └── sinks</span><br></pre></td></tr></table></figure><p>我们先看到，<code>jobs</code>目录下的，分为了2种类型，我们平时用的流计算任务可以分为2种，一种是常规的<code>告警属性</code>，另一种是<code>产品属性(类似BI系统需要的实时数据)</code>。</p><p>我们看到<code>alarm/handler/job_xxx</code>就是我们具体的项目。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">src/main/java/org/cp/flink/jobs/alarm/</span></span><br><span class="line"><span class="string">├──</span> <span class="string">Job_10002.java</span></span><br><span class="line"><span class="string">├──</span> <span class="string">Job_10008.java</span></span><br><span class="line"><span class="string">├──</span> <span class="string">Job_19.java</span></span><br><span class="line"><span class="string">├──</span> <span class="string">README.md</span></span><br><span class="line"><span class="string">└──</span> <span class="string">handler</span></span><br><span class="line">    <span class="string">├──</span> <span class="string">AbstractHandler.java</span></span><br><span class="line">    <span class="string">├──</span> <span class="string">errorReport_10008</span></span><br><span class="line">    <span class="string">│</span>   <span class="string">├──</span> <span class="string">Logic.java</span></span><br><span class="line">    <span class="string">│</span>   <span class="string">├──</span> <span class="string">Logic_10012.java</span></span><br><span class="line">    <span class="string">│</span>   <span class="string">├──</span> <span class="string">Logic_19.java</span></span><br><span class="line">    <span class="string">│</span>   <span class="string">└──</span> <span class="string">Logic_20.java</span></span><br><span class="line">    <span class="string">├──</span> <span class="string">job_10002</span></span><br><span class="line">    <span class="string">│</span>   <span class="string">├──</span> <span class="string">LogIndexRecordDataHandler.java</span></span><br><span class="line">    <span class="string">│</span>   <span class="string">├──</span> <span class="string">LogIndexRecordHandler.java</span></span><br><span class="line">    <span class="string">│</span>   <span class="string">└──</span> <span class="string">model</span></span><br><span class="line">    <span class="string">│</span>       <span class="string">├──</span> <span class="string">log_index_record</span></span><br><span class="line">    <span class="string">│</span>       <span class="string">│</span>   <span class="string">└──</span> <span class="string">StatisticsMcfx2Model.java</span></span><br><span class="line">    <span class="string">│</span>       <span class="string">└──</span> <span class="string">log_index_record_data</span></span><br><span class="line">    <span class="string">│</span>           <span class="string">└──</span> <span class="string">StatisticsMcfx1Model.java</span></span><br><span class="line">    <span class="string">├──</span> <span class="string">job_10008</span></span><br><span class="line">    <span class="string">│</span>   <span class="string">├──</span> <span class="string">AppErrorHandler.java</span></span><br><span class="line">    <span class="string">│</span>   <span class="string">├──</span> <span class="string">LogFcmErrorHandler.java</span></span><br><span class="line">    <span class="string">│</span>   <span class="string">└──</span> <span class="string">model</span></span><br><span class="line">    <span class="string">│</span>       <span class="string">├──</span> <span class="string">app_error</span></span><br><span class="line">    <span class="string">│</span>       <span class="string">│</span>   <span class="string">└──</span> <span class="string">StatisticsAppErrorModel.java</span></span><br><span class="line">    <span class="string">│</span>       <span class="string">└──</span> <span class="string">log_fcm_error</span></span><br><span class="line">    <span class="string">│</span>           <span class="string">└──</span> <span class="string">StatisticsFcmErrorModel.java</span></span><br><span class="line">    <span class="string">└──</span> <span class="string">job_19</span></span><br><span class="line">        <span class="string">├──</span> <span class="string">LogBanHandler.java</span></span><br><span class="line">        <span class="string">├──</span> <span class="string">LogClientLossHandler.java</span></span><br><span class="line">        <span class="string">├──</span> <span class="string">LogConsumeGoldHandler.java</span></span><br><span class="line">        <span class="string">├──</span> <span class="string">LogRoleCreateHandler.java</span></span><br><span class="line">        <span class="string">├──</span> <span class="string">TLogMarketHandler.java</span></span><br><span class="line">        <span class="string">└──</span> <span class="string">model</span></span><br><span class="line">            <span class="string">├──</span> <span class="string">log_ban</span></span><br><span class="line">            <span class="string">│</span>   <span class="string">└──</span> <span class="string">StatisticsModel.java</span></span><br><span class="line">            <span class="string">├──</span> <span class="string">log_client_loss</span></span><br><span class="line">            <span class="string">│</span>   <span class="string">└──</span> <span class="string">IpMonitorModel.java</span></span><br><span class="line">            <span class="string">├──</span> <span class="string">log_consume_gold</span></span><br><span class="line">            <span class="string">│</span>   <span class="string">├──</span> <span class="string">StatisticsBindGoldModel.java</span></span><br><span class="line">            <span class="string">│</span>   <span class="string">└──</span> <span class="string">StatisticsUnBindGoldModel.java</span></span><br><span class="line">            <span class="string">├──</span> <span class="string">log_role_create</span></span><br><span class="line">            <span class="string">│</span>   <span class="string">└──</span> <span class="string">SingleServerRoleCreateModel.java</span></span><br><span class="line">            <span class="string">└──</span> <span class="string">t_log_market</span></span><br><span class="line">                <span class="string">├──</span> <span class="string">MarketTransactionLogByBuyerModel.java</span></span><br><span class="line">                <span class="string">└──</span> <span class="string">MarketTransactionLogBySellerModel.java</span></span><br></pre></td></tr></table></figure><p>对于各个项目的<code>错误告警监控</code>，这里分为了多个<code>job</code>。</p><ul><li>Job_10002.java</li><li>Job_10008.java</li><li>Job_19.java</li></ul><p>我们从入口开始看</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> org.cp.flink.jobs.alarm;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.alibaba.fastjson.JSONObject;</span><br><span class="line"><span class="keyword">import</span> deps.util.Util;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.SimpleStringSchema;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.utils.ParameterTool;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.ProcessFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.Collector;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.OutputTag;</span><br><span class="line"><span class="keyword">import</span> org.cp.flink.Bootstrap;</span><br><span class="line"><span class="keyword">import</span> org.cp.flink.jobs.alarm.handler.job_10008.AppErrorHandler;</span><br><span class="line"><span class="keyword">import</span> org.cp.flink.jobs.alarm.handler.job_10008.LogFcmErrorHandler;</span><br><span class="line"><span class="keyword">import</span> org.cp.flink.events.CommonEvent;</span><br><span class="line"><span class="keyword">import</span> org.cp.flink.events.app_error.Event;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.Logger;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.LoggerFactory;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Job_10008</span> <span class="keyword">extends</span> <span class="title">Bootstrap</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger logger = LoggerFactory.getLogger(Job_10008<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> StreamExecutionEnvironment env = getStreamExecutionEnvironment(args, Job_10008<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        env.enableCheckpointing(<span class="number">5000</span>); <span class="comment">// checkpoint every 5000 msecs</span></span><br><span class="line"></span><br><span class="line">        ParameterTool parameterTool = (ParameterTool) env.getConfig().getGlobalJobParameters();</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.setProperty(<span class="string">"bootstrap.servers"</span>, parameterTool.get(<span class="string">"kafka.source.bootstrap.servers"</span>));</span><br><span class="line">        props.setProperty(<span class="string">"group.id"</span>, parameterTool.get(<span class="string">"kafka.source.group"</span>));</span><br><span class="line">        props.put(<span class="string">"enable.auto.commit"</span>, parameterTool.get(<span class="string">"kafka.source.enable.auto.commit"</span>));</span><br><span class="line">        props.put(<span class="string">"auto.commit.interval.ms"</span>, parameterTool.get(<span class="string">"kafka.source.auto.commit.interval.ms"</span>));</span><br><span class="line">        props.put(<span class="string">"session.timeout.ms"</span>, parameterTool.get(<span class="string">"kafka.source.session.timeout.ms"</span>));</span><br><span class="line">        props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置kafka并行度</span></span><br><span class="line">        env.setParallelism(parameterTool.getInt(<span class="string">"kafka.source.parallelism"</span>, <span class="number">1</span>));</span><br><span class="line"></span><br><span class="line">        DataStream&lt;String&gt; stream = env</span><br><span class="line">                .addSource(<span class="keyword">new</span> FlinkKafkaConsumer&lt;&gt;(Arrays.asList(parameterTool.get(<span class="string">"kafka.source.topic"</span>).split(<span class="string">","</span>)), <span class="keyword">new</span> SimpleStringSchema(), props));</span><br><span class="line"></span><br><span class="line">        env.setParallelism(parameterTool.getInt(<span class="string">"app.parallelism"</span>, <span class="number">1</span>));</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;CommonEvent&gt; s0 = stream.filter((String json) -&gt; &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                JSONObject.parseObject(json, CommonEvent<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">            &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">                logger.error(json);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        &#125;).map(</span><br><span class="line">                (String json) -&gt; JSONObject.parseObject(json, CommonEvent<span class="class">.<span class="keyword">class</span>).<span class="title">setOriginJson</span>(<span class="title">json</span>)</span></span><br><span class="line"><span class="class">        ).<span class="title">returns</span>(<span class="title">CommonEvent</span>.<span class="title">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> OutputTag&lt;CommonEvent&gt; outputTagAppError = <span class="keyword">new</span> OutputTag&lt;CommonEvent&gt;(AppErrorHandler<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>()) </span>&#123;</span><br><span class="line">        &#125;;</span><br><span class="line">        <span class="keyword">final</span> OutputTag&lt;CommonEvent&gt; outputTagLogFcmError = <span class="keyword">new</span> OutputTag&lt;CommonEvent&gt;(LogFcmErrorHandler<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>()) </span>&#123;</span><br><span class="line">        &#125;;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1. 主流不需要了, 所以不需要调用collector.collect()</span></span><br><span class="line">        <span class="comment">// 2. 只要旁路输出流，因为要区分数据进行处理</span></span><br><span class="line">        <span class="comment">// 利用low-level-api的process算子处理旁路输出采集数据</span></span><br><span class="line">        SingleOutputStreamOperator&lt;CommonEvent&gt; s1 = s0.process(<span class="keyword">new</span> ProcessFunction&lt;CommonEvent, CommonEvent&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(CommonEvent event, Context context, Collector&lt;CommonEvent&gt; collector)</span> </span>&#123;</span><br><span class="line">                <span class="keyword">switch</span> (event.getHeaders().getLogName()) &#123;</span><br><span class="line">                    <span class="keyword">case</span> <span class="string">"app_error"</span>:</span><br><span class="line">                        context.output(outputTagAppError, event);</span><br><span class="line">                        <span class="keyword">break</span>;</span><br><span class="line">                    <span class="keyword">case</span> <span class="string">"log_fcm_error"</span>:</span><br><span class="line">                        context.output(outputTagLogFcmError, event);</span><br><span class="line">                        <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        DataStream&lt;CommonEvent&gt; AppErrorSource = s1.getSideOutput(outputTagAppError);</span><br><span class="line">        DataStream&lt;CommonEvent&gt; LogFcmErrorSource = s1.getSideOutput(outputTagLogFcmError);</span><br><span class="line"></span><br><span class="line">        DataStream&lt;Event&gt; AppErrorSource_s0 = AppErrorSource.map((CommonEvent event) -&gt; JSONObject.parseObject(event.getOriginJson(), Event<span class="class">.<span class="keyword">class</span>)</span></span><br><span class="line"><span class="class">        ).<span class="title">returns</span>(<span class="title">Event</span>.<span class="title">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        DataStream&lt;org.cp.flink.events.log_fcm_error.Event&gt; LogFcmErrorSource_s0 = LogFcmErrorSource.map((CommonEvent event) -&gt; JSONObject.parseObject(event.getOriginJson(), org.cp.flink.events.log_fcm_error.Event<span class="class">.<span class="keyword">class</span>)</span></span><br><span class="line"><span class="class">        ).<span class="title">returns</span>(<span class="title">org</span>.<span class="title">cp</span>.<span class="title">flink</span>.<span class="title">events</span>.<span class="title">log_fcm_error</span>.<span class="title">Event</span>.<span class="title">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        AppErrorHandler.build().handle(AppErrorSource_s0);</span><br><span class="line">        LogFcmErrorHandler.build().handle(LogFcmErrorSource_s0);</span><br><span class="line"></span><br><span class="line">        env.execute(Util.getCurrentJobName(((ParameterTool) env.getConfig().getGlobalJobParameters())));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>由于我们一个topic只能够可能存在多种数据，所以这里利用了<code>旁路由</code>进行了分流。把数据流分发到不同的<code>子流</code>中，我们再把<code>子流</code>传递不同的<code>Handler</code>进行处理。</p><p>这里例如: <code>AppErrorHandler</code>。我们以此为例子进行说明。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> org.cp.flink.jobs.alarm.handler.job_10008;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> lombok.NoArgsConstructor;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.utils.ParameterTool;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.ProcessFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.Collector;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.OutputTag;</span><br><span class="line"><span class="keyword">import</span> org.cp.flink.jobs.alarm.handler.AbstractHandler;</span><br><span class="line"><span class="keyword">import</span> org.cp.flink.jobs.alarm.handler.job_10008.model.app_error.StatisticsAppErrorModel;</span><br><span class="line"><span class="keyword">import</span> org.cp.flink.events.app_error.Event;</span><br><span class="line"></span><br><span class="line"><span class="meta">@NoArgsConstructor</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">AppErrorHandler</span> <span class="keyword">extends</span> <span class="title">AbstractHandler</span>&lt;<span class="title">Event</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> AppErrorHandler instance;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> AppErrorHandler <span class="title">build</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (instance == <span class="keyword">null</span>) &#123;</span><br><span class="line">            instance = <span class="keyword">new</span> AppErrorHandler();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> instance;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">handle</span><span class="params">(DataStream&lt;Event&gt; s0)</span> </span>&#123;</span><br><span class="line">        ParameterTool parameterTool = <span class="keyword">this</span>.getParameterTool(s0);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 利用旁路输出多流到对应到model</span></span><br><span class="line">        <span class="comment">// StatisticsAppErrorModel</span></span><br><span class="line">        <span class="keyword">final</span> OutputTag&lt;Event&gt; outputTagStatisticsAppError = <span class="keyword">new</span> OutputTag&lt;Event&gt;(StatisticsAppErrorModel<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>()) </span>&#123;</span><br><span class="line">        &#125;;</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;Event&gt; s1 = s0.process(<span class="keyword">new</span> ProcessFunction&lt;Event, Event&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processElement</span><span class="params">(Event event, Context context, Collector&lt;Event&gt; collector)</span> </span>&#123;</span><br><span class="line">                context.output(outputTagStatisticsAppError, event);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        DataStream&lt;Event&gt; sideOutputStreamAppError = s1.getSideOutput(outputTagStatisticsAppError);</span><br><span class="line"></span><br><span class="line">        StatisticsAppErrorModel.build().handle(sideOutputStreamAppError);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (parameterTool.getBoolean(<span class="string">"app.handler.print.console"</span>, <span class="keyword">false</span>)) &#123;</span><br><span class="line">            s0.print(AppErrorHandler<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>由于，我们希望到一条数据从<code>kafka</code>被<code>pull</code>下来到时候，可以用于多个不同的<code>流计算模型model</code>，所以我们在这里需要<code>copy</code>到多个<code>旁路输出</code>，但是这里我们只有一个<code>stream-model</code>，所以我们就只用一个来处理即可，从旁路输出拿到<code>datastream</code>之后，在对应的模型中进行<code>核心逻辑</code>处理。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> org.cp.flink.jobs.alarm.handler.job_10008.model.app_error;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> deps.util.Util;</span><br><span class="line"><span class="keyword">import</span> lombok.NoArgsConstructor;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.functions.KeySelector;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple3;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.tuple.Tuple5;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.utils.ParameterTool;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.WindowedStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.timestamps.BoundedOutOfOrdernessTimestampExtractor;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.windowing.WindowFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.time.Time;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.windowing.windows.TimeWindow;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.runtime.operators.util.AssignerWithPeriodicWatermarksAdapter;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.Collector;</span><br><span class="line"><span class="keyword">import</span> org.cp.flink.jobs.alarm.handler.AbstractHandler;</span><br><span class="line"><span class="keyword">import</span> org.cp.flink.events.app_error.Event;</span><br><span class="line"><span class="keyword">import</span> org.cp.flink.jobs.alarm.handler.job_19.model.log_ban.StatisticsModel;</span><br><span class="line"><span class="keyword">import</span> org.cp.flink.sinks.MysqlItem;</span><br><span class="line"><span class="keyword">import</span> org.cp.flink.sinks.MysqlSink;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.Logger;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.LoggerFactory;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.time.Duration;</span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.TimeUnit;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 错误日志统计</span></span><br><span class="line"><span class="comment"> * 窗口：滚动事件窗口，每1分钟统计一次</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@NoArgsConstructor</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StatisticsAppErrorModel</span> <span class="keyword">extends</span> <span class="title">AbstractHandler</span>&lt;<span class="title">Event</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String DEFAULT_SINK_DATABASE = <span class="string">"db_app_log_alarm"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String DEFAULT_SINK_TABLE = <span class="string">"t_log_app_error_alarm_164"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger logger = LoggerFactory.getLogger(StatisticsAppErrorModel<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> StatisticsAppErrorModel instance;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> StatisticsAppErrorModel <span class="title">build</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (instance == <span class="keyword">null</span>) &#123;</span><br><span class="line">            instance = <span class="keyword">new</span> StatisticsAppErrorModel();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> instance;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">handle</span><span class="params">(DataStream&lt;Event&gt; s0)</span> </span>&#123;</span><br><span class="line">        s0.getExecutionConfig().setAutoWatermarkInterval(<span class="number">5000L</span>);</span><br><span class="line"></span><br><span class="line">        logger.debug(<span class="string">"getAutoWatermarkInterval: &#123;&#125;"</span>, s0.getExecutionConfig().getAutoWatermarkInterval());</span><br><span class="line">        ParameterTool parameterTool = <span class="keyword">this</span>.getParameterTool(s0);</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;Event&gt; s1 = s0.assignTimestampsAndWatermarks(<span class="keyword">new</span> AssignerWithPeriodicWatermarksAdapter.Strategy&lt;&gt;(</span><br><span class="line">                        <span class="keyword">new</span> BoundedOutOfOrdernessTimestampExtractor&lt;Event&gt;(Time.of(<span class="number">1</span>, TimeUnit.SECONDS)) &#123;</span><br><span class="line"></span><br><span class="line">                            <span class="meta">@Override</span></span><br><span class="line">                            <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Event event)</span> </span>&#123;</span><br><span class="line">                                Long ts = event.getLogs().getMtime() * <span class="number">1000L</span>;</span><br><span class="line">                                logger.debug(</span><br><span class="line">                                        <span class="string">"thread-id: &#123;&#125;, eventTime: [&#123;&#125;|&#123;&#125;], watermark: [&#123;&#125;|&#123;&#125;]"</span>,</span><br><span class="line">                                        Thread.currentThread().getId(),</span><br><span class="line">                                        ts,</span><br><span class="line">                                        sdf.format(ts),</span><br><span class="line">                                        <span class="keyword">this</span>.getCurrentWatermark().getTimestamp(),</span><br><span class="line">                                        sdf.format(<span class="keyword">this</span>.getCurrentWatermark().getTimestamp())</span><br><span class="line">                                );</span><br><span class="line"></span><br><span class="line">                                <span class="keyword">return</span> ts;</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                )</span><br><span class="line">                        <span class="comment">// 尽可能和窗口大小保持一致，所以如果其中一个并行度出现问题的情况下</span></span><br><span class="line">                        <span class="comment">// 最大的延迟计算结果是一个窗口大小的时间</span></span><br><span class="line">                        .withIdleness(Duration.ofMinutes(<span class="number">1L</span>))</span><br><span class="line">        );</span><br><span class="line"></span><br><span class="line">        WindowedStream&lt;Event, Tuple5&lt;Integer, String, String, Integer, String&gt;, TimeWindow&gt; s2 = s1.keyBy(<span class="keyword">new</span> KeySelector&lt;Event, Tuple5&lt;Integer, String, String, Integer, String&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Tuple5&lt;Integer, String, String, Integer, String&gt; <span class="title">getKey</span><span class="params">(Event event)</span> </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> Tuple5.of(</span><br><span class="line">                        event.getLogs().getRelatedAppId(),</span><br><span class="line">                        event.getLogs().getChildApp(),</span><br><span class="line">                        event.getLogs().getSummary(),</span><br><span class="line">                        event.getLogs().getLevel(),</span><br><span class="line">                        event.getLogs().getIp()</span><br><span class="line">                );</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">                .window(TumblingEventTimeWindows.of(Time.minutes(<span class="number">1L</span>)));</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;Tuple3&lt;Tuple5&lt;Integer, String, String, Integer, String&gt;, Event, Integer&gt;&gt; s3 = s2.apply(<span class="keyword">new</span> WindowFunction&lt;Event, Tuple3&lt;Tuple5&lt;Integer, String, String, Integer, String&gt;, Event, Integer&gt;, Tuple5&lt;Integer, String, String, Integer, String&gt;, TimeWindow&gt;() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">apply</span><span class="params">(Tuple5&lt;Integer, String, String, Integer, String&gt; key, TimeWindow timeWindow, Iterable&lt;Event&gt; iterable, Collector&lt;Tuple3&lt;Tuple5&lt;Integer, String, String, Integer, String&gt;, Event, Integer&gt;&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">                <span class="keyword">for</span> (Event event : iterable) &#123;</span><br><span class="line">                    sum++;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                logger.debug(<span class="string">"聚合窗口key: &#123;&#125;, 窗口中的数量:&#123;&#125;, 此时的窗口范围是[&#123;&#125;,&#123;&#125;)"</span>, key, sum, sdf.format(timeWindow.getStart()), sdf.format(timeWindow.getEnd()));</span><br><span class="line">                collector.collect(Tuple3.of(key, iterable.iterator().next(), sum));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        String sinkDatabase = parameterTool.get(StatisticsModel.class.getName() + ".sink_database", DEFAULT_SINK_DATABASE);</span><br><span class="line">        String sinkTable = parameterTool.get(StatisticsModel.class.getName() + ".sink_table", DEFAULT_SINK_TABLE);</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;MysqlItem&gt; s4 = s3.map(e -&gt; &#123;</span><br><span class="line">                    HashMap&lt;String, Object&gt; kv = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">                    kv.put(<span class="string">"related_app_id"</span>, e.f1.getLogs().getRelatedAppId());</span><br><span class="line">                    kv.put(<span class="string">"child_app"</span>, e.f1.getLogs().getChildApp());</span><br><span class="line">                    kv.put(<span class="string">"summary"</span>, e.f1.getLogs().getSummary());</span><br><span class="line">                    kv.put(<span class="string">"level"</span>, e.f1.getLogs().getLevel());</span><br><span class="line">                    kv.put(<span class="string">"ip"</span>, e.f1.getLogs().getIp());</span><br><span class="line"></span><br><span class="line">                    kv.put(<span class="string">"mtime"</span>, e.f1.getLogs().getMtime());</span><br><span class="line">                    kv.put(<span class="string">"mdate"</span>, Util.timeStamp2Date(Integer.toString(e.f1.getLogs().getMtime()), <span class="string">"yyyy-MM-dd"</span>));</span><br><span class="line"></span><br><span class="line">                    <span class="comment">// 来自聚合窗口统计的结果</span></span><br><span class="line">                    kv.put(<span class="string">"cnt"</span>, e.f2);</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">return</span> MysqlItem.builder()</span><br><span class="line">                            .database(sinkDatabase)</span><br><span class="line">                            .table(sinkTable)</span><br><span class="line">                            .kv(kv)</span><br><span class="line">                            .build();</span><br><span class="line">                &#125;</span><br><span class="line">        ).returns(MysqlItem<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        s4.addSink(<span class="keyword">new</span> MysqlSink(parameterTool))</span><br><span class="line">                .setParallelism(parameterTool.getInt(<span class="string">"mysql.sink.parallelism"</span>, <span class="number">1</span>))</span><br><span class="line">                .name(<span class="string">"MysqlSink"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (parameterTool.getBoolean(<span class="string">"app.handler.print.console"</span>, <span class="keyword">false</span>)) &#123;</span><br><span class="line">            s0.print(StatisticsAppErrorModel<span class="class">.<span class="keyword">class</span>.<span class="title">getName</span>())</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>从上面的整体中，我们这里先看到设置<code>watermark</code>的逻辑，这个<code>watermark</code>决定了我们的flink的数据的有序性，是一个比较重要的处理。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">// 每5s-flink需要获取新的watermark</span></span><br><span class="line">s0.getExecutionConfig().setAutoWatermarkInterval(<span class="number">5000L</span>);</span><br><span class="line"></span><br><span class="line">logger.debug(<span class="string">"getAutoWatermarkInterval: &#123;&#125;"</span>, s0.getExecutionConfig().getAutoWatermarkInterval());</span><br><span class="line">ParameterTool parameterTool = <span class="keyword">this</span>.getParameterTool(s0);</span><br><span class="line"></span><br><span class="line">SingleOutputStreamOperator&lt;Event&gt; s1 = s0.assignTimestampsAndWatermarks(<span class="keyword">new</span> AssignerWithPeriodicWatermarksAdapter.Strategy&lt;&gt;(</span><br><span class="line"><span class="keyword">new</span> BoundedOutOfOrdernessTimestampExtractor&lt;Event&gt;(Time.of(<span class="number">1</span>, TimeUnit.SECONDS)) &#123;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Event event)</span> </span>&#123;</span><br><span class="line">Long ts = event.getLogs().getMtime() * <span class="number">1000L</span>;</span><br><span class="line">logger.debug(</span><br><span class="line"><span class="string">"thread-id: &#123;&#125;, eventTime: [&#123;&#125;|&#123;&#125;], watermark: [&#123;&#125;|&#123;&#125;]"</span>,</span><br><span class="line">Thread.currentThread().getId(),</span><br><span class="line">ts,</span><br><span class="line">sdf.format(ts),</span><br><span class="line"><span class="keyword">this</span>.getCurrentWatermark().getTimestamp(),</span><br><span class="line">sdf.format(<span class="keyword">this</span>.getCurrentWatermark().getTimestamp())</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> ts;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">)</span><br><span class="line"><span class="comment">// 尽可能和窗口大小保持一致，所以如果其中一个并行度出现问题的情况下</span></span><br><span class="line"><span class="comment">// 最大的延迟计算结果是一个窗口大小的时间</span></span><br><span class="line">.withIdleness(Duration.ofMinutes(<span class="number">1L</span>))</span><br><span class="line">);</span><br></pre></td></tr></table></figure><p>我们这里通过<code>AssignerWithPeriodicWatermarksAdapter</code>设置一个<code>watermark</code>生成的策略。</p><p>当数据到来的时候，允许<code>1秒延迟</code>的情况下，解析数据的<code>事件时间(event-time)</code>作为我们的<code>watermark</code>，这里需要注意的是，这里从event-time提取的时间的单位需要是<code>毫秒</code>级别。</p><p>再通过<code>.withIdleness</code>，进行当某个窗口下<code>idle</code>了，那么也会刷新<code>watermark</code>。这个知识点，在kafka中是一个很重要的逻辑，由于flink在kafka的topic在多partition下，在partition的数据<code>watermark</code>对齐的情况，才会进行，所以为了防止，由于防止kafka的partition的数据倾斜对我们造成业务逻辑一直无法更新watermark的问题。这个十分必要。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">WindowedStream&lt;Event, Tuple5&lt;Integer, String, String, Integer, String&gt;, TimeWindow&gt; s2 = s1.keyBy(<span class="keyword">new</span> KeySelector&lt;Event, Tuple5&lt;Integer, String, String, Integer, String&gt;&gt;() &#123;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Tuple5&lt;Integer, String, String, Integer, String&gt; <span class="title">getKey</span><span class="params">(Event event)</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> Tuple5.of(</span><br><span class="line">event.getLogs().getRelatedAppId(),</span><br><span class="line">event.getLogs().getChildApp(),</span><br><span class="line">event.getLogs().getSummary(),</span><br><span class="line">event.getLogs().getLevel(),</span><br><span class="line">event.getLogs().getIp()</span><br><span class="line">);</span><br><span class="line">&#125;</span><br><span class="line">&#125;)</span><br><span class="line">.window(TumblingEventTimeWindows.of(Time.minutes(<span class="number">1L</span>)));</span><br></pre></td></tr></table></figure><p>对于<code>windowstream</code>，主要是定义<code>窗口的时间大小</code>， <code>窗口数据的唯一主键</code>。</p><p>在这里，由于我的需求是每1分钟统计一次，所以这里可以看到我的窗口是基于<code>EventTime（事件时间）</code>的窗口，并且大小范围为<code>1分钟</code>。而数据的唯一主键则是通过<code>getKet(Event event)</code>方法来处理。通过flink内置的便捷的<code>Tuple5</code>这个类来处理的原因是因为我这里有5个元素组成的key。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">SingleOutputStreamOperator&lt;Tuple3&lt;Tuple5&lt;Integer, String, String, Integer, String&gt;, Event, Integer&gt;&gt; s3 = s2.apply(<span class="keyword">new</span> WindowFunction&lt;Event, Tuple3&lt;Tuple5&lt;Integer, String, String, Integer, String&gt;, Event, Integer&gt;, Tuple5&lt;Integer, String, String, Integer, String&gt;, TimeWindow&gt;() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">apply</span><span class="params">(Tuple5&lt;Integer, String, String, Integer, String&gt; key, TimeWindow timeWindow, Iterable&lt;Event&gt; iterable, Collector&lt;Tuple3&lt;Tuple5&lt;Integer, String, String, Integer, String&gt;, Event, Integer&gt;&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">                <span class="keyword">for</span> (Event event : iterable) &#123;</span><br><span class="line">                    sum++;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                logger.debug(<span class="string">"聚合窗口key: &#123;&#125;, 窗口中的数量:&#123;&#125;, 此时的窗口范围是[&#123;&#125;,&#123;&#125;)"</span>, key, sum, sdf.format(timeWindow.getStart()), sdf.format(timeWindow.getEnd()));</span><br><span class="line">                collector.collect(Tuple3.of(key, iterable.iterator().next(), sum));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br></pre></td></tr></table></figure><p>接下来就是<code>聚合(统计)</code>的逻辑了，当<code>window-trigger-condition</code>满足条件之后，就会把当前窗口内的所有数据推到下一个<code>算子</code>，在这个<code>算子</code>的<code>apply()</code>中，我们可以看到我们只是简单的做了一个数据统计，也就是<code>sum++</code>，经过这一操作之后，经过<code>collector</code>对进行进行<code>收集</code>，准备用于下一个<code>算子</code>中。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">SingleOutputStreamOperator&lt;MysqlItem&gt; s4 = s3.map(e -&gt; &#123;</span><br><span class="line">                    HashMap&lt;String, Object&gt; kv = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">                    kv.put(<span class="string">"related_app_id"</span>, e.f1.getLogs().getRelatedAppId());</span><br><span class="line">                    kv.put(<span class="string">"child_app"</span>, e.f1.getLogs().getChildApp());</span><br><span class="line">                    kv.put(<span class="string">"summary"</span>, e.f1.getLogs().getSummary());</span><br><span class="line">                    kv.put(<span class="string">"level"</span>, e.f1.getLogs().getLevel());</span><br><span class="line">                    kv.put(<span class="string">"ip"</span>, e.f1.getLogs().getIp());</span><br><span class="line"></span><br><span class="line">                    kv.put(<span class="string">"mtime"</span>, e.f1.getLogs().getMtime());</span><br><span class="line">                    kv.put(<span class="string">"mdate"</span>, Util.timeStamp2Date(Integer.toString(e.f1.getLogs().getMtime()), <span class="string">"yyyy-MM-dd"</span>));</span><br><span class="line"></span><br><span class="line">                    <span class="comment">// 来自聚合窗口统计的结果</span></span><br><span class="line">                    kv.put(<span class="string">"cnt"</span>, e.f2);</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">return</span> MysqlItem.builder()</span><br><span class="line">                            .database(sinkDatabase)</span><br><span class="line">                            .table(sinkTable)</span><br><span class="line">                            .kv(kv)</span><br><span class="line">                            .build();</span><br><span class="line">                &#125;</span><br><span class="line">        ).returns(MysqlItem<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">s4.addSink(<span class="keyword">new</span> MysqlSink(parameterTool))</span><br><span class="line">.setParallelism(parameterTool.getInt(<span class="string">"mysql.sink.parallelism"</span>, <span class="number">1</span>))</span><br><span class="line">.name(<span class="string">"MysqlSink"</span>);</span><br></pre></td></tr></table></figure><p>在这个前面到算子中，我们拿到了一些我们所期待到数据了，接下来就是把数据转换成为我们需要入库的一个结构。通过<code>MysqlItem</code>对象，我们把所有的结构化的对象通过<code>MysqlSink</code>方法进行发送给mysql。<code>mysqlsink</code>是我们自己封的一个<code>sinker</code>，其中的代码实现如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> org.cp.flink.sinks;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> lombok.Setter;</span><br><span class="line"><span class="keyword">import</span> lombok.experimental.Accessors;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.utils.ParameterTool;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.configuration.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.sink.RichSinkFunction;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.Logger;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.LoggerFactory;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.nio.charset.StandardCharsets;</span><br><span class="line"><span class="keyword">import</span> java.sql.Connection;</span><br><span class="line"><span class="keyword">import</span> java.sql.DriverManager;</span><br><span class="line"><span class="keyword">import</span> java.sql.PreparedStatement;</span><br><span class="line"><span class="keyword">import</span> java.sql.SQLException;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Setter</span></span><br><span class="line"><span class="meta">@Accessors</span>(chain = <span class="keyword">true</span>)</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MysqlSink</span> <span class="keyword">extends</span> <span class="title">RichSinkFunction</span>&lt;<span class="title">MysqlItem</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger logger = LoggerFactory.getLogger(MysqlSink<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">    ParameterTool parameterTool;</span><br><span class="line">    <span class="keyword">private</span> Connection connection;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">MysqlSink</span><span class="params">(ParameterTool parameterTool)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.parameterTool = parameterTool;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>.open(parameters);</span><br><span class="line">        <span class="keyword">if</span> (connection == <span class="keyword">null</span>) &#123;</span><br><span class="line">            connection = <span class="keyword">this</span>.getConnection();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>.close();</span><br><span class="line">        <span class="keyword">if</span> (connection != <span class="keyword">null</span>) &#123;</span><br><span class="line">            connection.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * todo: 再考虑一下如果插入失败的话是否需要重试之类的</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> item</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> context</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">invoke</span><span class="params">(MysqlItem item, Context context)</span> </span>&#123;</span><br><span class="line">        logger.debug(<span class="string">"mysql-item: &#123;&#125;"</span>, item);</span><br><span class="line">        MysqlItem.Sql sqlInfo = item.toInsertIgnoreSql();</span><br><span class="line">        String sql = sqlInfo.getPreSql();</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            PreparedStatement ps = <span class="keyword">this</span>.connection.prepareStatement(sql);</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= sqlInfo.getValues().size(); i++) &#123;</span><br><span class="line">                ps.setObject(i, sqlInfo.getValues().get(i-<span class="number">1</span>));</span><br><span class="line">            &#125;</span><br><span class="line">            logger.debug(ps.toString());</span><br><span class="line">            ps.execute();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">            logger.error(e.getMessage());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> Connection <span class="title">getConnection</span><span class="params">()</span> <span class="keyword">throws</span> ClassNotFoundException, SQLException </span>&#123;</span><br><span class="line">        Class.forName(<span class="string">"com.mysql.cj.jdbc.Driver"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> DriverManager.getConnection(</span><br><span class="line">                String.format(</span><br><span class="line">                        <span class="string">"jdbc:mysql://%s:%s/?useUnicode=true&amp;characterEncoding=%s&amp;useSSL=false&amp;autoReconnect=true"</span>,</span><br><span class="line">                        <span class="keyword">this</span>.parameterTool.get(<span class="string">"mysql.sink.host"</span>),</span><br><span class="line">                        <span class="keyword">this</span>.parameterTool.get(<span class="string">"mysql.sink.port"</span>),</span><br><span class="line">                        <span class="keyword">this</span>.parameterTool.get(<span class="string">"mysql.sink.characterEncoding"</span>, StandardCharsets.UTF_8.toString())</span><br><span class="line">                ),</span><br><span class="line">                <span class="keyword">this</span>.parameterTool.get(<span class="string">"mysql.sink.user"</span>),</span><br><span class="line">                <span class="keyword">this</span>.parameterTool.get(<span class="string">"mysql.sink.password"</span>)</span><br><span class="line">        );</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>到此，一个基于<code>datastream-api</code>的job，就完成了。</p><p>但是由于这是<code>java技术栈</code>，对于不是<code>java技术栈</code>的团队而言，这是一件比较麻烦的事情。就算是<code>java技术栈</code>，也需要去属于了解flink的原理，然后去编写对应的flink代码，这对于不熟悉<code>datastream-api</code>的小伙伴来说，也是一种头痛的事情。</p><p>所以对于这个问题，我们考虑使用上层一些的api，也就是<code>table-api</code>和<code>sql-api</code>。</p><p>但是由于此类api还是需要熟悉api的细节，所以我们看到了flink提供了一个叫<code>sql-client</code>的东西。但是由于<code>sql-client</code>的不稳定性（某些版本下存在比较严重的bug），且某些需求无法满足我们，为了灵活和可控性，我们最终解决了自行开发<code>flink-sql-client</code>。</p><h2 id="基于自研sql-client的flink开发"><a href="#基于自研sql-client的flink开发" class="headerlink" title="基于自研sql-client的flink开发"></a>基于自研<code>sql-client</code>的flink开发</h2><p>具体的实现方式在 <a href="https://github.com/whiteCcinn/flink-sql-submit" target="_blank" rel="noopener">flink-sql-submit</a></p><p>实现原理其实也不复杂，其实就是通过一个flink项目，封装成为一个类似cmd的命令，然后通过此方式来提交我们的<code>sql或者sql文件</code></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">src/main/java/</span><br><span class="line">├── deps</span><br><span class="line">│   └── util</span><br><span class="line">│       ├── ParameterToolEnvironmentUtils.java</span><br><span class="line">│       ├── SqlCommandParser.java</span><br><span class="line">│       └── Util.java</span><br><span class="line">└── org</span><br><span class="line">    └── client</span><br><span class="line">        └── flink</span><br><span class="line">            ├── Bootstrap.java</span><br><span class="line">            ├── SqlSubmit.java</span><br><span class="line">            ├── cmds</span><br><span class="line">            │   ├── AbstractCommand.java</span><br><span class="line">            │   ├── HelpCommand.java</span><br><span class="line">            │   ├── HiveCatalogCommand.java</span><br><span class="line">            │   ├── ICommand.java</span><br><span class="line">            │   ├── JobCommand.java</span><br><span class="line">            │   └── SqlParserCommand.java</span><br><span class="line">            ├── enums</span><br><span class="line">            │   └── PlanType.java</span><br><span class="line">            ├── internals</span><br><span class="line">            └── udfs</span><br></pre></td></tr></table></figure><p>我们可以看到，整个项目只有少量文件。提供了几个命令：</p><ul><li>help 帮助命令</li><li>hivecatalog 管理<ul><li>增</li><li>删</li><li>查</li></ul></li><li>job 提交任务<ul><li>sql</li><li>sql-file</li></ul></li><li>sql-parser 调试解析sql</li></ul><p>我们以一个<code>sql-file</code>为例子，其他大家可以在github上查看源码。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 以":"为分隔符，分别代表：catalog_type, hive_conf_path, catalog_name</span></span><br><span class="line"><span class="comment">-- "-" 代表使用默认值</span></span><br><span class="line">CATALOG_INFO = hive:/opt/hadoopclient/Hive/config/:-;</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">DATABASE</span> mstream_alarm <span class="keyword">COMMENT</span> <span class="string">'告警系统流计算'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">USE</span> mstream_alarm;</span><br><span class="line"></span><br><span class="line"><span class="keyword">SET</span> <span class="string">'pipeline.name'</span> = <span class="string">'每1分钟基础服务告警'</span>;</span><br><span class="line"><span class="keyword">SET</span> <span class="string">'table.exec.emit.early-fire.enabled'</span> = <span class="string">'true'</span>;</span><br><span class="line"><span class="keyword">SET</span> <span class="string">'table.exec.emit.early-fire.delay'</span> = <span class="string">'10s'</span>;</span><br><span class="line"><span class="keyword">SET</span> <span class="string">'mc.local.time.zone'</span> = <span class="string">'Asia/Shanghai'</span>;</span><br><span class="line"><span class="keyword">SET</span> <span class="string">'table.exec.sink.not-null-enforcer'</span> = <span class="string">'drop'</span>;</span><br><span class="line"><span class="comment">-- checkpoint配置</span></span><br><span class="line"><span class="keyword">SET</span> <span class="string">'execution.checkpointing.mode'</span> = <span class="string">'EXACTLY_ONCE'</span>;</span><br><span class="line"><span class="keyword">SET</span> <span class="string">'execution.checkpointing.interval'</span> = <span class="string">'2min'</span>;</span><br><span class="line"><span class="keyword">SET</span> <span class="string">'execution.checkpointing.timeout'</span> = <span class="string">'1min'</span>;</span><br><span class="line"><span class="keyword">SET</span> <span class="string">'execution.checkpointing.prefer-checkpoint-for-recovery'</span> = <span class="literal">true</span>;</span><br><span class="line"><span class="keyword">SET</span> <span class="string">'execution.checkpointing.externalized-checkpoint-retention'</span> = <span class="string">'RETAIN_ON_CANCELLATION'</span>;</span><br><span class="line"><span class="keyword">SET</span> <span class="string">'mc.state.backend.fs.checkpointdir'</span> = <span class="string">'hdfs:///flink/checkpoints/&#123;db&#125;/&#123;pipeline.name&#125;'</span>;</span><br><span class="line"><span class="keyword">SET</span> <span class="string">'mc.execution.savepoint.dir'</span> = <span class="string">'hdfs:///flink/savepoints/&#123;db&#125;/&#123;pipeline.name&#125;'</span>;</span><br><span class="line"><span class="comment">-- 重启策略</span></span><br><span class="line"><span class="keyword">SET</span> <span class="string">'restart-strategy'</span> = <span class="string">'failure-rate'</span>;</span><br><span class="line"><span class="keyword">SET</span> <span class="string">'restart-strategy.failure-rate.delay'</span> = <span class="string">'10s'</span>;</span><br><span class="line"><span class="keyword">SET</span> <span class="string">'restart-strategy.failure-rate.failure-rate-interval'</span> = <span class="string">'5min'</span>;</span><br><span class="line"><span class="keyword">SET</span> <span class="string">'restart-strategy.failure-rate.max-failures-per-interval'</span> = <span class="string">'10'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> app_error_To_t_log_app_error_alarm_164 (</span><br><span class="line">    headers <span class="keyword">ROW</span>&lt;<span class="string">`app_id`</span> <span class="built_in">int</span>,<span class="string">`log_name`</span> <span class="keyword">string</span>&gt;,</span><br><span class="line">    <span class="keyword">logs</span> <span class="keyword">ROW</span>&lt;<span class="string">`related_app_id`</span> <span class="built_in">int</span>, <span class="string">`child_app`</span> <span class="built_in">varchar</span>(<span class="number">200</span>), <span class="string">`summary`</span> <span class="keyword">string</span>,<span class="string">`level`</span> <span class="built_in">int</span>,<span class="string">`ip`</span> <span class="built_in">varchar</span>(<span class="number">200</span>),<span class="string">`detail`</span> <span class="built_in">varchar</span>(<span class="number">100</span>), <span class="string">`mtime`</span> <span class="built_in">int</span>&gt;,</span><br><span class="line">    etime <span class="keyword">as</span> TO_TIMESTAMP(FROM_UNIXTIME(logs.<span class="string">`mtime`</span>)),</span><br><span class="line">    WATERMARK <span class="keyword">for</span> etime <span class="keyword">AS</span> etime <span class="comment">-- defines watermark on ts column, marks ts as event-time attribute</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">WITH</span> (</span><br><span class="line">    <span class="string">'connector'</span> = <span class="string">'kafka'</span>,</span><br><span class="line">    <span class="string">'topic'</span> = <span class="string">'mfeilog_dsp_10008_app_error'</span>,</span><br><span class="line">    <span class="string">'properties.bootstrap.servers'</span> = <span class="string">'127.0.0.1:9092'</span>,</span><br><span class="line">    <span class="string">'properties.group.id'</span> = <span class="string">'app_error_to_t_log_app_error_alarm_164'</span>,</span><br><span class="line">    <span class="string">'format'</span> = <span class="string">'json'</span>,</span><br><span class="line">    <span class="string">'scan.startup.mode'</span> = <span class="string">'latest-offset'</span>,</span><br><span class="line">    <span class="string">'json.fail-on-missing-field'</span> = <span class="string">'false'</span>,</span><br><span class="line">    <span class="string">'json.ignore-parse-errors'</span> = <span class="string">'false'</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`t_log_app_error_alarm_164`</span> (</span><br><span class="line">  <span class="string">`related_app_id`</span> <span class="built_in">int</span>,</span><br><span class="line">  <span class="string">`child_app`</span> <span class="built_in">varchar</span>(<span class="number">200</span>),</span><br><span class="line">  <span class="string">`summary`</span> <span class="keyword">string</span>,</span><br><span class="line">  <span class="string">`level`</span> <span class="built_in">int</span>,</span><br><span class="line">  <span class="string">`ip`</span> <span class="built_in">varchar</span>(<span class="number">200</span>) ,</span><br><span class="line">  <span class="string">`cnt`</span> <span class="built_in">varchar</span>(<span class="number">200</span>) <span class="keyword">COMMENT</span> <span class="string">'calculate the detail of count()'</span>,</span><br><span class="line">  <span class="string">`mdate`</span> <span class="keyword">string</span>,</span><br><span class="line">  <span class="string">`mtime`</span> <span class="built_in">int</span>,</span><br><span class="line">  PRIMARY <span class="keyword">KEY</span> (<span class="string">`related_app_id`</span>,<span class="string">`child_app`</span>,<span class="string">`summary`</span>,<span class="string">`level`</span>,<span class="string">`ip`</span>) <span class="keyword">NOT</span> <span class="keyword">ENFORCED</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">   <span class="string">'connector'</span> = <span class="string">'jdbc'</span>,</span><br><span class="line">   <span class="string">'url'</span> = <span class="string">'jdbc:mysql://127.0.0.1:60701/db_app_log_alarm?useUnicode=true&amp;characterEncoding=utf8&amp;autoReconnect=true'</span>,</span><br><span class="line">   <span class="string">'driver'</span> = <span class="string">'com.mysql.cj.jdbc.Driver'</span>,</span><br><span class="line">   <span class="string">'table-name'</span> = <span class="string">'t_log_app_error_alarm_164'</span>,</span><br><span class="line">   <span class="string">'username'</span> = <span class="string">'flink_mstream_alarm'</span>,</span><br><span class="line">   <span class="string">'password'</span> = <span class="string">'xxxx'</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> t_log_app_error_alarm_164 (</span><br><span class="line">    <span class="keyword">select</span> t1.<span class="string">`related_app_id`</span>,t1.<span class="string">`child_app`</span>,t1.<span class="string">`summary`</span>,t1.<span class="string">`level`</span>,t1.<span class="string">`ip`</span>,<span class="keyword">cast</span>(t1.<span class="string">`cnt`</span> <span class="keyword">as</span> <span class="built_in">VARCHAR</span>(<span class="number">200</span>)) <span class="keyword">as</span> <span class="string">`cnt`</span>,t1.<span class="string">`mdate`</span>,<span class="keyword">cast</span> (t1.<span class="string">`mtime`</span> <span class="keyword">as</span> <span class="built_in">INT</span>)  <span class="keyword">from</span> (</span><br><span class="line">        <span class="keyword">select</span></span><br><span class="line">            logs.<span class="string">`related_app_id`</span> <span class="keyword">as</span> <span class="string">`related_app_id`</span>,</span><br><span class="line">            logs.<span class="string">`child_app`</span> <span class="keyword">as</span> <span class="string">`child_app`</span>,</span><br><span class="line">            logs.<span class="string">`summary`</span> <span class="keyword">as</span> <span class="string">`summary`</span>,</span><br><span class="line">            logs.<span class="string">`level`</span> <span class="keyword">as</span> <span class="string">`level`</span>,</span><br><span class="line">            logs.<span class="string">`ip`</span> <span class="keyword">as</span> <span class="string">`ip`</span>,</span><br><span class="line">            <span class="keyword">DATE_FORMAT</span>(TUMBLE_START(etime, <span class="built_in">INTERVAL</span> <span class="string">'1'</span> <span class="keyword">MINUTE</span>), <span class="string">'yyyy-MM-dd'</span>) <span class="keyword">as</span> <span class="string">`mdate`</span>,</span><br><span class="line">            <span class="keyword">UNIX_TIMESTAMP</span>(<span class="keyword">DATE_FORMAT</span>(TUMBLE_START(etime, <span class="built_in">INTERVAL</span> <span class="string">'1'</span> <span class="keyword">MINUTE</span>), <span class="string">'yyyy-MM-dd HH:mm:ss'</span>)) <span class="keyword">as</span> <span class="string">`mtime`</span>,</span><br><span class="line">            <span class="keyword">COUNT</span>(logs.<span class="string">`detail`</span>) <span class="keyword">as</span> <span class="string">`cnt`</span></span><br><span class="line">        <span class="keyword">FROM</span> app_error_To_t_log_app_error_alarm_164</span><br><span class="line">        <span class="keyword">GROUP</span> <span class="keyword">BY</span> logs.<span class="string">`related_app_id`</span>, logs.<span class="string">`child_app`</span>,logs.<span class="string">`summary`</span>,logs.<span class="string">`level`</span>,logs.<span class="string">`ip`</span>,TUMBLE(etime, <span class="built_in">INTERVAL</span> <span class="string">'1'</span> <span class="keyword">MINUTE</span>)</span><br><span class="line">    ) t1</span><br><span class="line">);</span><br></pre></td></tr></table></figure><p>我们可以看到这个<code>sql-file</code>，支持了一些<code>关键字</code>，这些关键字被开发在<code>client</code>当中了，所以可以被正常解析到。</p><p>通过解析到关键字，再调用对应的API，我们就可以设置对应的行为了。</p><p>我们可以看到我们从繁杂的<code>datastreamapi</code>中，已经把剥离了出来，通过sql这种DSL的方式，让不同语言技术栈的同事都可以定制自己的job。</p><p>并且支持了自定义重启策略，保证每一个算子在异常或者正常的情况下，都可以从正确的数据中进行恢复重启。</p><p>这一套sql编写下来，做的事情和我们上面的<code>datastream</code>做的事情是一样的，但是却无需了解太多其中的细节。</p><h4 id="UDF的运用"><a href="#UDF的运用" class="headerlink" title="UDF的运用"></a>UDF的运用</h4><p>例如我们需要ip转地址字符串，这个时候，我们就需要udf来协助我们完成这件事。</p><p>client项目可以内置一些我们所需要的UDF，然后连同job一起生效。</p><p>例如：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@127.0,0.1_A ~]# flink run -yid `cat /data/flink-stream/mstream/mstream_xx/yid` /data/flink-stream/flink-sql-submit-1.0-SNAPSHOT.jar job --sql "CATALOG_INFO = hive:/opt/hadoopclient/Hive/config/:-;USE mstream_alarm;SELECT ip2location('219.135.155.76');"</span><br><span class="line"> Interface ana-group-1byez.dad44e53-24e6-41be-bfd5-a4055f4c6604.com:32263 of application 'application_1641337362340_6699'.</span><br><span class="line">Job has been submitted with JobID 824af5a31aba88db6e0137f5e834f26b</span><br><span class="line">+----+--------------------------------+</span><br><span class="line">| op |                         EXPR$0 |</span><br><span class="line">+----+--------------------------------+</span><br><span class="line">| +I |                 中国,广东,广州 |</span><br><span class="line">+----+--------------------------------+</span><br></pre></td></tr></table></figure><p>我们可以看到，通过<code>ip2localtion()</code>，我们完成了一个udf，并且可以实现在sql的模式上。用过ip地址转为为了地址。</p><h2 id="落地实战"><a href="#落地实战" class="headerlink" title="落地实战"></a>落地实战</h2><p>由于资源的有限，我们在flink的架构上，采用的是每个项目对应一个<code>application</code>的方法，每个<code>application通过yarn来分配来分配资源容器</code>，然后再通过<code>yarn-session</code>(非<code>per on job</code>)的方式来管理我们的flink应用。</p><h3 id="申请资源应用"><a href="#申请资源应用" class="headerlink" title="申请资源应用"></a>申请资源应用</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yarn-session.sh -jm 1024 -tm 1024 -s 16 -nm '告警流计算应用' -yd</span><br></pre></td></tr></table></figure><p><img src="/images/FLINK/application.png" alt="application"></p><h3 id="client-例子"><a href="#client-例子" class="headerlink" title="client 例子"></a>client 例子</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">help</span></span></span><br><span class="line">root@41c5967b5948:/www# flink run target/mc-flink-sql-submit-1.0-SNAPSHOT.jar help</span><br><span class="line">帮助命令</span><br><span class="line"></span><br><span class="line">Usage of "flink run &lt;.jar&gt; help [options]"</span><br><span class="line"></span><br><span class="line">Available Commands</span><br><span class="line">   job          提交job作业</span><br><span class="line">   sql-parser   解析sql文件</span><br><span class="line">   help         帮助命令</span><br><span class="line">   hive-catalog hive-catalog的相关</span><br><span class="line"></span><br><span class="line">Global Options:</span><br><span class="line">   --app.force.remote bool</span><br><span class="line">       是否启动远端环境变量: false</span><br><span class="line">   --app.config.debug bool</span><br><span class="line">       是否打印用户参数: false</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> job</span></span><br><span class="line">root@41c5967b5948:/www# flink run target/mc-flink-sql-submit-1.0-SNAPSHOT.jar job help</span><br><span class="line">提交job</span><br><span class="line"></span><br><span class="line">Usage of "flink run &lt;.jar&gt; job [options]"</span><br><span class="line">   --sql string</span><br><span class="line">       执行的sql (*)</span><br><span class="line">   --plan string</span><br><span class="line">       选择执行计划器:</span><br><span class="line">           flink-streaming</span><br><span class="line">           flink-batch</span><br><span class="line">           blink-streaming</span><br><span class="line">           flink-batch</span><br><span class="line"></span><br><span class="line">Global Options:</span><br><span class="line">   --app.force.remote bool</span><br><span class="line">       是否启动远端环境变量: false</span><br><span class="line">   --app.config.debug bool</span><br><span class="line">       是否打印用户参数: false</span><br></pre></td></tr></table></figure><h3 id="flink-stream-sql-mctl-用法"><a href="#flink-stream-sql-mctl-用法" class="headerlink" title="flink-stream-sql-mctl 用法"></a>flink-stream-sql-mctl 用法</h3><p>这是一个集成脚本，所以存在约定的规则和部署的架构约束。</p><p>这便于我们管理所有的applition和flink种的所有flink-job。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">flink-sql-submit git:(master) ✗ ./flink-stream-sql-mctl.sh</span><br><span class="line"></span><br><span class="line">  flink-stream-sql-mctl.sh [OPTION] &lt;COMMAND&gt;</span><br><span class="line"></span><br><span class="line">  Flink流计算SQL-Client的执行脚本</span><br><span class="line"></span><br><span class="line">  Command:</span><br><span class="line">    run          [FILE]            运行</span><br><span class="line">    stop         [FILE]            停止</span><br><span class="line">    list         [FILE]            列出FILE所在yid下的所有job任务列表</span><br><span class="line">    drop_table   [FILE]            删除所有表</span><br><span class="line">    rebuild_run  [FILE]            删除所有表，然后重跑(继承savepoint）</span><br><span class="line"></span><br><span class="line">  Command-Common-Options:</span><br><span class="line">    -c, --clientpath  [LEVEL]    flink-sql-submit.jar路径  (Default is '/data/tmp/mc-flink-sql-submit-1.0-SNAPSHOT.jar')</span><br><span class="line">    -f   是否强制运行，忽略以往savepoint</span><br><span class="line"></span><br><span class="line">  Common-Options:</span><br><span class="line">    -h, --help              Display this help and exit</span><br><span class="line">    --loglevel [LEVEL]      One of: FATAL, ERROR, WARN, INFO, NOTICE, DEBUG, ALL, OFF</span><br><span class="line">                            (Default is 'ERROR')</span><br><span class="line">    --logfile [FILE]        Full PATH to logfile.  (Default is '/Users/caiwenhui/logs/flink-stream-sql-mctl.sh.log')</span><br><span class="line">    -n, --dryrun            Non-destructive. Makes no permanent changes.</span><br><span class="line">    -q, --quiet             Quiet (no output)</span><br><span class="line">    -v, --verbose           Output more information. (Items echoed to 'verbose')</span><br><span class="line">    --force                 Skip all user interaction.  Implied 'Yes' to all actions.</span><br></pre></td></tr></table></figure><p>约定规则：</p><ul><li>模型所在父目录的至少有一个yid文件（取最近的一个父节点的yid）对应所在的应用id</li><li>默认情况下，模型启动的时候会取最近一次savepoint的数据进行恢复，如果不存在，则直接启动</li></ul><h3 id="停止所有模型"><a href="#停止所有模型" class="headerlink" title="停止所有模型"></a>停止所有模型</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">for i in $(find /data/flink-stream/mstream_alarm/ -type f -name "*.sql");do /data/flink-stream/flink-stream-sql-mctl stop $i;done</span><br></pre></td></tr></table></figure><h3 id="启动所有模型"><a href="#启动所有模型" class="headerlink" title="启动所有模型"></a>启动所有模型</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">for i in $(find /data/flink-stream/mstream_alarm/ -type f -name "*.sql");do /data/flink-stream/flink-stream-sql-mctl run $i;done</span><br></pre></td></tr></table></figure><h3 id="删除所有表"><a href="#删除所有表" class="headerlink" title="删除所有表"></a>删除所有表</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">for i in $(find /data/flink-stream/mstream_alarm/ -type f -name "*.sql");do /data/flink-stream/flink-stream-sql-mctl drop_table $i;done</span><br></pre></td></tr></table></figure><h3 id="相关的一些落地后截图信息"><a href="#相关的一些落地后截图信息" class="headerlink" title="相关的一些落地后截图信息"></a>相关的一些落地后截图信息</h3><p><img src="/images/FLINK/server.png" alt="server"></p><p><img src="/images/FLINK/detail-0.png" alt="detail-0"></p><p><img src="/images/FLINK/detail-1.png" alt="detail-1"></p><p><img src="/images/FLINK/detail-2.png" alt="detail-2"></p><p><img src="/images/FLINK/detail-3.png" alt="detail-3"></p><p>到此为止，我们的flink相关的流计算应用，从0到1的过程暂时画上一个里程碑。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;在公司落地一套flink，总结到目前为止做了的事情。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://blog.crazylaw.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="flink" scheme="http://blog.crazylaw.cn/tags/flink/"/>
    
  </entry>
  
  <entry>
    <title>【Golang】- 基于gnet的端口复用支持多协议的客服聊天监控服务</title>
    <link href="http://blog.crazylaw.cn/2022/02/12/Golang/%E5%9F%BA%E4%BA%8Egnet%E7%9A%84%E5%AE%A2%E6%9C%8D%E8%81%8A%E5%A4%A9%E7%9B%91%E6%8E%A7%E6%9C%8D%E5%8A%A1/"/>
    <id>http://blog.crazylaw.cn/2022/02/12/Golang/%E5%9F%BA%E4%BA%8Egnet%E7%9A%84%E5%AE%A2%E6%9C%8D%E8%81%8A%E5%A4%A9%E7%9B%91%E6%8E%A7%E6%9C%8D%E5%8A%A1/</id>
    <published>2022-02-11T16:46:51.000Z</published>
    <updated>2022-02-12T03:45:01.017Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近，公司以前有一些旧的服务，由于各种原因，导致各种问题，并且架构设计行也不是那么友好和不利于维护。<br>所以准备重构设计一些服务。</p><p>在游戏公司中，GM客服的其中一个职能就是监督舆论，从玩家平日的聊天中进行监控。</p><p>我们从<code>业务需求</code>+<code>技术架构</code>层面进行整理。</p><a id="more"></a><h2 id="历史"><a href="#历史" class="headerlink" title="历史"></a>历史</h2><p>在过去中，由于当时php还是如日中天，旧的则是采集的<code>swoole1.x</code>的版本进行开发的服务。<br>受限于php一个语言特性，注定无法实现一些高性能的中间件，或者说大数据生态十分欠缺。当时用php除了<code>fastcgi</code>的<code>web系统</code>外，最多就只能做一些基本的<code>常驻</code>任务。</p><p>消息中间件最多也就是用到<code>rabbitmq</code>，<code>rocketmq</code>等等。</p><p>而常驻，一般无非就是直接<code>cli</code>，外加一个<code>循环+sleep</code>的组合套餐。而要实现<code>websocket-server</code>这种常驻服务，一般是借助<code>swoole</code>来处理。毕竟<code>reactor</code>的模式，怎么都比<code>单进程</code>的实现好。</p><p>分为了3个模块（每个模块=每个角色=一个进程=一个服务）：</p><ul><li>chat_record （聊天记录角色）（weboccket_client, tcp_clinet）</li><li>db_server （数据层角色) (tcp_server)</li><li>websocket_server (连接层角色) (webocket_server)</li></ul><p>由于当时php基本无法多线程编程(可用，但是不友好)，只能采用这种委婉的<code>伪多进程</code>的模拟进行<code>不同任务的处理</code>和<code>数据的交互</code>。</p><p><img src="/images/Go/chat_monitor.png" alt="旧服务的数据流图"></p><h2 id="新服务"><a href="#新服务" class="headerlink" title="新服务"></a>新服务</h2><p><img src="/images/Go/chat_monitor_new.png" alt="新服务的数据流图"></p><blockquote><p>但是由于种种原因，后面并未如此拆分架构，而是将<code>websocket-server网络连接层</code>的和<code>业务层</code>合并成为了一个<code>单体服务</code></p></blockquote><p>技术选型上</p><ul><li>go</li><li>gnet</li><li>kafka</li></ul><h3 id="为什么核心的网络层需要采用gnet呢？"><a href="#为什么核心的网络层需要采用gnet呢？" class="headerlink" title="为什么核心的网络层需要采用gnet呢？"></a>为什么核心的网络层需要采用<code>gnet</code>呢？</h3><p>一般Go语言的TCP(和HTTP)的处理都是<code>每一个连接</code>启动<code>一个goroutine</code>去处理，因为我们被教导<code>goroutine</code>的不像<code>thread</code>, 它是很便宜的，可以在服务器上启动成<code>千上万的goroutine</code>。</p><p>但是对于<code>一百万</code>的连接，这种<code>goroutine-per-connection</code>的模式就<code>至少</code>要启动<code>一百万个goroutine</code>，这对资源的消耗也是极大的。</p><p>针对不同的操作系统和不同的Go版本，一个goroutine锁使用的最小的栈大小是<code>2KB ~ 8 KB (go stack)</code>,如果在每个goroutine中在<code>分配byte buffer</code>用以从连接中读写数据，<code>几十G的内存</code>轻轻松松就分配出去了。</p><p><code>吞吐率</code>和<code>延迟</code>需要数据来支撑，但是显然这个<code>单goroutine</code>处理的模式<code>不适合耗时较长</code>的业务处理，<code>&quot;hello world&quot;</code>或者<code>直接的简单的memory操作</code>应该没有问题。</p><p>对于百万连接<code>但是并发量很小</code>的场景，比如消息推送、页游等场景，这种实现应该是没有问题的。</p><p>但是对于并发量很大，延迟要求比较低的场景，这种实现可能会存在问题。</p><p><code>gnet</code>采用了类似<code>netty</code>的<code>reactor</code>模式，基于<code>epoll</code>或者<code>kqueue</code>实现io多路复用。并且基于golang的语言特性，其实现原理为<code>带线程/go程池的主从 Reactors 多线程</code>模式，在网络层上性能上有极大的优化。</p><p>我们通过gnet提供的tcp网络层，在应用层，实现了http和webocket的端口复用的形式。</p><p>http用于提供<code>prometheus</code>的<code>metrics</code>指标，例如<code>连接数/各种类型引发的error数/每条数据被多少个GM客服监视着</code>等等</p><p>websocket则是用于在我们的<code>GM客服</code>中，提一个实时的聊天数据获取</p><h3 id="为什么采用kafka"><a href="#为什么采用kafka" class="headerlink" title="为什么采用kafka"></a>为什么采用kafka</h3><p>由于我们整套日志服务都是基于kafka作为核心组件的，所以在数据的实时上，可以保证到数据的实效性。</p><p>从而取消了以往从mysql中分库分表去查询数据。也不需要通过其他<code>OLAP</code>的服务进行处理。</p><h3 id="端口复用实现支持多协议"><a href="#端口复用实现支持多协议" class="headerlink" title="端口复用实现支持多协议"></a>端口复用实现支持多协议</h3><p>这个是网络连接层，也是链接的核心业务逻辑，在gnet中当有数据到来的时候，由<code>IO多路复用</code>的<code>epoll</code>模型，会触发<code>OnTraffic(c gnet.Conn)</code>的回调函数，在这个过程中，我们就可以通过网络层中获取的数据进行加工处理，形成自己想要的<code>应用协议</code>。</p><p>由于刚才介绍到了，我们需要实现核心需求：<code>端口多协议复用</code></p><p>在这里，先列出核心的逻辑：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">type</span> ApplicationLayerProto <span class="keyword">int</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(alp ApplicationLayerProto)</span> <span class="title">String</span><span class="params">()</span> <span class="params">(s <span class="keyword">string</span>)</span></span> &#123;</span><br><span class="line"><span class="keyword">switch</span> alp &#123;</span><br><span class="line"><span class="keyword">case</span> HttpApplicationLayerProto:</span><br><span class="line">s = <span class="string">"http"</span></span><br><span class="line"><span class="keyword">case</span> WebsocketApplicationLayerProto:</span><br><span class="line">s = <span class="string">"websocket"</span></span><br><span class="line"><span class="keyword">default</span>:</span><br><span class="line">s = <span class="string">"unknown"</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> (</span><br><span class="line">HttpApplicationLayerProto ApplicationLayerProto = <span class="literal">iota</span></span><br><span class="line">WebsocketApplicationLayerProto</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> codec <span class="keyword">struct</span> &#123;</span><br><span class="line">proto ApplicationLayerProto</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *codec)</span> <span class="title">isHttp</span><span class="params">()</span> <span class="title">bool</span></span> &#123;</span><br><span class="line"><span class="keyword">if</span> c.proto == HttpApplicationLayerProto &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">true</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="literal">false</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *codec)</span> <span class="title">isWebsocket</span><span class="params">()</span> <span class="title">bool</span></span> &#123;</span><br><span class="line"><span class="keyword">if</span> c.proto == WebsocketApplicationLayerProto &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">true</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="literal">false</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> httpCodec <span class="keyword">struct</span> &#123;</span><br><span class="line">*codec</span><br><span class="line">parser *wildcat.HTTPParser</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> wsCodec <span class="keyword">struct</span> &#123;</span><br><span class="line">*codec</span><br><span class="line">connected <span class="keyword">bool</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(serv *server)</span> <span class="title">OnOpen</span><span class="params">(c gnet.Conn)</span> <span class="params">([]<span class="keyword">byte</span>, gnet.Action)</span></span> &#123;</span><br><span class="line">c.SetContext(<span class="built_in">new</span>(codec))</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, gnet.None</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(serv *server)</span> <span class="title">OnTraffic</span><span class="params">(c gnet.Conn)</span> <span class="title">gnet</span>.<span class="title">Action</span></span> &#123;</span><br><span class="line"><span class="keyword">var</span> buffer *bytes.Buffer</span><br><span class="line"><span class="keyword">var</span> buff []<span class="keyword">byte</span></span><br><span class="line">pipeline:</span><br><span class="line"><span class="keyword">switch</span> cdc := c.Context().(<span class="keyword">type</span>) &#123;</span><br><span class="line"><span class="keyword">case</span> *codec:</span><br><span class="line">buf, err := c.Next(<span class="number">-1</span>)</span><br><span class="line">buff = <span class="built_in">make</span>([]<span class="keyword">byte</span>, <span class="built_in">len</span>(buf))</span><br><span class="line"><span class="built_in">copy</span>(buff, buf)</span><br><span class="line">buffer = bytes.NewBuffer(buff)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> gnet.Close</span><br><span class="line">&#125;</span><br><span class="line">hc := &amp;httpCodec&#123;parser: wildcat.NewHTTPParser(), codec: cdc&#125;</span><br><span class="line">_, err = hc.parser.Parse(buf)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">log.Errorlog(log.NetServerErrorCategory&#123;Summary: fmt.Sprintf(<span class="string">"http parser error: %v"</span>, err)&#125;)</span><br><span class="line"><span class="keyword">return</span> gnet.Close</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> upgrade := hc.parser.FindHeader([]<span class="keyword">byte</span>(<span class="string">"Upgrade"</span>)); upgrade != <span class="literal">nil</span> &amp;&amp; bytes.Equal(upgrade, []<span class="keyword">byte</span>(<span class="string">"websocket"</span>)) &#123;</span><br><span class="line">cdc.proto = WebsocketApplicationLayerProto</span><br><span class="line">wc := &amp;wsCodec&#123;</span><br><span class="line">codec: cdc,</span><br><span class="line">&#125;</span><br><span class="line">c.SetContext(wc)</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">cdc.proto = HttpApplicationLayerProto</span><br><span class="line">c.SetContext(hc)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">goto</span> pipeline</span><br><span class="line"><span class="keyword">case</span> *httpCodec:</span><br><span class="line">buf := bufio.NewReader(buffer)</span><br><span class="line">req, err := http.ReadRequest(buf)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">log.Errorlog(log.NetServerErrorCategory&#123;Summary: fmt.Sprintf(<span class="string">"request from http error: %v"</span>, err)&#125;)</span><br><span class="line"><span class="keyword">return</span> gnet.Close</span><br><span class="line">&#125;</span><br><span class="line">metrics.TotalConnectedCounter.WithLabelValues(HttpApplicationLayerProto.String()).Inc()</span><br><span class="line">resp := route.NewResponse(c)</span><br><span class="line">h, _ := serv.serverMux.Handler(req)</span><br><span class="line">h.ServeHTTP(resp, req)</span><br><span class="line"><span class="keyword">if</span> _, err = resp.Close(); err != <span class="literal">nil</span> &#123;</span><br><span class="line">log.Errorlog(log.NetServerErrorCategory&#123;Summary: fmt.Sprintf(<span class="string">"write to http error: %v"</span>, err)&#125;)</span><br><span class="line"><span class="keyword">return</span> gnet.Close</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> gnet.Close</span><br><span class="line"><span class="keyword">case</span> *wsCodec:</span><br><span class="line"><span class="keyword">if</span> !cdc.connected &#123;</span><br><span class="line">wcb := &amp;wsConnBridge&#123;</span><br><span class="line">buff: buffer,</span><br><span class="line">c:    c,</span><br><span class="line">&#125;</span><br><span class="line">_, err := ws.Upgrade(wcb)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">log.Errorlog(log.NetServerErrorCategory&#123;Summary: fmt.Sprintf(<span class="string">"upgrade[%s] to websocket error: %v"</span>, c.RemoteAddr().String(), err)&#125;)</span><br><span class="line">&#125;</span><br><span class="line">log.Debugf(log.NetServerDebugCategory&#123;&#125;, <span class="string">"conn[%v] upgrade websocket protocol"</span>, c.RemoteAddr().String())</span><br><span class="line">cdc.connected = <span class="literal">true</span></span><br><span class="line">metrics.ConnectedGauge.Inc()</span><br><span class="line">metrics.TotalConnectedCounter.WithLabelValues(WebsocketApplicationLayerProto.String()).Inc()</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">msg, op, err := wsutil.ReadClientData(c)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">if</span> _, ok := err.(wsutil.ClosedError); !ok &#123;</span><br><span class="line">log.Errorlog(log.NetServerErrorCategory&#123;Summary: fmt.Sprintf(<span class="string">"[%s] receive ws message error: %v"</span>, c.RemoteAddr().String(), err)&#125;)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> gnet.Close</span><br><span class="line">&#125;</span><br><span class="line">log.Debugf(log.NetServerDebugCategory&#123;&#125;, <span class="string">"conn[%v] receive [op=%v] [msg=%v]"</span>, c.RemoteAddr().String(), op, <span class="keyword">string</span>(msg))</span><br><span class="line"><span class="keyword">if</span> op == ws.OpText &#123;</span><br><span class="line"><span class="keyword">if</span> rs := route.MatchRequestSpec(msg); rs == <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> route.GlobalWsRouter.DefaultHandler().ServeWebsocket(<span class="string">"/"</span>, msg, c, op)</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="keyword">return</span> route.GlobalWsRouter.MatchHandler(rs.Path).ServeWebsocket(rs.Path, rs.Params, c, op)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> gnet.None</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里，我们可以看到，当存在新链接进来的啥时候，首先经过<code>OnOpen(c gnet.Conn)</code>方法，这个时候，我们会在<code>gnet.Conn</code>中设置一个我们用户的一个<code>上下文环境Context</code>，在这个Context下，我们为每个连接都初始化了<code>codec</code>的结构体对象，当开始接收数据的时候，触发到了<code>OnTraffic(c gnet.Conn)</code>方法，这个以后，我们需要把网络层接收到的数据拿出来，由于<code>流</code>的存在，使得我们无法重复在同一个连接中，多次重复获取流，所以如果后面需要用到的话，利用取出来的<code>byte-buffer</code>生成一个新的<code>流</code>，以供后续使用。</p><p>所以你会发现有一段代码为:</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">buf, err := c.Next(<span class="number">-1</span>)</span><br><span class="line">buff = <span class="built_in">make</span>([]<span class="keyword">byte</span>, <span class="built_in">len</span>(buf))</span><br><span class="line"><span class="built_in">copy</span>(buff, buf)</span><br><span class="line">buffer = bytes.NewBuffer(buff)</span><br></pre></td></tr></table></figure><p>接下来，需要做的事情就是解析数据为http协议对象，由于我这里的<code>端口复用</code>的逻辑是<code>http+webocket</code>复用，所以都是基于<code>http协议</code>的，所以这里可以简单粗暴的处理，然后通过判断<code>http协议</code>中是否包含了需要升级为<code>webocket协议</code>的关键字段<code>Upgrade:webocket</code>，如果包含，则表示本次请求是一个websocket连接，否则就是一个单纯http连接。以此来达到复用的需求。</p><p>在这个基础之上，我们也更新了当前连接的<code>上下文环境Context</code>，升级为了<code>httpCodec</code>和<code>wsCodec</code>，通过<code>goto+断言</code>语法，我们可以进入到，我们所需要进入的逻辑阶段。不要觉得这就完事了，麻烦的事情才刚开始，现在你只是知道了开头。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">buf := bufio.NewReader(buffer)</span><br><span class="line">req, err := http.ReadRequest(buf)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">    log.Errorlog(log.NetServerErrorCategory&#123;Summary: fmt.Sprintf(<span class="string">"request from http error: %v"</span>, err)&#125;)</span><br><span class="line">    <span class="keyword">return</span> gnet.Close</span><br><span class="line">&#125;</span><br><span class="line">metrics.TotalConnectedCounter.WithLabelValues(HttpApplicationLayerProto.String()).Inc()</span><br><span class="line">resp := route.NewResponse(c)</span><br><span class="line">h, _ := serv.serverMux.Handler(req)</span><br><span class="line">h.ServeHTTP(resp, req)</span><br><span class="line"><span class="keyword">if</span> _, err = resp.Close(); err != <span class="literal">nil</span> &#123;</span><br><span class="line">    log.Errorlog(log.NetServerErrorCategory&#123;Summary: fmt.Sprintf(<span class="string">"write to http error: %v"</span>, err)&#125;)</span><br><span class="line">    <span class="keyword">return</span> gnet.Close</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> gnet.Close</span><br></pre></td></tr></table></figure><p>如果是<code>http协议</code>，那么我们就不需要升级协议了。但是有一个问题就是，在golang的<code>http/server.go</code>中，我们所熟悉的接口</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// A Handler responds to an HTTP request.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// ServeHTTP should write reply headers and data to the ResponseWriter</span></span><br><span class="line"><span class="comment">// and then return. Returning signals that the request is finished; it</span></span><br><span class="line"><span class="comment">// is not valid to use the ResponseWriter or read from the</span></span><br><span class="line"><span class="comment">// Request.Body after or concurrently with the completion of the</span></span><br><span class="line"><span class="comment">// ServeHTTP call.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// Depending on the HTTP client software, HTTP protocol version, and</span></span><br><span class="line"><span class="comment">// any intermediaries between the client and the Go server, it may not</span></span><br><span class="line"><span class="comment">// be possible to read from the Request.Body after writing to the</span></span><br><span class="line"><span class="comment">// ResponseWriter. Cautious handlers should read the Request.Body</span></span><br><span class="line"><span class="comment">// first, and then reply.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// Except for reading the body, handlers should not modify the</span></span><br><span class="line"><span class="comment">// provided Request.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// If ServeHTTP panics, the server (the caller of ServeHTTP) assumes</span></span><br><span class="line"><span class="comment">// that the effect of the panic was isolated to the active request.</span></span><br><span class="line"><span class="comment">// It recovers the panic, logs a stack trace to the server error log,</span></span><br><span class="line"><span class="comment">// and either closes the network connection or sends an HTTP/2</span></span><br><span class="line"><span class="comment">// RST_STREAM, depending on the HTTP protocol. To abort a handler so</span></span><br><span class="line"><span class="comment">// the client sees an interrupted response but the server doesn't log</span></span><br><span class="line"><span class="comment">// an error, panic with the value ErrAbortHandler.</span></span><br><span class="line"><span class="keyword">type</span> Handler <span class="keyword">interface</span> &#123;</span><br><span class="line">ServeHTTP(ResponseWriter, *Request)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们看到这个<code>Handler</code>interface，需要实现<code>ServeHTTP(ResponseWriter, *Request)</code>，而这个<code>Request</code>，对于我们目前来是，是不存在的，所以我们需要想办法构造一个<code>Request</code>对象出来。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ReadRequest reads and parses an incoming request from b.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// ReadRequest is a low-level function and should only be used for</span></span><br><span class="line"><span class="comment">// specialized applications; most code should use the Server to read</span></span><br><span class="line"><span class="comment">// requests and handle them via the Handler interface. ReadRequest</span></span><br><span class="line"><span class="comment">// only supports HTTP/1.x requests. For HTTP/2, use golang.org/x/net/http2.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">ReadRequest</span><span class="params">(b *bufio.Reader)</span> <span class="params">(*Request, error)</span></span> &#123;</span><br><span class="line"><span class="keyword">return</span> readRequest(b, deleteHostHeader)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>好在标准包中提供一个<code>ReadRequest(b *bufio.Reader) (*Request, error)</code>的方法，可以通过<code>bufio.Reader</code>去读取<code>http协议</code>，然后构造出我们所需要的<code>Request</code>对象，所以你会看到，我们在一开始<code>copy(buff, buf)</code>的意义就体现在此了。<br>还会那句话，因为这是一个<code>流</code>，无法重复读取，所以我们利用<code>[]byte</code>构造一个全新的可度的字节流。</p><p>解决了<code>Request</code>的问题之后，另外一个问题也来了，<code>ResponseWriter</code>是一个和Response相关可写的字节流。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// A ResponseWriter interface is used by an HTTP handler to</span></span><br><span class="line"><span class="comment">// construct an HTTP response.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// A ResponseWriter may not be used after the Handler.ServeHTTP method</span></span><br><span class="line"><span class="comment">// has returned.</span></span><br><span class="line"><span class="keyword">type</span> ResponseWriter <span class="keyword">interface</span> &#123;</span><br><span class="line"><span class="comment">// Header returns the header map that will be sent by</span></span><br><span class="line"><span class="comment">// WriteHeader. The Header map also is the mechanism with which</span></span><br><span class="line"><span class="comment">// Handlers can set HTTP trailers.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// Changing the header map after a call to WriteHeader (or</span></span><br><span class="line"><span class="comment">// Write) has no effect unless the modified headers are</span></span><br><span class="line"><span class="comment">// trailers.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// There are two ways to set Trailers. The preferred way is to</span></span><br><span class="line"><span class="comment">// predeclare in the headers which trailers you will later</span></span><br><span class="line"><span class="comment">// send by setting the "Trailer" header to the names of the</span></span><br><span class="line"><span class="comment">// trailer keys which will come later. In this case, those</span></span><br><span class="line"><span class="comment">// keys of the Header map are treated as if they were</span></span><br><span class="line"><span class="comment">// trailers. See the example. The second way, for trailer</span></span><br><span class="line"><span class="comment">// keys not known to the Handler until after the first Write,</span></span><br><span class="line"><span class="comment">// is to prefix the Header map keys with the TrailerPrefix</span></span><br><span class="line"><span class="comment">// constant value. See TrailerPrefix.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// To suppress automatic response headers (such as "Date"), set</span></span><br><span class="line"><span class="comment">// their value to nil.</span></span><br><span class="line">Header() Header</span><br><span class="line"></span><br><span class="line"><span class="comment">// Write writes the data to the connection as part of an HTTP reply.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// If WriteHeader has not yet been called, Write calls</span></span><br><span class="line"><span class="comment">// WriteHeader(http.StatusOK) before writing the data. If the Header</span></span><br><span class="line"><span class="comment">// does not contain a Content-Type line, Write adds a Content-Type set</span></span><br><span class="line"><span class="comment">// to the result of passing the initial 512 bytes of written data to</span></span><br><span class="line"><span class="comment">// DetectContentType. Additionally, if the total size of all written</span></span><br><span class="line"><span class="comment">// data is under a few KB and there are no Flush calls, the</span></span><br><span class="line"><span class="comment">// Content-Length header is added automatically.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// Depending on the HTTP protocol version and the client, calling</span></span><br><span class="line"><span class="comment">// Write or WriteHeader may prevent future reads on the</span></span><br><span class="line"><span class="comment">// Request.Body. For HTTP/1.x requests, handlers should read any</span></span><br><span class="line"><span class="comment">// needed request body data before writing the response. Once the</span></span><br><span class="line"><span class="comment">// headers have been flushed (due to either an explicit Flusher.Flush</span></span><br><span class="line"><span class="comment">// call or writing enough data to trigger a flush), the request body</span></span><br><span class="line"><span class="comment">// may be unavailable. For HTTP/2 requests, the Go HTTP server permits</span></span><br><span class="line"><span class="comment">// handlers to continue to read the request body while concurrently</span></span><br><span class="line"><span class="comment">// writing the response. However, such behavior may not be supported</span></span><br><span class="line"><span class="comment">// by all HTTP/2 clients. Handlers should read before writing if</span></span><br><span class="line"><span class="comment">// possible to maximize compatibility.</span></span><br><span class="line">Write([]<span class="keyword">byte</span>) (<span class="keyword">int</span>, error)</span><br><span class="line"></span><br><span class="line"><span class="comment">// WriteHeader sends an HTTP response header with the provided</span></span><br><span class="line"><span class="comment">// status code.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// If WriteHeader is not called explicitly, the first call to Write</span></span><br><span class="line"><span class="comment">// will trigger an implicit WriteHeader(http.StatusOK).</span></span><br><span class="line"><span class="comment">// Thus explicit calls to WriteHeader are mainly used to</span></span><br><span class="line"><span class="comment">// send error codes.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// The provided code must be a valid HTTP 1xx-5xx status code.</span></span><br><span class="line"><span class="comment">// Only one header may be written. Go does not currently</span></span><br><span class="line"><span class="comment">// support sending user-defined 1xx informational headers,</span></span><br><span class="line"><span class="comment">// with the exception of 100-continue response header that the</span></span><br><span class="line"><span class="comment">// Server sends automatically when the Request.Body is read.</span></span><br><span class="line">WriteHeader(statusCode <span class="keyword">int</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>秉着面向接口开发的原则，并且为了更好的兼容第三方的API，所以我们需要实现一个自己的<code>ResponseWriter</code>对象，于是就有了<code>route.NewResponse(c)</code>，这个<code>resp</code>实现了上述的接口.</p><p>兼容了<code>promhttp</code>提供的<code>Handler</code>，也兼容了自己的<code>helloworld</code>接口。</p><p>接着我们通过<code>cmux</code>进行一个路由匹配，然后调用到对应的<code>ServeHTTP</code>,处理完逻辑之后，在<code>resp</code>的<code>Close()</code>阶段，把缓存区的所有<code>[]byte</code>，推送到连接层，然后通过返回<code>gnet.Close</code>进行网络层的断开，至此，一个简单而完整的<code>http交互流程</code>完毕。</p><p>对于<code>Websocket</code>协议来说，要做的事情也是十分繁琐（由于用了开源协议库，相对简化了很多），请先看下面的应用层协议处理逻辑。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> !cdc.connected &#123;</span><br><span class="line">        wcb := &amp;wsConnBridge&#123;</span><br><span class="line">            buff: buffer,</span><br><span class="line">            c:    c,</span><br><span class="line">        &#125;</span><br><span class="line">        _, err := ws.Upgrade(wcb)</span><br><span class="line">        <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">            log.Errorlog(log.NetServerErrorCategory&#123;Summary: fmt.Sprintf(<span class="string">"upgrade[%s] to websocket error: %v"</span>, c.RemoteAddr().String(), err)&#125;)</span><br><span class="line">        &#125;</span><br><span class="line">        log.Debugf(log.NetServerDebugCategory&#123;&#125;, <span class="string">"conn[%v] upgrade websocket protocol"</span>, c.RemoteAddr().String())</span><br><span class="line">        cdc.connected = <span class="literal">true</span></span><br><span class="line">        metrics.ConnectedGauge.Inc()</span><br><span class="line">        metrics.TotalConnectedCounter.WithLabelValues(WebsocketApplicationLayerProto.String()).Inc()</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        msg, op, err := wsutil.ReadClientData(c)</span><br><span class="line">        <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> _, ok := err.(wsutil.ClosedError); !ok &#123;</span><br><span class="line">                log.Errorlog(log.NetServerErrorCategory&#123;Summary: fmt.Sprintf(<span class="string">"[%s] receive ws message error: %v"</span>, c.RemoteAddr().String(), err)&#125;)</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> gnet.Close</span><br><span class="line">        &#125;</span><br><span class="line">        log.Debugf(log.NetServerDebugCategory&#123;&#125;, <span class="string">"conn[%v] receive [op=%v] [msg=%v]"</span>, c.RemoteAddr().String(), op, <span class="keyword">string</span>(msg))</span><br><span class="line">        <span class="keyword">if</span> op == ws.OpText &#123;</span><br><span class="line">            <span class="keyword">if</span> rs := route.MatchRequestSpec(msg); rs == <span class="literal">nil</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> route.GlobalWsRouter.DefaultHandler().ServeWebsocket(<span class="string">"/"</span>, msg, c, op)</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> route.GlobalWsRouter.MatchHandler(rs.Path).ServeWebsocket(rs.Path, rs.Params, c, op)</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>升级协议的过程中，我们用到了<code>github.com/gobwas/ws</code>这个协议库。</p><p>我们在接受到<code>websocket</code>前的时候需要先升级为websocket协议，但是这里遇到了一个问题，还是同理，我们的<code>gnet.Conn</code>的数据已经被我们取出来了，而升级的API显然就是需要提供一个可读可写的IO。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Upgrade is like Upgrader&#123;&#125;.Upgrade().</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">Upgrade</span><span class="params">(conn io.ReadWriter)</span> <span class="params">(Handshake, error)</span></span> &#123;</span><br><span class="line"><span class="keyword">return</span> DefaultUpgrader.Upgrade(conn)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ReadWriter is the interface that groups the basic Read and Write methods.</span></span><br><span class="line"><span class="keyword">type</span> ReadWriter <span class="keyword">interface</span> &#123;</span><br><span class="line">Reader</span><br><span class="line">Writer</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> Reader <span class="keyword">interface</span> &#123;</span><br><span class="line">Read(p []<span class="keyword">byte</span>) (n <span class="keyword">int</span>, err error)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> Writer <span class="keyword">interface</span> &#123;</span><br><span class="line">Write(p []<span class="keyword">byte</span>) (n <span class="keyword">int</span>, err error)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>因此，我们又需要实现一个自己的<code>wsConnBridge</code>对象，主要是实现上述的接口，但是这个结构体相对来说就比较简单了，分别保存之前提出来的<code>[]byte</code>的buffer用于读行为，再保存一个<code>gnet.Conn</code>用于写行为即可。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> wsConnBridge <span class="keyword">struct</span> &#123;</span><br><span class="line">buff *bytes.Buffer</span><br><span class="line">c    gnet.Conn</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(w *wsConnBridge)</span> <span class="title">Read</span><span class="params">(p []<span class="keyword">byte</span>)</span> <span class="params">(n <span class="keyword">int</span>, err error)</span></span> &#123;</span><br><span class="line"><span class="keyword">return</span> w.buff.Read(p)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(w *wsConnBridge)</span> <span class="title">Write</span><span class="params">(p []<span class="keyword">byte</span>)</span> <span class="params">(n <span class="keyword">int</span>, err error)</span></span> &#123;</span><br><span class="line"><span class="keyword">return</span> w.c.Write(p)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>升级完了，我们需要给当前的<code>上下文环境的Context</code>标记为已经升级连接完毕。</p><p>然后就是进入到数据的收发环节了。</p><p><code>github.com/gobwas/ws</code>提供了<code>api</code>来进行数据的收发，分别有<code>high-level</code>和<code>low-level</code>，这里，我们可优先选择<code>high-level-api</code>，然后读取数据。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> WebsocketHandler <span class="keyword">interface</span> &#123;</span><br><span class="line">ServeWebsocket(path <span class="keyword">string</span>, data []<span class="keyword">byte</span>, w io.Writer, op ws.OpCode) gnet.Action</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>读取到数据之后，又因为我需要和http的route能有一个高度匹配的代码写法，所以在路由匹配上，也是做了一个类似的<code>Match</code>的行为，然后选择到对应的<code>Handler</code>，触发统一的<code>ServeWebsocket()</code>接口（为了和http的<code>ServeHttp()</code>对应）。</p><p>到此，从<code>网络层到应用层</code>的<code>端口复用实现多协议</code>原理就到此为止了。</p><p>接着就是处理自己的业务逻辑数据了。</p><h2 id="业务逻辑概述"><a href="#业务逻辑概述" class="headerlink" title="业务逻辑概述"></a>业务逻辑概述</h2><ol><li>记录客服需要监控的数据规则和连接关联</li><li>kafka-client从监控规则中匹配合适的数据，推送到对应的fd中 </li></ol><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ...</span></span><br><span class="line"><span class="keyword">var</span> i <span class="keyword">int64</span> = <span class="number">0</span></span><br><span class="line"><span class="keyword">var</span> wg sync.WaitGroup</span><br><span class="line">ListenChatRuleMap.Range(<span class="function"><span class="keyword">func</span><span class="params">(key, value <span class="keyword">interface</span>&#123;&#125;)</span> <span class="title">bool</span></span> &#123;</span><br><span class="line">    <span class="keyword">if</span> Match(key.(<span class="keyword">string</span>), kmsKey) &#123;</span><br><span class="line">        wg.Add(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">(c gnet.Conn, wsp *WsSendPayload)</span></span> &#123;</span><br><span class="line">            <span class="keyword">defer</span> wg.Done()</span><br><span class="line">            err := wsutil.WriteServerMessage(c, ws.OpText, wsp.Json())</span><br><span class="line">            <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">                log.Errorf(log.AppErrorCategory&#123;Summary: fmt.Sprintf(<span class="string">"[wsWriteServerMessage failed] [err=%v]"</span>, err)&#125;, <span class="string">"[key=%s],[data=%s]"</span>, key.(<span class="keyword">string</span>), <span class="keyword">string</span>(wsp.Json()))</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            &#125;</span><br><span class="line">            atomic.AddInt64(&amp;i, <span class="number">1</span>)</span><br><span class="line">        &#125;(value.(gnet.Conn), wsp)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span></span><br><span class="line">&#125;)</span><br><span class="line">wg.Wait()</span><br><span class="line">metrics.ChatLogCounterClientHistogram.WithLabelValues(strconv.FormatUint(<span class="keyword">uint64</span>(lrc.Pid), <span class="number">10</span>), strconv.Itoa(wsp.ServerId), strconv.Itoa(wsp.AgentId)).Observe(<span class="keyword">float64</span>(atomic.LoadInt64(&amp;i)))</span><br><span class="line"><span class="comment">// ...</span></span><br></pre></td></tr></table></figure><p>至此，网络层和业务层的所有需求大体已经完毕了。</p><h2 id="prometheus-指标"><a href="#prometheus-指标" class="headerlink" title="prometheus 指标"></a>prometheus 指标</h2><p>部分的指标如下，后续可以通过一些指标对服务的稳定和可靠性进行优化升级处理。</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"># HELP chat_monitor_app_handle_chat_total Counter of handle.</span><br><span class="line"># TYPE chat_monitor_app_handle_chat_total counter</span><br><span class="line">chat_monitor_app_handle_chat_total&#123;agent_id="29",app_id="19",server_id="6558"&#125; 3</span><br><span class="line"># HELP chat_monitor_net_client_recv_counter number of chat log for client</span><br><span class="line"># TYPE chat_monitor_net_client_recv_counter histogram</span><br><span class="line">chat_monitor_net_client_recv_counter_bucket&#123;agent_id="29",pid="1643890670000002",server_id="6558",le="1"&#125; 0</span><br><span class="line">chat_monitor_net_client_recv_counter_bucket&#123;agent_id="29",pid="1643890670000002",server_id="6558",le="2"&#125; 0</span><br><span class="line">chat_monitor_net_client_recv_counter_bucket&#123;agent_id="29",pid="1643890670000002",server_id="6558",le="4"&#125; 2</span><br><span class="line">chat_monitor_net_client_recv_counter_bucket&#123;agent_id="29",pid="1643890670000002",server_id="6558",le="8"&#125; 3</span><br><span class="line">chat_monitor_net_client_recv_counter_bucket&#123;agent_id="29",pid="1643890670000002",server_id="6558",le="16"&#125; 3</span><br><span class="line">chat_monitor_net_client_recv_counter_bucket&#123;agent_id="29",pid="1643890670000002",server_id="6558",le="32"&#125; 3</span><br><span class="line">chat_monitor_net_client_recv_counter_bucket&#123;agent_id="29",pid="1643890670000002",server_id="6558",le="64"&#125; 3</span><br><span class="line">chat_monitor_net_client_recv_counter_bucket&#123;agent_id="29",pid="1643890670000002",server_id="6558",le="+Inf"&#125; 3</span><br><span class="line">chat_monitor_net_client_recv_counter_sum&#123;agent_id="29",pid="1643890670000002",server_id="6558"&#125; 12</span><br><span class="line">chat_monitor_net_client_recv_counter_count&#123;agent_id="29",pid="1643890670000002",server_id="6558"&#125; 3</span><br><span class="line"># HELP chat_monitor_net_current_connected Current Counter Gauge of ws-connected.</span><br><span class="line"># TYPE chat_monitor_net_current_connected gauge</span><br><span class="line">chat_monitor_net_current_connected 4</span><br><span class="line"># HELP chat_monitor_net_total_connected The Total Counter of connected.</span><br><span class="line"># TYPE chat_monitor_net_total_connected counter</span><br><span class="line">chat_monitor_net_total_connected&#123;type="http"&#125; 15</span><br><span class="line">chat_monitor_net_total_connected&#123;type="websocket"&#125; 5</span><br><span class="line"># HELP chat_monitor_server_error_total Counter of error.</span><br><span class="line"># TYPE chat_monitor_server_error_total counter</span><br><span class="line">chat_monitor_server_error_total&#123;type="network_server_error"&#125; 1</span><br><span class="line"># HELP chat_monitor_server_gogc The value of GOGC</span><br><span class="line"># TYPE chat_monitor_server_gogc gauge</span><br><span class="line">chat_monitor_server_gogc 100</span><br><span class="line"># HELP chat_monitor_server_info Indicate the chat_monitor server info, and the value is the start timestamp (s).</span><br><span class="line"># TYPE chat_monitor_server_info gauge</span><br><span class="line">chat_monitor_server_info 1.644568978e+09</span><br><span class="line"># HELP chat_monitor_server_maxprocs The value of GOMAXPROCS.</span><br><span class="line"># TYPE chat_monitor_server_maxprocs gauge</span><br><span class="line">chat_monitor_server_maxprocs 6</span><br></pre></td></tr></table></figure><p>到这里，一些基础而核心的逻辑也介绍完了。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;最近，公司以前有一些旧的服务，由于各种原因，导致各种问题，并且架构设计行也不是那么友好和不利于维护。&lt;br&gt;所以准备重构设计一些服务。&lt;/p&gt;
&lt;p&gt;在游戏公司中，GM客服的其中一个职能就是监督舆论，从玩家平日的聊天中进行监控。&lt;/p&gt;
&lt;p&gt;我们从&lt;code&gt;业务需求&lt;/code&gt;+&lt;code&gt;技术架构&lt;/code&gt;层面进行整理。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Golang" scheme="http://blog.crazylaw.cn/categories/Golang/"/>
    
    
      <category term="Golang" scheme="http://blog.crazylaw.cn/tags/Golang/"/>
    
  </entry>
  
  <entry>
    <title>TIDB源码剖析（一）</title>
    <link href="http://blog.crazylaw.cn/2022/01/24/TIDB/TIDB%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>http://blog.crazylaw.cn/2022/01/24/TIDB/TIDB%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90%EF%BC%88%E4%B8%80%EF%BC%89/</id>
    <published>2022-01-24T02:28:33.000Z</published>
    <updated>2022-01-25T03:01:50.165Z</updated>
    
    <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>这一章，作为我们的起始章节，跟着源码，我们一步步来熟悉TIDB的整体代码结构</p><hr><a id="more"></a><h2 id="Select"><a href="#Select" class="headerlink" title="Select"></a>Select</h2><p>当我们有一条基本的sql如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> mysql.user;</span><br></pre></td></tr></table></figure><p>我们从接收到客户端连接开始，<code>执行</code>，<code>解析</code>，<code>逻辑优化器</code>，<code>物理优化器</code>，到<code>最终结果</code>开始分析。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">github.com&#x2F;pingcap&#x2F;tidb&#x2F;planner.optimize at optimize.go:335</span><br><span class="line">github.com&#x2F;pingcap&#x2F;tidb&#x2F;planner.Optimize at optimize.go:211</span><br><span class="line">github.com&#x2F;pingcap&#x2F;tidb&#x2F;executor.(*Compiler).Compile at compiler.go:77</span><br><span class="line">github.com&#x2F;pingcap&#x2F;tidb&#x2F;session.(*session).ExecuteStmt at session.go:1696</span><br><span class="line">github.com&#x2F;pingcap&#x2F;tidb&#x2F;server.(*TiDBContext).ExecuteStmt at driver_tidb.go:220</span><br><span class="line">github.com&#x2F;pingcap&#x2F;tidb&#x2F;server.(*clientConn).handleStmt at conn.go:1977</span><br><span class="line">github.com&#x2F;pingcap&#x2F;tidb&#x2F;server.(*clientConn).handleQuery at conn.go:1846</span><br><span class="line">github.com&#x2F;pingcap&#x2F;tidb&#x2F;server.(*clientConn).dispatch at conn.go:1341</span><br><span class="line">github.com&#x2F;pingcap&#x2F;tidb&#x2F;server.(*clientConn).Run at conn.go:1091</span><br><span class="line">github.com&#x2F;pingcap&#x2F;tidb&#x2F;server.(*Server).onConn at server.go:556</span><br><span class="line">runtime.goexit at asm_amd64.s:1371</span><br><span class="line"> - Async stack trace</span><br><span class="line">github.com&#x2F;pingcap&#x2F;tidb&#x2F;server.(*Server).startNetworkListener at server.go:453</span><br></pre></td></tr></table></figure><p>上面这是一个基本的执行流程，我们跟着这一段堆栈来进行分析。</p><h2 id="github-com-pingcap-tidb-server-Server-onConn-at-server-go-连接处理逻辑"><a href="#github-com-pingcap-tidb-server-Server-onConn-at-server-go-连接处理逻辑" class="headerlink" title="github.com/pingcap/tidb/server.(*Server).onConn at server.go (连接处理逻辑)"></a>github.com/pingcap/tidb/server.(*Server).onConn at server.go (连接处理逻辑)</h2><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conn.Run(ctx)</span><br></pre></td></tr></table></figure><p>这里，我们看到了这是进入到了一个<code>clientConn</code>的 <code>Run</code> 方法。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Run reads client query and writes query result to client in for loop, if there is a panic during query handling,</span></span><br><span class="line"><span class="comment">// it will be recovered and log the panic error.</span></span><br><span class="line"><span class="comment">// This function returns and the connection is closed if there is an IO error or there is a panic.</span></span><br><span class="line"><span class="comment">// 在for循环中，执行读取客户端查询，并将查询结果写入客户端，如果在处理查询时出现panic，</span></span><br><span class="line"><span class="comment">// 它将被恢复并记录panic错误。</span></span><br><span class="line"><span class="comment">// 如果出现IO错误或panic，该函数返回并关闭连接。</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(cc *clientConn)</span> <span class="title">Run</span><span class="params">(ctx context.Context)</span></span></span><br></pre></td></tr></table></figure><p>这里我们看到了有一段文字帮助我们理解注意事项。</p><p>我们按照过程式的顺序来从上往下看源码</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">   <span class="keyword">const</span> size = <span class="number">4096</span></span><br><span class="line"><span class="keyword">defer</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">r := <span class="built_in">recover</span>()</span><br><span class="line"><span class="keyword">if</span> r != <span class="literal">nil</span> &#123;</span><br><span class="line">buf := <span class="built_in">make</span>([]<span class="keyword">byte</span>, size)</span><br><span class="line">stackSize := runtime.Stack(buf, <span class="literal">false</span>)</span><br><span class="line">buf = buf[:stackSize]</span><br><span class="line">logutil.Logger(ctx).Error(<span class="string">"connection running loop panic"</span>,</span><br><span class="line">zap.Stringer(<span class="string">"lastSQL"</span>, getLastStmtInConn&#123;cc&#125;),</span><br><span class="line">zap.String(<span class="string">"err"</span>, fmt.Sprintf(<span class="string">"%v"</span>, r)),</span><br><span class="line">zap.String(<span class="string">"stack"</span>, <span class="keyword">string</span>(buf)),</span><br><span class="line">)</span><br><span class="line">err := cc.writeError(ctx, errors.New(fmt.Sprintf(<span class="string">"%v"</span>, r)))</span><br><span class="line">terror.Log(err)</span><br><span class="line">metrics.PanicCounter.WithLabelValues(metrics.LabelSession).Inc()</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> atomic.LoadInt32(&amp;cc.status) != connStatusShutdown &#123;</span><br><span class="line">err := cc.Close()</span><br><span class="line">terror.Log(err)</span><br><span class="line">&#125;</span><br><span class="line">&#125;()</span><br></pre></td></tr></table></figure><p>这段代码，我们看到了几点。</p><ul><li>通过 <code>recover()</code> 方法来阻止<code>panic</code>引起的程序异常崩溃，如果是panic的话，那么将会有一段特殊的逻辑处理<br>  1.1 通过 <code>runtime.Stack(buf,false)</code> 的第二个参数来控制只获取当前协程下的堆栈信息，并且写入到<code>buf</code>变量中<br>  1.2 由于 <code>const size = 4096</code> 的原因，我们拿到的buf未必是那么多，因此，通过 <code>buf[:stackSize]</code> 来进行切片处理，把变量的指针重新指向新的数据区域<br>  1.3 通过日志组件来记录详细信息， 有意思的是，这里通过了<code>getLastStmtInConn结构体</code>里面的<code>String()</code>方法来进行序列化自己想要的内容信息，其他的就是基本的<code>err</code>, <code>stack</code>的信息了<br>  1.4 我们不单单需要在服务器上记录信息，还要把对应的用户错误信息也记录下来并且发送给客户端。所以通过了 <code>err := cc.writeError(ctx, errors.New(fmt.Sprintf(&quot;%v&quot;, r)))</code> 来实现这一点。<br>  1.5 然后就是记录相关的<code>metrics</code>，因为发生了一次 <code>panic</code>，所以需要通过<code>PanicCounter</code>记录下来，用于统计由于<code>session</code>引起的<code>panic</code>总共有多少次</li><li>如果是非panic引起的函数析构，那么还要通过原子性草走来判断状态是否为关闭状态，如果是关闭状态，那么在这里就需要把连接断开，并且记录下错误信息</li></ul><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Usually, client connection status changes between [dispatching] &lt;=&gt; [reading].</span></span><br><span class="line"><span class="comment">// When some event happens, server may notify this client connection by setting</span></span><br><span class="line"><span class="comment">// the status to special values, for example: kill or graceful shutdown.</span></span><br><span class="line"><span class="comment">// The client connection would detect the events when it fails to change status</span></span><br><span class="line"><span class="comment">// by CAS operation, it would then take some actions accordingly.</span></span><br><span class="line"><span class="comment">// 通常情况下，客户端连接状态在[dispatching] &lt;=&gt; [reading]之间变化。</span></span><br><span class="line"><span class="comment">// 当某个事件发生时，服务器可以通过设置来通知这个客户端连接</span></span><br><span class="line"><span class="comment">// 将状态设置为特殊值，例如:kill或graceful shutdown。</span></span><br><span class="line"><span class="comment">// 当CAS操作改变状态失败时，客户端连接将检测到事件，然后采取相应的动作。</span></span><br><span class="line"><span class="keyword">for</span> &#123;</span><br><span class="line"><span class="keyword">if</span> !atomic.CompareAndSwapInt32(&amp;cc.status, connStatusDispatching, connStatusReading) ||</span><br><span class="line"><span class="comment">// The judge below will not be hit by all means,</span></span><br><span class="line"><span class="comment">// But keep it stayed as a reminder and for the code reference for connStatusWaitShutdown.</span></span><br><span class="line">atomic.LoadInt32(&amp;cc.status) == connStatusWaitShutdown &#123;</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">&#125;</span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>我们看到这是一个循环操作，并且通过原子性操作<code>atomic.CompareAndSwapInt32</code>（比较然后再交换，所以符合CAS原则，乐观锁）来判断session连接是否能是否能切换到<code>connStatusDispatching</code> =&gt; <code>connStatusReading</code> 状态</li><li>如果不可以切换，那么则结束该方法</li><li>如果连接状态为等待关闭状态，那么也结束该方法</li></ul><p>对于其中的 <code>...</code>，现在会在下面进一步说明。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">cc.alloc.Reset()</span><br><span class="line"><span class="comment">// close connection when idle time is more than wait_timeout</span></span><br><span class="line">waitTimeout := cc.getSessionVarsWaitTimeout(ctx)</span><br><span class="line">cc.pkt.setReadTimeout(time.Duration(waitTimeout) * time.Second)</span><br><span class="line">start := time.Now()</span><br><span class="line">data, err := cc.readPacket()</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">if</span> terror.ErrorNotEqual(err, io.EOF) &#123;</span><br><span class="line"><span class="keyword">if</span> netErr, isNetErr := errors.Cause(err).(net.Error); isNetErr &amp;&amp; netErr.Timeout() &#123;</span><br><span class="line">idleTime := time.Since(start)</span><br><span class="line">logutil.Logger(ctx).Info(<span class="string">"read packet timeout, close this connection"</span>,</span><br><span class="line">zap.Duration(<span class="string">"idle"</span>, idleTime),</span><br><span class="line">zap.Uint64(<span class="string">"waitTimeout"</span>, waitTimeout),</span><br><span class="line">zap.Error(err),</span><br><span class="line">)</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">errStack := errors.ErrorStack(err)</span><br><span class="line"><span class="keyword">if</span> !strings.Contains(errStack, <span class="string">"use of closed network connection"</span>) &#123;</span><br><span class="line">logutil.Logger(ctx).Warn(<span class="string">"read packet failed, close this connection"</span>,</span><br><span class="line">zap.Error(errors.SuspendStack(err)))</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">disconnectByClientWithError.Inc()</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><code>cc.alloc.Reset()</code>重置内存池大小</li><li>当空闲时间大于等待超时时间的话那么将会关闭丽连接。<code>cc.pkt.setReadTimeout(time.Duration(waitTimeout) * time.Second)</code></li><li>从客户端读取数据，如果存在错误，那么将会记录下来相关信息，例如从读取数据到最后的时间，来统计idletime，通过<code>metrics.DisconnectionCounter.WithLabelValues(metrics.LblError)</code>来记录因为err导致连接断开的次数</li></ul><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> !atomic.CompareAndSwapInt32(&amp;cc.status, connStatusReading, connStatusDispatching) &#123;</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>同理，经过cas乐观锁，把状态从 <code>connStatusReading</code> =&gt; <code>connStatusDispatching</code>如果，交换设置失败，那么就结束函数。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">startTime := time.Now()</span><br><span class="line">err = cc.dispatch(ctx, data)</span><br></pre></td></tr></table></figure><h2 id="github-com-pingcap-tidb-server-clientConn-dispatch-（分发逻辑）"><a href="#github-com-pingcap-tidb-server-clientConn-dispatch-（分发逻辑）" class="headerlink" title="github.com/pingcap/tidb/server.(*clientConn).dispatch （分发逻辑）"></a>github.com/pingcap/tidb/server.(*clientConn).dispatch （分发逻辑）</h2><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// dispatch handles client request based on command which is the first byte of the data.</span></span><br><span class="line"><span class="comment">// It also gets a token from server which is used to limit the concurrently handling clients.</span></span><br><span class="line"><span class="comment">// The most frequently used command is ComQuery.</span></span><br><span class="line"><span class="comment">// dispatch根据命令处理客户端请求，命令是数据的第一个字节。</span></span><br><span class="line"><span class="comment">// 它也从服务器获取一个令牌，用于限制并发处理客户端。</span></span><br><span class="line"><span class="comment">// 最常用的命令是ComQuery。</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(cc *clientConn)</span> <span class="title">dispatch</span><span class="params">(ctx context.Context, data []<span class="keyword">byte</span>)</span> <span class="title">error</span></span></span><br></pre></td></tr></table></figure><p>下面的方法都是dispatch的过程顺序逻辑</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">defer</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line"><span class="comment">// reset killed for each request</span></span><br><span class="line">atomic.StoreUint32(&amp;cc.ctx.GetSessionVars().Killed, <span class="number">0</span>)</span><br><span class="line">&#125;()</span><br><span class="line">t := time.Now()</span><br><span class="line"><span class="keyword">if</span> (cc.ctx.Status() &amp; mysql.ServerStatusInTrans) &gt; <span class="number">0</span> &#123;</span><br><span class="line">connIdleDurationHistogramInTxn.Observe(t.Sub(cc.lastActive).Seconds())</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">connIdleDurationHistogramNotInTxn.Observe(t.Sub(cc.lastActive).Seconds())</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>这里可以看到这里有一个defer，当函数结束的时候，会重置session的Killed次数</li><li><code>cc.ctx.Status() &amp; mysql.ServerStatusInTrans</code> 这里因为兼容了mysql的无状态协议，所以通过第一个<code>位运算</code>来判断当前状态<ol><li>如果当前链接处于一个<code>事务</code>状态下的话，那么通过<code>connIdleDurationHistogramInTxn.Observe(t.Sub(cc.lastActive).Seconds())</code> 用直方图监控从最后一次活跃时间到当前分发时间</li><li>否则则用另一个<code>metrics</code>来记录</li></ol></li></ul><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">span := opentracing.StartSpan(<span class="string">"server.dispatch"</span>)</span><br><span class="line">cfg := config.GetGlobalConfig()</span><br><span class="line"><span class="keyword">if</span> cfg.OpenTracing.Enable &#123;</span><br><span class="line">ctx = opentracing.ContextWithSpan(ctx, span)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> cancelFunc context.CancelFunc</span><br><span class="line">ctx, cancelFunc = context.WithCancel(ctx)</span><br><span class="line">cc.mu.Lock()</span><br><span class="line">cc.mu.cancelFunc = cancelFunc</span><br><span class="line">cc.mu.Unlock()</span><br></pre></td></tr></table></figure><ul><li>通过<code>opentracing</code>来开始进行<code>分布式追踪</code>，<code>cc.mu</code> 主要是用来在<code>事务</code>中取消事务用的。</li></ul><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">cc.lastPacket = data</span><br><span class="line">cmd := data[<span class="number">0</span>]</span><br><span class="line">data = data[<span class="number">1</span>:]</span><br><span class="line"><span class="keyword">if</span> topsqlstate.TopSQLEnabled() &#123;</span><br><span class="line"><span class="keyword">defer</span> pprof.SetGoroutineLabels(ctx)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> variable.EnablePProfSQLCPU.Load() &#123;</span><br><span class="line">label := getLastStmtInConn&#123;cc&#125;.PProfLabel()</span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(label) &gt; <span class="number">0</span> &#123;</span><br><span class="line"><span class="keyword">defer</span> pprof.SetGoroutineLabels(ctx)</span><br><span class="line">ctx = pprof.WithLabels(ctx, pprof.Labels(<span class="string">"sql"</span>, label))</span><br><span class="line">pprof.SetGoroutineLabels(ctx)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>把当前session接收到的数据记录在<code>lastPakcet</code>中</li><li><code>第一个字节</code>代表<code>命令</code></li><li><code>后面的字节</code>代表<code>数据</code></li></ul><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">token := cc.server.getToken()</span><br><span class="line"><span class="keyword">defer</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line"><span class="comment">// if handleChangeUser failed, cc.ctx may be nil</span></span><br><span class="line"><span class="keyword">if</span> cc.ctx != <span class="literal">nil</span> &#123;</span><br><span class="line">cc.ctx.SetProcessInfo(<span class="string">""</span>, t, mysql.ComSleep, <span class="number">0</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">cc.server.releaseToken(token)</span><br><span class="line">span.Finish()</span><br><span class="line">cc.lastActive = time.Now()</span><br><span class="line">&#125;()</span><br></pre></td></tr></table></figure><p>这里需要关注一下<code>defer</code>里面的内容</p><ul><li>根据mysql协议，当命令为<code>mysql.ComSleep</code>的时候，代表execute已经完成了。所以当结束的时候，需要设置一下这个<code>ProcessInfo</code></li><li>然后释放本次token，并且span也需要标记为完成</li><li>更新最后一次活跃时间</li></ul><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">vars := cc.ctx.GetSessionVars()</span><br><span class="line"><span class="comment">// reset killed for each request</span></span><br><span class="line">atomic.StoreUint32(&amp;vars.Killed, <span class="number">0</span>)</span><br><span class="line"><span class="keyword">if</span> cmd &lt; mysql.ComEnd &#123;</span><br><span class="line">cc.ctx.SetCommandValue(cmd)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>获取当前session的变量</li><li>重置其中的killed属性</li><li>如果<code>cmd</code>在范围内的，更新当前命令的值</li></ul><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">dataStr := <span class="keyword">string</span>(hack.String(data))</span><br><span class="line"><span class="keyword">switch</span> cmd &#123;</span><br><span class="line"><span class="keyword">case</span> mysql.ComPing, mysql.ComStmtClose, mysql.ComStmtSendLongData, mysql.ComStmtReset,</span><br><span class="line">mysql.ComSetOption, mysql.ComChangeUser:</span><br><span class="line">cc.ctx.SetProcessInfo(<span class="string">""</span>, t, cmd, <span class="number">0</span>)</span><br><span class="line"><span class="keyword">case</span> mysql.ComInitDB:</span><br><span class="line">cc.ctx.SetProcessInfo(<span class="string">"use "</span>+dataStr, t, cmd, <span class="number">0</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>这里利用了golang种的<code>hack（黑科技）</code>的方式来把<code>byte</code>转换成<code>string</code>，其实主要就是因为底层用的都有一样的结构体，所以可以直接通过<code>unsafe.pointer</code>来直接操作内容指针，进行<code>zero-copy</code></li><li>对cmd进行<code>processinfo</code>的处理，如果是<code>use db</code>的命令的话，则需要传递数据库</li></ul><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">switch</span> cmd &#123;</span><br><span class="line"><span class="keyword">case</span> mysql.ComSleep:</span><br><span class="line"><span class="comment">// <span class="doctag">TODO:</span> According to mysql document, this command is supposed to be used only internally.</span></span><br><span class="line"><span class="comment">// So it's just a temp fix, not sure if it's done right.</span></span><br><span class="line"><span class="comment">// Investigate this command and write test case later.</span></span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line"><span class="keyword">case</span> mysql.ComQuit:</span><br><span class="line"><span class="keyword">return</span> io.EOF</span><br><span class="line"><span class="keyword">case</span> mysql.ComInitDB:</span><br><span class="line"><span class="keyword">if</span> err := cc.useDB(ctx, dataStr); err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> err</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> cc.writeOK(ctx)</span><br><span class="line"><span class="keyword">case</span> mysql.ComQuery: <span class="comment">// Most frequently used command.</span></span><br><span class="line"><span class="comment">// For issue 1989</span></span><br><span class="line"><span class="comment">// Input payload may end with byte '\0', we didn't find related mysql document about it, but mysql</span></span><br><span class="line"><span class="comment">// implementation accept that case. So trim the last '\0' here as if the payload an EOF string.</span></span><br><span class="line"><span class="comment">// See http://dev.mysql.com/doc/internals/en/com-query.html</span></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(data) &gt; <span class="number">0</span> &amp;&amp; data[<span class="built_in">len</span>(data)<span class="number">-1</span>] == <span class="number">0</span> &#123;</span><br><span class="line">data = data[:<span class="built_in">len</span>(data)<span class="number">-1</span>]</span><br><span class="line">dataStr = <span class="keyword">string</span>(hack.String(data))</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> cc.handleQuery(ctx, dataStr)</span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里我复制了一部分，因为我们重点关注<code>mysql.ComQuery</code>命令。</p><ul><li>根据提示，我们发现因为mysql协议说明了输入载体可能以<code>\0</code>作为最后字节，所以这里一定要减去client发送的多余的最后一个字节。所以长度进行了-1操作</li><li>然后进入到<code>cc.handleQuery(ctx, dataStr)</code></li></ul><h2 id="github-com-pingcap-tidb-server-clientConn-handleQuery"><a href="#github-com-pingcap-tidb-server-clientConn-handleQuery" class="headerlink" title="github.com/pingcap/tidb/server.(*clientConn).handleQuery"></a>github.com/pingcap/tidb/server.(*clientConn).handleQuery</h2><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// handleQuery executes the sql query string and writes result set or result ok to the client.</span></span><br><span class="line"><span class="comment">// As the execution time of this function represents the performance of TiDB, we do time log and metrics here.</span></span><br><span class="line"><span class="comment">// There is a special query `load data` that does not return result, which is handled differently.</span></span><br><span class="line"><span class="comment">// Query `load stats` does not return result either.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(cc *clientConn)</span> <span class="title">handleQuery</span><span class="params">(ctx context.Context, sql <span class="keyword">string</span>)</span> <span class="params">(err error)</span></span></span><br></pre></td></tr></table></figure><p>这个方法，终于开始正式进入我们的主题了</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">defer</span> trace.StartRegion(ctx, <span class="string">"handleQuery"</span>).End()</span><br><span class="line">sc := cc.ctx.GetSessionVars().StmtCtx</span><br><span class="line">prevWarns := sc.GetWarnings()</span><br><span class="line">stmts, err := cc.ctx.Parse(ctx, sql)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> err</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(stmts) == <span class="number">0</span> &#123;</span><br><span class="line"><span class="keyword">return</span> cc.writeOK(ctx)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>defer进行了当函数结束的时候，标记<code>handleQuery</code>结束</li><li>拿到<code>statement</code>的上下文环境</li><li>从上下文中拿到所有的<code>warinning</code>警告</li><li>通过<code>cc.ctx.Parse(ctx, sql)</code>来进行解析sql，这里属于一个大的篇章，暂时不张开讲，主要涉及到的内容有<code>编译原理</code>,<code>AST-Tree</code>，<code>Yacc</code>。我们通过这里可以拿到一棵抽象语法树，实质是<code>SelectStmt</code>，内部包含了如下内容：<ol><li>dmlNode（因为select语句属于dml语句）</li><li>其他的都是常规的例如<code>FROM</code>, <code>WHERE</code>, <code>FIELDS</code>, <code>DISTINCT</code> 等等</li></ol></li><li>如果没有一个完成的抽象语法书，则直接返回响应协议和对应的内容</li></ul><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> pointPlans []plannercore.Plan</span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(stmts) &gt; <span class="number">1</span> &#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// The client gets to choose if it allows multi-statements, and</span></span><br><span class="line"><span class="comment">// probably defaults OFF. This helps prevent against SQL injection attacks</span></span><br><span class="line"><span class="comment">// by early terminating the first statement, and then running an entirely</span></span><br><span class="line"><span class="comment">// new statement.</span></span><br><span class="line"></span><br><span class="line">capabilities := cc.ctx.GetSessionVars().ClientCapability</span><br><span class="line"><span class="keyword">if</span> capabilities&amp;mysql.ClientMultiStatements &lt; <span class="number">1</span> &#123;</span><br><span class="line"><span class="comment">// The client does not have multi-statement enabled. We now need to determine</span></span><br><span class="line"><span class="comment">// how to handle an unsafe situation based on the multiStmt sysvar.</span></span><br><span class="line"><span class="keyword">switch</span> cc.ctx.GetSessionVars().MultiStatementMode &#123;</span><br><span class="line"><span class="keyword">case</span> variable.OffInt:</span><br><span class="line">err = errMultiStatementDisabled</span><br><span class="line"><span class="keyword">return</span> err</span><br><span class="line"><span class="keyword">case</span> variable.OnInt:</span><br><span class="line"><span class="comment">// multi statement is fully permitted, do nothing</span></span><br><span class="line"><span class="keyword">default</span>:</span><br><span class="line">warn := stmtctx.SQLWarn&#123;Level: stmtctx.WarnLevelWarning, Err: errMultiStatementDisabled&#125;</span><br><span class="line">parserWarns = <span class="built_in">append</span>(parserWarns, warn)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Only pre-build point plans for multi-statement query</span></span><br><span class="line">pointPlans, err = cc.prefetchPointPlanKeys(ctx, stmts)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> err</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>通过Session中的var中的<code>ClientCapability</code>的<code>位运算</code>来判断是否支持<code>mysql.ClientMultiStatements</code>（多sql语句）</li><li>如果<code>sysvar</code>也不支持<code>MultiStatementMode</code>,也就是<code>variable.OffInt</code>，那么就直接返回err</li><li>如果没有能力支持client多statement的话，但是var又开启了的话，目前啥事也没做</li><li>默认就是不支持，但是会通过warn来展示给客户端</li><li>只有在多statement的场景下预取目标计划关键字</li></ul><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i, stmt := <span class="keyword">range</span> stmts &#123;</span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(pointPlans) &gt; <span class="number">0</span> &#123;</span><br><span class="line"><span class="comment">// Save the point plan in Session, so we don't need to build the point plan again.</span></span><br><span class="line">cc.ctx.SetValue(plannercore.PointPlanKey, plannercore.PointPlanVal&#123;Plan: pointPlans[i]&#125;)</span><br><span class="line">&#125;</span><br><span class="line">retryable, err = cc.handleStmt(ctx, stmt, parserWarns, i == <span class="built_in">len</span>(stmts)<span class="number">-1</span>)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">if</span> !retryable || !errors.ErrorEqual(err, storeerr.ErrTiFlashServerTimeout) &#123;</span><br><span class="line"><span class="keyword">break</span></span><br><span class="line">&#125;</span><br><span class="line">_, allowTiFlashFallback := cc.ctx.GetSessionVars().AllowFallbackToTiKV[kv.TiFlash]</span><br><span class="line"><span class="keyword">if</span> !allowTiFlashFallback &#123;</span><br><span class="line"><span class="keyword">break</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// When the TiFlash server seems down, we append a warning to remind the user to check the status of the TiFlash</span></span><br><span class="line"><span class="comment">// server and fallback to TiKV.</span></span><br><span class="line">warns := <span class="built_in">append</span>(parserWarns, stmtctx.SQLWarn&#123;Level: stmtctx.WarnLevelError, Err: err&#125;)</span><br><span class="line"><span class="built_in">delete</span>(cc.ctx.GetSessionVars().IsolationReadEngines, kv.TiFlash)</span><br><span class="line">_, err = cc.handleStmt(ctx, stmt, warns, i == <span class="built_in">len</span>(stmts)<span class="number">-1</span>)</span><br><span class="line">cc.ctx.GetSessionVars().IsolationReadEngines[kv.TiFlash] = <span class="keyword">struct</span>&#123;&#125;&#123;&#125;</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">break</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>如果有目标计划的话，那么只需要在上下文中设置value即可，不需要再次构建目标计划</li><li><code>cc.handleStmt(ctx, stmt, parserWarns, i == len(stmts)-1)</code> 这是我们的核心中的核心，这里面就是处理<code>抽象语法树</code>的逻辑，包含了<code>逻辑优化</code>, <code>物理优化</code>, <code>执行器</code>，<code>tikv</code>交互等等</li><li>todo：留着回来分析</li></ul><h2 id="github-com-pingcap-tidb-server-clientConn-handleStmt"><a href="#github-com-pingcap-tidb-server-clientConn-handleStmt" class="headerlink" title="github.com/pingcap/tidb/server.(*clientConn).handleStmt"></a>github.com/pingcap/tidb/server.(*clientConn).handleStmt</h2><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// The first return value indicates whether the call of handleStmt has no side effect and can be retried.</span></span><br><span class="line"><span class="comment">// Currently, the first return value is used to fall back to TiKV when TiFlash is down.</span></span><br><span class="line"><span class="comment">// 第一个返回值表示调用handleStmt是否没有副作用，是否可以重试</span></span><br><span class="line"><span class="comment">// 当前，第一个返回值用于在TiFlash down时回落到TiKV</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(cc *clientConn)</span> <span class="title">handleStmt</span><span class="params">(ctx context.Context, stmt ast.StmtNode, warns []stmtctx.SQLWarn, lastStmt <span class="keyword">bool</span>)</span> <span class="params">(<span class="keyword">bool</span>, error)</span></span></span><br></pre></td></tr></table></figure><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ctx = context.WithValue(ctx, execdetails.StmtExecDetailKey, &amp;execdetails.StmtExecDetails&#123;&#125;)</span><br><span class="line">ctx = context.WithValue(ctx, util.ExecDetailsKey, &amp;util.ExecDetails&#123;&#125;)</span><br><span class="line">reg := trace.StartRegion(ctx, <span class="string">"ExecuteStmt"</span>)</span><br><span class="line">cc.audit(plugin.Starting)</span><br><span class="line">rs, err := cc.ctx.ExecuteStmt(ctx, stmt)</span><br></pre></td></tr></table></figure><ul><li>上下文带上value，设置主要是<code>StmtExecDetails</code>，里面记录了写入sql到响应的时间</li><li>上下文带上value，设置主要是<code>ExecDetails</code>，里面记录了<code>execution</code>的详情信息，分别有</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h2&gt;&lt;p&gt;这一章，作为我们的起始章节，跟着源码，我们一步步来熟悉TIDB的整体代码结构&lt;/p&gt;
&lt;hr&gt;
    
    </summary>
    
    
      <category term="TIDB" scheme="http://blog.crazylaw.cn/categories/TIDB/"/>
    
    
      <category term="TIDB" scheme="http://blog.crazylaw.cn/tags/TIDB/"/>
    
  </entry>
  
  <entry>
    <title>2022杂乱知识点总结</title>
    <link href="http://blog.crazylaw.cn/2022/01/19/2022%E6%9D%82%E4%B9%B1%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93/"/>
    <id>http://blog.crazylaw.cn/2022/01/19/2022%E6%9D%82%E4%B9%B1%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93/</id>
    <published>2022-01-19T06:38:06.000Z</published>
    <updated>2022-02-05T06:14:45.964Z</updated>
    
    <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>记录一下常规下的一些知识点。</p><hr><a id="more"></a><h2 id="Go"><a href="#Go" class="headerlink" title="Go"></a>Go</h2><h3 id="go-channel-close后读的问题"><a href="#go-channel-close后读的问题" class="headerlink" title="go channel close后读的问题"></a>go channel close后读的问题</h3>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h2&gt;&lt;p&gt;记录一下常规下的一些知识点。&lt;/p&gt;
&lt;hr&gt;
    
    </summary>
    
    
      <category term="知识点" scheme="http://blog.crazylaw.cn/categories/%E7%9F%A5%E8%AF%86%E7%82%B9/"/>
    
    
      <category term="2022杂乱知识点总结" scheme="http://blog.crazylaw.cn/tags/2022%E6%9D%82%E4%B9%B1%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>【2021】 2021年终总结</title>
    <link href="http://blog.crazylaw.cn/2022/01/02/2021%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93/"/>
    <id>http://blog.crazylaw.cn/2022/01/02/2021%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93/</id>
    <published>2022-01-02T15:54:00.000Z</published>
    <updated>2022-01-25T03:01:32.272Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>2021已经过去了，在这里回忆一下，我的2021的年终总结。</p><a id="more"></a><h2 id="困难与挑战"><a href="#困难与挑战" class="headerlink" title="困难与挑战"></a>困难与挑战</h2><p>工作中，在过去的一年里，迎接了不少的挑战，在公司中落地大数据相关的服务了从<code>erlang</code>到<code>golang</code>的转换。我们整套新的体系称为：<code>mfeilog</code></p><ol><li><p>在公司中，积累了许多go语言的组件，并且不断的完善修复相关的bug，例如，mfeilog的核心数据源：<code>msource</code>，mfeilog的基础组件：<code>daemon组件</code>/<code>no-named-pipe组件</code>，<code>logbdev组件</code>，<code>sink_mysql</code>/<code>sink_kafka</code>等组件。</p></li><li><p>说到这个<code>msource</code>服务，这是一个基于<code>rocksdb</code>实现的支持<code>sql</code>语法的轻量级小型定制<code>数据库服务</code>，可以说是经过了不断优化，升级，修复内部各种内置功能，才得以现在的稳定和高效。给应用层提供了一个高可靠，稳定，的数据源功能。其中有一个bug，印象特别深刻，也是我始料未及的一个bug，因为<code>发号器</code>是附属在<code>Table</code>当中的，我需要<code>自定义序列化</code>后存储在底层的<code>rocksdb</code>中，所以在我<code>反序列化</code>出来的时候，启动了多个发号器，导致每个<code>field-colunm</code>都实例化了一个新的对象，而非同一个对象，引发了发号器错乱，从而导致数据丢失的问题(内存被覆盖，错误ack掉了数据)。这个问题排查也是十分不易！！！所以可以说是血的教训。</p></li><li><p>并且也为公司推广 “服务器日志查询方案” 中，确定了以<code>GPL（grafna+protomal+loki）</code>的日志查询架构，统一了日志查询的入口，大大提高了日志查询的效率。</p></li><li><p>在 <code>go服务流量录制和回放方案和实现</code> 中，为了实现流量闭环，特意去调研查看了 滴滴的 <code>《写轮眼》</code>项目，并且知道了通过类似注入的方式，可以在用户层编解码上替换到原来的函数指针，从而实现在用户层替换go底层源码的方式，但是由于性价比问题，在公司中最后并未推广，也没有进行研发。该需求后面被搁置了。</p></li><li><p>在 <code>配置服务方案</code> 中，这个需求在以往其实已经试过才用<code>consule</code>来做配置服务中心，原因是早期我们想要对服务做一个<code>服务注册和发现</code>，在 <code>php</code> 这种<code>fastcgi</code>的方式来说，consule的主动发现服务，就比很多类似<code>etcd</code>被动发现注册服务好用。但是由于最终因为我们的服务目前来说比较零散，并且需求的任务不是着急，这件事后面也被搁置了，后面今天再次提了一个这样子的类似的需求，实现了通过 go语言写的的工具，类似于<code>k8s</code>的<code>kubectl</code>的工具，进行配置的同步和管理，分别分为了2个模式，一个是C端的工具，一个是S端的同步服务，中间的枢纽，最终选择了以<code>etcd</code>为配置分布式存储服务。我们服务的部署特点，利用<code>jenkins</code>的<code>多阶段自定义编译</code>的<code>jenkins-shared-library</code>实现了灵活编译，根据现有的服务灵活部署，从而达到非嵌入式的配置同步方式。</p></li><li><p>因为公司成立得比较早，代码仓库一直从未进行变更，所以其中一个需求就是 <code>gitbucket</code> 到 <code>gitlab</code> 的代码仓库迁移，写了一个小工具，从而实现了从 gitbucket到 gitlab一键自动化无缝迁移代码，包括项目组，项目的历史<code>commit</code>，<code>tag</code>，<code>branch</code>等等都自动化完成，大大减轻了项目迁移的负担。</p></li><li><p>对 <code>jenkins</code> 的<code>shard-library</code> 模块进行优化升级，编写了一个<code>python的支持多凭据认证的脚本</code>，并且支持自定义编译代码。</p></li><li><p>集成了一套，<code>本地的大数据docker环境</code>，我们都知道大数据环境十分的繁杂，并且还需要多台机器才能处理，这对我们本地开发来说十分的不友好，但是网上又没有那些很好的开箱即用的<code>docker-compose</code>环境，因此整理了一套本地的大数据docker环境(非CDH版本)</p></li><li><p>优化升级部分 <code>mproxy</code> 的代码，从而让测试人员更友好的在该项目中完成<code>自动化测试</code>的脚本功能。</p></li><li><p>推动<code>flink</code>的落地，由于我们早期的流计算，是单机的，并且存在严重的外部依赖属性，所以，我们推动了flink的落地，探究了几种开发方式，分别是用纯<code>java</code> 写的<code>datastream-api</code>方式，这个方式有一个好处就是，所有的上层的api都是基于<code>datastream</code>的，一些上层的api无法满足我们的需求，我们通过datastream可以很轻松的实现各类需求。在这个过程中，我踩了不少的坑，主要是来自于<code>watermark</code>和<code>window</code>的概念，咋一看其实都是一些比较好理解的概念，但是其实在配合大数据专用的<code>kafka</code>消息队列中间件，一切就变得复杂起来，由于<code>kafka</code>的<code>partition</code>只能有一个client去消费的原因，加上flink自身的概念<code>并行度</code>，这一切结合在一起，就会出现一些让你疑惑的点。经过了大量反复的摸索和钻研，最终才掌握了在多partition下flink的watermark和window-trigger机制方式，但是由于该方式不够直观，也不管灵活，所以我们最终推广了<code>sql-api</code>。好家伙，你以为这就完了吗，并没有，由于flink自身带有<code>sql-client</code>的原因，我一开始尝试了用<code>sql-client</code>来编写流计算的模型，但是发现这个工具有太多问题了，不同的版本有不用的调用方式，不同版本下，对于<code>SET</code>支持的粒度也是不一致的，这让我很头疼。所以最终选择了，基于flink编写了一个自研的<code>flink-sql-client</code>，通过<code>flink run flink-sql-client.jar</code>的方式，我们就可以轻松的提交sql任务或者做其他的需求（例如查看hive的cataglog等等）。然而到了这里还没有结束，由于<code>sql</code>的部分，我们没办法控制，是由<code>flink-core</code>自身标准化了流程，所以有一些bug我们没办法解决，例如在 <code>flink-1.12.0</code> 种，就会因为<code>watermark</code>在<code>idle</code>的情况下，无法推进watermark，从而导致窗口在少量数据情况下，根本不能及时的统计和计算（这和号称实时分析的流计算违背），所以我们只能想了一些版本做了一些迂回的操作。从而最终解决类似这种由于底层bug所带来的问题。</p></li></ol><h2 id="自己的学习上"><a href="#自己的学习上" class="headerlink" title="自己的学习上"></a>自己的学习上</h2><ol><li><p>对<code>golang</code>的<code>protobuf</code>服务有一些的了解，并且学会了<code>protobuf</code>的插件开发。了解了protobuf的协议。</p></li><li><p>对<code>rust</code>上的生态更为清晰了，利用了其中的一些<code>actor</code>模型，<code>async-io</code>等分别实现了一些基础的工具。</p></li><li><p>对<code>flutter</code>也有了一定的了解，利用<code>dart</code>编写了一个可以用于自定义通信的库。s</p></li><li><p>对<code>linux</code> 种的一些磁盘io，网络io，已经shell命令的灵活运用更加深刻。</p></li></ol><blockquote><p>未完待续！！</p></blockquote><p>(悄咪咪的告诉大家，我买房了。嘿嘿)</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;2021已经过去了，在这里回忆一下，我的2021的年终总结。&lt;/p&gt;
    
    </summary>
    
    
      <category term="年终总结" scheme="http://blog.crazylaw.cn/categories/%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="年终总结" scheme="http://blog.crazylaw.cn/tags/%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>【大数据】- flink开发环境搭建</title>
    <link href="http://blog.crazylaw.cn/2021/09/28/%E5%A4%A7%E6%95%B0%E6%8D%AE/flink%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"/>
    <id>http://blog.crazylaw.cn/2021/09/28/%E5%A4%A7%E6%95%B0%E6%8D%AE/flink%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/</id>
    <published>2021-09-28T01:56:40.000Z</published>
    <updated>2021-09-28T09:10:18.184Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>由于最近要推动flink的流计算代替我们的内部的自行研发的mstream流计算服务，所以需要对flink进行开发。</p><a id="more"></a><h2 id="环境搭建"><a href="#环境搭建" class="headerlink" title="环境搭建"></a>环境搭建</h2><p>我们的flink的版本上1.12.0</p><p>理论上只是jdk8和jdk11.所以我们需要安装jdk8和jdk8</p><p>因为我的是macOS，所以这里我说一下mac安装的过程。</p><p>首先，我们需要安装jdk。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">brew tap adoptopenjdk/openjdk</span><br><span class="line">brew install adoptopenjdk8</span><br><span class="line">brew install adoptopenjdk11</span><br></pre></td></tr></table></figure><blockquote><p>置于要用jdk8还是jdk11，自行抉择，我这里2个都安装了。</p></blockquote><p>但是这个时候你可能会找不到安装路径</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ /usr/libexec/java_home -V</span><br><span class="line">Matching Java Virtual Machines (3):</span><br><span class="line">    11.0.12 (x86_64) "Oracle Corporation" - "Java SE 11.0.12" /Library/Java/JavaVirtualMachines/jdk-11.0.12.jdk/Contents/Home</span><br><span class="line">    11.0.11 (x86_64) "AdoptOpenJDK" - "AdoptOpenJDK 11" /Library/Java/JavaVirtualMachines/adoptopenjdk-11.jdk/Contents/Home</span><br><span class="line">    1.8.0_292 (x86_64) "AdoptOpenJDK" - "AdoptOpenJDK 8" /Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home</span><br></pre></td></tr></table></figure><p>通过<code>/usr/libexec/java_home -V</code>内置的一个程序，就可以找到所有相关的<code>java_home</code>，这里我们就可以看到对应的<code>java_home</code>，然后找到对应的java解析器。</p><h2 id="IDE初始化项目"><a href="#IDE初始化项目" class="headerlink" title="IDE初始化项目"></a>IDE初始化项目</h2><p>我这里用的是<code>IDEA</code>，所以我这里列一下我的操作步骤。</p><h3 id="New-Project"><a href="#New-Project" class="headerlink" title="New Project"></a>New Project</h3><p>先把<code>Maven</code>包的路径确定下来。后面利用docker-maven工具的时候，指定挂载仓库有用。</p><p><img src="/images/%E5%A4%A7%E6%95%B0%E6%8D%AE/newproject13.png" alt="newproject13"></p><p>开始创建项目</p><p><img src="/images/%E5%A4%A7%E6%95%B0%E6%8D%AE/newproject1.png" alt="newproject1"></p><p>选择使用Maven来创建项目，并且选择刚才安装好的<code>JDK8</code>或者<code>JDK11</code>。</p><p>默认情况下，这是不带<code>archetype</code>的，这个是<code>Maven</code>模板的类型。我们需要勾选这个<code>archetype</code>，</p><p><img src="/images/%E5%A4%A7%E6%95%B0%E6%8D%AE/newproject2.png" alt="newproject2"></p><p>接下来添加<code>flink-quickstart-java</code>的<code>archetype</code>。</p><ul><li>GroupId = org.apache.flink</li><li>AryofactId = flink-quickstart-java</li><li>Version = 1.12.0</li></ul><p><img src="/images/%E5%A4%A7%E6%95%B0%E6%8D%AE/newproject3.png" alt="newproject3"></p><p>利用模版创建项目</p><p><img src="/images/%E5%A4%A7%E6%95%B0%E6%8D%AE/newproject4.png" alt="newproject4"></p><p>可以根据自行的需要，填写项目的路径以及对应的<code>GroupId</code>, <code>AryofactId</code>, <code>Version</code></p><p><img src="/images/%E5%A4%A7%E6%95%B0%E6%8D%AE/newproject5.png" alt="newproject5"></p><p>然后就是Maven的相关配置，这里使用的默认的就行，直接点击<code>Finish</code>完成项目初始化，然后项目会自动根据Maven的配置加载对应的Jar包。</p><p><img src="/images/%E5%A4%A7%E6%95%B0%E6%8D%AE/newproject6.png" alt="newproject6"></p><p>等待一切初始化完毕后，会看到如下图的模板</p><p><img src="/images/%E5%A4%A7%E6%95%B0%E6%8D%AE/newproject7.png" alt="newproject7"></p><p>其中包含了2个Job，分别是<code>BatchJob</code>和<code>StreamingJob</code>。</p><ul><li>BatchJob 代表批处理任务</li><li>StreamingJob 代表流处理任务</li></ul><h2 id="编写批处理代码并测试执行"><a href="#编写批处理代码并测试执行" class="headerlink" title="编写批处理代码并测试执行"></a>编写批处理代码并测试执行</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * Licensed to the Apache Software Foundation (ASF) under one</span></span><br><span class="line"><span class="comment"> * or more contributor license agreements.  See the NOTICE file</span></span><br><span class="line"><span class="comment"> * distributed with this work for additional information</span></span><br><span class="line"><span class="comment"> * regarding copyright ownership.  The ASF licenses this file</span></span><br><span class="line"><span class="comment"> * to you under the Apache License, Version 2.0 (the</span></span><br><span class="line"><span class="comment"> * "License"); you may not use this file except in compliance</span></span><br><span class="line"><span class="comment"> * with the License.  You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *     http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment"> * distributed under the License is distributed on an "AS IS" BASIS,</span></span><br><span class="line"><span class="comment"> * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment"> * See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment"> * limitations under the License.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">package</span> my.flink;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.ExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.operators.DataSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.util.Collector;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Skeleton for a Flink Batch Job.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;For a tutorial how to write a Flink batch application, check the</span></span><br><span class="line"><span class="comment"> * tutorials and examples on the &lt;a href="https://flink.apache.org/docs/stable/"&gt;Flink Website&lt;/a&gt;.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;To package your application into a JAR file for execution,</span></span><br><span class="line"><span class="comment"> * change the main class in the POM.xml file to this class (simply search for 'mainClass')</span></span><br><span class="line"><span class="comment"> * and run 'mvn clean package' on the command line.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BatchJob</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">// set up the batch execution environment</span></span><br><span class="line">        <span class="keyword">final</span> ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        DataSource&lt;String&gt; el = env.fromElements(<span class="string">"good good study"</span>, <span class="string">"day day up"</span>);</span><br><span class="line"></span><br><span class="line">        el.flatMap(</span><br><span class="line">                (String a, Collector&lt;String&gt; out) -&gt; Arrays.stream(a.split(<span class="string">" "</span>)).forEach(x -&gt; out.collect(x))</span><br><span class="line">        ).returns(String<span class="class">.<span class="keyword">class</span>).<span class="title">print</span>()</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 由于print()是调试模式，所以不能指定Jobname，print()内部会自动调用Execute()</span></span><br><span class="line">        <span class="comment">// 所以 env.execute() 将无法调用，需要注释掉，否则会有报错抛出，当然你也可以选择忽略这个异常</span></span><br><span class="line">        <span class="comment">// execute program</span></span><br><span class="line">        <span class="comment">// env.execute("Flink Batch Java API Skeleton");</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里我们把<code>BatchJob</code>加入了<code>具体</code>的任务。我这里的写法是Java8的lamba的写法，使用lamba的写法记得需要在后面加上<code>returns</code>的函数，这是因为使用<code>lamba</code>的情况下，会导致部分信息无法自动推导，需要手动显式指定，从而导致我们需要调用多这个函数。</p><p>我们初始化了一个数据源集合，这个集合类型为<code>String</code>类型，我们指定这个集合的元素有2个，分别是<code>good good study</code>, <code>day day up</code>。</p><p>然后我们通过<code>flatMap</code>的方法进行一个归并的操作，把每个元素通过<code>一个空格</code>进行切分，切分之后，我们通过<code>Collector</code>的<code>collect()</code>进行收集起来。最终输出在终端。</p><p>并且这个Job的名字，我们定义为<code>Flink Batch Java API Skeleton</code>。</p><p>我们运行这个Job.默认情况下，会遇到如下报错：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org&#x2F;apache&#x2F;flink&#x2F;api&#x2F;java&#x2F;ExecutionEnvironment</span><br><span class="line">at my.flink.BatchJob.main(BatchJob.java:41)</span><br><span class="line">Caused by: java.lang.ClassNotFoundException: org.apache.flink.api.java.ExecutionEnvironment</span><br><span class="line">at java.base&#x2F;jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581)</span><br><span class="line">at java.base&#x2F;jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178)</span><br><span class="line">at java.base&#x2F;java.lang.ClassLoader.loadClass(ClassLoader.java:522)</span><br><span class="line">... 1 more</span><br></pre></td></tr></table></figure><p>你可能会觉得很奇怪，明明<code>IDEA</code>已经把<code>Flink</code>的包加载进来，也能正常跳转，为什么在运行的时候却出现了这个，这是因为这是编译的行为，和IDEA加载包没有直接的关系。</p><p>打开你的<code>pom.xml</code>，找到<code>dependencies</code>下的<code>&lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</code>的所有依赖包，你会发现每个依赖包下面都有一个<code>&lt;scope /&gt;</code>的定义，里面的value写的是<code>provided</code>，我们只需要把这一整行注释掉就好了。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-streaming-java_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-clients_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>注释后</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- &lt;scope&gt;provided&lt;/scope&gt; --&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-streaming-java_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- &lt;scope&gt;provided&lt;/scope&gt; --&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-clients_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- &lt;scope&gt;provided&lt;/scope&gt; --&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>这样子，就等同于定义依赖包，使用默认的<code>scope</code>范围。</p><p>我们这里需要了解一下scope的一些细节。</p><p>对于<code>scope=compile</code>的情况（<code>默认scope</code>),也就是说这个项目在<code>编译</code>，<code>测试</code>，<code>运行阶段``都需要</code>这个artifact(模块)对应的jar包<code>在classpath中</code>。</p><p>而对于<code>scope=provided</code>的情况，则可以认为这个provided是<code>目标容器已经provide这个artifact</code>。换句话说，<code>它只影响到编译，测试阶段</code>。在编译测试阶段，我们需要这个artifact对应的jar包在classpath中，而在运行阶段，假定目标的容器（比如我们这里的liferay容器）已经提供了这个jar包，所以无需我们这个artifact对应的jar包了。</p><p>maven中三种classpath<br>编译，测试，运行</p><ul><li>compile：默认范围，编译测试运行都有效</li><li>provided：在编译和测试时有效</li><li>runtime：在测试和运行时有效</li><li>test：只在测试时有效</li><li>system：在编译和测试时有效，与本机系统关联，可移植性差</li></ul><p>所以我们需要改变的就是这个<code>scope</code>的范围，让某情况下可以运行。例如，我们需要在本机上运行，那么我们就可以注释掉，然后就会使用默认的<code>compile</code>。</p><p>但是需要注意的是，当你改动了这个<code>pom.xml</code>之后，idea我不知道是不是bug，反正我的当前版本不会自动刷新，怎么理解这句话？</p><p>通过<code>File -&gt; Project Structure</code>打开页面（因为我是mac），所以可以通过快捷键<code>Command + [:;]</code>打开。界面如下</p><p><img src="/images/%E5%A4%A7%E6%95%B0%E6%8D%AE/newproject8.png" alt="newproject8"></p><p>我们可以看到，<code>Flink</code>相关的依赖包其实已经存在了，这里显示了我们的Maven包下的scope是<code>Provided</code>，那就代表<code>IDEA</code>并未自动识别我刚才的注释。因为如果成功识别出来了，应该是会变成<code>Compile</code>。当然我可以直接在这里进行修改，但是为了统一维护的问题，不建议在此处修改，虽然直接修改很方便，但是下次加载还是从<code>pom.xml</code>加载的，并且间接依赖包也特别多，你无法掌握那么多依赖包的关系。</p><p>所以注释后，我们需要利用<code>pom.xml</code>进行<code>maven</code>的<code>reload project</code>。</p><p><img src="/images/%E5%A4%A7%E6%95%B0%E6%8D%AE/newproject10.png" alt="newproject10"></p><p>这个时候，我们就发现不管是直接还是间接的依赖包都变成了<code>Compile</code>。</p><p><img src="/images/%E5%A4%A7%E6%95%B0%E6%8D%AE/newproject11.png" alt="newproject11"></p><p>接下来，我们在运行一次我们的任务。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br></pre></td><td class="code"><pre><span class="line">15:33:55,353 INFO  org.apache.flink.api.java.utils.PlanGenerator                [] - The job has 0 registered types and 0 default Kryo serializers</span><br><span class="line">15:33:55,523 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils [] - The configuration option taskmanager.cpu.cores required for local execution is not set, setting it to the maximal possible value.</span><br><span class="line">15:33:55,523 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils [] - The configuration option taskmanager.memory.task.heap.size required for local execution is not set, setting it to the maximal possible value.</span><br><span class="line">15:33:55,523 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils [] - The configuration option taskmanager.memory.task.off-heap.size required for local execution is not set, setting it to the maximal possible value.</span><br><span class="line">15:33:55,523 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils [] - The configuration option taskmanager.memory.network.min required for local execution is not set, setting it to its default value 64 mb.</span><br><span class="line">15:33:55,524 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils [] - The configuration option taskmanager.memory.network.max required for local execution is not set, setting it to its default value 64 mb.</span><br><span class="line">15:33:55,524 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils [] - The configuration option taskmanager.memory.managed.size required for local execution is not set, setting it to its default value 128 mb.</span><br><span class="line">15:33:55,548 INFO  org.apache.flink.runtime.minicluster.MiniCluster             [] - Starting Flink Mini Cluster</span><br><span class="line">15:33:55,551 INFO  org.apache.flink.runtime.minicluster.MiniCluster             [] - Starting Metrics Registry</span><br><span class="line">15:33:55,627 INFO  org.apache.flink.runtime.metrics.MetricRegistryImpl          [] - No metrics reporter configured, no metrics will be exposed&#x2F;reported.</span><br><span class="line">15:33:55,627 INFO  org.apache.flink.runtime.minicluster.MiniCluster             [] - Starting RPC Service(s)</span><br><span class="line">15:33:55,780 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Trying to start local actor system</span><br><span class="line">15:33:56,203 INFO  akka.event.slf4j.Slf4jLogger                                 [] - Slf4jLogger started</span><br><span class="line">15:33:56,313 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Actor system started at akka:&#x2F;&#x2F;flink</span><br><span class="line">15:33:56,328 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Trying to start local actor system</span><br><span class="line">15:33:56,341 INFO  akka.event.slf4j.Slf4jLogger                                 [] - Slf4jLogger started</span><br><span class="line">15:33:56,356 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils        [] - Actor system started at akka:&#x2F;&#x2F;flink-metrics</span><br><span class="line">15:33:56,373 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.metrics.dump.MetricQueryService at akka:&#x2F;&#x2F;flink-metrics&#x2F;user&#x2F;rpc&#x2F;MetricQueryService .</span><br><span class="line">15:33:56,399 INFO  org.apache.flink.runtime.minicluster.MiniCluster             [] - Starting high-availability services</span><br><span class="line">15:33:56,418 INFO  org.apache.flink.runtime.blob.BlobServer                     [] - Created BLOB server storage directory &#x2F;var&#x2F;folders&#x2F;zq&#x2F;2b48w4_x5vq89_jrz3yns13h0000gn&#x2F;T&#x2F;blobStore-4ec8c72e-36f6-4b8d-aba8-70bb3d443f93</span><br><span class="line">15:33:56,430 INFO  org.apache.flink.runtime.blob.BlobServer                     [] - Started BLOB server at 0.0.0.0:58212 - max concurrent requests: 50 - max backlog: 1000</span><br><span class="line">15:33:56,434 INFO  org.apache.flink.runtime.blob.PermanentBlobCache             [] - Created BLOB cache storage directory &#x2F;var&#x2F;folders&#x2F;zq&#x2F;2b48w4_x5vq89_jrz3yns13h0000gn&#x2F;T&#x2F;blobStore-3433044e-b47e-445c-9df2-ceb5d1e8da6f</span><br><span class="line">15:33:56,436 INFO  org.apache.flink.runtime.blob.TransientBlobCache             [] - Created BLOB cache storage directory &#x2F;var&#x2F;folders&#x2F;zq&#x2F;2b48w4_x5vq89_jrz3yns13h0000gn&#x2F;T&#x2F;blobStore-4d06675d-1b13-4fa2-87e9-0a1609934f09</span><br><span class="line">15:33:56,436 INFO  org.apache.flink.runtime.minicluster.MiniCluster             [] - Starting 1 TaskManger(s)</span><br><span class="line">15:33:56,441 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerRunner      [] - Starting TaskManager with ResourceID: a7c681c7-48a2-4491-803a-535036a51fcb</span><br><span class="line">15:33:56,477 INFO  org.apache.flink.runtime.taskexecutor.TaskManagerServices    [] - Temporary file directory &#39;&#x2F;var&#x2F;folders&#x2F;zq&#x2F;2b48w4_x5vq89_jrz3yns13h0000gn&#x2F;T&#39;: total 233 GB, usable 25 GB (10.73% usable)</span><br><span class="line">15:33:56,482 INFO  org.apache.flink.runtime.io.disk.FileChannelManagerImpl      [] - FileChannelManager uses directory &#x2F;var&#x2F;folders&#x2F;zq&#x2F;2b48w4_x5vq89_jrz3yns13h0000gn&#x2F;T&#x2F;flink-io-dc01cff1-6b52-43ed-9d16-9085f49c732e for spill files.</span><br><span class="line">15:33:56,492 INFO  org.apache.flink.runtime.io.disk.FileChannelManagerImpl      [] - FileChannelManager uses directory &#x2F;var&#x2F;folders&#x2F;zq&#x2F;2b48w4_x5vq89_jrz3yns13h0000gn&#x2F;T&#x2F;flink-netty-shuffle-41571afb-b13e-494b-b937-0696d2c77ca1 for spill files.</span><br><span class="line">15:33:56,580 INFO  org.apache.flink.runtime.io.network.buffer.NetworkBufferPool [] - Allocated 64 MB for network buffer pool (number of memory segments: 2048, bytes per segment: 32768).</span><br><span class="line">15:33:56,594 INFO  org.apache.flink.runtime.io.network.NettyShuffleEnvironment  [] - Starting the network environment and its components.</span><br><span class="line">15:33:56,596 INFO  org.apache.flink.runtime.taskexecutor.KvStateService         [] - Starting the kvState service and its components.</span><br><span class="line">15:33:56,631 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.taskexecutor.TaskExecutor at akka:&#x2F;&#x2F;flink&#x2F;user&#x2F;rpc&#x2F;taskmanager_0 .</span><br><span class="line">15:33:56,650 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Start job leader service.</span><br><span class="line">15:33:56,653 INFO  org.apache.flink.runtime.filecache.FileCache                 [] - User file cache uses directory &#x2F;var&#x2F;folders&#x2F;zq&#x2F;2b48w4_x5vq89_jrz3yns13h0000gn&#x2F;T&#x2F;flink-dist-cache-fc4fd5a1-79fa-4a19-8d7d-f3072006c91e</span><br><span class="line">15:33:56,714 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   [] - Starting rest endpoint.</span><br><span class="line">15:33:56,717 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   [] - Failed to load web based job submission extension. Probable reason: flink-runtime-web is not in the classpath.</span><br><span class="line">15:33:57,089 WARN  org.apache.flink.runtime.webmonitor.WebMonitorUtils          [] - Log file environment variable &#39;log.file&#39; is not set.</span><br><span class="line">15:33:57,089 WARN  org.apache.flink.runtime.webmonitor.WebMonitorUtils          [] - JobManager log files are unavailable in the web dashboard. Log file location not found in environment variable &#39;log.file&#39; or configuration key &#39;web.log.path&#39;.</span><br><span class="line">15:33:57,300 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   [] - Rest endpoint listening at localhost:58223</span><br><span class="line">15:33:57,302 INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService [] - Proposing leadership to contender http:&#x2F;&#x2F;localhost:58223</span><br><span class="line">15:33:57,305 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   [] - http:&#x2F;&#x2F;localhost:58223 was granted leadership with leaderSessionID&#x3D;22a043f5-f263-4e6c-87e9-6e61beef3075</span><br><span class="line">15:33:57,306 INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService [] - Received confirmation of leadership for leader http:&#x2F;&#x2F;localhost:58223 , session&#x3D;22a043f5-f263-4e6c-87e9-6e61beef3075</span><br><span class="line">15:33:57,327 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.resourcemanager.StandaloneResourceManager at akka:&#x2F;&#x2F;flink&#x2F;user&#x2F;rpc&#x2F;resourcemanager_1 .</span><br><span class="line">15:33:57,344 INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService [] - Proposing leadership to contender LeaderContender: DefaultDispatcherRunner</span><br><span class="line">15:33:57,345 INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService [] - Proposing leadership to contender LeaderContender: StandaloneResourceManager</span><br><span class="line">15:33:57,347 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - ResourceManager akka:&#x2F;&#x2F;flink&#x2F;user&#x2F;rpc&#x2F;resourcemanager_1 was granted leadership with fencing token 99793e5c3d8a81ced62f8a03bd21494c</span><br><span class="line">15:33:57,350 INFO  org.apache.flink.runtime.minicluster.MiniCluster             [] - Flink Mini Cluster started successfully</span><br><span class="line">15:33:57,350 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerImpl [] - Starting the SlotManager.</span><br><span class="line">15:33:57,351 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Start SessionDispatcherLeaderProcess.</span><br><span class="line">15:33:57,353 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Recover all persisted job graphs.</span><br><span class="line">15:33:57,354 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Successfully recovered 0 persisted job graphs.</span><br><span class="line">15:33:57,355 INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService [] - Received confirmation of leadership for leader akka:&#x2F;&#x2F;flink&#x2F;user&#x2F;rpc&#x2F;resourcemanager_1 , session&#x3D;d62f8a03-bd21-494c-9979-3e5c3d8a81ce</span><br><span class="line">15:33:57,357 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Connecting to ResourceManager akka:&#x2F;&#x2F;flink&#x2F;user&#x2F;rpc&#x2F;resourcemanager_1(99793e5c3d8a81ced62f8a03bd21494c).</span><br><span class="line">15:33:57,365 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher at akka:&#x2F;&#x2F;flink&#x2F;user&#x2F;rpc&#x2F;dispatcher_2 .</span><br><span class="line">15:33:57,378 INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService [] - Received confirmation of leadership for leader akka:&#x2F;&#x2F;flink&#x2F;user&#x2F;rpc&#x2F;dispatcher_2 , session&#x3D;0a8eb324-f6f9-44d7-a452-87c855415b0e</span><br><span class="line">15:33:57,387 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Resolved ResourceManager address, beginning registration</span><br><span class="line">15:33:57,393 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registering TaskManager with ResourceID a7c681c7-48a2-4491-803a-535036a51fcb (akka:&#x2F;&#x2F;flink&#x2F;user&#x2F;rpc&#x2F;taskmanager_0) at ResourceManager</span><br><span class="line">15:33:57,395 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Successful registration at resource manager akka:&#x2F;&#x2F;flink&#x2F;user&#x2F;rpc&#x2F;resourcemanager_1 under registration id 3e9b649958365e1a080d0b1102807505.</span><br><span class="line">15:33:57,396 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Received JobGraph submission c9c27c95a1e3b4a8bfd7250101fa1126 (Flink Java Job at Tue Sep 28 15:33:55 CST 2021).</span><br><span class="line">15:33:57,396 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Submitting job c9c27c95a1e3b4a8bfd7250101fa1126 (Flink Java Job at Tue Sep 28 15:33:55 CST 2021).</span><br><span class="line">15:33:57,423 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Starting RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at akka:&#x2F;&#x2F;flink&#x2F;user&#x2F;rpc&#x2F;jobmanager_3 .</span><br><span class="line">15:33:57,433 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Initializing job Flink Java Job at Tue Sep 28 15:33:55 CST 2021 (c9c27c95a1e3b4a8bfd7250101fa1126).</span><br><span class="line">15:33:57,452 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Using restart back off time strategy NoRestartBackoffTimeStrategy for Flink Java Job at Tue Sep 28 15:33:55 CST 2021 (c9c27c95a1e3b4a8bfd7250101fa1126).</span><br><span class="line">15:33:57,487 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Running initialization on master for job Flink Java Job at Tue Sep 28 15:33:55 CST 2021 (c9c27c95a1e3b4a8bfd7250101fa1126).</span><br><span class="line">15:33:57,490 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Successfully ran initialization on master in 3 ms.</span><br><span class="line">15:33:57,512 INFO  org.apache.flink.runtime.scheduler.adapter.DefaultExecutionTopology [] - Built 1 pipelined regions in 3 ms</span><br><span class="line">15:33:57,518 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Using failover strategy org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy@4fe83a40 for Flink Java Job at Tue Sep 28 15:33:55 CST 2021 (c9c27c95a1e3b4a8bfd7250101fa1126).</span><br><span class="line">15:33:57,527 INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService [] - Proposing leadership to contender akka:&#x2F;&#x2F;flink&#x2F;user&#x2F;rpc&#x2F;jobmanager_3</span><br><span class="line">15:33:57,528 INFO  org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl      [] - JobManager runner for job Flink Java Job at Tue Sep 28 15:33:55 CST 2021 (c9c27c95a1e3b4a8bfd7250101fa1126) was granted leadership with session id 00c173d1-6a96-47ad-a2d9-da1ebc4d6a41 at akka:&#x2F;&#x2F;flink&#x2F;user&#x2F;rpc&#x2F;jobmanager_3.</span><br><span class="line">15:33:57,532 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Starting execution of job Flink Java Job at Tue Sep 28 15:33:55 CST 2021 (c9c27c95a1e3b4a8bfd7250101fa1126) under job master id a2d9da1ebc4d6a4100c173d16a9647ad.</span><br><span class="line">15:33:57,533 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Starting scheduling with scheduling strategy [org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy]</span><br><span class="line">15:33:57,533 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job Flink Java Job at Tue Sep 28 15:33:55 CST 2021 (c9c27c95a1e3b4a8bfd7250101fa1126) switched from state CREATED to RUNNING.</span><br><span class="line">15:33:57,537 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - CHAIN DataSource (at main(BatchJob.java:43) (org.apache.flink.api.java.io.CollectionInputFormat)) -&gt; FlatMap (FlatMap at main(BatchJob.java:45)) (1&#x2F;1) (2d0c18f32aaefecbd6f3d76a781d54b9) switched from CREATED to SCHEDULED.</span><br><span class="line">15:33:57,537 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - DataSink (collect()) (1&#x2F;1) (1feb48784b233306f550eda82cf1b5e9) switched from CREATED to SCHEDULED.</span><br><span class="line">15:33:57,546 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Cannot serve slot request, no ResourceManager connected. Adding as pending request [SlotRequestId&#123;1e28fd68790f78b4b48f557e8ba4d92f&#125;]</span><br><span class="line">15:33:57,552 INFO  org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService [] - Received confirmation of leadership for leader akka:&#x2F;&#x2F;flink&#x2F;user&#x2F;rpc&#x2F;jobmanager_3 , session&#x3D;00c173d1-6a96-47ad-a2d9-da1ebc4d6a41</span><br><span class="line">15:33:57,552 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Connecting to ResourceManager akka:&#x2F;&#x2F;flink&#x2F;user&#x2F;rpc&#x2F;resourcemanager_1(99793e5c3d8a81ced62f8a03bd21494c)</span><br><span class="line">15:33:57,554 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Resolved ResourceManager address, beginning registration</span><br><span class="line">15:33:57,555 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registering job manager a2d9da1ebc4d6a4100c173d16a9647ad@akka:&#x2F;&#x2F;flink&#x2F;user&#x2F;rpc&#x2F;jobmanager_3 for job c9c27c95a1e3b4a8bfd7250101fa1126.</span><br><span class="line">15:33:57,559 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registered job manager a2d9da1ebc4d6a4100c173d16a9647ad@akka:&#x2F;&#x2F;flink&#x2F;user&#x2F;rpc&#x2F;jobmanager_3 for job c9c27c95a1e3b4a8bfd7250101fa1126.</span><br><span class="line">15:33:57,561 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - JobManager successfully registered at ResourceManager, leader id: 99793e5c3d8a81ced62f8a03bd21494c.</span><br><span class="line">15:33:57,562 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Requesting new slot [SlotRequestId&#123;1e28fd68790f78b4b48f557e8ba4d92f&#125;] and profile ResourceProfile&#123;UNKNOWN&#125; with allocation id d73fe42189235dfaf22a937eb4556ee1 from resource manager.</span><br><span class="line">15:33:57,562 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Request slot with profile ResourceProfile&#123;UNKNOWN&#125; for job c9c27c95a1e3b4a8bfd7250101fa1126 with allocation id d73fe42189235dfaf22a937eb4556ee1.</span><br><span class="line">15:33:57,565 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Receive slot request d73fe42189235dfaf22a937eb4556ee1 for job c9c27c95a1e3b4a8bfd7250101fa1126 from resource manager with leader id 99793e5c3d8a81ced62f8a03bd21494c.</span><br><span class="line">15:33:57,570 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Allocated slot for d73fe42189235dfaf22a937eb4556ee1.</span><br><span class="line">15:33:57,571 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Add job c9c27c95a1e3b4a8bfd7250101fa1126 for job leader monitoring.</span><br><span class="line">15:33:57,573 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Try to register at job manager akka:&#x2F;&#x2F;flink&#x2F;user&#x2F;rpc&#x2F;jobmanager_3 with leader id 00c173d1-6a96-47ad-a2d9-da1ebc4d6a41.</span><br><span class="line">15:33:57,574 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Resolved JobManager address, beginning registration</span><br><span class="line">15:33:57,577 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Successful registration at job manager akka:&#x2F;&#x2F;flink&#x2F;user&#x2F;rpc&#x2F;jobmanager_3 for job c9c27c95a1e3b4a8bfd7250101fa1126.</span><br><span class="line">15:33:57,578 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Establish JobManager connection for job c9c27c95a1e3b4a8bfd7250101fa1126.</span><br><span class="line">15:33:57,580 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Offer reserved slots to the leader of job c9c27c95a1e3b4a8bfd7250101fa1126.</span><br><span class="line">15:33:57,588 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - CHAIN DataSource (at main(BatchJob.java:43) (org.apache.flink.api.java.io.CollectionInputFormat)) -&gt; FlatMap (FlatMap at main(BatchJob.java:45)) (1&#x2F;1) (2d0c18f32aaefecbd6f3d76a781d54b9) switched from SCHEDULED to DEPLOYING.</span><br><span class="line">15:33:57,590 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying CHAIN DataSource (at main(BatchJob.java:43) (org.apache.flink.api.java.io.CollectionInputFormat)) -&gt; FlatMap (FlatMap at main(BatchJob.java:45)) (1&#x2F;1) (attempt #0) with attempt id 2d0c18f32aaefecbd6f3d76a781d54b9 to a7c681c7-48a2-4491-803a-535036a51fcb @ localhost (dataPort&#x3D;-1) with allocation id d73fe42189235dfaf22a937eb4556ee1</span><br><span class="line">15:33:57,595 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - DataSink (collect()) (1&#x2F;1) (1feb48784b233306f550eda82cf1b5e9) switched from SCHEDULED to DEPLOYING.</span><br><span class="line">15:33:57,595 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Deploying DataSink (collect()) (1&#x2F;1) (attempt #0) with attempt id 1feb48784b233306f550eda82cf1b5e9 to a7c681c7-48a2-4491-803a-535036a51fcb @ localhost (dataPort&#x3D;-1) with allocation id d73fe42189235dfaf22a937eb4556ee1</span><br><span class="line">15:33:57,595 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot d73fe42189235dfaf22a937eb4556ee1.</span><br><span class="line">15:33:57,627 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task CHAIN DataSource (at main(BatchJob.java:43) (org.apache.flink.api.java.io.CollectionInputFormat)) -&gt; FlatMap (FlatMap at main(BatchJob.java:45)) (1&#x2F;1)#0 (2d0c18f32aaefecbd6f3d76a781d54b9), deploy into slot with allocation id d73fe42189235dfaf22a937eb4556ee1.</span><br><span class="line">15:33:57,628 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - CHAIN DataSource (at main(BatchJob.java:43) (org.apache.flink.api.java.io.CollectionInputFormat)) -&gt; FlatMap (FlatMap at main(BatchJob.java:45)) (1&#x2F;1)#0 (2d0c18f32aaefecbd6f3d76a781d54b9) switched from CREATED to DEPLOYING.</span><br><span class="line">15:33:57,630 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot d73fe42189235dfaf22a937eb4556ee1.</span><br><span class="line">15:33:57,630 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot d73fe42189235dfaf22a937eb4556ee1.</span><br><span class="line">15:33:57,633 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task CHAIN DataSource (at main(BatchJob.java:43) (org.apache.flink.api.java.io.CollectionInputFormat)) -&gt; FlatMap (FlatMap at main(BatchJob.java:45)) (1&#x2F;1)#0 (2d0c18f32aaefecbd6f3d76a781d54b9) [DEPLOYING].</span><br><span class="line">15:33:57,634 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Registering task at network: CHAIN DataSource (at main(BatchJob.java:43) (org.apache.flink.api.java.io.CollectionInputFormat)) -&gt; FlatMap (FlatMap at main(BatchJob.java:45)) (1&#x2F;1)#0 (2d0c18f32aaefecbd6f3d76a781d54b9) [DEPLOYING].</span><br><span class="line">15:33:57,642 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Received task DataSink (collect()) (1&#x2F;1)#0 (1feb48784b233306f550eda82cf1b5e9), deploy into slot with allocation id d73fe42189235dfaf22a937eb4556ee1.</span><br><span class="line">15:33:57,642 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - DataSink (collect()) (1&#x2F;1)#0 (1feb48784b233306f550eda82cf1b5e9) switched from CREATED to DEPLOYING.</span><br><span class="line">15:33:57,643 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Loading JAR files for task DataSink (collect()) (1&#x2F;1)#0 (1feb48784b233306f550eda82cf1b5e9) [DEPLOYING].</span><br><span class="line">15:33:57,644 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Registering task at network: DataSink (collect()) (1&#x2F;1)#0 (1feb48784b233306f550eda82cf1b5e9) [DEPLOYING].</span><br><span class="line">15:33:57,647 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - CHAIN DataSource (at main(BatchJob.java:43) (org.apache.flink.api.java.io.CollectionInputFormat)) -&gt; FlatMap (FlatMap at main(BatchJob.java:45)) (1&#x2F;1)#0 (2d0c18f32aaefecbd6f3d76a781d54b9) switched from DEPLOYING to RUNNING.</span><br><span class="line">15:33:57,648 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - DataSink (collect()) (1&#x2F;1)#0 (1feb48784b233306f550eda82cf1b5e9) switched from DEPLOYING to RUNNING.</span><br><span class="line">15:33:57,648 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - CHAIN DataSource (at main(BatchJob.java:43) (org.apache.flink.api.java.io.CollectionInputFormat)) -&gt; FlatMap (FlatMap at main(BatchJob.java:45)) (1&#x2F;1) (2d0c18f32aaefecbd6f3d76a781d54b9) switched from DEPLOYING to RUNNING.</span><br><span class="line">15:33:57,649 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - DataSink (collect()) (1&#x2F;1) (1feb48784b233306f550eda82cf1b5e9) switched from DEPLOYING to RUNNING.</span><br><span class="line">15:33:57,659 WARN  org.apache.flink.metrics.MetricGroup                         [] - The operator name DataSource (at main(BatchJob.java:43) (org.apache.flink.api.java.io.CollectionInputFormat)) exceeded the 80 characters length limit and was truncated.</span><br><span class="line">15:33:57,667 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - CHAIN DataSource (at main(BatchJob.java:43) (org.apache.flink.api.java.io.CollectionInputFormat)) -&gt; FlatMap (FlatMap at main(BatchJob.java:45)) (1&#x2F;1)#0 (2d0c18f32aaefecbd6f3d76a781d54b9) switched from RUNNING to FINISHED.</span><br><span class="line">15:33:57,667 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for CHAIN DataSource (at main(BatchJob.java:43) (org.apache.flink.api.java.io.CollectionInputFormat)) -&gt; FlatMap (FlatMap at main(BatchJob.java:45)) (1&#x2F;1)#0 (2d0c18f32aaefecbd6f3d76a781d54b9).</span><br><span class="line">15:33:57,670 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state FINISHED to JobManager for task CHAIN DataSource (at main(BatchJob.java:43) (org.apache.flink.api.java.io.CollectionInputFormat)) -&gt; FlatMap (FlatMap at main(BatchJob.java:45)) (1&#x2F;1)#0 2d0c18f32aaefecbd6f3d76a781d54b9.</span><br><span class="line">15:33:57,677 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - CHAIN DataSource (at main(BatchJob.java:43) (org.apache.flink.api.java.io.CollectionInputFormat)) -&gt; FlatMap (FlatMap at main(BatchJob.java:45)) (1&#x2F;1) (2d0c18f32aaefecbd6f3d76a781d54b9) switched from RUNNING to FINISHED.</span><br><span class="line">15:33:57,678 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - DataSink (collect()) (1&#x2F;1)#0 (1feb48784b233306f550eda82cf1b5e9) switched from RUNNING to FINISHED.</span><br><span class="line">15:33:57,678 INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Freeing task resources for DataSink (collect()) (1&#x2F;1)#0 (1feb48784b233306f550eda82cf1b5e9).</span><br><span class="line">15:33:57,679 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Un-registering task and sending final execution state FINISHED to JobManager for task DataSink (collect()) (1&#x2F;1)#0 1feb48784b233306f550eda82cf1b5e9.</span><br><span class="line">15:33:57,682 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - DataSink (collect()) (1&#x2F;1) (1feb48784b233306f550eda82cf1b5e9) switched from RUNNING to FINISHED.</span><br><span class="line">15:33:57,685 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job Flink Java Job at Tue Sep 28 15:33:55 CST 2021 (c9c27c95a1e3b4a8bfd7250101fa1126) switched from state RUNNING to FINISHED.</span><br><span class="line">15:33:57,691 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Job c9c27c95a1e3b4a8bfd7250101fa1126 reached globally terminal state FINISHED.</span><br><span class="line">15:33:57,691 INFO  org.apache.flink.runtime.minicluster.MiniCluster             [] - Shutting down Flink Mini Cluster</span><br><span class="line">15:33:57,691 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   [] - Shutting down rest endpoint.</span><br><span class="line">15:33:57,691 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Stopping TaskExecutor akka:&#x2F;&#x2F;flink&#x2F;user&#x2F;rpc&#x2F;taskmanager_0.</span><br><span class="line">15:33:57,692 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Close ResourceManager connection 01714233597d70de71bbfbda09ac665e.</span><br><span class="line">15:33:57,692 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Closing TaskExecutor connection a7c681c7-48a2-4491-803a-535036a51fcb because: The TaskExecutor is shutting down.</span><br><span class="line">15:33:57,693 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Close JobManager connection for job c9c27c95a1e3b4a8bfd7250101fa1126.</span><br><span class="line">15:33:57,694 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Stopping the JobMaster for job Flink Java Job at Tue Sep 28 15:33:55 CST 2021(c9c27c95a1e3b4a8bfd7250101fa1126).</span><br><span class="line">15:33:57,695 INFO  org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:0, state:ALLOCATED, resource profile: ResourceProfile&#123;managedMemory&#x3D;128.000mb (134217728 bytes), networkMemory&#x3D;64.000mb (67108864 bytes)&#125;, allocationId: d73fe42189235dfaf22a937eb4556ee1, jobId: c9c27c95a1e3b4a8bfd7250101fa1126).</span><br><span class="line">15:33:57,697 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Suspending SlotPool.</span><br><span class="line">15:33:57,697 INFO  org.apache.flink.runtime.jobmaster.JobMaster                 [] - Close ResourceManager connection 01714233597d70de71bbfbda09ac665e: Stopping JobMaster for job Flink Java Job at Tue Sep 28 15:33:55 CST 2021(c9c27c95a1e3b4a8bfd7250101fa1126)..</span><br><span class="line">15:33:57,697 INFO  org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl     [] - Stopping SlotPool.</span><br><span class="line">15:33:57,697 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Disconnect job manager a2d9da1ebc4d6a4100c173d16a9647ad@akka:&#x2F;&#x2F;flink&#x2F;user&#x2F;rpc&#x2F;jobmanager_3 for job c9c27c95a1e3b4a8bfd7250101fa1126 from the resource manager.</span><br><span class="line">15:33:57,699 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Stop job leader service.</span><br><span class="line">15:33:57,699 INFO  org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager [] - Shutting down TaskExecutorLocalStateStoresManager.</span><br><span class="line">good</span><br><span class="line">good</span><br><span class="line">study</span><br><span class="line">day</span><br><span class="line">day</span><br><span class="line">up</span><br><span class="line">15:33:57,725 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   [] - Removing cache directory &#x2F;var&#x2F;folders&#x2F;zq&#x2F;2b48w4_x5vq89_jrz3yns13h0000gn&#x2F;T&#x2F;flink-web-ui</span><br><span class="line">15:33:57,727 INFO  org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint   [] - Shut down complete.</span><br><span class="line">15:33:57,729 INFO  org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Shut down cluster because application is in CANCELED, diagnostics DispatcherResourceManagerComponent has been closed..</span><br><span class="line">15:33:57,729 INFO  org.apache.flink.runtime.io.disk.FileChannelManagerImpl      [] - FileChannelManager removed spill file directory &#x2F;var&#x2F;folders&#x2F;zq&#x2F;2b48w4_x5vq89_jrz3yns13h0000gn&#x2F;T&#x2F;flink-io-dc01cff1-6b52-43ed-9d16-9085f49c732e</span><br><span class="line">15:33:57,730 INFO  org.apache.flink.runtime.io.network.NettyShuffleEnvironment  [] - Shutting down the network environment and its components.</span><br><span class="line">15:33:57,730 INFO  org.apache.flink.runtime.entrypoint.component.DispatcherResourceManagerComponent [] - Closing components.</span><br><span class="line">15:33:57,730 INFO  org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Stopping SessionDispatcherLeaderProcess.</span><br><span class="line">15:33:57,730 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Stopping dispatcher akka:&#x2F;&#x2F;flink&#x2F;user&#x2F;rpc&#x2F;dispatcher_2.</span><br><span class="line">15:33:57,731 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Stopping all currently running jobs of dispatcher akka:&#x2F;&#x2F;flink&#x2F;user&#x2F;rpc&#x2F;dispatcher_2.</span><br><span class="line">15:33:57,731 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerImpl [] - Closing the SlotManager.</span><br><span class="line">15:33:57,731 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerImpl [] - Suspending the SlotManager.</span><br><span class="line">15:33:57,731 INFO  org.apache.flink.runtime.rest.handler.legacy.backpressure.BackPressureRequestCoordinator [] - Shutting down back pressure request coordinator.</span><br><span class="line">15:33:57,731 INFO  org.apache.flink.runtime.io.disk.FileChannelManagerImpl      [] - FileChannelManager removed spill file directory &#x2F;var&#x2F;folders&#x2F;zq&#x2F;2b48w4_x5vq89_jrz3yns13h0000gn&#x2F;T&#x2F;flink-netty-shuffle-41571afb-b13e-494b-b937-0696d2c77ca1</span><br><span class="line">15:33:57,732 INFO  org.apache.flink.runtime.taskexecutor.KvStateService         [] - Shutting down the kvState service and its components.</span><br><span class="line">15:33:57,732 INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Stopped dispatcher akka:&#x2F;&#x2F;flink&#x2F;user&#x2F;rpc&#x2F;dispatcher_2.</span><br><span class="line">15:33:57,732 INFO  org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Stop job leader service.</span><br><span class="line">15:33:57,734 INFO  org.apache.flink.runtime.filecache.FileCache                 [] - removed file cache directory &#x2F;var&#x2F;folders&#x2F;zq&#x2F;2b48w4_x5vq89_jrz3yns13h0000gn&#x2F;T&#x2F;flink-dist-cache-fc4fd5a1-79fa-4a19-8d7d-f3072006c91e</span><br><span class="line">15:33:57,735 INFO  org.apache.flink.runtime.taskexecutor.TaskExecutor           [] - Stopped TaskExecutor akka:&#x2F;&#x2F;flink&#x2F;user&#x2F;rpc&#x2F;taskmanager_0.</span><br><span class="line">15:33:57,735 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Stopping Akka RPC service.</span><br><span class="line">15:33:57,760 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Stopping Akka RPC service.</span><br><span class="line">15:33:57,760 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Stopped Akka RPC service.</span><br><span class="line">15:33:57,766 INFO  org.apache.flink.runtime.blob.PermanentBlobCache             [] - Shutting down BLOB cache</span><br><span class="line">15:33:57,768 INFO  org.apache.flink.runtime.blob.TransientBlobCache             [] - Shutting down BLOB cache</span><br><span class="line">15:33:57,772 INFO  org.apache.flink.runtime.blob.BlobServer                     [] - Stopped BLOB server at 0.0.0.0:58212</span><br><span class="line">15:33:57,772 INFO  org.apache.flink.runtime.rpc.akka.AkkaRpcService             [] - Stopped Akka RPC service.</span><br></pre></td></tr></table></figure><p><img src="/images/%E5%A4%A7%E6%95%B0%E6%8D%AE/newproject12.png" alt="newproject12"></p><p>可以看到，我们的代码已经执行，并且生效。这样子我们的开发环境就搭建完毕了。其他的基本大同小异，如需记录，后面会额外的篇章进行记录。</p><h2 id="项目打包并提交Flink集群执行"><a href="#项目打包并提交Flink集群执行" class="headerlink" title="项目打包并提交Flink集群执行"></a>项目打包并提交Flink集群执行</h2><p>直到刚才为止，我们都是本地开发的模式，但是如果要在生产环境运行，那么我们需要打包成jar，然后借助flink-client提交job图对象给flink-job-manager，然后再分发给各个的taskManager进行执行。</p><p>所以这里我们需要打包出<code>jar</code>包。</p><p>我们使用Maven的<code>mvn clean package</code>命令可以很方便地进行打包。</p><p>如果需要额外指定一些内容的话，可以使用<code>mvn clean package -Dfile.encoding=UTF-8 -DskipTests=true</code>，这样子可以忽略测试阶段。</p><p>利用docker</p><ul><li>构架一个flink1.12的集群</li><li>构建一个maven工具，版本3.6.3（由于idea采用的是内置的maven，这是是一个独立的jar包，所以外部无法直接引用mvn命令）</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker pull flink:1.12-scala_2.11-java8</span><br><span class="line">docker pull maven:3.6.3</span><br></pre></td></tr></table></figure><p>在代码目录下打包出jar包</p><blockquote><p>记得打包成<code>生成环境的jar包</code>的时候，把<code>&lt;scope /&gt;</code>改回 <code>provided</code>, 也就是把注释取消掉。</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">➜  my-flink-jdk11 docker run --rm -it -v  ~/.m2:/root/.m2 -v $(PWD):/www -w /www maven:3.6.3 mvn clean package</span><br><span class="line">[INFO] Scanning for projects...</span><br><span class="line">[INFO] </span><br><span class="line">[INFO] ----------------------&lt; my-flink:my-flink-jdk11 &gt;-----------------------</span><br><span class="line">[INFO] Building Flink Quickstart Job 1.0-SNAPSHOT</span><br><span class="line">[INFO] --------------------------------[ jar ]---------------------------------</span><br><span class="line">[INFO] </span><br><span class="line">[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ my-flink-jdk11 ---</span><br><span class="line">[INFO] Deleting /www/target</span><br><span class="line">[INFO] </span><br><span class="line">[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ my-flink-jdk11 ---</span><br><span class="line">[INFO] Using 'UTF-8' encoding to copy filtered resources.</span><br><span class="line">[INFO] Copying 1 resource</span><br><span class="line">[INFO] </span><br><span class="line">[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ my-flink-jdk11 ---</span><br><span class="line">[INFO] Changes detected - recompiling the module!</span><br><span class="line">[INFO] Compiling 2 source files to /www/target/classes</span><br><span class="line">[INFO] </span><br><span class="line">[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ my-flink-jdk11 ---</span><br><span class="line">[INFO] Using 'UTF-8' encoding to copy filtered resources.</span><br><span class="line">[INFO] skip non existing resourceDirectory /www/src/test/resources</span><br><span class="line">[INFO] </span><br><span class="line">[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ my-flink-jdk11 ---</span><br><span class="line">[INFO] No sources to compile</span><br><span class="line">[INFO] </span><br><span class="line">[INFO] --- maven-surefire-plugin:2.12.4:test (default-test) @ my-flink-jdk11 ---</span><br><span class="line">[INFO] No tests to run.</span><br><span class="line">[INFO] </span><br><span class="line">[INFO] --- maven-jar-plugin:2.4:jar (default-jar) @ my-flink-jdk11 ---</span><br><span class="line">[INFO] Building jar: /www/target/my-flink-jdk11-1.0-SNAPSHOT.jar</span><br><span class="line">[INFO] </span><br><span class="line">[INFO] --- maven-shade-plugin:3.1.1:shade (default) @ my-flink-jdk11 ---</span><br><span class="line">[INFO] Excluding org.slf4j:slf4j-api:jar:1.7.15 from the shaded jar.</span><br><span class="line">[INFO] Excluding org.apache.logging.log4j:log4j-slf4j-impl:jar:2.12.1 from the shaded jar.</span><br><span class="line">[INFO] Excluding org.apache.logging.log4j:log4j-api:jar:2.12.1 from the shaded jar.</span><br><span class="line">[INFO] Excluding org.apache.logging.log4j:log4j-core:jar:2.12.1 from the shaded jar.</span><br><span class="line">[INFO] Replacing original artifact with shaded artifact.</span><br><span class="line">[INFO] Replacing /www/target/my-flink-jdk11-1.0-SNAPSHOT.jar with /www/target/my-flink-jdk11-1.0-SNAPSHOT-shaded.jar</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] BUILD SUCCESS</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] Total time:  8.988 s</span><br><span class="line">[INFO] Finished at: 2021-09-28T08:30:35Z</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br></pre></td></tr></table></figure><p>转移就构建成功了。</p><p>在<code>target</code>目录下查看<code>jar包</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">➜  my-flink-jdk11 ll target/my-flink-jdk11-1.0-SNAPSHOT.jar </span><br><span class="line">-rw-r--r--  1 caiwenhui  staff   6.6K Sep 28 16:30 target/my-flink-jdk11-1.0-SNAPSHOT.jar</span><br></pre></td></tr></table></figure><p>同样把代码目录挂载进flink容器，然后构建flink容器（此步骤只要是拿到target目录下的jar包，如果你指定了其他路径换个挂载目录也可以）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -it --name flinkc --privileged  -w /www -v$(PWD):/www -p 8081:8081 flink:1.12-scala_2.11-java8 bash</span><br></pre></td></tr></table></figure><blockquote><p>8081是flink webui的服务，具体的内容后面再说</p></blockquote><p>进到容器后，启动单机版flink集群</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">flink@ed5e7ea28514:/www$ start-cluster.sh</span><br><span class="line">Starting cluster.</span><br><span class="line">Starting standalonesession daemon on host ed5e7ea28514.</span><br><span class="line">Starting taskexecutor daemon on host ed5e7ea28514</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">flink@ed5e7ea28514:&#x2F;www$ flink run --class my.flink.BatchJob .&#x2F;target&#x2F;my-flink-jdk11-1.0-SNAPSHOT.jar</span><br><span class="line">Job has been submitted with JobID ca99d6d7ef6f913ac334d7123d63658b</span><br><span class="line">Program execution finished</span><br><span class="line">Job with JobID ca99d6d7ef6f913ac334d7123d63658b has finished.</span><br><span class="line">Job Runtime: 187 ms</span><br><span class="line">Accumulator Results:</span><br><span class="line">- 40a6a5d6af948dba01cbb7bee71f2d4e (java.util.ArrayList) [6 elements]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">good</span><br><span class="line">good</span><br><span class="line">study</span><br><span class="line">day</span><br><span class="line">day</span><br><span class="line">up</span><br></pre></td></tr></table></figure><p>可以看到，可以这个结果和我们再IDEA执行的结果一致，所以开发环境搭建完毕。后面的篇章将会是具体的流计算内容。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;由于最近要推动flink的流计算代替我们的内部的自行研发的mstream流计算服务，所以需要对flink进行开发。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="http://blog.crazylaw.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="flink" scheme="http://blog.crazylaw.cn/tags/flink/"/>
    
  </entry>
  
</feed>

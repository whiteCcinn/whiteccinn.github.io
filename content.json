{"meta":{"title":"白菜君の技术库","subtitle":null,"description":"知道做不到，等于不知道","author":"白菜(whiteCcinn)","url":"http://blog.crazylaw.cn","root":"/"},"pages":[{"title":"关于我","date":"2022-04-11T16:00:00.000Z","updated":"2022-04-12T15:26:27.405Z","comments":true,"path":"about/index.html","permalink":"http://blog.crazylaw.cn/about/index.html","excerpt":"","text":"Hi there 👋Hello, I’m whiteCcinn, Chinese name is wen-hui CAI, nickname can call me “白菜🥬” in Chinese, I am a full stack development engineer, standard enterprise, has a lot of interest in the application of distributed storage, so language are: I like Rust, Golang, C, C++ , of course, I also like PHP, because this is my most good at language, hope you like me, too Contact me Company: @Topon Address：GuangZhou, GuangDong, China Github: https://github.com/whiteCcinn Blog: https://whiteccinn.github.io/ QQ: &#52;&#55;&#x31;&#x31;&#49;&#51;&#55;&#x34;&#x34;&#x40;&#113;&#x71;&#x2e;&#x63;&#x6f;&#x6d;"},{"title":"分类","date":"2019-03-18T16:00:00.000Z","updated":"2021-03-20T16:25:01.824Z","comments":true,"path":"categories/index.html","permalink":"http://blog.crazylaw.cn/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2019-03-18T16:00:00.000Z","updated":"2021-03-20T16:25:01.824Z","comments":true,"path":"tags/index.html","permalink":"http://blog.crazylaw.cn/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"mock服务架构设计(二)","slug":"架构设计/mock服务架构设计(二)","date":"2024-06-18T14:52:40.000Z","updated":"2024-06-18T13:51:14.857Z","comments":true,"path":"2024/06/18/架构设计/mock服务架构设计(二)/","link":"","permalink":"http://blog.crazylaw.cn/2024/06/18/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/mock%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1(%E4%BA%8C)/","excerpt":"前言上一篇《mock服务架构设计（一）》中，我们发现","text":"前言上一篇《mock服务架构设计（一）》中，我们发现 以下会以一个常规的adx架构来进行说明。 在这个架构中，我们简单的把整体的架构，分为了三部分，分别是： 中间件: redis集群，mongo服务 业务程序: adx 上游角色: dsp 在实际业务中我们的redis可能会丢服务进行一些例如qps，临时缓存的动作，所以是必不可少的一部分。mongo服务则是充当我们的数据存储层，dsp就是我们adx服务的上游，我们的广告竞价的玩家就是来自于dsp。 这三个角色大致了解和理解之后，我们会发现，一般的设计中，由于要符合adx的低延时，高并发的行为，我们一般会把服务至于同一个内网中（虽然实际的架构设计可能是多个网关，相互行为NAT行为的内部链路），用以减少我们内部之间的网络耗时。对外的请求，只有向上游DSP进行询价的时候。 所以既然我们要mock的话，那很显然，我们需要mock我们的dsp服务，但是我们并不希望去调整所谓的上游EP，或者在内部编写hard code代码，虽然在github开源库上我们找到了一些类似滴滴开发的https://github.com/didi/sharingan的流量录制回放服务。但是由于其使用上的繁琐和不便等因素，以及需求的本质性，我认为，暂时不需呀用到这种大规模的流量录制回放服务。 所以对于这个问题，只需要采用我们新的架构设计，就可以实现这一点。 在上图中，我们可以发现，我们引入了一个常见的角色iptables，这是一个网络防火墙工具，也是内核netfiler的hook工具，他能让我们的流量在经过系统内部的时候，对其进行额外的行为。 Netfilter-packet-flow，这是netfilter系统的工作流程，感兴趣的需要额外去了解哈 理论我们借助iptables去实现每个hook节点的行为。用以实现 adx服务器可以与Redis集群正常通信 adx服务器可以与mongo正常通信 adx对dsp的请求需要转发到mock-dsp服务器 配置adx服务器的iptables规则如下： ADX服务器的IP为 172.18.0.100/16 目标mock-dsp的IP为 172.18.0.200/16, 端口为8080 Redis集群的IP范围为 172.18.0.0/16(具体ip不用理会), Redis集群的端口范围为 7001-7006, 总共6台机器 MongoDB服务IP范围为 172.18.0.0/16(具体ip不用理会), 的端口为 27017 配置NAT表将非Redis和非MongoDB流量转发到目标内网(MOCK-DSP机器: 1234567# 确保 Redis 和 MongoDB 的流量直接通过iptables -t nat -A OUTPUT -p tcp -d 172.18.0.0/16 --dport 7001:7006 -j ACCEPTiptables -t nat -A OUTPUT -p tcp -d 172.18.0.0/16 --dport 27017 -j ACCEPT# 将非 Redis 和非 MongoDB 的流量转发到 MOCK-DSP机器# 也就是外部的DSP请求全部转发到mock-dsp的机器上iptables -t nat -A OUTPUT -p tcp ! -d 172.18.0.0/16 -m multiport ! --dports 7001:7006,27017 -j DNAT --to-destination 172.18.0.200:8080 配置SNAT/MASQUERADE规则 (确保返回路径正确)123456# 确保 Redis 和 MongoDB 的流量直接通过iptables -t nat -A POSTROUTING -p tcp -d 172.18.0.0/16 --dport 7001:7006 -j ACCEPTiptables -t nat -A POSTROUTING -p tcp -d 172.18.0.0/16 --dport 27017 -j ACCEPT# 确保转发到mock-dsp器的流量可以正确返回iptables -t nat -A POSTROUTING -d 172.18.0.200 -j MASQUERADE 配置filter表规则（可选，但推荐） 但是实际上我感觉没必要，只是按照规范来说，是需要加上 配置INPUT规则（允许从Redis集群和MongoDB服务来的连接） 1234# 允许来自 Redis 集群的连接iptables -A INPUT -p tcp -s 172.18.0.0/16 --dport 7001:7006 -m state --state NEW,ESTABLISHED -j ACCEPT# 允许来自 MongoDB 服务的连接iptables -A INPUT -p tcp -s 172.18.0.0/16 --dport 27017 -m state --state NEW,ESTABLISHED -j ACCEPT 配置OUTPUT规则（允许到Redis集群和MongoDB服务的连接） 123456# 允许到 Redis 集群的连接iptables -A OUTPUT -p tcp -d 172.18.0.0/16 --dport 7001:7006 -m state --state NEW,ESTABLISHED -j ACCEPT# 允许到 MongoDB 服务的连接iptables -A OUTPUT -p tcp -d 172.18.0.0/16 --dport 27017 -m state --state NEW,ESTABLISHED -j ACCEPT# 允许到mock-dsp的转发流量（非 Redis 和非 MongoDB 流量）iptables -A OUTPUT -p tcp -d 172.18.0.200 -m state --state NEW,ESTABLISHED -j ACCEPT 完整的例子如下：123456789101112131415161718192021222324# DNAT规则：将非 Redis 和非 MongoDB 的流量转发到MOCK-DSP机器iptables -t nat -A OUTPUT -p tcp -d 172.18.0.0/16 --dport 7001:7006 -j ACCEPTiptables -t nat -A OUTPUT -p tcp -d 172.18.0.0/16 --dport 27017 -j ACCEPTiptables -t nat -A OUTPUT -p tcp ! -d 172.18.0.0/16 -m multiport ! --dports 7001:7006,27017 -j DNAT --to-destination 172.18.0.200:8080# SNAT/MASQUERADE规则：确保返回路径正确iptables -t nat -A POSTROUTING -p tcp -d 172.18.0.0/16 --dport 7001:7006 -j ACCEPTiptables -t nat -A POSTROUTING -p tcp -d 172.18.0.0/16 --dport 27017 -j ACCEPTiptables -t nat -A POSTROUTING -d 172.18.0.200 -j MASQUERADE# 允许来自 Redis 集群的连接iptables -A INPUT -p tcp -s 172.18.0.0/16 --dport 7001:7006 -m state --state NEW,ESTABLISHED -j ACCEPT# 允许来自 MongoDB 服务的连接iptables -A INPUT -p tcp -s 172.18.0.0/16 --dport 27017 -m state --state NEW,ESTABLISHED -j ACCEPT# 允许到 Redis 集群的连接iptables -A OUTPUT -p tcp -d 172.18.0.0/16 --dport 7001:7006 -m state --state NEW,ESTABLISHED -j ACCEPT# 允许到 MongoDB 服务的连接iptables -A OUTPUT -p tcp -d 172.18.0.0/16 --dport 27017 -m state --state NEW,ESTABLISHED -j ACCEPT# 允许到mock-dsp的转发流量（非 Redis 和非 MongoDB 流量）iptables -A OUTPUT -p tcp -d 172.18.0.200 -m state --state NEW,ESTABLISHED -j ACCEPT 理论看明白之后，我们再回过头来看这个问题，是不是发现**清晰明了** 实战本地docker模拟整体架构 这里，我们直接上2张图，分别代表了docker容器服务, 容器的network信息。可以看到基本信息都不变, 重点关注一下这里的&lt;dsp&gt; = &lt;mock dsp&gt;， 并且实际ip从172.18.0.200/16 =&gt; 172.168.0.10/16。 获取这些信息之后，对我们iptables-rule文件调整了一下，代码如下： 12345678910111213141516171819202122232425262728# 文件名：iptables-rules# filter表的规则*filter:INPUT ACCEPT [0:0]:FORWARD ACCEPT [0:0]:OUTPUT ACCEPT [0:0]# 允许宿主机的请求请求可以在宿主机直接打到到我们的adx服务# 其实不设置也可以，因为上面已经设置了默认协议为`ACCEPT`-A INPUT -p tcp -m state --state NEW -m tcp --dport 1372 -j ACCEPTCOMMIT# nat表的规则*nat:PREROUTING ACCEPT [0:0]:INPUT ACCEPT [0:0]:POSTROUTING ACCEPT [0:0]:OUTPUT ACCEPT [0:0]-A OUTPUT -p tcp -d 172.18.0.0/16 --dport 7001:7006 -j ACCEPT-A OUTPUT -p tcp -d 172.18.0.0/16 --dport 27017 -j ACCEPT-A OUTPUT -p tcp ! -d 172.18.0.0/16 -m multiport ! --dports 7001:7006,27017 -j DNAT --to-destination 172.18.0.10:8080-A POSTROUTING -p tcp -d 172.18.0.0/16 --dport 7001:7006 -j ACCEPT-A POSTROUTING -p tcp -d 172.18.0.0/16 --dport 27017 -j ACCEPT-A POSTROUTING -d 172.18.0.10 -j MASQUERADECOMMIT 在我们的adx服务器上设置iptables规则，执行命令 1iptables-restore &lt; iptables-rules 查看我们服务器上的iptables规则： 12345678910111213141516171819root@87cb4c5933af:# iptables -nvL -t natChain PREROUTING (policy ACCEPT 2 packets, 120 bytes) pkts bytes target prot opt in out source destinationChain INPUT (policy ACCEPT 2 packets, 120 bytes) pkts bytes target prot opt in out source destinationChain POSTROUTING (policy ACCEPT 367 packets, 21822 bytes) pkts bytes target prot opt in out source destination 11 660 ACCEPT tcp -- * * 0.0.0.0/0 172.18.0.0/16 tcp dpts:7001:7006 2 120 ACCEPT tcp -- * * 0.0.0.0/0 172.18.0.0/16 tcp dpt:27017 5 300 MASQUERADE all -- * * 0.0.0.0/0 172.18.0.10Chain OUTPUT (policy ACCEPT 367 packets, 21822 bytes) pkts bytes target prot opt in out source destination 11 660 ACCEPT tcp -- * * 0.0.0.0/0 172.18.0.0/16 tcp dpts:7001:7006 2 120 ACCEPT tcp -- * * 0.0.0.0/0 172.18.0.0/16 tcp dpt:27017 5 300 DNAT tcp -- * * 0.0.0.0/0 !172.18.0.0/16 multiport dports !7001:7006,27017 to:172.18.0.10:8080# Warning: iptables-legacy tables present, use iptables-legacy to see them 这里可以看到，我们的规则已经按要求生效 接下来就是接着我们的启动adx服务和 mock-dsp服务 启动完毕之后，我们在宿主机把入参请求发到容器中的adx服务中 1req=`cat filter.json| jq '.req'`;curl -X POST 'http://127.0.0.1:1372/adx_api?pubid=xxxxx' -d \"$req\" 这里的filter.json，是从线上bid日志进行match下来的信息，也是后续我们要放到mock-dsp中的数据源当然，更好的方式是从kafaka中自动更新信息 通过 tcpdump -n，我们可以看到，这里三次握手的SYN第一步都是向着mock-dsp的机器发出的请求，也就是172.18.0.10/16 这个地址发出，所以我们的nat表的对流量转发完美的处理了所有流量的流向。 并且不一会儿我们也得到了一段正常的广告offer填充！ mock-dsp的简易代码：123456789101112131415161718192021222324252627282930313233343536373839404142package mainimport ( \"fmt\" \"github.com/gin-gonic/gin\" \"github.com/goccy/go-json\" \"io/ioutil\" \"net/http\")type FileT struct &#123; Req interface&#123;&#125; `json:\"req\"` Resp interface&#123;&#125; `json:\"resp\"`&#125;func main() &#123; // 启动gin框架，采用默认配置 router := gin.Default() b, err := ioutil.ReadFile(\"filter.json\") if err != nil &#123; panic(err) &#125; var f FileT err = json.Unmarshal(b, &amp;f) if err != nil &#123; panic(err) &#125; // 编写匿名的handler函数 router.POST(\"/request/:dsp_id\", func(c *gin.Context) &#123; dspId := c.Param(\"dsp_id\") c.JSON(http.StatusOK, f.Resp) &#125;) router.GET(\"/request/:dsp_id\", func(c *gin.Context) &#123; dspId := c.Param(\"dsp_id\") c.JSON(http.StatusOK, f.Resp) &#125;) router.Run() //:8080&#125; 后续我们可以对这个mock-dsp进行更加丰富的针对性处理 通过kafka代替文件的方式定时更新offer 针对adformat的和dsp_id进行匹配，得到不同offer的返回 总结这一次，我们通过这种架构设计，可以做到后续我们对我们的一些需求得到更好的支持。 代码调优 开发同学自测逻辑 测试同学测试逻辑 对服务进行压测","categories":[{"name":"架构设计","slug":"架构设计","permalink":"http://blog.crazylaw.cn/categories/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/"}],"tags":[{"name":"mock","slug":"mock","permalink":"http://blog.crazylaw.cn/tags/mock/"},{"name":"机器学习","slug":"机器学习","permalink":"http://blog.crazylaw.cn/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"随机森林","slug":"随机森林","permalink":"http://blog.crazylaw.cn/tags/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/"}]},{"title":"mock服务架构设计(一)","slug":"架构设计/mock服务架构设计(一)","date":"2024-05-25T03:52:40.000Z","updated":"2024-06-18T13:47:47.003Z","comments":true,"path":"2024/05/25/架构设计/mock服务架构设计(一)/","link":"","permalink":"http://blog.crazylaw.cn/2024/05/25/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/mock%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1(%E4%B8%80)/","excerpt":"前言在公司的日常adx开发流程中，我发现其中一个很费心，很费时的行为，那就是要拿到上游的广告offer。 要拿到一个正常的offer，不单单需要匹配内部的逻辑，还要看上游是否需要对你的这次展示机会进行竞价。 由于在adx的业务逻辑中，一次请求会同时请求N次上游，所以必然少不了对上游的询价(请求)行为，那我们在日常开发，以及做代码调试的过程中，有没有办法可以更好的做到能拿到offer来对我们的竞价行为进行调试和开发呢？ 还有一种场景就是测试同学会面临的，就是如何在尽量不影响线上数据的情况下，能安全无痛的进行测试adx呢，如果因为测试的行为，导致线上业务数据异常，或者出现了结算问题，对于大规模流量的广告自动化交易程序来说，这都是致命的，严重点来说，可能随时导致破产。 又换句话说，有没有办法能在我们想要做模拟压测的时候，也能进行正常的内部逻辑呢？其中一个难点就是我们在压测的环节，并不想正式去对上游询价，但是我又想要拿到上游的offer，除了 hard code offer的动作外，有没有一些更理想的架构方案呢？ 答案是：有的，接下来就是我们的主题","text":"前言在公司的日常adx开发流程中，我发现其中一个很费心，很费时的行为，那就是要拿到上游的广告offer。 要拿到一个正常的offer，不单单需要匹配内部的逻辑，还要看上游是否需要对你的这次展示机会进行竞价。 由于在adx的业务逻辑中，一次请求会同时请求N次上游，所以必然少不了对上游的询价(请求)行为，那我们在日常开发，以及做代码调试的过程中，有没有办法可以更好的做到能拿到offer来对我们的竞价行为进行调试和开发呢？ 还有一种场景就是测试同学会面临的，就是如何在尽量不影响线上数据的情况下，能安全无痛的进行测试adx呢，如果因为测试的行为，导致线上业务数据异常，或者出现了结算问题，对于大规模流量的广告自动化交易程序来说，这都是致命的，严重点来说，可能随时导致破产。 又换句话说，有没有办法能在我们想要做模拟压测的时候，也能进行正常的内部逻辑呢？其中一个难点就是我们在压测的环节，并不想正式去对上游询价，但是我又想要拿到上游的offer，除了 hard code offer的动作外，有没有一些更理想的架构方案呢？ 答案是：有的，接下来就是我们的主题 以下会以一个常规的adx架构来进行说明。 在这个架构中，我们简单的把整体的架构，分为了三部分，分别是： 中间件: redis集群，mongo服务 业务程序: adx 上游角色: dsp 在实际业务中我们的redis可能会丢服务进行一些例如qps，临时缓存的动作，所以是必不可少的一部分。mongo服务则是充当我们的数据存储层，dsp就是我们adx服务的上游，我们的广告竞价的玩家就是来自于dsp。 这三个角色大致了解和理解之后，我们会发现，一般的设计中，由于要符合adx的低延时，高并发的行为，我们一般会把服务至于同一个内网中（虽然实际的架构设计可能是多个网关，相互行为NAT行为的内部链路），用以减少我们内部之间的网络耗时。对外的请求，只有向上游DSP进行询价的时候。 所以既然我们要mock的话，那很显然，我们需要mock我们的dsp服务，但是我们并不希望去调整所谓的上游EP，或者在内部编写hard code代码，虽然在github开源库上我们找到了一些类似滴滴开发的https://github.com/didi/sharingan的流量录制回放服务。但是由于其使用上的繁琐和不便等因素，以及需求的本质性，我认为，暂时不需呀用到这种大规模的流量录制回放服务。 所以对于这个问题，只需要采用我们新的架构设计，就可以实现这一点。 在上图中，我们可以发现，我们引入了一个常见的角色iptables，这是一个网络防火墙工具，也是内核netfiler的hook工具，他能让我们的流量在经过系统内部的时候，对其进行额外的行为。 Netfilter-packet-flow，这是netfilter系统的工作流程，感兴趣的需要额外去了解哈 理论我们借助iptables去实现每个hook节点的行为。用以实现 adx服务器可以与Redis集群正常通信 adx服务器可以与mongo正常通信 adx对dsp的请求需要转发到mock-dsp服务器 配置adx服务器的iptables规则如下： ADX服务器的IP为 172.18.0.100/16 目标mock-dsp的IP为 172.18.0.200/16, 端口为8080 Redis集群的IP范围为 172.18.0.0/16(具体ip不用理会), Redis集群的端口范围为 7001-7006, 总共6台机器 MongoDB服务IP范围为 172.18.0.0/16(具体ip不用理会), 的端口为 27017 配置NAT表将非Redis和非MongoDB流量转发到目标内网(MOCK-DSP机器: 1234567# 确保 Redis 和 MongoDB 的流量直接通过iptables -t nat -A OUTPUT -p tcp -d 172.18.0.0/16 --dport 7001:7006 -j ACCEPTiptables -t nat -A OUTPUT -p tcp -d 172.18.0.0/16 --dport 27017 -j ACCEPT# 将非 Redis 和非 MongoDB 的流量转发到 MOCK-DSP机器# 也就是外部的DSP请求全部转发到mock-dsp的机器上iptables -t nat -A OUTPUT -p tcp ! -d 172.18.0.0/16 -m multiport ! --dports 7001:7006,27017 -j DNAT --to-destination 172.18.0.200:8080 配置SNAT/MASQUERADE规则 (确保返回路径正确)123456# 确保 Redis 和 MongoDB 的流量直接通过iptables -t nat -A POSTROUTING -p tcp -d 172.18.0.0/16 --dport 7001:7006 -j ACCEPTiptables -t nat -A POSTROUTING -p tcp -d 172.18.0.0/16 --dport 27017 -j ACCEPT# 确保转发到mock-dsp器的流量可以正确返回iptables -t nat -A POSTROUTING -d 172.18.0.200 -j MASQUERADE 配置filter表规则（可选，但推荐） 但是实际上我感觉没必要，只是按照规范来说，是需要加上 配置INPUT规则（允许从Redis集群和MongoDB服务来的连接） 1234# 允许来自 Redis 集群的连接iptables -A INPUT -p tcp -s 172.18.0.0/16 --dport 7001:7006 -m state --state NEW,ESTABLISHED -j ACCEPT# 允许来自 MongoDB 服务的连接iptables -A INPUT -p tcp -s 172.18.0.0/16 --dport 27017 -m state --state NEW,ESTABLISHED -j ACCEPT 配置OUTPUT规则（允许到Redis集群和MongoDB服务的连接） 123456# 允许到 Redis 集群的连接iptables -A OUTPUT -p tcp -d 172.18.0.0/16 --dport 7001:7006 -m state --state NEW,ESTABLISHED -j ACCEPT# 允许到 MongoDB 服务的连接iptables -A OUTPUT -p tcp -d 172.18.0.0/16 --dport 27017 -m state --state NEW,ESTABLISHED -j ACCEPT# 允许到mock-dsp的转发流量（非 Redis 和非 MongoDB 流量）iptables -A OUTPUT -p tcp -d 172.18.0.200 -m state --state NEW,ESTABLISHED -j ACCEPT 完整的例子如下：123456789101112131415161718192021222324# DNAT规则：将非 Redis 和非 MongoDB 的流量转发到MOCK-DSP机器iptables -t nat -A OUTPUT -p tcp -d 172.18.0.0/16 --dport 7001:7006 -j ACCEPTiptables -t nat -A OUTPUT -p tcp -d 172.18.0.0/16 --dport 27017 -j ACCEPTiptables -t nat -A OUTPUT -p tcp ! -d 172.18.0.0/16 -m multiport ! --dports 7001:7006,27017 -j DNAT --to-destination 172.18.0.200:8080# SNAT/MASQUERADE规则：确保返回路径正确iptables -t nat -A POSTROUTING -p tcp -d 172.18.0.0/16 --dport 7001:7006 -j ACCEPTiptables -t nat -A POSTROUTING -p tcp -d 172.18.0.0/16 --dport 27017 -j ACCEPTiptables -t nat -A POSTROUTING -d 172.18.0.200 -j MASQUERADE# 允许来自 Redis 集群的连接iptables -A INPUT -p tcp -s 172.18.0.0/16 --dport 7001:7006 -m state --state NEW,ESTABLISHED -j ACCEPT# 允许来自 MongoDB 服务的连接iptables -A INPUT -p tcp -s 172.18.0.0/16 --dport 27017 -m state --state NEW,ESTABLISHED -j ACCEPT# 允许到 Redis 集群的连接iptables -A OUTPUT -p tcp -d 172.18.0.0/16 --dport 7001:7006 -m state --state NEW,ESTABLISHED -j ACCEPT# 允许到 MongoDB 服务的连接iptables -A OUTPUT -p tcp -d 172.18.0.0/16 --dport 27017 -m state --state NEW,ESTABLISHED -j ACCEPT# 允许到mock-dsp的转发流量（非 Redis 和非 MongoDB 流量）iptables -A OUTPUT -p tcp -d 172.18.0.200 -m state --state NEW,ESTABLISHED -j ACCEPT 理论看明白之后，我们再回过头来看这个问题，是不是发现**清晰明了** 实战本地docker模拟整体架构 这里，我们直接上2张图，分别代表了docker容器服务, 容器的network信息。可以看到基本信息都不变, 重点关注一下这里的&lt;dsp&gt; = &lt;mock dsp&gt;， 并且实际ip从172.18.0.200/16 =&gt; 172.168.0.10/16。 获取这些信息之后，对我们iptables-rule文件调整了一下，代码如下： 12345678910111213141516171819202122232425262728# 文件名：iptables-rules# filter表的规则*filter:INPUT ACCEPT [0:0]:FORWARD ACCEPT [0:0]:OUTPUT ACCEPT [0:0]# 允许宿主机的请求请求可以在宿主机直接打到到我们的adx服务# 其实不设置也可以，因为上面已经设置了默认协议为`ACCEPT`-A INPUT -p tcp -m state --state NEW -m tcp --dport 1372 -j ACCEPTCOMMIT# nat表的规则*nat:PREROUTING ACCEPT [0:0]:INPUT ACCEPT [0:0]:POSTROUTING ACCEPT [0:0]:OUTPUT ACCEPT [0:0]-A OUTPUT -p tcp -d 172.18.0.0/16 --dport 7001:7006 -j ACCEPT-A OUTPUT -p tcp -d 172.18.0.0/16 --dport 27017 -j ACCEPT-A OUTPUT -p tcp ! -d 172.18.0.0/16 -m multiport ! --dports 7001:7006,27017 -j DNAT --to-destination 172.18.0.10:8080-A POSTROUTING -p tcp -d 172.18.0.0/16 --dport 7001:7006 -j ACCEPT-A POSTROUTING -p tcp -d 172.18.0.0/16 --dport 27017 -j ACCEPT-A POSTROUTING -d 172.18.0.10 -j MASQUERADECOMMIT 在我们的adx服务器上设置iptables规则，执行命令 1iptables-restore &lt; iptables-rules 查看我们服务器上的iptables规则： 12345678910111213141516171819root@87cb4c5933af:# iptables -nvL -t natChain PREROUTING (policy ACCEPT 2 packets, 120 bytes) pkts bytes target prot opt in out source destinationChain INPUT (policy ACCEPT 2 packets, 120 bytes) pkts bytes target prot opt in out source destinationChain POSTROUTING (policy ACCEPT 367 packets, 21822 bytes) pkts bytes target prot opt in out source destination 11 660 ACCEPT tcp -- * * 0.0.0.0/0 172.18.0.0/16 tcp dpts:7001:7006 2 120 ACCEPT tcp -- * * 0.0.0.0/0 172.18.0.0/16 tcp dpt:27017 5 300 MASQUERADE all -- * * 0.0.0.0/0 172.18.0.10Chain OUTPUT (policy ACCEPT 367 packets, 21822 bytes) pkts bytes target prot opt in out source destination 11 660 ACCEPT tcp -- * * 0.0.0.0/0 172.18.0.0/16 tcp dpts:7001:7006 2 120 ACCEPT tcp -- * * 0.0.0.0/0 172.18.0.0/16 tcp dpt:27017 5 300 DNAT tcp -- * * 0.0.0.0/0 !172.18.0.0/16 multiport dports !7001:7006,27017 to:172.18.0.10:8080# Warning: iptables-legacy tables present, use iptables-legacy to see them 这里可以看到，我们的规则已经按要求生效 接下来就是接着我们的启动adx服务和 mock-dsp服务 启动完毕之后，我们在宿主机把入参请求发到容器中的adx服务中 1req=`cat filter.json| jq '.req'`;curl -X POST 'http://127.0.0.1:1372/adx_api?pubid=xxxxx' -d \"$req\" 这里的filter.json，是从线上bid日志进行match下来的信息，也是后续我们要放到mock-dsp中的数据源当然，更好的方式是从kafaka中自动更新信息 通过 tcpdump -n，我们可以看到，这里三次握手的SYN第一步都是向着mock-dsp的机器发出的请求，也就是172.18.0.10/16 这个地址发出，所以我们的nat表的对流量转发完美的处理了所有流量的流向。 并且不一会儿我们也得到了一段正常的广告offer填充！ mock-dsp的简易代码：123456789101112131415161718192021222324252627282930313233343536373839404142package mainimport ( \"fmt\" \"github.com/gin-gonic/gin\" \"github.com/goccy/go-json\" \"io/ioutil\" \"net/http\")type FileT struct &#123; Req interface&#123;&#125; `json:\"req\"` Resp interface&#123;&#125; `json:\"resp\"`&#125;func main() &#123; // 启动gin框架，采用默认配置 router := gin.Default() b, err := ioutil.ReadFile(\"filter.json\") if err != nil &#123; panic(err) &#125; var f FileT err = json.Unmarshal(b, &amp;f) if err != nil &#123; panic(err) &#125; // 编写匿名的handler函数 router.POST(\"/request/:dsp_id\", func(c *gin.Context) &#123; dspId := c.Param(\"dsp_id\") c.JSON(http.StatusOK, f.Resp) &#125;) router.GET(\"/request/:dsp_id\", func(c *gin.Context) &#123; dspId := c.Param(\"dsp_id\") c.JSON(http.StatusOK, f.Resp) &#125;) router.Run() //:8080&#125; 后续我们可以对这个mock-dsp进行更加丰富的针对性处理 通过kafka代替文件的方式定时更新offer 针对adformat的和dsp_id进行匹配，得到不同offer的返回 总结这一次，我们通过这种架构设计，可以做到后续我们对我们的一些需求得到更好的支持。 代码调优 开发同学自测逻辑 测试同学测试逻辑 对服务进行压测","categories":[{"name":"架构设计","slug":"架构设计","permalink":"http://blog.crazylaw.cn/categories/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/"}],"tags":[{"name":"mock","slug":"mock","permalink":"http://blog.crazylaw.cn/tags/mock/"},{"name":"iptables","slug":"iptables","permalink":"http://blog.crazylaw.cn/tags/iptables/"}]},{"title":"树莓派4b Home Assistant","slug":"智能家居/Home-Assistant-pi","date":"2024-01-04T16:00:19.000Z","updated":"2024-01-10T10:55:23.444Z","comments":true,"path":"2024/01/05/智能家居/Home-Assistant-pi/","link":"","permalink":"http://blog.crazylaw.cn/2024/01/05/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/Home-Assistant-pi/","excerpt":"前言前面做了几期关于Home assistant的教程，在前面我是完全使用docker手动部署，并且采用基本都是通过安装home-assistant-core去处理。后面我发现很多东西的使用方式都特别别扭，而且确实很多东西都比较麻烦，所以后面再反过来看，发现了一个叫hass的生态，这个其实就是home-assistant的周边生态，而这些周边，几乎都是必备的，于是我妥协了使用了hass的生态，但是有一点不同的是，我这里并没有使用hassOS，而是采用hassIO，也就是home-assistant-supervisor。 下面开始，我就会我的正式体现记录我后面的智能家居做法。","text":"前言前面做了几期关于Home assistant的教程，在前面我是完全使用docker手动部署，并且采用基本都是通过安装home-assistant-core去处理。后面我发现很多东西的使用方式都特别别扭，而且确实很多东西都比较麻烦，所以后面再反过来看，发现了一个叫hass的生态，这个其实就是home-assistant的周边生态，而这些周边，几乎都是必备的，于是我妥协了使用了hass的生态，但是有一点不同的是，我这里并没有使用hassOS，而是采用hassIO，也就是home-assistant-supervisor。 下面开始，我就会我的正式体现记录我后面的智能家居做法。 硬件 树莓派4b+一个（我在写这篇文章的时候，已经出了树莓派5，但是由于ha还没公布稳定支持树莓派5，所以我还是购入了4b+，后面稳定后再替换） 效果展示 Home Assistant Supervisor 安装切换为root用户ccinn@raspberrypi:~ $ sudo su - 12sudo apt update -ysudo apt upgrade -y 查看树莓派版本 123456➜ ~ lsb_release -aNo LSB modules are available.Distributor ID: DebianDescription: Debian GNU/Linux 12 (bookworm)Release: 12Codename: bookworm 如果要完整支持Home Assistant Supervised，最好根据 看本地的系统是否满足 可以看到，截止到文章发布，官方文档说明是需要Debian12，我的树莓派系统是满足的。 NetworkManagerHomeAssistantSupervised需要NetworkManager的支持，树莓派官方使用的是ModemManager、openresolv和dhcpcd5。 原本的配置可以不动，但是需要固定Mac地址并禁用ModemManger。 因为NetworkManager一旦安装就会开始工作。所以我们先创建一个NetworkManager的配置文件来固定Mac地址，防止我们后续操作重启时候，树莓派的Mac地址频繁变更。 固定Mac地址预编写NetworkManager的配置： 1234# 创建配置目录和文件sudo mkdir -p /etc/NetworkManager/conf.d/# 对文件追加内容sudo vim /etc/NetworkManager/conf.d/100-disable-wifi-mac-randomization.conf 追加的内容： 12345[connection]wifi.mac-address-randomization=1[device]wifi.scan-rand-mac-address=no 之后就可以安装network-manager了。 安装完Network-Manager后，网络可能会出现短暂的丢包。这个时候多等一下就好，并且在完成ModemManager的禁用前，请勿重启树莓派系统！！！ 安装： 1sudo apt install -y network-manager 禁用ModemManager有些教程会让你卸载dhcpcd5，但是这样重启后需要重新配置网络，并且不能用树莓派的方法配置，这让我这没有显示器的用户很苦恼……所以这里我们就不卸载dhcpcd5，直接禁用ModemManager即可。 1234# 停止ModemManagersudo systemctl stop ModemManager# 禁止ModemManager开机自启sudo systemctl disable ModemManager 到此，NetworkManager部分就准备好了。 Apparmor1sudo apt install -y apparmor-utils jq software-properties-common apt-transport-https avahi-daemon ca-certificates curl dbus socat 但是需要注意，需要把Apparmor的启动配置参数加到树莓派的启动参数内（参考自：https://github.com/Kanga-Who/home-assistant/issues/25）： 12# 使用vim打开/boot/cmdline.txtsudo vim /boot/cmdline.txt 末尾添加：apparmor=1 security=apparmor OS-Agent还需要安装OS Agent。这个并没有在Debian的软件源内，所以我们需要使用dpkg安装。最新OS Agent的下载地址：https://github.com/home-assistant/os-agent/releases/latest 记得找最新的aarch版 1234# 下载OS Agent 1.2.2wget https://github.com/home-assistant/os-agent/releases/download/1.2.2/os-agent_1.2.2_linux_aarch64.deb# 使用dpkg安装sudo dpkg -i os-agent_1.2.2_linux_aarch64.deb 其他依赖12345678sudo apt-get install \\vim \\jq \\wget \\curl \\udisks2 \\libglib2.0-bin \\dbus -y 在2022.11.27后Homeassistant正式需要Systemd Journal的支持；我们同样可以使用软件包管理器进行安装： 1sudo apt install systemd-journal-remote -y Docker1234# 下载Docker安装脚本sudo curl -fsSL https://get.docker.com -o get-docker.sh# 使用阿里镜像源下载并安装Dockersudo sh get-docker.sh --mirror Aliyun 把我们自带的ccinn用户添加到docker用户组内： 这个用户根据自己的实际用户来有的人默认就是pi用户 1sudo usermod -aG docker ccinn 重启设备上诉操作，我们已经重新配置了网络、安装了依赖和添加了启动参数，所以在正式安装Home Assisistant Supervised前，我们需要重启设备。 Supervised现在开始安装Home Assisistant Supervised啦。 1234# 下载deb安装包wget https://github.com/home-assistant/supervised-installer/releases/latest/download/homeassistant-supervised.deb# 安装sudo dpkg -i homeassistant-supervised.deb 之后，没有问题就会出现选项卡，我们选择树莓派4B： 安装过程……根据自己的网络，这一步可能会卡很久，如果还是不行，记得科学一下再重新安装。 成果使用docker命令，查看Supervised的容器状态（如果并没有Homeassistant容器；那么等10min～20min再试试，期间保持树莓派运行，Homeassistant会组建初始化完成）： 进入IP:4357，可以查看Supervised的状态： 等待5分钟左右（Home Assisistant Supervised第一次启动比较慢），就可以通过IP:8123在浏览器访问了 至此ha的安装就ok了，接下来会继续一些辅助性的东西。 安装zsh12apt install zshsh -c \"$(curl -fsSL https://gitee.com/shmhlsy/oh-my-zsh-install.sh/raw/master/install.sh)\" 1234vim ~/.zshrc# 在最后一行加入export PROMPT='%n@%m:%F&#123;13&#125;%~ %F&#123;50&#125;%B%# %f%b' 如果想要复制ha容器中的配置出来，可以使用 1docker cp $(docker ps | grep -v NAMES | grep homeassistant | awk '&#123;print $1&#125;'):/config /home/ccinn/.homeassistant/ 这样子，/home/ccinn/.homeassistant 目录就是当前的了。 如果我们想要找到hassio中的ha容器挂载的目录是那里的话，可以用以下命令 123456789101112131415161718cat /var/lib/docker/containers/$(docker inspect homeassistant | jq -r '.[] | .Id')/config.v2.json | jq '.MountPoints.\"/config\"'# 下面为标准的路径&#123; \"Source\": \"/usr/share/hassio/homeassistant\", \"Destination\": \"/config\", \"RW\": true, \"Name\": \"\", \"Driver\": \"\", \"Type\": \"bind\", \"Propagation\": \"rprivate\", \"Spec\": &#123; \"Type\": \"bind\", \"Source\": \"/usr/share/hassio/homeassistant\", \"Target\": \"/config\" &#125;, \"SkipMountpointCreation\": true&#125; 可以看到，容器和宿主机的目录地址是/usr/share/hassio/homeassistant 所以，当我们需要手动安装插件，修改配置等等，直接操作这个目录即可。 安装 Samba我们知道，树莓派是一个小型主机，我们日常使用，肯定有我们自己的主机。我们可以通过Samba服务来进行文件的二次管理。把文件备份到我们的日常使用的主机上，或者直接在我们日常使用的主机上进行操作文件配置。 1sudo apt-get install samba samba-common-bin 修改配置文件 123456789sudo vim /etc/samba/smb.conf# 在文件底部添加[pi]path = /home/ccinn/.homeassistantwriteable=Yes create mask=0777 directory mask=0777 public=no 因为我上面用的路径和宿主机下的路径不一致，所以需要做个软连接 ln -s /usr/share/hassio/homeassistant /home/ccinn/.homeassistant, 这样子就能实现，samba和树莓派和ha容器的文件实时同步了。 保存文件后，添加samba用户 1sudo smbpasswd -a ccinn 注意：使用 sudo smbpasswd -a 命令创建用户时，创建的用户必须为 Linux 系统账户，如我这里的ccinn 输入自己的samba用户的密码，比如 a123456 重启服务让其生效 1sudo systemctl restart smbd Samba 安装完成后，树莓派的 Hass 开发环境搭建完毕。 通过 Samba 访问 Home Assistant 文件夹因为我的是macOS,所以这里用mac为例子。 在电脑桌面上打开 访达。 选择桌面左上角菜单中的 前往 &gt; 连接服务器。 在弹出的窗口中，输入以下地址后单击 连接。 1smb://192.168.8.189/pi 安装hacs在树莓派中执行如下命令。 1wget -U - nttps://get.nacs.xyz | bash - 可以看到，hacs会自动找到对应的目录进行安装。 然后外面通过samba，也可以看到在custom_components中多了个hacs的自定义组件。 回到ha-web(IP:8123)中，在用户-集成，添加集成，然后搜索HACS，点击一下hacs，就可以安装hacs了，那么接下来就可以用第三方内容了","categories":[{"name":"智能家居","slug":"智能家居","permalink":"http://blog.crazylaw.cn/categories/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/"},{"name":"树莓派","slug":"智能家居/树莓派","permalink":"http://blog.crazylaw.cn/categories/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/%E6%A0%91%E8%8E%93%E6%B4%BE/"}],"tags":[{"name":"智能家居","slug":"智能家居","permalink":"http://blog.crazylaw.cn/tags/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/"},{"name":"Home Assistant","slug":"Home-Assistant","permalink":"http://blog.crazylaw.cn/tags/Home-Assistant/"},{"name":"HA","slug":"HA","permalink":"http://blog.crazylaw.cn/tags/HA/"}]},{"title":"ssp-adx-localcache优化","slug":"公司/localcache","date":"2023-08-27T01:53:00.000Z","updated":"2023-08-28T09:40:43.131Z","comments":true,"path":"2023/08/27/公司/localcache/","link":"","permalink":"http://blog.crazylaw.cn/2023/08/27/%E5%85%AC%E5%8F%B8/localcache/","excerpt":"前言最近在处理ssp-adx-rtb的服务的性能优化，做了好多方面的优化，其中一个就是我们的本地的localcache的问题。 经过pprof的性能分析，发现cache2go，在 CPU Flame Graph 中，占比十分严重，基本大于1/3，既然是localcache，那么，我们的目的本意就是为了提速，所以占比那么大，是十分不合理的。 所以需要找到原因，并且解决它。降低cpu使用率，从而提高服务的QPS，减少服务器成本。","text":"前言最近在处理ssp-adx-rtb的服务的性能优化，做了好多方面的优化，其中一个就是我们的本地的localcache的问题。 经过pprof的性能分析，发现cache2go，在 CPU Flame Graph 中，占比十分严重，基本大于1/3，既然是localcache，那么，我们的目的本意就是为了提速，所以占比那么大，是十分不合理的。 所以需要找到原因，并且解决它。降低cpu使用率，从而提高服务的QPS，减少服务器成本。 cache2go旧版 cache2go https://github.com/muesli/cache2go 项目的描述为： 1Concurrency-safe Go caching library with expiration capabilities and access counters 并发安全，并且带有效期的和访问计数器的一个类库组件 我们需要用他来解决我们的3大核心问题 本地缓存 并发安全 带ttl功能 对于开源版本第一版本，我们已经做为处理了。 就是他的淘汰策略，是ttl+lru，当一个缓存在一定时间内被连续访问，或者在一个key，准备过期的时候，如果被访问，那么他的过期时间将继续延长到下一个周期。 这一特点，并不是我们需求，所以我们需要对这一点进行了调整，过期时间，只需要判断为 ttl 过期即可，不需要加上 lru 的方式。 这里就不展开细说。 cache2go新版 cache2go-new https://github.com/whiteCcinn/cache2go 在这一个版本，基本把整个库都按需重构了。主要是以下几个方面。 加入hash分片机制，把key打散到不同的bucket中，让bucket-lock的争抢降低 同一个cache-table，有且仅有一个goroutine，来处理 ttl 数据，并不会因为分片的个数调整带来更多的无效goroutine 没有采用渐进式的方式来删除key, 在 add, get 的阶段，尽量保持服务的高效性能，方式由于锁带来的性能衰减 采用双写机制，实现L1和L2的二级包装级别，从而做到 读写分离, 尽可能的避免在必要的场景下由于整个写锁导致读锁阻塞的问题，让后台在处理 ttl 和 重建map的过程中，服务依然高效提供服务 定期重建底层map属性，来释放map申请的内存，让整个服务相对处于一个内存稳定的状态 需求+机制，就可以在读写较多或者后台需要处理map的情况下，性能依旧保持有一个较好的性能体现。 为了实现这几点： 123456789101112131415161718192021type CacheTable struct &#123; sync.RWMutex hash *fnv64a // 用于hash shardMask uint64 // 用于hash的mask，在做按位与操作的时候，实现求余一样的行为，由于是位运算，效率一般都偏高 name string // cache的命名 L1Shards shardItems // L1的分片组 L2Shards shardItems // L2的分片组 cleanupInterval time.Duration // 定时处理ttl的数据 l1BlockChan []*CacheItem // 用于在L1分片组被后台处理过程中，暂时把数据缓存起来 l2BlockChan []*CacheItem // 用于在L2分片组被后台处理过程中，暂时把数据缓存起来 l1Mask int32 // L1原子计数器，用来代替lock，防止lock的堵塞现象，导致服务被影响 l2Mask int32 // L2原子计数器，用来代替lock，防止lock的堵塞现象，导致服务被影响 switchMask uint8 // 记录当前 cache-table是否在处理L1,L2分片组&#125; Add这是一个写入过程，实现起来也不算太复杂 123456789101112131415161718192021222324252627282930313233343536373839func (table *CacheTable) Add(key interface&#123;&#125;, lifeSpan time.Duration, data interface&#123;&#125;) *CacheItem &#123; item := NewCacheItem(key, lifeSpan, data) // 判断当前表是否在处理L1 if table.switchMask != 1&lt;&lt;1 &#123; // 记录L1正在处理写入行为，+1操作 atomic.AddInt32(&amp;table.l1Mask, 1) // 结束的时候L1写入的时候，-1操作 defer atomic.AddInt32(&amp;table.l1Mask, -1) // L1内部分片片级写锁开发 table.L1Shards[item.hashedKey&amp;table.shardMask].lock.Lock() // L1内部分片写入item table.L1Shards[item.hashedKey&amp;table.shardMask].m[item.key] = item // L1内部分片片级写锁结束 table.L1Shards[item.hashedKey&amp;table.shardMask].lock.Unlock() &#125; else &#123; // 如果当前后台在处理L1的话，那么先缓存起来 table.l1BlockChan = append(table.l1BlockChan, item) &#125; // 判断当前表是否在处理L2 if table.switchMask != 1&lt;&lt;2 &#123; // 记录L2正在处理写入行为，+1操作 atomic.AddInt32(&amp;table.l2Mask, 1) // 结束的时候L2写入的时候，-1操作 defer atomic.AddInt32(&amp;table.l2Mask, -1) // L2内部分片片级写锁开发 table.L2Shards[item.hashedKey&amp;table.shardMask].lock.Lock() // L2内部分片写入item table.L2Shards[item.hashedKey&amp;table.shardMask].m[item.key] = item // L2内部分片片级写锁结束 table.L2Shards[item.hashedKey&amp;table.shardMask].lock.Unlock() &#125; else &#123; // 如果当前后台在处理L2的话，那么先缓存起来 table.l2BlockChan = append(table.l2BlockChan, item) &#125; return item&#125; 通过双写的方式，实现L1和L2的同时写入,以此达到空间换时间的做法。 其中 (&amp; 2^(n-1)) 做到 (%m)的效果，并且由于是位运算，所以按理说效率会更高 Value Value 和 就是Get方法，获取key的item 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960func (table *CacheTable) Value(key interface&#123;&#125;, args ...interface&#123;&#125;) (*CacheItem, error) &#123; keyBytes, _ := json.Marshal(key) // 哈希的key hashedKey := table.hash.Sum64(string(keyBytes)) var sm *shardItem if table.switchMask == 1&gt;&gt;1 &#123; // 先查l1 sm = table.L1Shards[hashedKey&amp;table.shardMask] sm.lock.RLock() r, ok := sm.m[key] sm.lock.RUnlock() if ok &#123; // 正常返回结果 return r, nil &#125; // 再查l2 sm = table.L2Shards[hashedKey&amp;table.shardMask] sm.lock.RLock() r, ok = sm.m[key] sm.lock.RUnlock() if ok &#123; // 正常返回结果 return r, nil &#125; // 找不到key return nil, ErrKeyNotFound &#125; else if table.switchMask == 1&lt;&lt;1 &#123; // 正在处理l1，需要从l2读 sm = table.L2Shards[hashedKey&amp;table.shardMask] sm.lock.RLock() r, ok := sm.m[key] sm.lock.RUnlock() if ok &#123; // 正常返回结果 return r, nil &#125; // 找不到key return nil, ErrKeyNotFound &#125; else &#123; // 正在处理l2，需要从l1读 sm = table.L1Shards[hashedKey&amp;table.shardMask] sm.lock.RLock() r, ok := sm.m[key] sm.lock.RUnlock() if ok &#123; // 正常返回结果 return r, nil &#125; // 找不到key return nil, ErrKeyNotFound &#125;&#125; 可以看到这里，如果后台没有在操作L1, L2 的话，那么先从L1拿数据，然后再从L2拿数据 如果后台在操作L1, 那么只能从 L2 读取 如果后台在操作L2, 那么只能从 L1 读取 所以通过L1和L2，我们实现了一个读写分离的策略，并且在最大的程度上减少分片锁的读写锁冲突，从而提高服务的效率 后台任务123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169// 定时清理过期缓存go func(t *CacheTable, ctx context.Context) &#123; // 定时监测ttl数据 ticker := time.NewTicker(cleanInterval) // 定期重建map，以此来释放map申请的空间 reBuildTicker := time.NewTicker(30 * time.Minute) for &#123; select &#123; case &lt;-ctx.Done(): ticker.Stop() reBuildTicker.Stop() return case &lt;-ticker.C: // 表锁 t.Lock() // 扫描需要删除的key var deleteList []*CacheItem // 先处理l1，再处理l2 t.switchMask = 1 &lt;&lt; 1 now := time.Now() // 处理l1 // 不允许l1读写入，读写通过l2 for &#123; if atomic.LoadInt32(&amp;t.l1Mask) == 0 &#123; // 当L1已经操作完Add操作的时候继续往下走 break &#125; &#125; for i, sad := range t.L1Shards &#123; // 分片片级别读锁 sad.lock.RLock() for _, r := range sad.m &#123; // ttl数据校验处理 if now.Sub(r.createdOn).Seconds() &gt; r.lifeSpan.Seconds() &#123; deleteList = append(deleteList, r) &#125; &#125; sad.lock.RUnlock() &#125; // 开始删除 for _, item := range deleteList &#123; // 分片片级别写锁，防止在 Value操作的时候，并行读写异常 t.L1Shards[item.hashedKey&amp;t.shardMask].lock.Lock() delete(t.L1Shards[item.hashedKey&amp;t.shardMask].m, item.key) t.L1Shards[item.hashedKey&amp;t.shardMask].lock.Unlock() &#125; // 重置deleteList deleteList = make([]*CacheItem, 0) // 处理完l1,处理l2 t.switchMask = 1 &lt;&lt; 2 // 堵塞的item加回来到l1 l1Length := len(t.l1BlockChan) for _, item := range t.l1BlockChan &#123; if item != nil &#123; t.L1Shards[item.hashedKey&amp;t.shardMask].lock.Lock() t.L1Shards[item.hashedKey&amp;t.shardMask].m[item.key] = item t.L1Shards[item.hashedKey&amp;t.shardMask].lock.Unlock() &#125; &#125; // 重置l1BlockChan, 预先申请大小为原来到一半 t.l1BlockChan = make([]*CacheItem, 0, l1Length/2) // 不允许l2读写入，读写通过l1 for &#123; if atomic.LoadInt32(&amp;t.l2Mask) == 0 &#123; break &#125; &#125; for i, sad := range t.L2Shards &#123; sad.lock.RLock() for _, r := range sad.m &#123; if now.Sub(r.createdOn).Seconds() &gt; r.lifeSpan.Seconds() &#123; deleteList = append(deleteList, r) &#125; &#125; sad.lock.RUnlock() &#125; // 开始删除 for _, item := range deleteList &#123; t.L2Shards[item.hashedKey&amp;t.shardMask].lock.Lock() delete(t.L2Shards[item.hashedKey&amp;t.shardMask].m, item.key) t.L2Shards[item.hashedKey&amp;t.shardMask].lock.Unlock() &#125; // 恢复正常 t.switchMask = 1 &gt;&gt; 1 for _, item := range t.l2BlockChan &#123; //fmt.Println(t.name, t.L1Shards[item.hashedKey&amp;t.shardMask]) if item != nil &#123; t.L2Shards[item.hashedKey&amp;t.shardMask].lock.Lock() t.L2Shards[item.hashedKey&amp;t.shardMask].m[item.key] = item t.L2Shards[item.hashedKey&amp;t.shardMask].lock.Unlock() &#125; &#125; // 重置l2BlockChan t.l2BlockChan = make([]*CacheItem, 0, l2Length/2) t.Unlock() case &lt;-reBuildTicker.C: t.Lock() // 为了释放map内存 // 先处理l1，再处理l2 t.switchMask = 1 &lt;&lt; 1 now := time.Now() // 处理l1 // 不允许l1读写入，读写通过l2 for &#123; if atomic.LoadInt32(&amp;t.l1Mask) == 0 &#123; break &#125; &#125; for _, sad := range t.L1Shards &#123; sad.lock.Lock() nm := make(shard, len(sad.m)) for key, r := range sad.m &#123; if now.Sub(r.createdOn).Seconds() &lt; r.lifeSpan.Seconds() &#123; nm[key] = r &#125; &#125; sad.m = nil sad.m = nm sad.lock.Unlock() &#125; // 先处理l1，再处理l2 t.switchMask = 1 &lt;&lt; 2 for &#123; if atomic.LoadInt32(&amp;t.l2Mask) == 0 &#123; break &#125; &#125; for _, sad := range t.L2Shards &#123; sad.lock.Lock() nm := make(shard, len(sad.m)) for key, r := range sad.m &#123; if now.Sub(r.createdOn).Seconds() &lt; r.lifeSpan.Seconds() &#123; nm[key] = r &#125; &#125; sad.m = nil sad.m = nm sad.lock.Unlock() &#125; // 恢复正常 t.switchMask = 1 &gt;&gt; 1 runtime.GC() debug.FreeOSMemory() t.Unlock() &#125; &#125;&#125;(t, ctx)","categories":[{"name":"组件优化","slug":"组件优化","permalink":"http://blog.crazylaw.cn/categories/%E7%BB%84%E4%BB%B6%E4%BC%98%E5%8C%96/"}],"tags":[{"name":"cache","slug":"cache","permalink":"http://blog.crazylaw.cn/tags/cache/"}]},{"title":"Home Assistant （二）","slug":"智能家居/Home-Assistant2","date":"2023-07-31T16:00:19.000Z","updated":"2023-08-02T16:45:58.961Z","comments":true,"path":"2023/08/01/智能家居/Home-Assistant2/","link":"","permalink":"http://blog.crazylaw.cn/2023/08/01/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/Home-Assistant2/","excerpt":"前言本次主要是讲解一下进阶版的HA，为什么说是进阶版本呢，因为我也是花了几天的时候，去了解各种知识才了解得到的内容。 内容包括：什么叫hassio，hassos，并且他们和Home Assistant 是什么关系。 并且我会重点讲解docker版本的HA都是需要怎么玩（不借助hassio），你问我为什么，我就告诉你因为过于繁琐，并且多了很多无用的容器占用系统资源。 为什么我偏执于docker版本的ha？因为我不想一台机器，只做一件事，这对高配置和高性能的机器是一种绝对的浪费。 在这个时候，环境隔离就成了重要的因素, 而docker就很好的做到了这一点，并且不像hassio的底层依赖。","text":"前言本次主要是讲解一下进阶版的HA，为什么说是进阶版本呢，因为我也是花了几天的时候，去了解各种知识才了解得到的内容。 内容包括：什么叫hassio，hassos，并且他们和Home Assistant 是什么关系。 并且我会重点讲解docker版本的HA都是需要怎么玩（不借助hassio），你问我为什么，我就告诉你因为过于繁琐，并且多了很多无用的容器占用系统资源。 为什么我偏执于docker版本的ha？因为我不想一台机器，只做一件事，这对高配置和高性能的机器是一种绝对的浪费。 在这个时候，环境隔离就成了重要的因素, 而docker就很好的做到了这一点，并且不像hassio的底层依赖。 MQTT 什么是MQTT协议，这个得大家去了解了，这里我只和大家说，这是一个开源的发布订阅的简易协议它可以做到服务发现的功能，也可以做到事件通知等等它是传统的物联网大家都会选择的一个协议 这里，我们需要安装一个MQTT的服务器，这个项目就是如下 eclipse-mosquitto (https://github.com/eclipse/mosquitto) (https://hub.docker.com/r/amd64/eclipse-mosquitto) 这是一个一个由于C语言实现的MQTT协议的服务器，但是我们不直接用他。我们借助docker的特性，用docker来安装。 插一个题外话，其实Home Assistant的 ADD-ON 功能，也是创建一些服务的容器，所以我们手动操作，也是一样的。这个时候就是解答有一些小伙伴经常会纠结的一个问题，为什么我用docker安装的HA，它不存在 Add-on 这个加载项，我要怎么使用这个功能。答案就是：手动处理。当然如果你有编程能力的话，也可以做成一个自动化脚本，这也是我接下来准备做的。 好的，话不多说，我们继续我们的操作，基于 docker 安装 eclipse-mosquitto 服务。 找到一个目录，接着执行如下命令创建目录 123mkdir -p ~/ha/mqtt/configmkdir -p ~/ha/mqtt/datamkdir -p ~/ha/mqtt/log 在 ~/ha/mqtt/config 目录下创建配置文件 mosquitto.conf 1vim mosquitto.conf 内容如下，主要分别是需要持久化数据，存储的路径是容器内部的路径 12345678910# 是否允许未提供用户名的客户端进行连接allow_anonymous true# 0.0.0.0很重要，否则你的宿主机转发的端口，也无法被mqtt接收到listener 1883 0.0.0.0# 是否持久化数据persistence true# 持久化数据的路径persistence_location /mosquitto/data# 落盘日志log_dest file /mosquitto/log/mosquitto.log 具体配置文档 https://mosquitto.org/man/mosquitto-conf-5.html 最后执行如下命令即可启动 mosquitto 容器： 123456docker run -d --name=mosquitto --privileged-p 1883:1883 \\-v $(PWD)/config/mosquitto.conf:/mosquitto/config/mosquitto.conf \\-v $(PWD)/data:/mosquitto/data \\-v $(PWD)/log:/mosquitto/log \\eclipse-mosquitto 我的 $(PWD)代表当前目录，你可以改成上面的 ~/ha/mqtt/目录 查看容器是否启动： 12CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESb4a8f33dc8b2 eclipse-mosquitto \"/docker-entrypoint.…\" 17 seconds ago Up 17 seconds 0.0.0.0:1883-&gt;1883/tcp mosquitto 查看日志信息: 12345678910tail log/mosquitto.log1690908079: mosquitto version 2.0.15 starting1690908079: Config loaded from /mosquitto/config/mosquitto.conf.1690908079: Starting in local only mode. Connections will only be possible from clients running on this machine.1690908079: Create a configuration file which defines a listener to allow remote access.1690908079: For more details see https://mosquitto.org/documentation/authentication-methods/1690908079: Opening ipv4 listen socket on port 1883.1690908079: Opening ipv6 listen socket on port 1883.1690908079: Error: Address not available1690908079: mosquitto version 2.0.15 running OK，我们看到一切正常。 让我们来测试一下是不是真的ok了。 我们同样，用2个容器，分别创建发布者和订阅者 由于我的是macbook，所以我的容器可以通过host.docker.internal来访问宿主机，如果是linux环境下的，可以使用 --net host 模式 订阅者（窗口1）: 1docker run --rm -it eclipse-mosquitto sh -c 'mosquitto_sub -h host.docker.internal -p 1883 -t \"#\" -v' 对应的参数就不一一解释了 发布者（窗口2）: 1docker run --rm -it eclipse-mosquitto sh -c 'mosquitto_pub -h host.docker.internal -p 1883 -t \"test/testdevice\" -m caiwenhui-hello' 回到窗口1查看订阅者的情况 12docker run --rm -it eclipse-mosquitto sh -c 'mosquitto_sub -h host.docker.internal -p 1883 -t \"#\" -v' test/testdevice caiwenhui-hello 可以看见，我们的消息和对应的topic都正常接受到了。非常好！ 接下来，我们接入我们的HA！！ 让HA和MQTT连接我们打开回到我们的HA, http://127.0.0.1:8123 由于我这里没有账号密码，并且允许匿名发送，所以这里可以先忽视这些，正常的情况下，我们需要设置，否则一旦mqtt被破解，被发送恶意指令，可能会让你的设备“发疯” 看到这里，我们配置完成了。接下里，就是在HA里面测试是否真的配置正确并且可通的了。 可以看到关键信息: i am ha&#39;s caiwenhui , 是因为我在我的电脑发送的消息被HA接收到了 可以看到，这个图，代表，我接收到了从ha系统发出的消息。ok，我们ha和mqtt的连接在不借助 hassio 和 add-on的情况下已经打通了。","categories":[{"name":"智能家居","slug":"智能家居","permalink":"http://blog.crazylaw.cn/categories/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/"}],"tags":[{"name":"智能家居","slug":"智能家居","permalink":"http://blog.crazylaw.cn/tags/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/"},{"name":"Home Assistant","slug":"Home-Assistant","permalink":"http://blog.crazylaw.cn/tags/Home-Assistant/"},{"name":"HA","slug":"HA","permalink":"http://blog.crazylaw.cn/tags/HA/"}]},{"title":"Home Assistant （一）","slug":"智能家居/Home-Assistant","date":"2023-07-29T16:00:19.000Z","updated":"2023-08-01T16:22:01.707Z","comments":true,"path":"2023/07/30/智能家居/Home-Assistant/","link":"","permalink":"http://blog.crazylaw.cn/2023/07/30/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/Home-Assistant/","excerpt":"前言既然是技术博客，那么这一期，我会开始做一系列的关于智能家居的玩法。 Home Assistant 也叫 HA + Zigbee协议(硬件之间的通信协议) HACS 是 HA 的一个第三方插件，这个插件包括了许多开发者所贡献的插件，如果你是一名开发人员，那么你一定了解什么叫依赖库。所以可以理解为 HACS 就是 HA 的第三方插件依赖库，你可以在连找到需要对你有用的插件，而你不必重新开发。 为什么选择 Zigbee协议 ？ 在对比过蓝牙mesh,wifi协议之后，我最终选择了zigbee协议，在于硬件之间的控制信号量传输本身就少，所以它自身的稳定性和分布式的特点，可以很好的支持到家庭中的各个角落，不会出现由于信号差导致失灵的情况，并且zigbee协议的特点，电量的消耗也是十分的低。","text":"前言既然是技术博客，那么这一期，我会开始做一系列的关于智能家居的玩法。 Home Assistant 也叫 HA + Zigbee协议(硬件之间的通信协议) HACS 是 HA 的一个第三方插件，这个插件包括了许多开发者所贡献的插件，如果你是一名开发人员，那么你一定了解什么叫依赖库。所以可以理解为 HACS 就是 HA 的第三方插件依赖库，你可以在连找到需要对你有用的插件，而你不必重新开发。 为什么选择 Zigbee协议 ？ 在对比过蓝牙mesh,wifi协议之后，我最终选择了zigbee协议，在于硬件之间的控制信号量传输本身就少，所以它自身的稳定性和分布式的特点，可以很好的支持到家庭中的各个角落，不会出现由于信号差导致失灵的情况，并且zigbee协议的特点，电量的消耗也是十分的低。 硬件由于我现在没有树莓派或者nas等本地服务器，所以我以我的macbook pro来例。但是实际情况下的智能家居，我们需要有一台连接在本地局域网的服务，它需要要求的特点包括如下： 低功耗 便携，不占用家庭空间 支持更多的其他功能，例如用于nas，time machine, 软路由(科学上学) 等等（非必要，但是可以做在一起，所以机器的性能越出色越好） 由于是测试用的，所以我只买了2个 涂鸦 品牌的 e27 的智能灯泡 和 zigbee2mqtt 的 zigbee协议信号网关 ** 需要去淘宝买 ** zigbee信号网关 2个智能灯泡 zigbee信号网关 支持zigbee协议的智能灯泡 Home Assistant 安装由于目前我在尝试阶段，并且是macbook，并且我更倾向于用docker安装各种服务，在容器化的时代，物理环境的隔离是十分重要的一个环节。 docker 安装 这里不介绍docker怎么安装了，如果是完全没编程的基础知识的话，折腾起来确实有点麻烦 创建数据存储目录，mkdir -p ~/ha;cd ~/ha，然后执行如下 docker命令 进行安装 1234567docker run -d \\ --name homeassistant \\ --privileged \\ -e TZ=Asia/Shanghai \\ -v $(PWD):/config \\ -p 8123:8123 \\ homeassistant/home-assistant 网上很多都说要用–net=host模式，但是其实没必要的，指定端口暴露就好ha的默认端口就是8123 docker ps 查看容器情况 12CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESb2f9de1af25b homeassistant/home-assistant \"/init\" 2 seconds ago Up 1 second 0.0.0.0:8123-&gt;8123/tcp homeassistant 可以看到我们的容器启动了。并且正常运行了。如果你还不放心，可以看一下日志也可以 docker log homeassistant 123456789s6-rc: info: service s6rc-oneshot-runner: startings6-rc: info: service s6rc-oneshot-runner successfully starteds6-rc: info: service fix-attrs: startings6-rc: info: service fix-attrs successfully starteds6-rc: info: service legacy-cont-init: startings6-rc: info: service legacy-cont-init successfully starteds6-rc: info: service legacy-services: startingservices-up: info: copying legacy longrun home-assistant (no readiness notification)s6-rc: info: service legacy-services successfully started 打开浏览器，在浏览器中输入 127.0.0.1:8123，就会看到如下界面 接下来，我们创建好账号密码，切勿忘记密码，好记性不如烂笔头，记得记录下来 这里我们，不怎么需要理会，正常填写即可。 这里默认的都是关闭的，也推荐关闭。然后点击下一步。 这里其实就可以选择你要连接的品牌了，但是这里我先跳过，后面再设置。直接点击 完成 即可 ok，现在我们可以看到了，完成之后，进到 HA系统 了，可以看到，内置了一个谷歌的TTS的功能，可以不用理会（这个是谷歌推出的一个文本转语音的机器学习的功能）。 HACS 安装Home Assistant Community Store 为社区建设的HomeAssistant商店，可以安装第三方集成、主题、表盘以及自动化等。 GitHub 地址如下: https://github.com/hacs https://github.com/hacs/integration 安装之前，让我们先把资源目录整理好. 1234# 存放等会要安装的 HACS文件mkdir -p ~/ha/custom_components/hacs# 存放未来 HACS 安装的各种首页磁贴啥的（官方叫Lovelace ）mkdir -p ~/ha/www 打开浏览器，输入github 的地址 https://github.com/hacs/integration/releases ，在这里下载最新的hacs的releases包。 这里看见，我当前看到的目前的版本是去到了 1.32.1，所以本篇文章将以 1.32.1 为例子. 下载这个 zip 的压缩包，然后解压到 ~/ha/custom_components/hacs 目录 终端命令如下： unzip ~/Downloads/hacs.zip -d ~/ha/custom_components/hacs 解压后的情况如上图。 在这里点击开发者工具页面，然后点击检测配置。 接下来就可以看到检测通过，然后我们在同一个页面中找到重启按钮, 就可以重启我们的HA系统，让插件加载生效. 点击重启按钮之后，可以看到系统正在重启，目前失去了连接. 接着，我们手动刷新一下页面，用户登陆失效了，代表重启成功了，所以我们需要重新登陆到我们的HA系统，这个时候，需要输入我们记下来的账号密码 ** 换另外一种 ** 方式去安装HACS也可以的，直接在docker容器内部安装，通过hacs脚本自动安装。 1docker exec -it homeassistant bash -c 'wget -q -O - https://install.hacs.xyz | bash -' 可以看到，现在安装完毕了，我需要重启然后让插件加载成功。 接下来，我们点击 配置 ，找到 设备与服务 项 点击 添加集成 搜索一下hacs，然后我们就能识别到的刚才安装的hacs了。 这里把全部选中，因为这个是第三方插件，所以HA需要确保你会排查问题。所以需要看你是否懂得这些，你只需要全选了即可. 这是设备注册。需要先拥有Github账号，没有的话注册一个并在浏览器上登录,github.com 然后打开上面的提供的连接: https://github.com/login/device, 输入提供的 设备码 点击 账号授权给 hacs 服务，即可。 回到 HA系统，会发现已经识别到了，所以完成安装了 HACS 。 homeassistant AppHA官方提供了APP，iOS和Android都有，可自行下载~ 12ios: https://www.github.com/home-assistant/iOSandroid: https://github.com/home-assistant/android 当然，也可以自行去各大应用商店下载，例如 appstore 温馨提示： App需要填入自己的HA地址，所以如果服务跑在家里的话，需要内网穿透或者公网才能在外面使用噢~ 如果HA内网穿透，configuration.yaml需要加上下面内容，同时内网穿透服务器(如ngrok、frp等)的nginx需要开启websocket支持，否则会出现外网无法访问、能访问但是无法登录等问题。 HA配置文件configuration.yaml 12345http: use_x_forwarded_for: True trusted_proxies: - 127.0.0.1/24 - ::1/128 nginx配置参考(用frp内网穿透) 1234567891011121314server &#123; listen 80; server_name *.frp.yourdomain.cn frp.yourdomain.cn; location / &#123; proxy_redirect off; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; # 下面两行提供websocket支持,homeassistant需要 proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \"upgrade\"; proxy_pass http://127.0.0.1:8123; &#125;&#125; 最后附上同一个局域网下的访问效果图","categories":[{"name":"智能家居","slug":"智能家居","permalink":"http://blog.crazylaw.cn/categories/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/"}],"tags":[{"name":"智能家居","slug":"智能家居","permalink":"http://blog.crazylaw.cn/tags/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/"},{"name":"Home Assistant","slug":"Home-Assistant","permalink":"http://blog.crazylaw.cn/tags/Home-Assistant/"},{"name":"HA","slug":"HA","permalink":"http://blog.crazylaw.cn/tags/HA/"}]},{"title":"【多媒体】- AAC音频","slug":"多媒体/aac","date":"2022-08-20T03:10:40.000Z","updated":"2022-08-20T16:00:19.452Z","comments":true,"path":"2022/08/20/多媒体/aac/","link":"","permalink":"http://blog.crazylaw.cn/2022/08/20/%E5%A4%9A%E5%AA%92%E4%BD%93/aac/","excerpt":"前言音频中的audio有很多格式，分别有aac,mp3等等 在海外市场中，aac格式比较普遍，所以最近对其进行了一些研究。","text":"前言音频中的audio有很多格式，分别有aac,mp3等等 在海外市场中，aac格式比较普遍，所以最近对其进行了一些研究。 说明格式协议解码id3v2adts","categories":[{"name":"多媒体","slug":"多媒体","permalink":"http://blog.crazylaw.cn/categories/%E5%A4%9A%E5%AA%92%E4%BD%93/"}],"tags":[{"name":"aac","slug":"aac","permalink":"http://blog.crazylaw.cn/tags/aac/"}]},{"title":"【广告行业】知识点","slug":"公司/广告行业知识点","date":"2022-04-12T01:53:00.000Z","updated":"2022-04-12T12:20:15.427Z","comments":true,"path":"2022/04/12/公司/广告行业知识点/","link":"","permalink":"http://blog.crazylaw.cn/2022/04/12/%E5%85%AC%E5%8F%B8/%E5%B9%BF%E5%91%8A%E8%A1%8C%E4%B8%9A%E7%9F%A5%E8%AF%86%E7%82%B9/","excerpt":"前言由于广告行业有众多的知识点，所以记录一篇文章用以了解相关的专业名词和术语，并且加深了解的印象。","text":"前言由于广告行业有众多的知识点，所以记录一篇文章用以了解相关的专业名词和术语，并且加深了解的印象。 头部竞价Header Bidding，顾名思义，就是头部竞价，跟其相对应的就是Waterfall，瀑布流。在Header Bidding风靡之前，Waterfall才是各家广告平台的主流竞价方式。想要了解Header Bidding，那我们就需要先弄清楚什么是Waterfall了。 这里涉及到的角色有开发者（publisher），广告调解平台（mediation platform），需求方（demand partner）。首先，广告调解平台扮演一个（中立）的第三方角色，接入了多家广告平台的广告源，比如Facebook，Vungle，Fyber，Applovin等，即将各家的广告放到一个SDK中，这时候开发者如果想要变现自家的流量，只需接入广告调解平台的SDK即可，再通过一个简单的配置即可自主决定接通哪几家的广告源了，而不需要每家的广告SDK都接一遍，省时省力，是不是很棒？ 那么，问题来了。开发者通过调解平台的SDK介入了多家的广告源后，流量改怎么分配呢？同一个广告请求（ad request）到底是该发给Facebook，还是google还是vungle还是其他人呢？这时候就涉及到调解平台的算法逻辑了，通常，这个调解平台会对给新介入的广告平台分配一定的流量，测试下其表现，并得到一个大概的eCPM的值；然后，调解平台会根据不同的广告平台在过去24小时内的eCPM的高低来排序，广告请求优先发给eCPM最高的第一位选手，若没有填充（fill），就下一个，没有填充就再下一个，如此循环往复。Waterfall刚出来的时候真的是一个超级棒的概念，很好地解决了开发者流量变现最大化的问题。 但是，问题又来了。昨天排在第一的广告平台，谁能保证它今天给的eCPM也是足够高而排在第一位呢？又或者，昨天排在第三位的广告平台，今天有个爆款的单子推广，可以给到很高的价格，但是由于调解平台的算法机制，它不具有优先选择广告位的权利，而导致它并不能买到多少量。 这时候，聪明的移动互联网人就想到了借鉴桌面端广告的做法，没错，就是Header Bidding。头部竞价技术源于网页端，开发者通过在网页头部嵌入代码，从而似的广告请求在公开竞价之前可以发给开发者优选的合作伙伴，拿到优选合作伙伴的返回后将其价格与公开竞价价格做对比，价高者得。而这个模型也得以运用到移动端了。 调解平台一改往日的瀑布流的形式，将一个接一个地发广告请求的方式 改为 同时像所有的广告平台发请求，在一定的时间内将收到的返回最比较，最高出价即赢得广告的展示权。 DSP（Demand-Side Platform）需求方平台互联网广告DSP（Demand-Side Platform），就是需求方平台 DSP为需求方（即广告主或代理商）提供实时竞价投放平台。广告需求方可以在平台上管理广告活动及其投放策略，包括设置目标受众的定向条件、预算、出价、创意等。 SSP（Supply-Side Platform）供应方平台SSP服务于媒体方，可以作为分散的流量入口、大小媒体的聚合平台。 ADN（Ad Network）广告网盟ADN可以被理解为媒体代理公司，通过为广告主采购媒体方流量，赚取中间差价，其代表有百度网盟等。 ADX（Ad Exchange）广告交易平台ADX提供的功能是交换，实现实时竞价、广告库存和广告需求的匹配。广告需求方可以随时改变自己的出价策略和所选择的资源。 程序化广告-交易模式术语RTB (real time bidding) 实时竞价Real Time Bidding(实时竞价)，也叫Open Auction（公开竞价），简称RTB 流量需求方在广告交易平台中，设定广告流量底价的情况下，当有流量过来时，与其他程序化广告买家一起对流量出价，广告交易平台收到各个程序化买家的出价后，进行比价，价高者获得流量并同步竞价成功的结果。整个过程都是通过程序化的方式在 100 毫秒内完成的。 品牌能够对单个展示广告位置进行竞价购买，而不是以预先固定的价格进行购买，从而使购买决策更加划算，避免广告主预算浪费 PDB（Private Direct Buy）程序化直接购买 这个概念应该和直投是同一个概念 是目前国内市场最为常见和主流应用的一种私有交易模式。 指流量需求方用确定的价格买断固定、优质的媒体资源，然后进行程序化广告的精准定向投放。常说的“保价保量”。 PD（Preferred Deals）优先交易与 PDB 区别在于，这种私有交易方式在广告资源上具有一定的不确定性。即流量需求方可以购买某一优质广告位，但其能获得多少曝光展示量却不能预先保证。常说的“保价不保量”。 PA（Private Auction）私有竞价供应方平台将较优质的固定广告位资源专门拿出来，放在一个半公开市场中，仅由进入白名单的买方（VIP）进行竞价，价高者得。因此，广告位可以锁定，但采买价格和是否最终获得曝光都不能预先保证。常说的“不保价不保量”。 程序化广告-效果术语广告传播影响受众的认知、心理、行为和态度，由此带来的直接和间接广告效益，对广告效果的评估的也有着多方面要素和维度。 ROIReturn On Investment（投资回报率），简称ROI。即营销者通过广告投放得到的经济回报占广告投入（花费）的比例。 ImpressionImpression，即曝光量，也被称为“展示量”、“展现量”。即投放期广告被展示的总次数。一般用户每浏览一次页面，同时页面中广告位的广告被展示一次，就是一个曝光。 ClickClick，即点击量，为投放期用户点击某个广告的总次数。 CTR(Click-Through-Rate) 点击率广告被点击的次数与广告曝光次数的比例 计算公式：Click/Impression*100% 反映了广告的受关注程度，或用来衡量广告的吸引程度。 RR（Reach Rate）到达率到达量与点击量的比例（到达量/点击量*100%） 到达量：即有多少用户点击广告后进入落地页。 CR（Conversion Rate）转化率转化量与点击量的比例（转化量/点击量*100%） 转化量：即有多少用户点击广告并进入落地页（活动页）后，继续发生咨询、注册、下载、加入购物车、下单等行为。 留存率特定周期内（如次日留存、七日留存等），留存用户数量（有多少用户留下来）占广告（当时）导入的新增用户数量的比例。留存率=留存用户数/新增用户数量*100% LT（Life Time）生命周期一个用户从第1次到最后1次参与游戏之间的时间段，一般按月计算平均值 LTV(Life Time Value) 用户终生价值用户在生命周期内为该游戏创造的收入总计，可以看成是一个ARPU 值的长期累计。 计算公式：LTV = ARPUxLT。 DAU(Daily Active Users) 日活跃用户数量DAU 指的是某产品（网站、软件或游戏等）在一日之内登录或使用过的用户总数（不包括重复的用户）。 DAU 是一个比较基本的指标，能够相对片面地展现产品短时间内的热度。 一般来说只看产品的 DAU 其实意义不大，单纯通过 DAU 无法判断产品的真实质量，且 DAU 很容易伪造。 MAU(Monthly Active Users) 月活跃用户数量MAU 指的是某产品（网站、软件或游戏等）在一个月（统计月）之内登录或使用过的用户总数（不包括重复的用户）。 MAU 同样也是一个比较基本的指标，能够相对片面的展现产品一段时间内的热度。 通过 DAU 和 MAU 虽然能够看出产品在一段时间内的热度，但是无法精确地判断产品的留存率，因为你无法得知用户的属性（是否为老用户）。此时 DNU 和 DOU 是时候出来救场了。 DNU &amp; DOU「Daily New Users（日新增用户数量 ）&amp; Daily Old Users（日非新增用户数量）」 这两个词的意义非常明确，就是直接展现了产品短时间内的新老用户情况。 通过 DNU 和 DOU 加上 DAU 和 MAU 这几个指标能够比较直接地判断产品的留存率（即用户粘性）。 ARPU(Average Revenue Per User) 每用户平均收益目前 ARPU 这个概念在许多行业都有着广泛的应用，特别是在互联网游戏产业里面，随着免费游戏的兴起，ARPU 日渐成为游戏运营商着重关注的元素。 一般来说，不同的行业乃至不同的企业都有自己专属的 ARPU 计算方式。 常用计算公式：每用户平均收益 = 总收益 ÷ 总用户数 广告相关eCPM（Effective Cost Per Mille） 每千次展示收益eCPM 是一个主要面向产品方的指标。 指的是在产品（网页、应用等等）中展示某广告 1000 次所带来的收益。 计算公式：每千次展示收益 = 总收益 ÷ 广告展示总次数 × 1000 只要是有效展示就行，可以说是很简单粗暴的变现方式了… CPM「Cost Per Mille / Cost Per Thousand Impressions（每千次印象成本）」 CPM 是一种主要面向广告主的广告计费模式。 指的是广告投放过程中，平均每向 1000 人展示某广告 1 次需要的成本。一般同一 IP 在 24 小时内最多只有一次有效展示。 计算公式：每千次印象成本 = 总成本 ÷ 广告达到人数 × 1000 你可以不看，但是我这个广告一定要播！ CPC（Cost Per Click） 每点击成本CPC 是一种主要面向广告主的广告计费模式。 指的是在广告投放中，广告主仅为用户的有效点击行为付费，而不再为广告的展示次数付费。 计算公式：每点击成本 = 总成本 ÷ 点击数 不点不给钱！ CPA（Cost Per Action） 每行动成本CPA 是一种主要面向广告主的广告计费模式。 CPA 顾名思义是按照用户的行为（Action）作为指标来计费，这个行为可以是注册、咨询或加入购物车等等。 意思就是用户点击了广告还不算，需要用户有注册成功之类的行为才行，不过这种模式下广告费也相对较高。 这是一个不受产品方待见的模式，条件太苛刻了… CPS(Cost Per Sale) 每销售成本CPS 是一种面向广告主的广告计费模式。 在该模式下，广告主的商品成功销售出去之后，产品方才可以获取到一定比例的佣金（提成）。 这么说来似乎有点销售的意思（吃提成） CPP（Cost Per Purchase）每购买成本CPP 是一种面向广告主的广告计费模式。 在该模式下，用户点击广告并成功进行交易后，广告主按照销售笔数付给产品方广告费用。 要注意了，CPP 是按照销售笔数来算钱的 CPR（Cost Per Response）每回应成本CPR 是一种面向广告主的广告计费模式，这种模式的特点为：及时反应、直接互动、准确记录。 在 CPR 模式下，广告展示后，还需要用户给予广告主回应才算有效。所谓回应，一般是拨打电话之类的形式。例如电视购物广告，一般在固定时段播出，当用户拨打了广告中的电话之后才算作有效传播。 这种模式要求相对较高，也挺不受待见的，这广告费太难赚了… 又想起了被电视购物广告支配的日子.. PPC（Pay Per Click）点击付费广告PPC 是大公司最常用的网络广告形式，这种方法费用很高，但效果也很好，比如百度竞价、搜狐和新浪首页上的 Banner 广告。 计价公式：起价 + (点击数 × 每次点击的价格) 越是著名的搜索引擎，起价越高，最高可达数万甚至数十万，而每次点击的价格在 0.30 元左右。 提供点击付费的网站非常多，主要有各大门户网站(如搜狐、新浪)和搜索引擎（Google 和百度），以及其他浏览量较大的网站，比如提供软件下载的华军等等。 就是那种在搜索引擎搜素关键词，然后在展示在搜索结果前面的那种广告… PPS（Pay Per Sale） 按销售付费PPS 广告是根据网络广告所产生的直接销售数量而付费的一种定价模式。 PPS 和 CPS 基本一个意思，类似于淘宝客这类的服务。广义上不仅仅是指互联网广告范畴，应包括所有形式的基于成功销售而收取一定比例佣金的商业合作方式。 网站相关UV（Unique Visitors）独立访客数UV 表示某网站 1 天内（00:00 - 24:00）的独立访客总数。 所谓的“独立”访客，是以浏览器的 Cookie 为依据的，只要 Cookie 相同，那么就算更换了 IP 也都将被视为同一个访客，且当天无论访问多少次 UV 都只会加 1 个。 通常是一部手机（电脑）一个坑~ PV（Page Views）页面浏览量PV 表示某页面在一定统计周期内的总浏览量。 在一定周期内，用户每次打开或刷新该页面都将被视为 1 次浏览。 刷新一下多一个真好玩~ IP（Internet Protocol）独立 IP 数IP 表示 1 天内（00:00 - 24:00）访问某网站的 IP 总数。 该指标以广域网 IP 为依据，同一 IP 的设备无论访问多少次都只算一个计数，也就是说，连接同一路由器的不同设备在同一天内多次访问该网站，在正常情况下都只会增加一个 IP 计数。 一条网线一个坑~ RV（Repeat Visitors）重复访客数RV 指的是在一定统计周期内，访问某网站两次或两次以上的访客总数。 俗称：回头客 TP（Time On Page）页面停留时间TP 指的是（总的）用户在某个页面的平均停留时长。 TP 时长可以反映出某个页面对用户的吸引力，帮助判断用户的喜好。 能看完这篇文章的话 TP 应该能有 5 分钟吧 TS（Time On Site） 网站停留时间TS 指的是（总的）用户在某网站（包括了该网站下的所有页面）的平均停留时长。 反正视频网站的 TS 肯定都挺长的 SD（Session Duration ）平均会话时长SD 是在 Google Analytics 中使用的一个指标，用来统计网站的平均停留时长。 SD 的作用类似于 Time On Site，但是这两者的计算方式不一样。","categories":[{"name":"广告行业","slug":"广告行业","permalink":"http://blog.crazylaw.cn/categories/%E5%B9%BF%E5%91%8A%E8%A1%8C%E4%B8%9A/"}],"tags":[{"name":"广告行业","slug":"广告行业","permalink":"http://blog.crazylaw.cn/tags/%E5%B9%BF%E5%91%8A%E8%A1%8C%E4%B8%9A/"}]},{"title":"【Mac】Mac安装软件流程","slug":"Mac/Mac安装软件流程","date":"2022-03-29T08:35:30.000Z","updated":"2022-04-11T10:31:25.337Z","comments":true,"path":"2022/03/29/Mac/Mac安装软件流程/","link":"","permalink":"http://blog.crazylaw.cn/2022/03/29/Mac/Mac%E5%AE%89%E8%A3%85%E8%BD%AF%E4%BB%B6%E6%B5%81%E7%A8%8B/","excerpt":"前言最近，准备重新打造我的mac电脑。","text":"前言最近，准备重新打造我的mac电脑。 iterm2 安装zsh 安装 powerlevel10k (安装方式参考官网文档) 替换 .vimrc 中的 zsh主题, ZSH_THEME=&quot;powerlevel10k/powerlevel10k&quot; vim相关我的个人vim配置 ccinn-vim vim的插件管理选择有 Vundle, vim-plug vim的目录插件 scrooloose/nerdtree vim的目录增强插件,字体和icon的下载和安装可以在nerd目录下显示图标，其他字体等 ryanoasis/nerd-fonts ryanoasis/vim-devicons 主体iterm2 的字体需要和nerd-fonts一致 vim 窗口状态栏 vim-airline/vim-airline vim-airline/vim-airline-themes Clean my mac付费软件，清理电脑 ntfs for mac外置移动硬盘专用","categories":[{"name":"Mac","slug":"Mac","permalink":"http://blog.crazylaw.cn/categories/Mac/"}],"tags":[{"name":"Mac","slug":"Mac","permalink":"http://blog.crazylaw.cn/tags/Mac/"}]},{"title":"【DevOps】git命令场景用法","slug":"DevOps/git命令场景用法","date":"2022-03-19T06:35:30.000Z","updated":"2022-04-15T07:40:12.764Z","comments":true,"path":"2022/03/19/DevOps/git命令场景用法/","link":"","permalink":"http://blog.crazylaw.cn/2022/03/19/DevOps/git%E5%91%BD%E4%BB%A4%E5%9C%BA%E6%99%AF%E7%94%A8%E6%B3%95/","excerpt":"前言现在，git已经成为了大家的代码仓库管理的一个工具了。在日常工作，我们会遇到各种个样的git问题，因此，用一篇文章来累计记录，日常生活中，我们会遇到，但是不常用的命令。","text":"前言现在，git已经成为了大家的代码仓库管理的一个工具了。在日常工作，我们会遇到各种个样的git问题，因此，用一篇文章来累计记录，日常生活中，我们会遇到，但是不常用的命令。 概念其实 git 的概念，我们开发者应该很多有会了，存在以下几个区域： 工作区 （你的任何改动） 暂存区 （git add） 本地仓库 （git commit） 远端代码仓库 （git push） 合并我们一般说合并其实是有","categories":[{"name":"DevOps","slug":"DevOps","permalink":"http://blog.crazylaw.cn/categories/DevOps/"}],"tags":[{"name":"DevOps","slug":"DevOps","permalink":"http://blog.crazylaw.cn/tags/DevOps/"}]},{"title":"【Golang】- go map源码阅读","slug":"Golang/go map源码阅读","date":"2022-03-10T07:55:51.000Z","updated":"2022-03-22T08:54:11.241Z","comments":true,"path":"2022/03/10/Golang/go map源码阅读/","link":"","permalink":"http://blog.crazylaw.cn/2022/03/10/Golang/go%20map%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/","excerpt":"前言最近发现同事去面试，发现很多时候会被问到go map的底层结构。今天我们来记录一下map的底层实现。","text":"前言最近发现同事去面试，发现很多时候会被问到go map的底层结构。今天我们来记录一下map的底层实现。 当下的源码阅读基于1.17 12345678910111213141516// A header for a Go map.type hmap struct &#123; // Note: the format of the hmap is also encoded in cmd/compile/internal/gc/reflect.go. // Make sure this stays in sync with the compiler's definition. count int // # live cells == size of map. Must be first (used by len() builtin) flags uint8 B uint8 // log_2 of # of buckets (can hold up to loadFactor * 2^B items) noverflow uint16 // approximate number of overflow buckets; see incrnoverflow for details hash0 uint32 // hash seed buckets unsafe.Pointer // array of 2^B Buckets. may be nil if count==0. oldbuckets unsafe.Pointer // previous bucket array of half the size, non-nil only when growing nevacuate uintptr // progress counter for evacuation (buckets less than this have been evacuated) extra *mapextra // optional fields&#125; 可以看到这是一个map的头部结构，其中有几个关键结构，分别是 count 当前map的元素个数 buckets 桶的数量，一般是2^B个 oldbuckets 扩容前的buckets 12345678910111213141516// mapextra holds fields that are not present on all maps.type mapextra struct &#123; // If both key and elem do not contain pointers and are inline, then we mark bucket // type as containing no pointers. This avoids scanning such maps. // However, bmap.overflow is a pointer. In order to keep overflow buckets // alive, we store pointers to all overflow buckets in hmap.extra.overflow and hmap.extra.oldoverflow. // overflow and oldoverflow are only used if key and elem do not contain pointers. // overflow contains overflow buckets for hmap.buckets. // oldoverflow contains overflow buckets for hmap.oldbuckets. // The indirection allows to store a pointer to the slice in hiter. overflow *[]*bmap oldoverflow *[]*bmap // nextOverflow holds a pointer to a free overflow bucket. nextOverflow *bmap&#125; 简单来说，这个可以忽略。 123456789101112// A bucket for a Go map.type bmap struct &#123; // tophash generally contains the top byte of the hash value // for each key in this bucket. If tophash[0] &lt; minTopHash, // tophash[0] is a bucket evacuation state instead. tophash [bucketCnt]uint8 // Followed by bucketCnt keys and then bucketCnt elems. // NOTE: packing all the keys together and then all the elems together makes the // code a bit more complicated than alternating key/elem/key/elem/... but it allows // us to eliminate padding which would be needed for, e.g., map[int64]int8. // Followed by an overflow pointer.&#125; bmap有2个模块的属性是在编译注入的，在源码上没办法浏览。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556 ┌─────────────────────────────────────┐ │bmap │┌─────────────────────────────────────┐ │ ││bmap │ │ ││ │ │ ││ │ │ ┌──────────────────────────────┐ ││ │ │ │tohash[bucketCnt]uint8 │ ││ ┌──────────────────────────────┐ │ │ │ │ ││ │tohash[bucketCnt]uint8 │ │ │ │ │ ││ │ │ │ │ │ │ ││ │ │ │ │ └──────────────────────────────┘ ││ │ │ │ │ ││ └──────────────────────────────┘ │ │ ++++++++++++++++++++++++++++++++ ││ │ │ + byte-array + ││ ++++++++++++++++++++++++++++++++ │ │ + (save key-value) + ││ + byte-array + │ ┌──────►│ + + ││ + (save key-value) + │ │ │ ++++++++++++++++++++++++++++++++ ││ + + │ │ │ ││ ++++++++++++++++++++++++++++++++ │ │ │ ++++++++++++++++++++++++++++++++ ││ │ │ │ + point to growed bucket + ││ ++++++++++++++++++++++++++++++++ │ │ │ + + ││ + point to growed bucket + │ │ │ + + ││ + + ├─────┘ │ +++++++++++++─┐+++++++++++++++++ ││ + + │ │ │ ││ ++++++++++++++++++++++++++++++++ │ │ │ ││ │ └──────────────────┼──────────────────┘│ │ │└─────────────────────────────────────┘ │ │ ▼ ┌─────────────────────────────────────┐ │bmap │ │ │ │ │ │ │ │ ┌──────────────────────────────┐ │ │ │tohash[bucketCnt]uint8 │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ └──────────────────────────────┘ │ │ │ │ ++++++++++++++++++++++++++++++++ │ │ + byte-array + │ │ + (save key-value) + │ │ + + │ │ ++++++++++++++++++++++++++++++++ │ │ │ │ ++++++++++++++++++++++++++++++++ │ │ + point to growed bucket + │ │ + + │ │ + + │ │ ++++++++++++++++++++++++++++++++ │ │ │ │ │ └─────────────────────────────────────┘s 相比于hmap，bucket的结构显得简单一些，byte-array是我们使用的map中的key和value就存储在这里。高位哈希值数组记录的是当前bucket中key相关的索引 mapassign 赋值过程123456789101112131415161718192021222324252627282930313233343536373839// 1. 判断会否当前已经进行了写保护if h.flags&amp;hashWriting != 0 &#123; throw(\"concurrent map writes\")&#125;// 2. 根据key计算哈希值hash := t.hasher(key, uintptr(h.hash0))// 3. 进行写保护h.flags ^= hashWriting// 4. 计算hash的低位部分bucket := hash &amp; bucketMask(h.B)// 5. 判断是否正在扩容，如果是，则数据迁移if h.growing() &#123; growWork(t, h, bucket)&#125;// 6. 根据低位hash找到对应的bucketb := (*bmap)(add(h.buckets, bucket*uintptr(t.bucketsize)))// 7. 计算高位hashtop := tophash(hash)// 8. 从对应的bucket以及overflow buckets中找到对应的key的位置// 9. 判断是否需要扩容，如果需要，则重新找到key的位置if !h.growing() &amp;&amp; (overLoadFactor(h.count+1, h.B) || tooManyOverflowBuckets(h.noverflow, h.B)) &#123; hashGrow(t, h) goto again // Growing the table invalidates everything, so try again&#125;// 10. 拿着可以插入kv的内存地址进行赋值if t.indirectkey() &#123; kmem := newobject(t.key) *(*unsafe.Pointer)(insertk) = kmem insertk = kmem&#125;if t.indirectelem() &#123; vmem := newobject(t.elem) *(*unsafe.Pointer)(elem) = vmem&#125;// 11. 写保护检查，并且解除写保护 if h.flags&amp;hashWriting == 0 &#123; throw(\"concurrent map writes\")&#125;h.flags &amp;^= hashWriting 赋值过程就是： 进行写保护 根据key计算哈希值 在低位哈希中找到bucket 计算高位hash 在bucket和overflow bucket桶中找到能插入key/value的位置 找到了就赋值 解除锁保护 其中有多次判断bucket是否需要扩容和是否正在扩容","categories":[{"name":"Go源码剖析系列","slug":"Go源码剖析系列","permalink":"http://blog.crazylaw.cn/categories/Go%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90%E7%B3%BB%E5%88%97/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://blog.crazylaw.cn/tags/Golang/"},{"name":"Go源码剖析","slug":"Go源码剖析","permalink":"http://blog.crazylaw.cn/tags/Go%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90/"}]},{"title":"【Golang】- go time.Sleep源码阅读","slug":"Golang/go time.Sleep源码阅读","date":"2022-03-10T07:55:51.000Z","updated":"2022-03-10T14:09:34.592Z","comments":true,"path":"2022/03/10/Golang/go time.Sleep源码阅读/","link":"","permalink":"http://blog.crazylaw.cn/2022/03/10/Golang/go%20time.Sleep%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/","excerpt":"前言由于time.Sleep()会挂起我们的协程，我们来看一下它的底层原理。","text":"前言由于time.Sleep()会挂起我们的协程，我们来看一下它的底层原理。 sleep 的实现我们通常使用 time.Sleep(1 * time.Second) 来将 goroutine 暂时休眠一段时间。sleep 操作在底层实现也是基于 timer 实现的。 有一些比较有意思的地方，单独拿出来讲下。 我们固然也可以这么做来实现 goroutine 的休眠: 12timer := time.NewTimer(2 * time.Seconds)&lt;-timer.C 这么做当然可以。但 golang 底层显然不是这么做的，因为这样有两个明显的额外性能损耗。 每次调用 sleep 的时候，都要创建一个 timer 对象 需要一个 channel 来传递事件 既然都可以放在 runtime 里面做。golang 里面做的更加干净： 123456789101112131415161718192021// timeSleep puts the current goroutine to sleep for at least ns nanoseconds.//go:linkname timeSleep time.Sleepfunc timeSleep(ns int64) &#123; if ns &lt;= 0 &#123; return &#125; gp := getg() t := gp.timer if t == nil &#123; t = new(timer) gp.timer = t &#125; t.f = goroutineReady t.arg = gp t.nextwhen = nanotime() + ns if t.nextwhen &lt; 0 &#123; // check for overflow. t.nextwhen = maxWhen &#125; gopark(resetForSleep, unsafe.Pointer(t), waitReasonSleep, traceEvGoSleep, 1)&#125; 在G对象上存在一个timer属性，在G的生命周期里timer都是唯一存在，解决了重复新建对象的问题 如果不存在timer，则在第一次的时候创建timer 并且把t.f设置成goroutineReay(这个意思是time到了时间之后设置一个触发函数，这个触发函数就是唤醒我们当前G任务)。 然后通过gopark来挂起当前的G任务 定时器的触发机制共分两种方式，分别为 调度器触发 和 监控线程sysmon 触发，两者主要是通过调用函数 checkTimers() 来实现的。 主要有两个地方会检查计时器，一个是 runtime.schedule，另一个是 findrunnable。 1234567891011121314// runtime/proc.gofunc schedule() &#123; _g_ := getg() top: pp := _g_.m.p.ptr() pp.preempt = false // 处理调度时的计时器触发 checkTimers(pp, 0) ... execute(gp, inheritTime) &#125; 另外一种是当前处理器 P 没有可执行的 Timer，且没有可执行的 G。那么按照调度模型，就会去窃取其他计时器和 G： 12345678910// runtime/proc.gofunc findrunnable() (gp *g, inheritTime bool) &#123; _g_ := getg() top: _p_ := _g_.m.p.ptr() ... now, pollUntil, _ := checkTimers(_p_, 0) ... &#125;","categories":[{"name":"Go源码剖析系列","slug":"Go源码剖析系列","permalink":"http://blog.crazylaw.cn/categories/Go%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90%E7%B3%BB%E5%88%97/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://blog.crazylaw.cn/tags/Golang/"},{"name":"Go源码剖析","slug":"Go源码剖析","permalink":"http://blog.crazylaw.cn/tags/Go%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90/"}]},{"title":"【Golang】- go channel源码阅读","slug":"Golang/go channel源码阅读","date":"2022-03-03T16:43:51.000Z","updated":"2022-03-29T02:24:03.596Z","comments":true,"path":"2022/03/04/Golang/go channel源码阅读/","link":"","permalink":"http://blog.crazylaw.cn/2022/03/04/Golang/go%20channel%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/","excerpt":"前言channel 是 Golang 中一个非常重要的特性，也是 Golang CSP 并发模型的一个重要体现。简单来说就是，goroutine 之间可以通过 channel 进行通信。 channel 在 Golang 如此重要，在代码中使用频率非常高，以至于不得不好奇其内部实现。本文将基于 go 1.17 的源码，分析 channel 的内部实现原理。","text":"前言channel 是 Golang 中一个非常重要的特性，也是 Golang CSP 并发模型的一个重要体现。简单来说就是，goroutine 之间可以通过 channel 进行通信。 channel 在 Golang 如此重要，在代码中使用频率非常高，以至于不得不好奇其内部实现。本文将基于 go 1.17 的源码，分析 channel 的内部实现原理。 channel 的基本使用在正式分析 channel 的实现之前，我们先看下 channel 的最基本用法，代码如下： 1234567891011121314package mainimport \"fmt\"func main() &#123; c := make(chan int) go func() &#123; c &lt;- 1 // send to channel &#125;() x := &lt;-c // recv from channel fmt.Println(x)&#125; 在以上代码中，我们通过 make(chan int) 来创建了一个类型为 int 的 channel。在一个 goroutine 中使用 c &lt;- 1 将数据发送到 channel 中。在主 goroutine 中通过 x := &lt;- c 从 channel 中读取数据并赋值给 x。 以上代码对应了 channel 的两种基本操作： send 操作 c &lt;- 1 表示发送数据到 channel recv 操作 x := &lt;- c 表示从 channel 中接收数据。 此外，channel 还分为有缓存 channel 和无缓存 channel。上述代码中，我们使用的是无缓冲的 channel。对于无缓冲的 channel，如果当前没有其他 goroutine 正在接收 channel 数据，则发送方会阻塞在发送语句处。 我们可以在 channel 初始化时指定缓冲区大小。例如，make(chan int, 2) 则指定缓冲区大小为 2。在缓冲区未满之前，发送方无阻塞地可以往 channel 发送数据，无需等待接收方准备好。而如果缓冲区已满，则发送方依然会阻塞。 channel 对应的底层实现函数在探究 channel 源码之前，我们肯定首先需要先找到 channel 在 Golang 的具体实现在哪。因为我们在使用 channel 时，用的是 &lt;- 符号，并不能直接在 go 源码中找到其实现。但是 Golang 编译器必然会将 &lt;- 符号翻译成底层对应的实现。 我们可以使用 Go 自带的命令: go tool compile -N -l -S hello.go, 将代码翻译成对应的汇编指令。 或者，直接可以使用 Compiler Explorer 这个在线工具。对于上述示例代码可以直接在这个链接看其汇编结果: go.godbolt.org/z/3xw5Cj。如下图： chansend1 chanrevc1 通过仔细查看以上示例代码对应的汇编指令，可以发现以下的对应关系： channel 的构造语句 make(chan int), 对应的是 runtime.makechan 函数发送语句 c &lt;- 1, 对应的是 runtime.chansend1 函数接收语句 x := &lt;- c, 对应的是 runtime.chanrecv1 函数以上几个函数的实现都位于 go 源码中的 runtime/chan.go 代码文件中。我们接下来针对这几个函数，探究下 channel 的实现。 channel 的构造channel 的构造语句 make(chan int)，将会被 golang 编译器翻译为 runtime.makechan 函数, 其函数签名如下： 1func makechan(t *chantype, size int) *hchan 其中，t *chantype 即构造 channel 时传入的元素类型。size int 即用户指定的 channel 缓冲区大小，不指定则为 0。该函数的返回值是 *hchan。hchan 则是 channel 在 golang 中的内部实现。其定义如下： 1234567891011121314type hchan struct &#123; qcount uint // buffer 中已放入的元素个数 dataqsiz uint // 用户构造 channel 时指定的 buf 大小 buf unsafe.Pointer // buffer elemsize uint16 // buffer 中每个元素的大小 closed uint32 // channel 是否关闭，== 0 代表未 closed elemtype *_type // channel 元素的类型信息 sendx uint // buffer 中已发送的索引位置 send index recvx uint // buffer 中已接收的索引位置 receive index recvq waitq // 等待接收的 goroutine list of recv waiters sendq waitq // 等待发送的 goroutine list of send waiters lock mutex&#125; hchan 中的所有属性大致可以分为三类： buffer 相关的属性。例如 buf、dataqsiz、qcount 等。 当 channel 的缓冲区大小不为 0 时，buffer 中存放了待接收的数据。使用 ring buffer 实现。 waitq 相关的属性，可以理解为是一个 FIFO 的标准队列。其中 recvq 中是正在等待接收数据的 goroutine，sendq 中是等待发送数据的 goroutine。waitq 使用双向链表实现。 其他属性，例如 lock、elemtype、closed。 通过简单分析 hchan 的属性，我们可以知道其中有两个重要的组件，buffer 和 waitq。hchan 所有行为和实现都是围绕这两个组件进行的。 向 channel 中发送数据channel 的发送和接收流程很相似，我们先分析下 channel 的发送过程 (如 c &lt;- 1), 对应于 runtime.chansend 函数的实现。 在尝试向 channel 中发送数据时，如果 recvq 队列不为空，则首先会从 recvq 中头部取出一个等待接收数据的 goroutine 出来。并将数据直接发送给该 goroutine。代码如下： 1234567891011lock(&amp;c.lock)if c.closed != 0 &#123; unlock(&amp;c.lock) panic(plainError(\"send on closed channel\"))&#125;if sg := c.recvq.dequeue(); sg != nil &#123; send(c, sg, ep, func() &#123; unlock(&amp;c.lock) &#125;, 3) return true&#125; 我们看到当我们整个send的过程是需要加锁处理的，并且也可以看到我们老生常谈的一个问题，当向cloesd的channel数据的时候，会导致panic产生 recvq 中是正在等待接收数据的 goroutine。当某个 goroutine 使用 recv 操作 (例如，x := &lt;- c)，如果此时 channel 的缓存中没有数据，且没有其他 goroutine 正在等待发送数据 (即 sendq 为空)，会将该 goroutine 以及要接收的数据地址打包成 sudog 对象，并放入到 recvq 中。 继续接着讲上面的代码，如果此时 recvq 不为空，则调用 send 函数将数据拷贝到对应的 goroutine 的堆栈上。 这个时候不经过我们的环形缓存！！！ send 函数的实现主要包含两点： memmove(dst, src, t.size) 进行数据的转移，本质上就是一个内存拷贝。 goready(gp, skip+1) goready 的作用是唤醒对应的 goroutine。 而如果 recvq 队列为空，则说明此时没有等待接收数据的 goroutine，那么此时 channel 会尝试把数据放到缓存中。 123456789101112131415if c.qcount &lt; c.dataqsiz &#123; // Space is available in the channel buffer. Enqueue the element to send. qp := chanbuf(c, c.sendx) if raceenabled &#123; racenotify(c, c.sendx, nil) &#125; typedmemmove(c.elemtype, qp, ep) c.sendx++ if c.sendx == c.dataqsiz &#123; c.sendx = 0 &#125; c.qcount++ unlock(&amp;c.lock) return true&#125; 以上代码的作用其实非常简单，就是把数据放到 buffer 中而已。此过程涉及了 ring buffer 的操作，其中 dataqsiz 代表用户指定的 channel 的 buffer 大小，如果不指定则默认为 0。 如果用户使用的是无缓冲 channel 或者此时 buffer 已满，则 c.qcount &lt; c.dataqsiz 条件不会满足, 以上流程也并不会执行到。此时会将当前的 goroutine 以及要发送的数据放入到 sendq 队列中，同时会切出该 goroutine 12345678910111213141516171819202122232425262728293031// Block on the channel. Some receiver will complete our operation for us.gp := getg()mysg := acquireSudog()mysg.releasetime = 0if t0 != 0 &#123; mysg.releasetime = -1&#125;// No stack splits between assigning elem and enqueuing mysg// on gp.waiting where copystack can find it.mysg.elem = epmysg.waitlink = nilmysg.g = gpmysg.isSelect = falsemysg.c = cgp.waiting = mysggp.param = nilc.sendq.enqueue(mysg)// Signal to anyone trying to shrink our stack that we're about// to park on a channel. The window between when this G's status// changes and when we set gp.activeStackChans is not safe for// stack shrinking.atomic.Store8(&amp;gp.parkingOnChan, 1)// 将 goroutine 转入 waiting 状态gopark(chanparkcommit, unsafe.Pointer(&amp;c.lock), waitReasonChanSend, traceEvGoBlockSend, 2)// Ensure the value being sent is kept alive until the// receiver copies it out. The sudog has a pointer to the// stack object, but sudogs aren't considered as roots of the// stack tracer.KeepAlive(ep)// 确保正在发送的值保持活动状态，直到接收者将其复制出来。sudog有一个指向堆栈对象的指针，但是sudog不被认为是堆栈跟踪程序的根。// 总而言之：防止被GC 调用 gopark 后，对于用户侧来看，该向 channel 发送数据的代码语句会进行阻塞。 以上过程就是 channel 的发送语句 (如，c &lt;- 1) 的内部工作流程，同时整个发送过程都使用 c.lock 进行加锁，保证并发安全。 简单来说，整个流程如下： 检查 recvq 是否为空，如果不为空，则从 recvq 头部取一个 goroutine，将数据发送过去，并唤醒对应的 goroutine 即可 如果 recvq 为空，则将数据放入到 buffer 中 如果 buffer 已满，则将要发送的数据和当前 goroutine 打包成 sudog 对象放入到 sendq 中。并将当前 goroutine 置为 waiting 状态。 从 channel 中接收数据的过程基本与发送过程类似，此处不再赘述了。 这里需要注意的是，channel 的整个发送过程和接收过程都使用 runtime.mutex 进行加锁。runtime.mutex 是 runtime 相关源码中常用到的一个轻量级锁。整个过程并不是最高效的 lockfree 的做法。 golang 在这里有个 issue:go/issues#8899，给出了 lockfree 的 channel 的方案。","categories":[{"name":"Go源码剖析系列","slug":"Go源码剖析系列","permalink":"http://blog.crazylaw.cn/categories/Go%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90%E7%B3%BB%E5%88%97/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://blog.crazylaw.cn/tags/Golang/"},{"name":"Go源码剖析","slug":"Go源码剖析","permalink":"http://blog.crazylaw.cn/tags/Go%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90/"}]},{"title":"【Golang】- Sync包详解","slug":"Golang/sync包详解","date":"2022-03-03T16:43:51.000Z","updated":"2022-04-15T07:40:12.764Z","comments":true,"path":"2022/03/04/Golang/sync包详解/","link":"","permalink":"http://blog.crazylaw.cn/2022/03/04/Golang/sync%E5%8C%85%E8%AF%A6%E8%A7%A3/","excerpt":"前言我们直到sync包给我们提供了一系列并发安全的数据结构。之前有见过一次sync-map，但是这一次刚好复习整理一下sync包的知识点。","text":"前言我们直到sync包给我们提供了一系列并发安全的数据结构。之前有见过一次sync-map，但是这一次刚好复习整理一下sync包的知识点。 Sync Sync.Map Sync.Once Sync.Pool Sync.Cond Sync.WaitGroup sync.Map sync.Map主要针对于Map对于并发读写不支持的场景下提出实现的，其原理是通过对map的写操作进行加锁：Sync.RWMutex 同时sync.Map实现了读写分离，当对map进行读操作时，通过读read Map, 当read Map中不存在是去dirty map中读取 123456789101112131415161718type Map struct &#123; me Mutex read atomic.Value // readOnly,读数据 dirty map[interface&#123;&#125;]*entry // 包含最新的写入数据，当missed达到一定的值时，将值赋给read misses int // 计数作用，每次从read中读失败，则missed加一&#125;// readOnly的数据结构type readOnly struct&#123; m map[interface&#123;&#125;]*entry amended bool // Map.dirty中的数据和这里的m中的数据不同时值为true&#125;// entry的数据结构：type entry struct &#123; p unsafe.Pointer // *interface&#123;&#125; // 可见value是一个指针值，虽然read和dirty存在冗余情况，但由于是指针类型，存储空间不会太多&#125; sync.Map相关问题 sync.Map的核心实现：两个map,一个用于写，一个用于读，这样的设计思想可以类比于缓存与数据库 sync.Map的局限性：如果写远高于读，dirty -&gt; readOnly这个类似于刷新数据的频率较高，不如直接使用mutex + map的效率高 sync.Map的设计思想：保证高频率读的无锁结构，空间换时间的思想 sync.WaitGroup sync.WaitGroup常用于针对goroutine的并发执行，通过WaitGroup可以等待所有的go程序执行结束之后再执行之后的逻辑 WaitGroup对象内部有一个计数器，最初重0开始，提供了三个方法：Add(),Done(),Wait()用来控制计数器的数量。Add(n)把计数器设置为n,Done()每次把计数器减一，Wait()会阻塞代码的执行，直到计数器的值减到0为止。","categories":[{"name":"Golang","slug":"Golang","permalink":"http://blog.crazylaw.cn/categories/Golang/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://blog.crazylaw.cn/tags/Golang/"}]},{"title":"【大数据】- 在公司从0到1落地flink流计算任务","slug":"大数据/在公司从0到1落地flink流计算任务","date":"2022-02-15T03:10:40.000Z","updated":"2022-02-17T01:14:41.637Z","comments":true,"path":"2022/02/15/大数据/在公司从0到1落地flink流计算任务/","link":"","permalink":"http://blog.crazylaw.cn/2022/02/15/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%9C%A8%E5%85%AC%E5%8F%B8%E4%BB%8E0%E5%88%B01%E8%90%BD%E5%9C%B0flink%E6%B5%81%E8%AE%A1%E7%AE%97%E4%BB%BB%E5%8A%A1/","excerpt":"前言在公司落地一套flink，总结到目前为止做了的事情。","text":"前言在公司落地一套flink，总结到目前为止做了的事情。 开发环境的部署我们默认场景下，flink使用hive-catalog，所以hive安装在这里。 Hive使用mysql作为外部数据存储，所以这里使用mysql 对于flink的开发，如果我想要一整套的本地的docker开发环境。 需要集成如下服务： hadoop hive flink kafka mysql 所以做了一个flink-docker-compose 在该项目中，由于不是采用CDH来集成的，都是一个个源码包手动安装的。所以需要下载源码包。 目前的版本为： flink: 1.12.0_2.11 mysql: 5.6 （8.0-jdbc） kafka: 2.12_2.11 maven: 3.6.3 jdk: 8/11 (默认jdk8) 本地环境的话，jdk需要自行处理好 hadoop: 3.1.1 hive: 3.1.0 一键下载源码包为了方便方便大家下载，对应的镜像链接，也都集成在了download.sh中，如果需要利用迅雷等p2p加速下载软件，可以通过从中提取出来 url 进行下载。 1./download.sh all 可设置的.env利用docker-compose对 .env的支持，可以在当中设置build image的一些环境变量和参数 12345678910111213141516171819# HadoopHADOOP_VERSION=3.1.1# HiveHIVE_VERSION=3.1.0# ScalaSCALA_VERSION=2.11# FlinkFLINK_VERSION=1.12.0# KafkaKAFKA_VERSION=2.4.0# ZookeeperZOOKEEPER_VERSION=3.5.6# MysqlMYSQL_VERSION=5.6MYSQL_DATABASE=defaultMYSQL_PORT=3306MYSQL_ROOT_PASSWORD=lnhzjm/B4qrScMYSQL_ENTRYPOINT_INITDB=./deploy/mysql/docker-entrypoint-initdb.dMYSQL_TIMEZONE=UTC kafka的网络我们知道kafka的网络协议是支持多端口的，由于我们有时候flink是在本地，有时候是在容器中，所以我们希望我们的kafka集群，支持容器内的网络，也支持和我们物理机的网络。 这个时候，我们需要设置kafka的2套端口协议。所以你可以看到 123456789101112131415161718192021222324252627kafka1: build: context: ./deploy/kafka args: scala_version: $&#123;SCALA_VERSION&#125; kafka_version: $&#123;KAFKA_VERSION&#125; container_name: flink-kafka1 ports: - '19092:19092' environment: KAFKA_PORT: 19092 KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://:9092,EXTERNAL_PLAINTEXT://kafka1:19092 KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,EXTERNAL_PLAINTEXT:PLAINTEXT KAFKA_LISTENERS: PLAINTEXT://:9092,EXTERNAL_PLAINTEXT://:19092 KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181 KAFKA_DEFAULT_REPLICATION_FACTOR: 3 networks: flink-networks: ipv4_address: 192.168.6.211 extra_hosts: - 'zookeeper:192.168.6.215' - 'kafka1:192.168.6.211' - 'kafka2:192.168.6.212' - 'kafka3:192.168.6.213' - 'kafka4:192.168.6.214' depends_on: - zookeeper 看到这里的 123KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://:9092,EXTERNAL_PLAINTEXT://kafka1:19092KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,EXTERNAL_PLAINTEXT:PLAINTEXTKAFKA_LISTENERS: PLAINTEXT://:9092,EXTERNAL_PLAINTEXT://:19092 这个就是决定我们的2套协议的关键所在，分别是对9092（容器内）和19092(和物理机)端口的支持。 但是设置完了这个，由于一般kafka-client会从可本机的可访问的dns服务器上寻找host映射，在连接的时候必备的流程。 在本地连接的时候，会通过kafka1/kafka2等hostname返回到client，client需要在本机找到所有的ip映射，所以我们需要设置一下etc/hosts 1echo \"127.0.0.1 kafka1 kafka2 kafka3 kafka4\" &gt;&gt; /etc/hosts 目前为止，我们所需要的环境变量已经处理完了。 基于datastream-api的flink开发我们知道flink提供了3种API，分别是datastream-api,table-api,sql-api datastream，也是flink的最原始的api，和flink集成一体，通过datastream-api，我们可以实现各种灵活的数据流处理。 按照我们以往对流计算数据的处理，在游戏公司中，一个游戏项目部署一个流计算的任务即为合理。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149.├── README.md├── pom.xml└── src └── main ├── java │ ├── deps │ │ ├── oaYdSdk │ │ │ ├── Youdu.java │ │ │ └── test │ │ │ └── YouduTest.java │ │ └── util │ │ ├── ParameterToolEnvironmentUtils.java │ │ └── Util.java │ └── org │ └── cp │ └── flink │ ├── Bootstrap.java │ ├── async │ │ └── AsyncOaYdHttpClient.java │ ├── events │ │ ├── CommonEvent.java │ │ ├── CommonEventHeader.java │ │ ├── app_error │ │ │ ├── Event.java │ │ │ ├── EventHeader.java │ │ │ └── EventLog.java │ │ ├── log_ban │ │ │ ├── Event.java │ │ │ ├── EventHeader.java │ │ │ └── EventLog.java │ │ ├── log_client_loss │ │ │ ├── Event.java │ │ │ ├── EventHeader.java │ │ │ └── EventLog.java │ │ ├── log_consume_gold │ │ │ ├── Event.java │ │ │ ├── EventHeader.java │ │ │ └── EventLog.java │ │ ├── log_fcm_error │ │ │ ├── Event.java │ │ │ ├── EventHeader.java │ │ │ └── EventLog.java │ │ ├── log_index_record │ │ │ ├── Event.java │ │ │ ├── EventHeader.java │ │ │ └── EventLog.java │ │ ├── log_index_record_data │ │ │ ├── Event.java │ │ │ ├── EventHeader.java │ │ │ └── EventLog.java │ │ ├── log_role_create │ │ │ ├── Event.java │ │ │ ├── EventHeader.java │ │ │ └── EventLog.java │ │ └── t_log_market │ │ ├── Event.java │ │ ├── EventHeader.java │ │ └── EventLog.java │ ├── jobs │ │ ├── alarm │ │ │ ├── ErrorReport_10008.java │ │ │ ├── Job_10002.java │ │ │ ├── Job_10008.java │ │ │ ├── Job_19.java │ │ │ ├── README.md │ │ │ └── handler │ │ │ ├── AbstractHandler.java │ │ │ ├── errorReport_10008 │ │ │ │ ├── Logic.java │ │ │ │ ├── Logic_10012.java │ │ │ │ ├── Logic_19.java │ │ │ │ └── Logic_20.java │ │ │ ├── job_10002 │ │ │ │ ├── LogIndexRecordDataHandler.java │ │ │ │ ├── LogIndexRecordHandler.java │ │ │ │ └── model │ │ │ │ ├── log_index_record │ │ │ │ │ └── StatisticsMcfx2Model.java │ │ │ │ └── log_index_record_data │ │ │ │ └── StatisticsMcfx1Model.java │ │ │ ├── job_10008 │ │ │ │ ├── AppErrorHandler.java │ │ │ │ ├── LogFcmErrorHandler.java │ │ │ │ └── model │ │ │ │ ├── app_error │ │ │ │ │ └── StatisticsAppErrorModel.java │ │ │ │ └── log_fcm_error │ │ │ │ └── StatisticsFcmErrorModel.java │ │ │ └── job_19 │ │ │ ├── LogBanHandler.java │ │ │ ├── LogClientLossHandler.java │ │ │ ├── LogConsumeGoldHandler.java │ │ │ ├── LogRoleCreateHandler.java │ │ │ ├── TLogMarketHandler.java │ │ │ └── model │ │ │ ├── log_ban │ │ │ │ └── StatisticsModel.java │ │ │ ├── log_client_loss │ │ │ │ └── IpMonitorModel.java │ │ │ ├── log_consume_gold │ │ │ │ ├── StatisticsBindGoldModel.java │ │ │ │ └── StatisticsUnBindGoldModel.java │ │ │ ├── log_role_create │ │ │ │ └── SingleServerRoleCreateModel.java │ │ │ └── t_log_market │ │ │ ├── MarketTransactionLogByBuyerModel.java │ │ │ └── MarketTransactionLogBySellerModel.java │ │ └── stream │ │ └── README.md │ ├── mock │ │ ├── MockAppError.java │ │ ├── MockLogFcmError.java │ │ └── README.md │ ├── serializer │ │ ├── AbstractSerializer.java │ │ └── log_role_create │ │ └── LogRoleCreateDeSerializer.java │ └── sinks │ ├── AsyncOaYdSdkHttpSink.java │ ├── MysqlItem.java │ └── MysqlSink.java └── resources ├── application-dev.properties ├── application-local.properties ├── application-pro.properties ├── application.properties ├── jobs │ ├── org.cp.flink.jobs.alarm.ErrorReport_10008 │ │ ├── application-dev.properties │ │ ├── application-local.properties │ │ ├── application-pro.properties │ │ └── application.properties │ ├── org.cp.flink.jobs.alarm.Job_10002 │ │ ├── application-dev.properties │ │ ├── application-local.properties │ │ ├── application-pro.properties │ │ └── application.properties │ ├── org.cp.flink.jobs.alarm.Job_10008 │ │ ├── application-dev.properties │ │ ├── application-local.properties │ │ ├── application-pro.properties │ │ └── application.properties │ └── org.cp.flink.jobs.alarm.Job_19 │ ├── application-dev.properties │ ├── application-local.properties │ ├── application-pro.properties │ └── application.properties └── log4j2.properties 这是我们早期的一个代码层级结构，所有的流计算任务基于一个flink项目下，resources下的配置根据当前需要提交的项目和环境来进行区分加载具体的配置，可以做到支持多环境,多项目下配置灵活配置。 我们看到 org.cp.flink目下，就是我们的所有flink代码。 1234567891011121314151617181920212223242526272829303132333435363738➜ flinkjob git:(master) ✗ tree -d src/main/java/orgsrc/main/java/org└── cp └── flink ├── async ├── events │ ├── app_error │ ├── log_ban │ ├── log_client_loss │ ├── log_consume_gold │ ├── log_fcm_error │ ├── log_index_record │ ├── log_index_record_data │ ├── log_role_create │ └── t_log_market ├── jobs │ ├── alarm │ │ └── handler │ │ ├── job_10002 │ │ │ └── model │ │ │ ├── log_index_record │ │ │ └── log_index_record_data │ │ ├── job_10008 │ │ │ └── model │ │ │ ├── app_error │ │ │ └── log_fcm_error │ │ └── job_19 │ │ └── model │ │ ├── log_ban │ │ ├── log_client_loss │ │ ├── log_consume_gold │ │ ├── log_role_create │ │ └── t_log_market │ └── stream ├── mock ├── serializer │ └── log_role_create └── sinks 我们先看到，jobs目录下的，分为了2种类型，我们平时用的流计算任务可以分为2种，一种是常规的告警属性，另一种是产品属性(类似BI系统需要的实时数据)。 我们看到alarm/handler/job_xxx就是我们具体的项目。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647src/main/java/org/cp/flink/jobs/alarm/├── Job_10002.java├── Job_10008.java├── Job_19.java├── README.md└── handler ├── AbstractHandler.java ├── errorReport_10008 │ ├── Logic.java │ ├── Logic_10012.java │ ├── Logic_19.java │ └── Logic_20.java ├── job_10002 │ ├── LogIndexRecordDataHandler.java │ ├── LogIndexRecordHandler.java │ └── model │ ├── log_index_record │ │ └── StatisticsMcfx2Model.java │ └── log_index_record_data │ └── StatisticsMcfx1Model.java ├── job_10008 │ ├── AppErrorHandler.java │ ├── LogFcmErrorHandler.java │ └── model │ ├── app_error │ │ └── StatisticsAppErrorModel.java │ └── log_fcm_error │ └── StatisticsFcmErrorModel.java └── job_19 ├── LogBanHandler.java ├── LogClientLossHandler.java ├── LogConsumeGoldHandler.java ├── LogRoleCreateHandler.java ├── TLogMarketHandler.java └── model ├── log_ban │ └── StatisticsModel.java ├── log_client_loss │ └── IpMonitorModel.java ├── log_consume_gold │ ├── StatisticsBindGoldModel.java │ └── StatisticsUnBindGoldModel.java ├── log_role_create │ └── SingleServerRoleCreateModel.java └── t_log_market ├── MarketTransactionLogByBuyerModel.java └── MarketTransactionLogBySellerModel.java 对于各个项目的错误告警监控，这里分为了多个job。 Job_10002.java Job_10008.java Job_19.java 我们从入口开始看 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101package org.cp.flink.jobs.alarm;import com.alibaba.fastjson.JSONObject;import deps.util.Util;import org.apache.flink.api.common.serialization.SimpleStringSchema;import org.apache.flink.api.java.utils.ParameterTool;import org.apache.flink.streaming.api.datastream.DataStream;import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.api.functions.ProcessFunction;import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;import org.apache.flink.util.Collector;import org.apache.flink.util.OutputTag;import org.cp.flink.Bootstrap;import org.cp.flink.jobs.alarm.handler.job_10008.AppErrorHandler;import org.cp.flink.jobs.alarm.handler.job_10008.LogFcmErrorHandler;import org.cp.flink.events.CommonEvent;import org.cp.flink.events.app_error.Event;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.util.Arrays;import java.util.Properties;public class Job_10008 extends Bootstrap &#123; private static final Logger logger = LoggerFactory.getLogger(Job_10008.class); public static void main(String[] args) throws Exception &#123; final StreamExecutionEnvironment env = getStreamExecutionEnvironment(args, Job_10008.class); env.enableCheckpointing(5000); // checkpoint every 5000 msecs ParameterTool parameterTool = (ParameterTool) env.getConfig().getGlobalJobParameters(); Properties props = new Properties(); props.setProperty(\"bootstrap.servers\", parameterTool.get(\"kafka.source.bootstrap.servers\")); props.setProperty(\"group.id\", parameterTool.get(\"kafka.source.group\")); props.put(\"enable.auto.commit\", parameterTool.get(\"kafka.source.enable.auto.commit\")); props.put(\"auto.commit.interval.ms\", parameterTool.get(\"kafka.source.auto.commit.interval.ms\")); props.put(\"session.timeout.ms\", parameterTool.get(\"kafka.source.session.timeout.ms\")); props.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); props.put(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); // 设置kafka并行度 env.setParallelism(parameterTool.getInt(\"kafka.source.parallelism\", 1)); DataStream&lt;String&gt; stream = env .addSource(new FlinkKafkaConsumer&lt;&gt;(Arrays.asList(parameterTool.get(\"kafka.source.topic\").split(\",\")), new SimpleStringSchema(), props)); env.setParallelism(parameterTool.getInt(\"app.parallelism\", 1)); SingleOutputStreamOperator&lt;CommonEvent&gt; s0 = stream.filter((String json) -&gt; &#123; try &#123; JSONObject.parseObject(json, CommonEvent.class); return true; &#125; catch (Exception e) &#123; e.printStackTrace(); logger.error(json); &#125; return false; &#125;).map( (String json) -&gt; JSONObject.parseObject(json, CommonEvent.class).setOriginJson(json) ).returns(CommonEvent.class); final OutputTag&lt;CommonEvent&gt; outputTagAppError = new OutputTag&lt;CommonEvent&gt;(AppErrorHandler.class.getName()) &#123; &#125;; final OutputTag&lt;CommonEvent&gt; outputTagLogFcmError = new OutputTag&lt;CommonEvent&gt;(LogFcmErrorHandler.class.getName()) &#123; &#125;; // 1. 主流不需要了, 所以不需要调用collector.collect() // 2. 只要旁路输出流，因为要区分数据进行处理 // 利用low-level-api的process算子处理旁路输出采集数据 SingleOutputStreamOperator&lt;CommonEvent&gt; s1 = s0.process(new ProcessFunction&lt;CommonEvent, CommonEvent&gt;() &#123; @Override public void processElement(CommonEvent event, Context context, Collector&lt;CommonEvent&gt; collector) &#123; switch (event.getHeaders().getLogName()) &#123; case \"app_error\": context.output(outputTagAppError, event); break; case \"log_fcm_error\": context.output(outputTagLogFcmError, event); break; &#125; &#125; &#125;); DataStream&lt;CommonEvent&gt; AppErrorSource = s1.getSideOutput(outputTagAppError); DataStream&lt;CommonEvent&gt; LogFcmErrorSource = s1.getSideOutput(outputTagLogFcmError); DataStream&lt;Event&gt; AppErrorSource_s0 = AppErrorSource.map((CommonEvent event) -&gt; JSONObject.parseObject(event.getOriginJson(), Event.class) ).returns(Event.class); DataStream&lt;org.cp.flink.events.log_fcm_error.Event&gt; LogFcmErrorSource_s0 = LogFcmErrorSource.map((CommonEvent event) -&gt; JSONObject.parseObject(event.getOriginJson(), org.cp.flink.events.log_fcm_error.Event.class) ).returns(org.cp.flink.events.log_fcm_error.Event.class); AppErrorHandler.build().handle(AppErrorSource_s0); LogFcmErrorHandler.build().handle(LogFcmErrorSource_s0); env.execute(Util.getCurrentJobName(((ParameterTool) env.getConfig().getGlobalJobParameters()))); &#125;&#125; 由于我们一个topic只能够可能存在多种数据，所以这里利用了旁路由进行了分流。把数据流分发到不同的子流中，我们再把子流传递不同的Handler进行处理。 这里例如: AppErrorHandler。我们以此为例子进行说明。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package org.cp.flink.jobs.alarm.handler.job_10008;import lombok.NoArgsConstructor;import org.apache.flink.api.java.utils.ParameterTool;import org.apache.flink.streaming.api.datastream.DataStream;import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;import org.apache.flink.streaming.api.functions.ProcessFunction;import org.apache.flink.util.Collector;import org.apache.flink.util.OutputTag;import org.cp.flink.jobs.alarm.handler.AbstractHandler;import org.cp.flink.jobs.alarm.handler.job_10008.model.app_error.StatisticsAppErrorModel;import org.cp.flink.events.app_error.Event;@NoArgsConstructorpublic class AppErrorHandler extends AbstractHandler&lt;Event&gt; &#123; private static AppErrorHandler instance; public static AppErrorHandler build() &#123; if (instance == null) &#123; instance = new AppErrorHandler(); &#125; return instance; &#125; @Override public void handle(DataStream&lt;Event&gt; s0) &#123; ParameterTool parameterTool = this.getParameterTool(s0); // 利用旁路输出多流到对应到model // StatisticsAppErrorModel final OutputTag&lt;Event&gt; outputTagStatisticsAppError = new OutputTag&lt;Event&gt;(StatisticsAppErrorModel.class.getName()) &#123; &#125;; SingleOutputStreamOperator&lt;Event&gt; s1 = s0.process(new ProcessFunction&lt;Event, Event&gt;() &#123; @Override public void processElement(Event event, Context context, Collector&lt;Event&gt; collector) &#123; context.output(outputTagStatisticsAppError, event); &#125; &#125;); DataStream&lt;Event&gt; sideOutputStreamAppError = s1.getSideOutput(outputTagStatisticsAppError); StatisticsAppErrorModel.build().handle(sideOutputStreamAppError); if (parameterTool.getBoolean(\"app.handler.print.console\", false)) &#123; s0.print(AppErrorHandler.class.getName()); &#125; &#125;&#125; 由于，我们希望到一条数据从kafka被pull下来到时候，可以用于多个不同的流计算模型model，所以我们在这里需要copy到多个旁路输出，但是这里我们只有一个stream-model，所以我们就只用一个来处理即可，从旁路输出拿到datastream之后，在对应的模型中进行核心逻辑处理。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144package org.cp.flink.jobs.alarm.handler.job_10008.model.app_error;import deps.util.Util;import lombok.NoArgsConstructor;import org.apache.flink.api.java.functions.KeySelector;import org.apache.flink.api.java.tuple.Tuple3;import org.apache.flink.api.java.tuple.Tuple5;import org.apache.flink.api.java.utils.ParameterTool;import org.apache.flink.streaming.api.datastream.DataStream;import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;import org.apache.flink.streaming.api.datastream.WindowedStream;import org.apache.flink.streaming.api.functions.timestamps.BoundedOutOfOrdernessTimestampExtractor;import org.apache.flink.streaming.api.functions.windowing.WindowFunction;import org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows;import org.apache.flink.streaming.api.windowing.time.Time;import org.apache.flink.streaming.api.windowing.windows.TimeWindow;import org.apache.flink.streaming.runtime.operators.util.AssignerWithPeriodicWatermarksAdapter;import org.apache.flink.util.Collector;import org.cp.flink.jobs.alarm.handler.AbstractHandler;import org.cp.flink.events.app_error.Event;import org.cp.flink.jobs.alarm.handler.job_19.model.log_ban.StatisticsModel;import org.cp.flink.sinks.MysqlItem;import org.cp.flink.sinks.MysqlSink;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.time.Duration;import java.util.HashMap;import java.util.concurrent.TimeUnit;/*/** * 错误日志统计 * 窗口：滚动事件窗口，每1分钟统计一次 */@NoArgsConstructorpublic class StatisticsAppErrorModel extends AbstractHandler&lt;Event&gt; &#123; private static final String DEFAULT_SINK_DATABASE = \"db_app_log_alarm\"; private static final String DEFAULT_SINK_TABLE = \"t_log_app_error_alarm_164\"; private static final Logger logger = LoggerFactory.getLogger(StatisticsAppErrorModel.class); private static StatisticsAppErrorModel instance; public static StatisticsAppErrorModel build() &#123; if (instance == null) &#123; instance = new StatisticsAppErrorModel(); &#125; return instance; &#125; @Override public void handle(DataStream&lt;Event&gt; s0) &#123; s0.getExecutionConfig().setAutoWatermarkInterval(5000L); logger.debug(\"getAutoWatermarkInterval: &#123;&#125;\", s0.getExecutionConfig().getAutoWatermarkInterval()); ParameterTool parameterTool = this.getParameterTool(s0); SingleOutputStreamOperator&lt;Event&gt; s1 = s0.assignTimestampsAndWatermarks(new AssignerWithPeriodicWatermarksAdapter.Strategy&lt;&gt;( new BoundedOutOfOrdernessTimestampExtractor&lt;Event&gt;(Time.of(1, TimeUnit.SECONDS)) &#123; @Override public long extractTimestamp(Event event) &#123; Long ts = event.getLogs().getMtime() * 1000L; logger.debug( \"thread-id: &#123;&#125;, eventTime: [&#123;&#125;|&#123;&#125;], watermark: [&#123;&#125;|&#123;&#125;]\", Thread.currentThread().getId(), ts, sdf.format(ts), this.getCurrentWatermark().getTimestamp(), sdf.format(this.getCurrentWatermark().getTimestamp()) ); return ts; &#125; &#125; ) // 尽可能和窗口大小保持一致，所以如果其中一个并行度出现问题的情况下 // 最大的延迟计算结果是一个窗口大小的时间 .withIdleness(Duration.ofMinutes(1L)) ); WindowedStream&lt;Event, Tuple5&lt;Integer, String, String, Integer, String&gt;, TimeWindow&gt; s2 = s1.keyBy(new KeySelector&lt;Event, Tuple5&lt;Integer, String, String, Integer, String&gt;&gt;() &#123; @Override public Tuple5&lt;Integer, String, String, Integer, String&gt; getKey(Event event) &#123; return Tuple5.of( event.getLogs().getRelatedAppId(), event.getLogs().getChildApp(), event.getLogs().getSummary(), event.getLogs().getLevel(), event.getLogs().getIp() ); &#125; &#125;) .window(TumblingEventTimeWindows.of(Time.minutes(1L))); SingleOutputStreamOperator&lt;Tuple3&lt;Tuple5&lt;Integer, String, String, Integer, String&gt;, Event, Integer&gt;&gt; s3 = s2.apply(new WindowFunction&lt;Event, Tuple3&lt;Tuple5&lt;Integer, String, String, Integer, String&gt;, Event, Integer&gt;, Tuple5&lt;Integer, String, String, Integer, String&gt;, TimeWindow&gt;() &#123; public void apply(Tuple5&lt;Integer, String, String, Integer, String&gt; key, TimeWindow timeWindow, Iterable&lt;Event&gt; iterable, Collector&lt;Tuple3&lt;Tuple5&lt;Integer, String, String, Integer, String&gt;, Event, Integer&gt;&gt; collector) throws Exception &#123; int sum = 0; for (Event event : iterable) &#123; sum++; &#125; logger.debug(\"聚合窗口key: &#123;&#125;, 窗口中的数量:&#123;&#125;, 此时的窗口范围是[&#123;&#125;,&#123;&#125;)\", key, sum, sdf.format(timeWindow.getStart()), sdf.format(timeWindow.getEnd())); collector.collect(Tuple3.of(key, iterable.iterator().next(), sum)); &#125; &#125;); String sinkDatabase = parameterTool.get(StatisticsModel.class.getName() + \".sink_database\", DEFAULT_SINK_DATABASE); String sinkTable = parameterTool.get(StatisticsModel.class.getName() + \".sink_table\", DEFAULT_SINK_TABLE); SingleOutputStreamOperator&lt;MysqlItem&gt; s4 = s3.map(e -&gt; &#123; HashMap&lt;String, Object&gt; kv = new HashMap&lt;&gt;(); kv.put(\"related_app_id\", e.f1.getLogs().getRelatedAppId()); kv.put(\"child_app\", e.f1.getLogs().getChildApp()); kv.put(\"summary\", e.f1.getLogs().getSummary()); kv.put(\"level\", e.f1.getLogs().getLevel()); kv.put(\"ip\", e.f1.getLogs().getIp()); kv.put(\"mtime\", e.f1.getLogs().getMtime()); kv.put(\"mdate\", Util.timeStamp2Date(Integer.toString(e.f1.getLogs().getMtime()), \"yyyy-MM-dd\")); // 来自聚合窗口统计的结果 kv.put(\"cnt\", e.f2); return MysqlItem.builder() .database(sinkDatabase) .table(sinkTable) .kv(kv) .build(); &#125; ).returns(MysqlItem.class); s4.addSink(new MysqlSink(parameterTool)) .setParallelism(parameterTool.getInt(\"mysql.sink.parallelism\", 1)) .name(\"MysqlSink\"); if (parameterTool.getBoolean(\"app.handler.print.console\", false)) &#123; s0.print(StatisticsAppErrorModel.class.getName()); &#125; &#125;&#125; 从上面的整体中，我们这里先看到设置watermark的逻辑，这个watermark决定了我们的flink的数据的有序性，是一个比较重要的处理。 123456789101112131415161718192021222324252627282930// 每5s-flink需要获取新的watermarks0.getExecutionConfig().setAutoWatermarkInterval(5000L);logger.debug(\"getAutoWatermarkInterval: &#123;&#125;\", s0.getExecutionConfig().getAutoWatermarkInterval());ParameterTool parameterTool = this.getParameterTool(s0);SingleOutputStreamOperator&lt;Event&gt; s1 = s0.assignTimestampsAndWatermarks(new AssignerWithPeriodicWatermarksAdapter.Strategy&lt;&gt;( new BoundedOutOfOrdernessTimestampExtractor&lt;Event&gt;(Time.of(1, TimeUnit.SECONDS)) &#123; @Override public long extractTimestamp(Event event) &#123; Long ts = event.getLogs().getMtime() * 1000L; logger.debug( \"thread-id: &#123;&#125;, eventTime: [&#123;&#125;|&#123;&#125;], watermark: [&#123;&#125;|&#123;&#125;]\", Thread.currentThread().getId(), ts, sdf.format(ts), this.getCurrentWatermark().getTimestamp(), sdf.format(this.getCurrentWatermark().getTimestamp()) ); return ts; &#125; &#125; ) // 尽可能和窗口大小保持一致，所以如果其中一个并行度出现问题的情况下 // 最大的延迟计算结果是一个窗口大小的时间 .withIdleness(Duration.ofMinutes(1L))); 我们这里通过AssignerWithPeriodicWatermarksAdapter设置一个watermark生成的策略。 当数据到来的时候，允许1秒延迟的情况下，解析数据的事件时间(event-time)作为我们的watermark，这里需要注意的是，这里从event-time提取的时间的单位需要是毫秒级别。 再通过.withIdleness，进行当某个窗口下idle了，那么也会刷新watermark。这个知识点，在kafka中是一个很重要的逻辑，由于flink在kafka的topic在多partition下，在partition的数据watermark对齐的情况，才会进行，所以为了防止，由于防止kafka的partition的数据倾斜对我们造成业务逻辑一直无法更新watermark的问题。这个十分必要。 12345678910111213WindowedStream&lt;Event, Tuple5&lt;Integer, String, String, Integer, String&gt;, TimeWindow&gt; s2 = s1.keyBy(new KeySelector&lt;Event, Tuple5&lt;Integer, String, String, Integer, String&gt;&gt;() &#123; @Override public Tuple5&lt;Integer, String, String, Integer, String&gt; getKey(Event event) &#123; return Tuple5.of( event.getLogs().getRelatedAppId(), event.getLogs().getChildApp(), event.getLogs().getSummary(), event.getLogs().getLevel(), event.getLogs().getIp() ); &#125;&#125;) .window(TumblingEventTimeWindows.of(Time.minutes(1L))); 对于windowstream，主要是定义窗口的时间大小， 窗口数据的唯一主键。 在这里，由于我的需求是每1分钟统计一次，所以这里可以看到我的窗口是基于EventTime（事件时间）的窗口，并且大小范围为1分钟。而数据的唯一主键则是通过getKet(Event event)方法来处理。通过flink内置的便捷的Tuple5这个类来处理的原因是因为我这里有5个元素组成的key。 1234567891011SingleOutputStreamOperator&lt;Tuple3&lt;Tuple5&lt;Integer, String, String, Integer, String&gt;, Event, Integer&gt;&gt; s3 = s2.apply(new WindowFunction&lt;Event, Tuple3&lt;Tuple5&lt;Integer, String, String, Integer, String&gt;, Event, Integer&gt;, Tuple5&lt;Integer, String, String, Integer, String&gt;, TimeWindow&gt;() &#123; public void apply(Tuple5&lt;Integer, String, String, Integer, String&gt; key, TimeWindow timeWindow, Iterable&lt;Event&gt; iterable, Collector&lt;Tuple3&lt;Tuple5&lt;Integer, String, String, Integer, String&gt;, Event, Integer&gt;&gt; collector) throws Exception &#123; int sum = 0; for (Event event : iterable) &#123; sum++; &#125; logger.debug(\"聚合窗口key: &#123;&#125;, 窗口中的数量:&#123;&#125;, 此时的窗口范围是[&#123;&#125;,&#123;&#125;)\", key, sum, sdf.format(timeWindow.getStart()), sdf.format(timeWindow.getEnd())); collector.collect(Tuple3.of(key, iterable.iterator().next(), sum)); &#125; &#125;); 接下来就是聚合(统计)的逻辑了，当window-trigger-condition满足条件之后，就会把当前窗口内的所有数据推到下一个算子，在这个算子的apply()中，我们可以看到我们只是简单的做了一个数据统计，也就是sum++，经过这一操作之后，经过collector对进行进行收集，准备用于下一个算子中。 12345678910111213141516171819202122232425SingleOutputStreamOperator&lt;MysqlItem&gt; s4 = s3.map(e -&gt; &#123; HashMap&lt;String, Object&gt; kv = new HashMap&lt;&gt;(); kv.put(\"related_app_id\", e.f1.getLogs().getRelatedAppId()); kv.put(\"child_app\", e.f1.getLogs().getChildApp()); kv.put(\"summary\", e.f1.getLogs().getSummary()); kv.put(\"level\", e.f1.getLogs().getLevel()); kv.put(\"ip\", e.f1.getLogs().getIp()); kv.put(\"mtime\", e.f1.getLogs().getMtime()); kv.put(\"mdate\", Util.timeStamp2Date(Integer.toString(e.f1.getLogs().getMtime()), \"yyyy-MM-dd\")); // 来自聚合窗口统计的结果 kv.put(\"cnt\", e.f2); return MysqlItem.builder() .database(sinkDatabase) .table(sinkTable) .kv(kv) .build(); &#125; ).returns(MysqlItem.class);s4.addSink(new MysqlSink(parameterTool)) .setParallelism(parameterTool.getInt(\"mysql.sink.parallelism\", 1)) .name(\"MysqlSink\"); 在这个前面到算子中，我们拿到了一些我们所期待到数据了，接下来就是把数据转换成为我们需要入库的一个结构。通过MysqlItem对象，我们把所有的结构化的对象通过MysqlSink方法进行发送给mysql。mysqlsink是我们自己封的一个sinker，其中的代码实现如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081package org.cp.flink.sinks;import lombok.Setter;import lombok.experimental.Accessors;import org.apache.flink.api.java.utils.ParameterTool;import org.apache.flink.configuration.Configuration;import org.apache.flink.streaming.api.functions.sink.RichSinkFunction;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.nio.charset.StandardCharsets;import java.sql.Connection;import java.sql.DriverManager;import java.sql.PreparedStatement;import java.sql.SQLException;@Setter@Accessors(chain = true)public class MysqlSink extends RichSinkFunction&lt;MysqlItem&gt; &#123; private static final Logger logger = LoggerFactory.getLogger(MysqlSink.class); ParameterTool parameterTool; private Connection connection; public MysqlSink(ParameterTool parameterTool) &#123; this.parameterTool = parameterTool; &#125; @Override public void open(Configuration parameters) throws Exception &#123; super.open(parameters); if (connection == null) &#123; connection = this.getConnection(); &#125; &#125; @Override public void close() throws Exception &#123; super.close(); if (connection != null) &#123; connection.close(); &#125; &#125; /** * todo: 再考虑一下如果插入失败的话是否需要重试之类的 * * @param item * @param context */ public void invoke(MysqlItem item, Context context) &#123; logger.debug(\"mysql-item: &#123;&#125;\", item); MysqlItem.Sql sqlInfo = item.toInsertIgnoreSql(); String sql = sqlInfo.getPreSql(); try &#123; PreparedStatement ps = this.connection.prepareStatement(sql); for (int i = 1; i &lt;= sqlInfo.getValues().size(); i++) &#123; ps.setObject(i, sqlInfo.getValues().get(i-1)); &#125; logger.debug(ps.toString()); ps.execute(); &#125; catch (Exception e) &#123; e.printStackTrace(); logger.error(e.getMessage()); &#125; &#125; private Connection getConnection() throws ClassNotFoundException, SQLException &#123; Class.forName(\"com.mysql.cj.jdbc.Driver\"); return DriverManager.getConnection( String.format( \"jdbc:mysql://%s:%s/?useUnicode=true&amp;characterEncoding=%s&amp;useSSL=false&amp;autoReconnect=true\", this.parameterTool.get(\"mysql.sink.host\"), this.parameterTool.get(\"mysql.sink.port\"), this.parameterTool.get(\"mysql.sink.characterEncoding\", StandardCharsets.UTF_8.toString()) ), this.parameterTool.get(\"mysql.sink.user\"), this.parameterTool.get(\"mysql.sink.password\") ); &#125;&#125; 到此，一个基于datastream-api的job，就完成了。 但是由于这是java技术栈，对于不是java技术栈的团队而言，这是一件比较麻烦的事情。就算是java技术栈，也需要去属于了解flink的原理，然后去编写对应的flink代码，这对于不熟悉datastream-api的小伙伴来说，也是一种头痛的事情。 所以对于这个问题，我们考虑使用上层一些的api，也就是table-api和sql-api。 但是由于此类api还是需要熟悉api的细节，所以我们看到了flink提供了一个叫sql-client的东西。但是由于sql-client的不稳定性（某些版本下存在比较严重的bug），且某些需求无法满足我们，为了灵活和可控性，我们最终解决了自行开发flink-sql-client。 基于自研sql-client的flink开发具体的实现方式在 flink-sql-submit 实现原理其实也不复杂，其实就是通过一个flink项目，封装成为一个类似cmd的命令，然后通过此方式来提交我们的sql或者sql文件 12345678910111213141516171819202122src/main/java/├── deps│ └── util│ ├── ParameterToolEnvironmentUtils.java│ ├── SqlCommandParser.java│ └── Util.java└── org └── client └── flink ├── Bootstrap.java ├── SqlSubmit.java ├── cmds │ ├── AbstractCommand.java │ ├── HelpCommand.java │ ├── HiveCatalogCommand.java │ ├── ICommand.java │ ├── JobCommand.java │ └── SqlParserCommand.java ├── enums │ └── PlanType.java ├── internals └── udfs 我们可以看到，整个项目只有少量文件。提供了几个命令： help 帮助命令 hivecatalog 管理 增 删 查 job 提交任务 sql sql-file sql-parser 调试解析sql 我们以一个sql-file为例子，其他大家可以在github上查看源码。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778-- 以\":\"为分隔符，分别代表：catalog_type, hive_conf_path, catalog_name-- \"-\" 代表使用默认值CATALOG_INFO = hive:/opt/hadoopclient/Hive/config/:-;CREATE DATABASE mstream_alarm COMMENT '告警系统流计算';USE mstream_alarm;SET 'pipeline.name' = '每1分钟基础服务告警';SET 'table.exec.emit.early-fire.enabled' = 'true';SET 'table.exec.emit.early-fire.delay' = '10s';SET 'mc.local.time.zone' = 'Asia/Shanghai';SET 'table.exec.sink.not-null-enforcer' = 'drop';-- checkpoint配置SET 'execution.checkpointing.mode' = 'EXACTLY_ONCE';SET 'execution.checkpointing.interval' = '2min';SET 'execution.checkpointing.timeout' = '1min';SET 'execution.checkpointing.prefer-checkpoint-for-recovery' = true;SET 'execution.checkpointing.externalized-checkpoint-retention' = 'RETAIN_ON_CANCELLATION';SET 'mc.state.backend.fs.checkpointdir' = 'hdfs:///flink/checkpoints/&#123;db&#125;/&#123;pipeline.name&#125;';SET 'mc.execution.savepoint.dir' = 'hdfs:///flink/savepoints/&#123;db&#125;/&#123;pipeline.name&#125;';-- 重启策略SET 'restart-strategy' = 'failure-rate';SET 'restart-strategy.failure-rate.delay' = '10s';SET 'restart-strategy.failure-rate.failure-rate-interval' = '5min';SET 'restart-strategy.failure-rate.max-failures-per-interval' = '10';CREATE TABLE app_error_To_t_log_app_error_alarm_164 ( headers ROW&lt;`app_id` int,`log_name` string&gt;, logs ROW&lt;`related_app_id` int, `child_app` varchar(200), `summary` string,`level` int,`ip` varchar(200),`detail` varchar(100), `mtime` int&gt;, etime as TO_TIMESTAMP(FROM_UNIXTIME(logs.`mtime`)), WATERMARK for etime AS etime -- defines watermark on ts column, marks ts as event-time attribute)WITH ( 'connector' = 'kafka', 'topic' = 'mfeilog_dsp_10008_app_error', 'properties.bootstrap.servers' = '127.0.0.1:9092', 'properties.group.id' = 'app_error_to_t_log_app_error_alarm_164', 'format' = 'json', 'scan.startup.mode' = 'latest-offset', 'json.fail-on-missing-field' = 'false', 'json.ignore-parse-errors' = 'false');CREATE TABLE `t_log_app_error_alarm_164` ( `related_app_id` int, `child_app` varchar(200), `summary` string, `level` int, `ip` varchar(200) , `cnt` varchar(200) COMMENT 'calculate the detail of count()', `mdate` string, `mtime` int, PRIMARY KEY (`related_app_id`,`child_app`,`summary`,`level`,`ip`) NOT ENFORCED) WITH ( 'connector' = 'jdbc', 'url' = 'jdbc:mysql://127.0.0.1:60701/db_app_log_alarm?useUnicode=true&amp;characterEncoding=utf8&amp;autoReconnect=true', 'driver' = 'com.mysql.cj.jdbc.Driver', 'table-name' = 't_log_app_error_alarm_164', 'username' = 'flink_mstream_alarm', 'password' = 'xxxx');insert into t_log_app_error_alarm_164 ( select t1.`related_app_id`,t1.`child_app`,t1.`summary`,t1.`level`,t1.`ip`,cast(t1.`cnt` as VARCHAR(200)) as `cnt`,t1.`mdate`,cast (t1.`mtime` as INT) from ( select logs.`related_app_id` as `related_app_id`, logs.`child_app` as `child_app`, logs.`summary` as `summary`, logs.`level` as `level`, logs.`ip` as `ip`, DATE_FORMAT(TUMBLE_START(etime, INTERVAL '1' MINUTE), 'yyyy-MM-dd') as `mdate`, UNIX_TIMESTAMP(DATE_FORMAT(TUMBLE_START(etime, INTERVAL '1' MINUTE), 'yyyy-MM-dd HH:mm:ss')) as `mtime`, COUNT(logs.`detail`) as `cnt` FROM app_error_To_t_log_app_error_alarm_164 GROUP BY logs.`related_app_id`, logs.`child_app`,logs.`summary`,logs.`level`,logs.`ip`,TUMBLE(etime, INTERVAL '1' MINUTE) ) t1); 我们可以看到这个sql-file，支持了一些关键字，这些关键字被开发在client当中了，所以可以被正常解析到。 通过解析到关键字，再调用对应的API，我们就可以设置对应的行为了。 我们可以看到我们从繁杂的datastreamapi中，已经把剥离了出来，通过sql这种DSL的方式，让不同语言技术栈的同事都可以定制自己的job。 并且支持了自定义重启策略，保证每一个算子在异常或者正常的情况下，都可以从正确的数据中进行恢复重启。 这一套sql编写下来，做的事情和我们上面的datastream做的事情是一样的，但是却无需了解太多其中的细节。 UDF的运用例如我们需要ip转地址字符串，这个时候，我们就需要udf来协助我们完成这件事。 client项目可以内置一些我们所需要的UDF，然后连同job一起生效。 例如： 12345678[root@127.0,0.1_A ~]# flink run -yid `cat /data/flink-stream/mstream/mstream_xx/yid` /data/flink-stream/flink-sql-submit-1.0-SNAPSHOT.jar job --sql \"CATALOG_INFO = hive:/opt/hadoopclient/Hive/config/:-;USE mstream_alarm;SELECT ip2location('219.135.155.76');\" Interface ana-group-1byez.dad44e53-24e6-41be-bfd5-a4055f4c6604.com:32263 of application 'application_1641337362340_6699'.Job has been submitted with JobID 824af5a31aba88db6e0137f5e834f26b+----+--------------------------------+| op | EXPR$0 |+----+--------------------------------+| +I | 中国,广东,广州 |+----+--------------------------------+ 我们可以看到，通过ip2localtion()，我们完成了一个udf，并且可以实现在sql的模式上。用过ip地址转为为了地址。 落地实战由于资源的有限，我们在flink的架构上，采用的是每个项目对应一个application的方法，每个application通过yarn来分配来分配资源容器，然后再通过yarn-session(非per on job)的方式来管理我们的flink应用。 申请资源应用1yarn-session.sh -jm 1024 -tm 1024 -s 16 -nm '告警流计算应用' -yd client 例子1234567891011121314151617181920212223242526272829303132333435363738# helproot@41c5967b5948:/www# flink run target/mc-flink-sql-submit-1.0-SNAPSHOT.jar help帮助命令Usage of \"flink run &lt;.jar&gt; help [options]\"Available Commands job 提交job作业 sql-parser 解析sql文件 help 帮助命令 hive-catalog hive-catalog的相关Global Options: --app.force.remote bool 是否启动远端环境变量: false --app.config.debug bool 是否打印用户参数: false# jobroot@41c5967b5948:/www# flink run target/mc-flink-sql-submit-1.0-SNAPSHOT.jar job help提交jobUsage of \"flink run &lt;.jar&gt; job [options]\" --sql string 执行的sql (*) --plan string 选择执行计划器: flink-streaming flink-batch blink-streaming flink-batchGlobal Options: --app.force.remote bool 是否启动远端环境变量: false --app.config.debug bool 是否打印用户参数: false flink-stream-sql-mctl 用法这是一个集成脚本，所以存在约定的规则和部署的架构约束。 这便于我们管理所有的applition和flink种的所有flink-job。 1234567891011121314151617181920212223242526flink-sql-submit git:(master) ✗ ./flink-stream-sql-mctl.sh flink-stream-sql-mctl.sh [OPTION] &lt;COMMAND&gt; Flink流计算SQL-Client的执行脚本 Command: run [FILE] 运行 stop [FILE] 停止 list [FILE] 列出FILE所在yid下的所有job任务列表 drop_table [FILE] 删除所有表 rebuild_run [FILE] 删除所有表，然后重跑(继承savepoint） Command-Common-Options: -c, --clientpath [LEVEL] flink-sql-submit.jar路径 (Default is '/data/tmp/mc-flink-sql-submit-1.0-SNAPSHOT.jar') -f 是否强制运行，忽略以往savepoint Common-Options: -h, --help Display this help and exit --loglevel [LEVEL] One of: FATAL, ERROR, WARN, INFO, NOTICE, DEBUG, ALL, OFF (Default is 'ERROR') --logfile [FILE] Full PATH to logfile. (Default is '/Users/caiwenhui/logs/flink-stream-sql-mctl.sh.log') -n, --dryrun Non-destructive. Makes no permanent changes. -q, --quiet Quiet (no output) -v, --verbose Output more information. (Items echoed to 'verbose') --force Skip all user interaction. Implied 'Yes' to all actions. 约定规则： 模型所在父目录的至少有一个yid文件（取最近的一个父节点的yid）对应所在的应用id 默认情况下，模型启动的时候会取最近一次savepoint的数据进行恢复，如果不存在，则直接启动 停止所有模型1for i in $(find /data/flink-stream/mstream_alarm/ -type f -name \"*.sql\");do /data/flink-stream/flink-stream-sql-mctl stop $i;done 启动所有模型1for i in $(find /data/flink-stream/mstream_alarm/ -type f -name \"*.sql\");do /data/flink-stream/flink-stream-sql-mctl run $i;done 删除所有表1for i in $(find /data/flink-stream/mstream_alarm/ -type f -name \"*.sql\");do /data/flink-stream/flink-stream-sql-mctl drop_table $i;done 相关的一些落地后截图信息 到此为止，我们的flink相关的流计算应用，从0到1的过程暂时画上一个里程碑。","categories":[{"name":"大数据","slug":"大数据","permalink":"http://blog.crazylaw.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"flink","slug":"flink","permalink":"http://blog.crazylaw.cn/tags/flink/"}]},{"title":"【Golang】- 基于gnet的端口复用支持多协议的客服聊天监控服务","slug":"Golang/基于gnet的客服聊天监控服务","date":"2022-02-11T16:46:51.000Z","updated":"2022-02-12T03:45:01.017Z","comments":true,"path":"2022/02/12/Golang/基于gnet的客服聊天监控服务/","link":"","permalink":"http://blog.crazylaw.cn/2022/02/12/Golang/%E5%9F%BA%E4%BA%8Egnet%E7%9A%84%E5%AE%A2%E6%9C%8D%E8%81%8A%E5%A4%A9%E7%9B%91%E6%8E%A7%E6%9C%8D%E5%8A%A1/","excerpt":"前言最近，公司以前有一些旧的服务，由于各种原因，导致各种问题，并且架构设计行也不是那么友好和不利于维护。所以准备重构设计一些服务。 在游戏公司中，GM客服的其中一个职能就是监督舆论，从玩家平日的聊天中进行监控。 我们从业务需求+技术架构层面进行整理。","text":"前言最近，公司以前有一些旧的服务，由于各种原因，导致各种问题，并且架构设计行也不是那么友好和不利于维护。所以准备重构设计一些服务。 在游戏公司中，GM客服的其中一个职能就是监督舆论，从玩家平日的聊天中进行监控。 我们从业务需求+技术架构层面进行整理。 历史在过去中，由于当时php还是如日中天，旧的则是采集的swoole1.x的版本进行开发的服务。受限于php一个语言特性，注定无法实现一些高性能的中间件，或者说大数据生态十分欠缺。当时用php除了fastcgi的web系统外，最多就只能做一些基本的常驻任务。 消息中间件最多也就是用到rabbitmq，rocketmq等等。 而常驻，一般无非就是直接cli，外加一个循环+sleep的组合套餐。而要实现websocket-server这种常驻服务，一般是借助swoole来处理。毕竟reactor的模式，怎么都比单进程的实现好。 分为了3个模块（每个模块=每个角色=一个进程=一个服务）： chat_record （聊天记录角色）（weboccket_client, tcp_clinet） db_server （数据层角色) (tcp_server) websocket_server (连接层角色) (webocket_server) 由于当时php基本无法多线程编程(可用，但是不友好)，只能采用这种委婉的伪多进程的模拟进行不同任务的处理和数据的交互。 新服务 但是由于种种原因，后面并未如此拆分架构，而是将websocket-server网络连接层的和业务层合并成为了一个单体服务 技术选型上 go gnet kafka 为什么核心的网络层需要采用gnet呢？一般Go语言的TCP(和HTTP)的处理都是每一个连接启动一个goroutine去处理，因为我们被教导goroutine的不像thread, 它是很便宜的，可以在服务器上启动成千上万的goroutine。 但是对于一百万的连接，这种goroutine-per-connection的模式就至少要启动一百万个goroutine，这对资源的消耗也是极大的。 针对不同的操作系统和不同的Go版本，一个goroutine锁使用的最小的栈大小是2KB ~ 8 KB (go stack),如果在每个goroutine中在分配byte buffer用以从连接中读写数据，几十G的内存轻轻松松就分配出去了。 吞吐率和延迟需要数据来支撑，但是显然这个单goroutine处理的模式不适合耗时较长的业务处理，&quot;hello world&quot;或者直接的简单的memory操作应该没有问题。 对于百万连接但是并发量很小的场景，比如消息推送、页游等场景，这种实现应该是没有问题的。 但是对于并发量很大，延迟要求比较低的场景，这种实现可能会存在问题。 gnet采用了类似netty的reactor模式，基于epoll或者kqueue实现io多路复用。并且基于golang的语言特性，其实现原理为带线程/go程池的主从 Reactors 多线程模式，在网络层上性能上有极大的优化。 我们通过gnet提供的tcp网络层，在应用层，实现了http和webocket的端口复用的形式。 http用于提供prometheus的metrics指标，例如连接数/各种类型引发的error数/每条数据被多少个GM客服监视着等等 websocket则是用于在我们的GM客服中，提一个实时的聊天数据获取 为什么采用kafka由于我们整套日志服务都是基于kafka作为核心组件的，所以在数据的实时上，可以保证到数据的实效性。 从而取消了以往从mysql中分库分表去查询数据。也不需要通过其他OLAP的服务进行处理。 端口复用实现支持多协议这个是网络连接层，也是链接的核心业务逻辑，在gnet中当有数据到来的时候，由IO多路复用的epoll模型，会触发OnTraffic(c gnet.Conn)的回调函数，在这个过程中，我们就可以通过网络层中获取的数据进行加工处理，形成自己想要的应用协议。 由于刚才介绍到了，我们需要实现核心需求：端口多协议复用 在这里，先列出核心的逻辑： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137type ApplicationLayerProto intfunc (alp ApplicationLayerProto) String() (s string) &#123; switch alp &#123; case HttpApplicationLayerProto: s = \"http\" case WebsocketApplicationLayerProto: s = \"websocket\" default: s = \"unknown\" &#125; return&#125;const ( HttpApplicationLayerProto ApplicationLayerProto = iota WebsocketApplicationLayerProto)type codec struct &#123; proto ApplicationLayerProto&#125;func (c *codec) isHttp() bool &#123; if c.proto == HttpApplicationLayerProto &#123; return true &#125; return false&#125;func (c *codec) isWebsocket() bool &#123; if c.proto == WebsocketApplicationLayerProto &#123; return true &#125; return false&#125;type httpCodec struct &#123; *codec parser *wildcat.HTTPParser&#125;type wsCodec struct &#123; *codec connected bool&#125;func (serv *server) OnOpen(c gnet.Conn) ([]byte, gnet.Action) &#123; c.SetContext(new(codec)) return nil, gnet.None&#125;func (serv *server) OnTraffic(c gnet.Conn) gnet.Action &#123; var buffer *bytes.Buffer var buff []bytepipeline: switch cdc := c.Context().(type) &#123; case *codec: buf, err := c.Next(-1) buff = make([]byte, len(buf)) copy(buff, buf) buffer = bytes.NewBuffer(buff) if err != nil &#123; return gnet.Close &#125; hc := &amp;httpCodec&#123;parser: wildcat.NewHTTPParser(), codec: cdc&#125; _, err = hc.parser.Parse(buf) if err != nil &#123; log.Errorlog(log.NetServerErrorCategory&#123;Summary: fmt.Sprintf(\"http parser error: %v\", err)&#125;) return gnet.Close &#125; if upgrade := hc.parser.FindHeader([]byte(\"Upgrade\")); upgrade != nil &amp;&amp; bytes.Equal(upgrade, []byte(\"websocket\")) &#123; cdc.proto = WebsocketApplicationLayerProto wc := &amp;wsCodec&#123; codec: cdc, &#125; c.SetContext(wc) &#125; else &#123; cdc.proto = HttpApplicationLayerProto c.SetContext(hc) &#125; goto pipeline case *httpCodec: buf := bufio.NewReader(buffer) req, err := http.ReadRequest(buf) if err != nil &#123; log.Errorlog(log.NetServerErrorCategory&#123;Summary: fmt.Sprintf(\"request from http error: %v\", err)&#125;) return gnet.Close &#125; metrics.TotalConnectedCounter.WithLabelValues(HttpApplicationLayerProto.String()).Inc() resp := route.NewResponse(c) h, _ := serv.serverMux.Handler(req) h.ServeHTTP(resp, req) if _, err = resp.Close(); err != nil &#123; log.Errorlog(log.NetServerErrorCategory&#123;Summary: fmt.Sprintf(\"write to http error: %v\", err)&#125;) return gnet.Close &#125; return gnet.Close case *wsCodec: if !cdc.connected &#123; wcb := &amp;wsConnBridge&#123; buff: buffer, c: c, &#125; _, err := ws.Upgrade(wcb) if err != nil &#123; log.Errorlog(log.NetServerErrorCategory&#123;Summary: fmt.Sprintf(\"upgrade[%s] to websocket error: %v\", c.RemoteAddr().String(), err)&#125;) &#125; log.Debugf(log.NetServerDebugCategory&#123;&#125;, \"conn[%v] upgrade websocket protocol\", c.RemoteAddr().String()) cdc.connected = true metrics.ConnectedGauge.Inc() metrics.TotalConnectedCounter.WithLabelValues(WebsocketApplicationLayerProto.String()).Inc() &#125; else &#123; msg, op, err := wsutil.ReadClientData(c) if err != nil &#123; if _, ok := err.(wsutil.ClosedError); !ok &#123; log.Errorlog(log.NetServerErrorCategory&#123;Summary: fmt.Sprintf(\"[%s] receive ws message error: %v\", c.RemoteAddr().String(), err)&#125;) &#125; return gnet.Close &#125; log.Debugf(log.NetServerDebugCategory&#123;&#125;, \"conn[%v] receive [op=%v] [msg=%v]\", c.RemoteAddr().String(), op, string(msg)) if op == ws.OpText &#123; if rs := route.MatchRequestSpec(msg); rs == nil &#123; return route.GlobalWsRouter.DefaultHandler().ServeWebsocket(\"/\", msg, c, op) &#125; else &#123; return route.GlobalWsRouter.MatchHandler(rs.Path).ServeWebsocket(rs.Path, rs.Params, c, op) &#125; &#125; &#125; &#125; return gnet.None&#125; 这里，我们可以看到，当存在新链接进来的啥时候，首先经过OnOpen(c gnet.Conn)方法，这个时候，我们会在gnet.Conn中设置一个我们用户的一个上下文环境Context，在这个Context下，我们为每个连接都初始化了codec的结构体对象，当开始接收数据的时候，触发到了OnTraffic(c gnet.Conn)方法，这个以后，我们需要把网络层接收到的数据拿出来，由于流的存在，使得我们无法重复在同一个连接中，多次重复获取流，所以如果后面需要用到的话，利用取出来的byte-buffer生成一个新的流，以供后续使用。 所以你会发现有一段代码为: 1234buf, err := c.Next(-1)buff = make([]byte, len(buf))copy(buff, buf)buffer = bytes.NewBuffer(buff) 接下来，需要做的事情就是解析数据为http协议对象，由于我这里的端口复用的逻辑是http+webocket复用，所以都是基于http协议的，所以这里可以简单粗暴的处理，然后通过判断http协议中是否包含了需要升级为webocket协议的关键字段Upgrade:webocket，如果包含，则表示本次请求是一个websocket连接，否则就是一个单纯http连接。以此来达到复用的需求。 在这个基础之上，我们也更新了当前连接的上下文环境Context，升级为了httpCodec和wsCodec，通过goto+断言语法，我们可以进入到，我们所需要进入的逻辑阶段。不要觉得这就完事了，麻烦的事情才刚开始，现在你只是知道了开头。 123456789101112131415buf := bufio.NewReader(buffer)req, err := http.ReadRequest(buf)if err != nil &#123; log.Errorlog(log.NetServerErrorCategory&#123;Summary: fmt.Sprintf(\"request from http error: %v\", err)&#125;) return gnet.Close&#125;metrics.TotalConnectedCounter.WithLabelValues(HttpApplicationLayerProto.String()).Inc()resp := route.NewResponse(c)h, _ := serv.serverMux.Handler(req)h.ServeHTTP(resp, req)if _, err = resp.Close(); err != nil &#123; log.Errorlog(log.NetServerErrorCategory&#123;Summary: fmt.Sprintf(\"write to http error: %v\", err)&#125;) return gnet.Close&#125;return gnet.Close 如果是http协议，那么我们就不需要升级协议了。但是有一个问题就是，在golang的http/server.go中，我们所熟悉的接口 123456789101112131415161718192021222324252627// A Handler responds to an HTTP request.//// ServeHTTP should write reply headers and data to the ResponseWriter// and then return. Returning signals that the request is finished; it// is not valid to use the ResponseWriter or read from the// Request.Body after or concurrently with the completion of the// ServeHTTP call.//// Depending on the HTTP client software, HTTP protocol version, and// any intermediaries between the client and the Go server, it may not// be possible to read from the Request.Body after writing to the// ResponseWriter. Cautious handlers should read the Request.Body// first, and then reply.//// Except for reading the body, handlers should not modify the// provided Request.//// If ServeHTTP panics, the server (the caller of ServeHTTP) assumes// that the effect of the panic was isolated to the active request.// It recovers the panic, logs a stack trace to the server error log,// and either closes the network connection or sends an HTTP/2// RST_STREAM, depending on the HTTP protocol. To abort a handler so// the client sees an interrupted response but the server doesn't log// an error, panic with the value ErrAbortHandler.type Handler interface &#123; ServeHTTP(ResponseWriter, *Request)&#125; 我们看到这个Handlerinterface，需要实现ServeHTTP(ResponseWriter, *Request)，而这个Request，对于我们目前来是，是不存在的，所以我们需要想办法构造一个Request对象出来。 123456789// ReadRequest reads and parses an incoming request from b.//// ReadRequest is a low-level function and should only be used for// specialized applications; most code should use the Server to read// requests and handle them via the Handler interface. ReadRequest// only supports HTTP/1.x requests. For HTTP/2, use golang.org/x/net/http2.func ReadRequest(b *bufio.Reader) (*Request, error) &#123; return readRequest(b, deleteHostHeader)&#125; 好在标准包中提供一个ReadRequest(b *bufio.Reader) (*Request, error)的方法，可以通过bufio.Reader去读取http协议，然后构造出我们所需要的Request对象，所以你会看到，我们在一开始copy(buff, buf)的意义就体现在此了。还会那句话，因为这是一个流，无法重复读取，所以我们利用[]byte构造一个全新的可度的字节流。 解决了Request的问题之后，另外一个问题也来了，ResponseWriter是一个和Response相关可写的字节流。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566// A ResponseWriter interface is used by an HTTP handler to// construct an HTTP response.//// A ResponseWriter may not be used after the Handler.ServeHTTP method// has returned.type ResponseWriter interface &#123; // Header returns the header map that will be sent by // WriteHeader. The Header map also is the mechanism with which // Handlers can set HTTP trailers. // // Changing the header map after a call to WriteHeader (or // Write) has no effect unless the modified headers are // trailers. // // There are two ways to set Trailers. The preferred way is to // predeclare in the headers which trailers you will later // send by setting the \"Trailer\" header to the names of the // trailer keys which will come later. In this case, those // keys of the Header map are treated as if they were // trailers. See the example. The second way, for trailer // keys not known to the Handler until after the first Write, // is to prefix the Header map keys with the TrailerPrefix // constant value. See TrailerPrefix. // // To suppress automatic response headers (such as \"Date\"), set // their value to nil. Header() Header // Write writes the data to the connection as part of an HTTP reply. // // If WriteHeader has not yet been called, Write calls // WriteHeader(http.StatusOK) before writing the data. If the Header // does not contain a Content-Type line, Write adds a Content-Type set // to the result of passing the initial 512 bytes of written data to // DetectContentType. Additionally, if the total size of all written // data is under a few KB and there are no Flush calls, the // Content-Length header is added automatically. // // Depending on the HTTP protocol version and the client, calling // Write or WriteHeader may prevent future reads on the // Request.Body. For HTTP/1.x requests, handlers should read any // needed request body data before writing the response. Once the // headers have been flushed (due to either an explicit Flusher.Flush // call or writing enough data to trigger a flush), the request body // may be unavailable. For HTTP/2 requests, the Go HTTP server permits // handlers to continue to read the request body while concurrently // writing the response. However, such behavior may not be supported // by all HTTP/2 clients. Handlers should read before writing if // possible to maximize compatibility. Write([]byte) (int, error) // WriteHeader sends an HTTP response header with the provided // status code. // // If WriteHeader is not called explicitly, the first call to Write // will trigger an implicit WriteHeader(http.StatusOK). // Thus explicit calls to WriteHeader are mainly used to // send error codes. // // The provided code must be a valid HTTP 1xx-5xx status code. // Only one header may be written. Go does not currently // support sending user-defined 1xx informational headers, // with the exception of 100-continue response header that the // Server sends automatically when the Request.Body is read. WriteHeader(statusCode int)&#125; 秉着面向接口开发的原则，并且为了更好的兼容第三方的API，所以我们需要实现一个自己的ResponseWriter对象，于是就有了route.NewResponse(c)，这个resp实现了上述的接口. 兼容了promhttp提供的Handler，也兼容了自己的helloworld接口。 接着我们通过cmux进行一个路由匹配，然后调用到对应的ServeHTTP,处理完逻辑之后，在resp的Close()阶段，把缓存区的所有[]byte，推送到连接层，然后通过返回gnet.Close进行网络层的断开，至此，一个简单而完整的http交互流程完毕。 对于Websocket协议来说，要做的事情也是十分繁琐（由于用了开源协议库，相对简化了很多），请先看下面的应用层协议处理逻辑。 12345678910111213141516171819202122232425262728293031if !cdc.connected &#123; wcb := &amp;wsConnBridge&#123; buff: buffer, c: c, &#125; _, err := ws.Upgrade(wcb) if err != nil &#123; log.Errorlog(log.NetServerErrorCategory&#123;Summary: fmt.Sprintf(\"upgrade[%s] to websocket error: %v\", c.RemoteAddr().String(), err)&#125;) &#125; log.Debugf(log.NetServerDebugCategory&#123;&#125;, \"conn[%v] upgrade websocket protocol\", c.RemoteAddr().String()) cdc.connected = true metrics.ConnectedGauge.Inc() metrics.TotalConnectedCounter.WithLabelValues(WebsocketApplicationLayerProto.String()).Inc() &#125; else &#123; msg, op, err := wsutil.ReadClientData(c) if err != nil &#123; if _, ok := err.(wsutil.ClosedError); !ok &#123; log.Errorlog(log.NetServerErrorCategory&#123;Summary: fmt.Sprintf(\"[%s] receive ws message error: %v\", c.RemoteAddr().String(), err)&#125;) &#125; return gnet.Close &#125; log.Debugf(log.NetServerDebugCategory&#123;&#125;, \"conn[%v] receive [op=%v] [msg=%v]\", c.RemoteAddr().String(), op, string(msg)) if op == ws.OpText &#123; if rs := route.MatchRequestSpec(msg); rs == nil &#123; return route.GlobalWsRouter.DefaultHandler().ServeWebsocket(\"/\", msg, c, op) &#125; else &#123; return route.GlobalWsRouter.MatchHandler(rs.Path).ServeWebsocket(rs.Path, rs.Params, c, op) &#125; &#125; &#125;&#125; 升级协议的过程中，我们用到了github.com/gobwas/ws这个协议库。 我们在接受到websocket前的时候需要先升级为websocket协议，但是这里遇到了一个问题，还是同理，我们的gnet.Conn的数据已经被我们取出来了，而升级的API显然就是需要提供一个可读可写的IO。 1234// Upgrade is like Upgrader&#123;&#125;.Upgrade().func Upgrade(conn io.ReadWriter) (Handshake, error) &#123; return DefaultUpgrader.Upgrade(conn)&#125; 12345678910111213// ReadWriter is the interface that groups the basic Read and Write methods.type ReadWriter interface &#123; Reader Writer&#125;type Reader interface &#123; Read(p []byte) (n int, err error)&#125;type Writer interface &#123; Write(p []byte) (n int, err error)&#125; 因此，我们又需要实现一个自己的wsConnBridge对象，主要是实现上述的接口，但是这个结构体相对来说就比较简单了，分别保存之前提出来的[]byte的buffer用于读行为，再保存一个gnet.Conn用于写行为即可。 123456789101112type wsConnBridge struct &#123; buff *bytes.Buffer c gnet.Conn&#125;func (w *wsConnBridge) Read(p []byte) (n int, err error) &#123; return w.buff.Read(p)&#125;func (w *wsConnBridge) Write(p []byte) (n int, err error) &#123; return w.c.Write(p)&#125; 升级完了，我们需要给当前的上下文环境的Context标记为已经升级连接完毕。 然后就是进入到数据的收发环节了。 github.com/gobwas/ws提供了api来进行数据的收发，分别有high-level和low-level，这里，我们可优先选择high-level-api，然后读取数据。 123type WebsocketHandler interface &#123; ServeWebsocket(path string, data []byte, w io.Writer, op ws.OpCode) gnet.Action&#125; 读取到数据之后，又因为我需要和http的route能有一个高度匹配的代码写法，所以在路由匹配上，也是做了一个类似的Match的行为，然后选择到对应的Handler，触发统一的ServeWebsocket()接口（为了和http的ServeHttp()对应）。 到此，从网络层到应用层的端口复用实现多协议原理就到此为止了。 接着就是处理自己的业务逻辑数据了。 业务逻辑概述 记录客服需要监控的数据规则和连接关联 kafka-client从监控规则中匹配合适的数据，推送到对应的fd中 123456789101112131415161718192021// ...var i int64 = 0var wg sync.WaitGroupListenChatRuleMap.Range(func(key, value interface&#123;&#125;) bool &#123; if Match(key.(string), kmsKey) &#123; wg.Add(1) go func(c gnet.Conn, wsp *WsSendPayload) &#123; defer wg.Done() err := wsutil.WriteServerMessage(c, ws.OpText, wsp.Json()) if err != nil &#123; log.Errorf(log.AppErrorCategory&#123;Summary: fmt.Sprintf(\"[wsWriteServerMessage failed] [err=%v]\", err)&#125;, \"[key=%s],[data=%s]\", key.(string), string(wsp.Json())) return &#125; atomic.AddInt64(&amp;i, 1) &#125;(value.(gnet.Conn), wsp) &#125; return true&#125;)wg.Wait()metrics.ChatLogCounterClientHistogram.WithLabelValues(strconv.FormatUint(uint64(lrc.Pid), 10), strconv.Itoa(wsp.ServerId), strconv.Itoa(wsp.AgentId)).Observe(float64(atomic.LoadInt64(&amp;i)))// ... 至此，网络层和业务层的所有需求大体已经完毕了。 prometheus 指标部分的指标如下，后续可以通过一些指标对服务的稳定和可靠性进行优化升级处理。 12345678910111213141516171819202122232425262728293031323334# HELP chat_monitor_app_handle_chat_total Counter of handle.# TYPE chat_monitor_app_handle_chat_total counterchat_monitor_app_handle_chat_total&#123;agent_id=\"29\",app_id=\"19\",server_id=\"6558\"&#125; 3# HELP chat_monitor_net_client_recv_counter number of chat log for client# TYPE chat_monitor_net_client_recv_counter histogramchat_monitor_net_client_recv_counter_bucket&#123;agent_id=\"29\",pid=\"1643890670000002\",server_id=\"6558\",le=\"1\"&#125; 0chat_monitor_net_client_recv_counter_bucket&#123;agent_id=\"29\",pid=\"1643890670000002\",server_id=\"6558\",le=\"2\"&#125; 0chat_monitor_net_client_recv_counter_bucket&#123;agent_id=\"29\",pid=\"1643890670000002\",server_id=\"6558\",le=\"4\"&#125; 2chat_monitor_net_client_recv_counter_bucket&#123;agent_id=\"29\",pid=\"1643890670000002\",server_id=\"6558\",le=\"8\"&#125; 3chat_monitor_net_client_recv_counter_bucket&#123;agent_id=\"29\",pid=\"1643890670000002\",server_id=\"6558\",le=\"16\"&#125; 3chat_monitor_net_client_recv_counter_bucket&#123;agent_id=\"29\",pid=\"1643890670000002\",server_id=\"6558\",le=\"32\"&#125; 3chat_monitor_net_client_recv_counter_bucket&#123;agent_id=\"29\",pid=\"1643890670000002\",server_id=\"6558\",le=\"64\"&#125; 3chat_monitor_net_client_recv_counter_bucket&#123;agent_id=\"29\",pid=\"1643890670000002\",server_id=\"6558\",le=\"+Inf\"&#125; 3chat_monitor_net_client_recv_counter_sum&#123;agent_id=\"29\",pid=\"1643890670000002\",server_id=\"6558\"&#125; 12chat_monitor_net_client_recv_counter_count&#123;agent_id=\"29\",pid=\"1643890670000002\",server_id=\"6558\"&#125; 3# HELP chat_monitor_net_current_connected Current Counter Gauge of ws-connected.# TYPE chat_monitor_net_current_connected gaugechat_monitor_net_current_connected 4# HELP chat_monitor_net_total_connected The Total Counter of connected.# TYPE chat_monitor_net_total_connected counterchat_monitor_net_total_connected&#123;type=\"http\"&#125; 15chat_monitor_net_total_connected&#123;type=\"websocket\"&#125; 5# HELP chat_monitor_server_error_total Counter of error.# TYPE chat_monitor_server_error_total counterchat_monitor_server_error_total&#123;type=\"network_server_error\"&#125; 1# HELP chat_monitor_server_gogc The value of GOGC# TYPE chat_monitor_server_gogc gaugechat_monitor_server_gogc 100# HELP chat_monitor_server_info Indicate the chat_monitor server info, and the value is the start timestamp (s).# TYPE chat_monitor_server_info gaugechat_monitor_server_info 1.644568978e+09# HELP chat_monitor_server_maxprocs The value of GOMAXPROCS.# TYPE chat_monitor_server_maxprocs gaugechat_monitor_server_maxprocs 6 到这里，一些基础而核心的逻辑也介绍完了。","categories":[{"name":"Golang","slug":"Golang","permalink":"http://blog.crazylaw.cn/categories/Golang/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://blog.crazylaw.cn/tags/Golang/"}]},{"title":"TIDB源码剖析（一）","slug":"TIDB/TIDB源码剖析（一）","date":"2022-01-24T02:28:33.000Z","updated":"2022-01-25T03:01:50.165Z","comments":true,"path":"2022/01/24/TIDB/TIDB源码剖析（一）/","link":"","permalink":"http://blog.crazylaw.cn/2022/01/24/TIDB/TIDB%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90%EF%BC%88%E4%B8%80%EF%BC%89/","excerpt":"简介这一章，作为我们的起始章节，跟着源码，我们一步步来熟悉TIDB的整体代码结构","text":"简介这一章，作为我们的起始章节，跟着源码，我们一步步来熟悉TIDB的整体代码结构 Select当我们有一条基本的sql如下： 1select * from mysql.user; 我们从接收到客户端连接开始，执行，解析，逻辑优化器，物理优化器，到最终结果开始分析。 12345678910111213github.com&#x2F;pingcap&#x2F;tidb&#x2F;planner.optimize at optimize.go:335github.com&#x2F;pingcap&#x2F;tidb&#x2F;planner.Optimize at optimize.go:211github.com&#x2F;pingcap&#x2F;tidb&#x2F;executor.(*Compiler).Compile at compiler.go:77github.com&#x2F;pingcap&#x2F;tidb&#x2F;session.(*session).ExecuteStmt at session.go:1696github.com&#x2F;pingcap&#x2F;tidb&#x2F;server.(*TiDBContext).ExecuteStmt at driver_tidb.go:220github.com&#x2F;pingcap&#x2F;tidb&#x2F;server.(*clientConn).handleStmt at conn.go:1977github.com&#x2F;pingcap&#x2F;tidb&#x2F;server.(*clientConn).handleQuery at conn.go:1846github.com&#x2F;pingcap&#x2F;tidb&#x2F;server.(*clientConn).dispatch at conn.go:1341github.com&#x2F;pingcap&#x2F;tidb&#x2F;server.(*clientConn).Run at conn.go:1091github.com&#x2F;pingcap&#x2F;tidb&#x2F;server.(*Server).onConn at server.go:556runtime.goexit at asm_amd64.s:1371 - Async stack tracegithub.com&#x2F;pingcap&#x2F;tidb&#x2F;server.(*Server).startNetworkListener at server.go:453 上面这是一个基本的执行流程，我们跟着这一段堆栈来进行分析。 github.com/pingcap/tidb/server.(*Server).onConn at server.go (连接处理逻辑)1conn.Run(ctx) 这里，我们看到了这是进入到了一个clientConn的 Run 方法。 1234567// Run reads client query and writes query result to client in for loop, if there is a panic during query handling,// it will be recovered and log the panic error.// This function returns and the connection is closed if there is an IO error or there is a panic.// 在for循环中，执行读取客户端查询，并将查询结果写入客户端，如果在处理查询时出现panic，// 它将被恢复并记录panic错误。// 如果出现IO错误或panic，该函数返回并关闭连接。func (cc *clientConn) Run(ctx context.Context) 这里我们看到了有一段文字帮助我们理解注意事项。 我们按照过程式的顺序来从上往下看源码 123456789101112131415161718192021 const size = 4096defer func() &#123; r := recover() if r != nil &#123; buf := make([]byte, size) stackSize := runtime.Stack(buf, false) buf = buf[:stackSize] logutil.Logger(ctx).Error(\"connection running loop panic\", zap.Stringer(\"lastSQL\", getLastStmtInConn&#123;cc&#125;), zap.String(\"err\", fmt.Sprintf(\"%v\", r)), zap.String(\"stack\", string(buf)), ) err := cc.writeError(ctx, errors.New(fmt.Sprintf(\"%v\", r))) terror.Log(err) metrics.PanicCounter.WithLabelValues(metrics.LabelSession).Inc() &#125; if atomic.LoadInt32(&amp;cc.status) != connStatusShutdown &#123; err := cc.Close() terror.Log(err) &#125;&#125;() 这段代码，我们看到了几点。 通过 recover() 方法来阻止panic引起的程序异常崩溃，如果是panic的话，那么将会有一段特殊的逻辑处理 1.1 通过 runtime.Stack(buf,false) 的第二个参数来控制只获取当前协程下的堆栈信息，并且写入到buf变量中 1.2 由于 const size = 4096 的原因，我们拿到的buf未必是那么多，因此，通过 buf[:stackSize] 来进行切片处理，把变量的指针重新指向新的数据区域 1.3 通过日志组件来记录详细信息， 有意思的是，这里通过了getLastStmtInConn结构体里面的String()方法来进行序列化自己想要的内容信息，其他的就是基本的err, stack的信息了 1.4 我们不单单需要在服务器上记录信息，还要把对应的用户错误信息也记录下来并且发送给客户端。所以通过了 err := cc.writeError(ctx, errors.New(fmt.Sprintf(&quot;%v&quot;, r))) 来实现这一点。 1.5 然后就是记录相关的metrics，因为发生了一次 panic，所以需要通过PanicCounter记录下来，用于统计由于session引起的panic总共有多少次 如果是非panic引起的函数析构，那么还要通过原子性草走来判断状态是否为关闭状态，如果是关闭状态，那么在这里就需要把连接断开，并且记录下错误信息 123456789101112131415161718// Usually, client connection status changes between [dispatching] &lt;=&gt; [reading].// When some event happens, server may notify this client connection by setting// the status to special values, for example: kill or graceful shutdown.// The client connection would detect the events when it fails to change status// by CAS operation, it would then take some actions accordingly.// 通常情况下，客户端连接状态在[dispatching] &lt;=&gt; [reading]之间变化。// 当某个事件发生时，服务器可以通过设置来通知这个客户端连接// 将状态设置为特殊值，例如:kill或graceful shutdown。// 当CAS操作改变状态失败时，客户端连接将检测到事件，然后采取相应的动作。for &#123; if !atomic.CompareAndSwapInt32(&amp;cc.status, connStatusDispatching, connStatusReading) || // The judge below will not be hit by all means, // But keep it stayed as a reminder and for the code reference for connStatusWaitShutdown. atomic.LoadInt32(&amp;cc.status) == connStatusWaitShutdown &#123; return &#125;...&#125; 我们看到这是一个循环操作，并且通过原子性操作atomic.CompareAndSwapInt32（比较然后再交换，所以符合CAS原则，乐观锁）来判断session连接是否能是否能切换到connStatusDispatching =&gt; connStatusReading 状态 如果不可以切换，那么则结束该方法 如果连接状态为等待关闭状态，那么也结束该方法 对于其中的 ...，现在会在下面进一步说明。 1234567891011121314151617181920212223242526cc.alloc.Reset()// close connection when idle time is more than wait_timeoutwaitTimeout := cc.getSessionVarsWaitTimeout(ctx)cc.pkt.setReadTimeout(time.Duration(waitTimeout) * time.Second)start := time.Now()data, err := cc.readPacket()if err != nil &#123; if terror.ErrorNotEqual(err, io.EOF) &#123; if netErr, isNetErr := errors.Cause(err).(net.Error); isNetErr &amp;&amp; netErr.Timeout() &#123; idleTime := time.Since(start) logutil.Logger(ctx).Info(\"read packet timeout, close this connection\", zap.Duration(\"idle\", idleTime), zap.Uint64(\"waitTimeout\", waitTimeout), zap.Error(err), ) &#125; else &#123; errStack := errors.ErrorStack(err) if !strings.Contains(errStack, \"use of closed network connection\") &#123; logutil.Logger(ctx).Warn(\"read packet failed, close this connection\", zap.Error(errors.SuspendStack(err))) &#125; &#125; &#125; disconnectByClientWithError.Inc() return&#125; cc.alloc.Reset()重置内存池大小 当空闲时间大于等待超时时间的话那么将会关闭丽连接。cc.pkt.setReadTimeout(time.Duration(waitTimeout) * time.Second) 从客户端读取数据，如果存在错误，那么将会记录下来相关信息，例如从读取数据到最后的时间，来统计idletime，通过metrics.DisconnectionCounter.WithLabelValues(metrics.LblError)来记录因为err导致连接断开的次数 123if !atomic.CompareAndSwapInt32(&amp;cc.status, connStatusReading, connStatusDispatching) &#123; return&#125; 同理，经过cas乐观锁，把状态从 connStatusReading =&gt; connStatusDispatching如果，交换设置失败，那么就结束函数。 12startTime := time.Now()err = cc.dispatch(ctx, data) github.com/pingcap/tidb/server.(*clientConn).dispatch （分发逻辑）1234567// dispatch handles client request based on command which is the first byte of the data.// It also gets a token from server which is used to limit the concurrently handling clients.// The most frequently used command is ComQuery.// dispatch根据命令处理客户端请求，命令是数据的第一个字节。// 它也从服务器获取一个令牌，用于限制并发处理客户端。// 最常用的命令是ComQuery。func (cc *clientConn) dispatch(ctx context.Context, data []byte) error 下面的方法都是dispatch的过程顺序逻辑 12345678910defer func() &#123; // reset killed for each request atomic.StoreUint32(&amp;cc.ctx.GetSessionVars().Killed, 0)&#125;()t := time.Now()if (cc.ctx.Status() &amp; mysql.ServerStatusInTrans) &gt; 0 &#123; connIdleDurationHistogramInTxn.Observe(t.Sub(cc.lastActive).Seconds())&#125; else &#123; connIdleDurationHistogramNotInTxn.Observe(t.Sub(cc.lastActive).Seconds())&#125; 这里可以看到这里有一个defer，当函数结束的时候，会重置session的Killed次数 cc.ctx.Status() &amp; mysql.ServerStatusInTrans 这里因为兼容了mysql的无状态协议，所以通过第一个位运算来判断当前状态 如果当前链接处于一个事务状态下的话，那么通过connIdleDurationHistogramInTxn.Observe(t.Sub(cc.lastActive).Seconds()) 用直方图监控从最后一次活跃时间到当前分发时间 否则则用另一个metrics来记录 1234567891011span := opentracing.StartSpan(\"server.dispatch\")cfg := config.GetGlobalConfig()if cfg.OpenTracing.Enable &#123; ctx = opentracing.ContextWithSpan(ctx, span)&#125;var cancelFunc context.CancelFuncctx, cancelFunc = context.WithCancel(ctx)cc.mu.Lock()cc.mu.cancelFunc = cancelFunccc.mu.Unlock() 通过opentracing来开始进行分布式追踪，cc.mu 主要是用来在事务中取消事务用的。 1234567891011121314cc.lastPacket = datacmd := data[0]data = data[1:]if topsqlstate.TopSQLEnabled() &#123; defer pprof.SetGoroutineLabels(ctx)&#125;if variable.EnablePProfSQLCPU.Load() &#123; label := getLastStmtInConn&#123;cc&#125;.PProfLabel() if len(label) &gt; 0 &#123; defer pprof.SetGoroutineLabels(ctx) ctx = pprof.WithLabels(ctx, pprof.Labels(\"sql\", label)) pprof.SetGoroutineLabels(ctx) &#125;&#125; 把当前session接收到的数据记录在lastPakcet中 第一个字节代表命令 后面的字节代表数据 1234567891011token := cc.server.getToken()defer func() &#123; // if handleChangeUser failed, cc.ctx may be nil if cc.ctx != nil &#123; cc.ctx.SetProcessInfo(\"\", t, mysql.ComSleep, 0) &#125; cc.server.releaseToken(token) span.Finish() cc.lastActive = time.Now()&#125;() 这里需要关注一下defer里面的内容 根据mysql协议，当命令为mysql.ComSleep的时候，代表execute已经完成了。所以当结束的时候，需要设置一下这个ProcessInfo 然后释放本次token，并且span也需要标记为完成 更新最后一次活跃时间 123456vars := cc.ctx.GetSessionVars()// reset killed for each requestatomic.StoreUint32(&amp;vars.Killed, 0)if cmd &lt; mysql.ComEnd &#123; cc.ctx.SetCommandValue(cmd)&#125; 获取当前session的变量 重置其中的killed属性 如果cmd在范围内的，更新当前命令的值 12345678dataStr := string(hack.String(data))switch cmd &#123;case mysql.ComPing, mysql.ComStmtClose, mysql.ComStmtSendLongData, mysql.ComStmtReset, mysql.ComSetOption, mysql.ComChangeUser: cc.ctx.SetProcessInfo(\"\", t, cmd, 0)case mysql.ComInitDB: cc.ctx.SetProcessInfo(\"use \"+dataStr, t, cmd, 0)&#125; 这里利用了golang种的hack（黑科技）的方式来把byte转换成string，其实主要就是因为底层用的都有一样的结构体，所以可以直接通过unsafe.pointer来直接操作内容指针，进行zero-copy 对cmd进行processinfo的处理，如果是use db的命令的话，则需要传递数据库 12345678910111213141516171819202122232425switch cmd &#123; case mysql.ComSleep: // TODO: According to mysql document, this command is supposed to be used only internally. // So it's just a temp fix, not sure if it's done right. // Investigate this command and write test case later. return nil case mysql.ComQuit: return io.EOF case mysql.ComInitDB: if err := cc.useDB(ctx, dataStr); err != nil &#123; return err &#125; return cc.writeOK(ctx) case mysql.ComQuery: // Most frequently used command. // For issue 1989 // Input payload may end with byte '\\0', we didn't find related mysql document about it, but mysql // implementation accept that case. So trim the last '\\0' here as if the payload an EOF string. // See http://dev.mysql.com/doc/internals/en/com-query.html if len(data) &gt; 0 &amp;&amp; data[len(data)-1] == 0 &#123; data = data[:len(data)-1] dataStr = string(hack.String(data)) &#125; return cc.handleQuery(ctx, dataStr) ...&#125; 这里我复制了一部分，因为我们重点关注mysql.ComQuery命令。 根据提示，我们发现因为mysql协议说明了输入载体可能以\\0作为最后字节，所以这里一定要减去client发送的多余的最后一个字节。所以长度进行了-1操作 然后进入到cc.handleQuery(ctx, dataStr) github.com/pingcap/tidb/server.(*clientConn).handleQuery12345// handleQuery executes the sql query string and writes result set or result ok to the client.// As the execution time of this function represents the performance of TiDB, we do time log and metrics here.// There is a special query `load data` that does not return result, which is handled differently.// Query `load stats` does not return result either.func (cc *clientConn) handleQuery(ctx context.Context, sql string) (err error) 这个方法，终于开始正式进入我们的主题了 1234567891011defer trace.StartRegion(ctx, \"handleQuery\").End()sc := cc.ctx.GetSessionVars().StmtCtxprevWarns := sc.GetWarnings()stmts, err := cc.ctx.Parse(ctx, sql)if err != nil &#123; return err&#125;if len(stmts) == 0 &#123; return cc.writeOK(ctx)&#125; defer进行了当函数结束的时候，标记handleQuery结束 拿到statement的上下文环境 从上下文中拿到所有的warinning警告 通过cc.ctx.Parse(ctx, sql)来进行解析sql，这里属于一个大的篇章，暂时不张开讲，主要涉及到的内容有编译原理,AST-Tree，Yacc。我们通过这里可以拿到一棵抽象语法树，实质是SelectStmt，内部包含了如下内容： dmlNode（因为select语句属于dml语句） 其他的都是常规的例如FROM, WHERE, FIELDS, DISTINCT 等等 如果没有一个完成的抽象语法书，则直接返回响应协议和对应的内容 123456789101112131415161718192021222324252627282930var pointPlans []plannercore.Planif len(stmts) &gt; 1 &#123; // The client gets to choose if it allows multi-statements, and // probably defaults OFF. This helps prevent against SQL injection attacks // by early terminating the first statement, and then running an entirely // new statement. capabilities := cc.ctx.GetSessionVars().ClientCapability if capabilities&amp;mysql.ClientMultiStatements &lt; 1 &#123; // The client does not have multi-statement enabled. We now need to determine // how to handle an unsafe situation based on the multiStmt sysvar. switch cc.ctx.GetSessionVars().MultiStatementMode &#123; case variable.OffInt: err = errMultiStatementDisabled return err case variable.OnInt: // multi statement is fully permitted, do nothing default: warn := stmtctx.SQLWarn&#123;Level: stmtctx.WarnLevelWarning, Err: errMultiStatementDisabled&#125; parserWarns = append(parserWarns, warn) &#125; &#125; // Only pre-build point plans for multi-statement query pointPlans, err = cc.prefetchPointPlanKeys(ctx, stmts) if err != nil &#123; return err &#125;&#125; 通过Session中的var中的ClientCapability的位运算来判断是否支持mysql.ClientMultiStatements（多sql语句） 如果sysvar也不支持MultiStatementMode,也就是variable.OffInt，那么就直接返回err 如果没有能力支持client多statement的话，但是var又开启了的话，目前啥事也没做 默认就是不支持，但是会通过warn来展示给客户端 只有在多statement的场景下预取目标计划关键字 12345678910111213141516171819202122232425for i, stmt := range stmts &#123; if len(pointPlans) &gt; 0 &#123; // Save the point plan in Session, so we don't need to build the point plan again. cc.ctx.SetValue(plannercore.PointPlanKey, plannercore.PointPlanVal&#123;Plan: pointPlans[i]&#125;) &#125; retryable, err = cc.handleStmt(ctx, stmt, parserWarns, i == len(stmts)-1) if err != nil &#123; if !retryable || !errors.ErrorEqual(err, storeerr.ErrTiFlashServerTimeout) &#123; break &#125; _, allowTiFlashFallback := cc.ctx.GetSessionVars().AllowFallbackToTiKV[kv.TiFlash] if !allowTiFlashFallback &#123; break &#125; // When the TiFlash server seems down, we append a warning to remind the user to check the status of the TiFlash // server and fallback to TiKV. warns := append(parserWarns, stmtctx.SQLWarn&#123;Level: stmtctx.WarnLevelError, Err: err&#125;) delete(cc.ctx.GetSessionVars().IsolationReadEngines, kv.TiFlash) _, err = cc.handleStmt(ctx, stmt, warns, i == len(stmts)-1) cc.ctx.GetSessionVars().IsolationReadEngines[kv.TiFlash] = struct&#123;&#125;&#123;&#125; if err != nil &#123; break &#125; &#125;&#125; 如果有目标计划的话，那么只需要在上下文中设置value即可，不需要再次构建目标计划 cc.handleStmt(ctx, stmt, parserWarns, i == len(stmts)-1) 这是我们的核心中的核心，这里面就是处理抽象语法树的逻辑，包含了逻辑优化, 物理优化, 执行器，tikv交互等等 todo：留着回来分析 github.com/pingcap/tidb/server.(*clientConn).handleStmt12345// The first return value indicates whether the call of handleStmt has no side effect and can be retried.// Currently, the first return value is used to fall back to TiKV when TiFlash is down.// 第一个返回值表示调用handleStmt是否没有副作用，是否可以重试// 当前，第一个返回值用于在TiFlash down时回落到TiKVfunc (cc *clientConn) handleStmt(ctx context.Context, stmt ast.StmtNode, warns []stmtctx.SQLWarn, lastStmt bool) (bool, error) 12345ctx = context.WithValue(ctx, execdetails.StmtExecDetailKey, &amp;execdetails.StmtExecDetails&#123;&#125;)ctx = context.WithValue(ctx, util.ExecDetailsKey, &amp;util.ExecDetails&#123;&#125;)reg := trace.StartRegion(ctx, \"ExecuteStmt\")cc.audit(plugin.Starting)rs, err := cc.ctx.ExecuteStmt(ctx, stmt) 上下文带上value，设置主要是StmtExecDetails，里面记录了写入sql到响应的时间 上下文带上value，设置主要是ExecDetails，里面记录了execution的详情信息，分别有","categories":[{"name":"TIDB","slug":"TIDB","permalink":"http://blog.crazylaw.cn/categories/TIDB/"}],"tags":[{"name":"TIDB","slug":"TIDB","permalink":"http://blog.crazylaw.cn/tags/TIDB/"}]},{"title":"2022杂乱知识点总结","slug":"2022杂乱知识点总结","date":"2022-01-19T06:38:06.000Z","updated":"2022-02-05T06:14:45.964Z","comments":true,"path":"2022/01/19/2022杂乱知识点总结/","link":"","permalink":"http://blog.crazylaw.cn/2022/01/19/2022%E6%9D%82%E4%B9%B1%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93/","excerpt":"简介记录一下常规下的一些知识点。","text":"简介记录一下常规下的一些知识点。 Gogo channel close后读的问题","categories":[{"name":"知识点","slug":"知识点","permalink":"http://blog.crazylaw.cn/categories/%E7%9F%A5%E8%AF%86%E7%82%B9/"}],"tags":[{"name":"2022杂乱知识点总结","slug":"2022杂乱知识点总结","permalink":"http://blog.crazylaw.cn/tags/2022%E6%9D%82%E4%B9%B1%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93/"}]},{"title":"【2021】 2021年终总结","slug":"2021年终总结","date":"2022-01-02T15:54:00.000Z","updated":"2022-01-25T03:01:32.272Z","comments":true,"path":"2022/01/02/2021年终总结/","link":"","permalink":"http://blog.crazylaw.cn/2022/01/02/2021%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93/","excerpt":"前言2021已经过去了，在这里回忆一下，我的2021的年终总结。","text":"前言2021已经过去了，在这里回忆一下，我的2021的年终总结。 困难与挑战工作中，在过去的一年里，迎接了不少的挑战，在公司中落地大数据相关的服务了从erlang到golang的转换。我们整套新的体系称为：mfeilog 在公司中，积累了许多go语言的组件，并且不断的完善修复相关的bug，例如，mfeilog的核心数据源：msource，mfeilog的基础组件：daemon组件/no-named-pipe组件，logbdev组件，sink_mysql/sink_kafka等组件。 说到这个msource服务，这是一个基于rocksdb实现的支持sql语法的轻量级小型定制数据库服务，可以说是经过了不断优化，升级，修复内部各种内置功能，才得以现在的稳定和高效。给应用层提供了一个高可靠，稳定，的数据源功能。其中有一个bug，印象特别深刻，也是我始料未及的一个bug，因为发号器是附属在Table当中的，我需要自定义序列化后存储在底层的rocksdb中，所以在我反序列化出来的时候，启动了多个发号器，导致每个field-colunm都实例化了一个新的对象，而非同一个对象，引发了发号器错乱，从而导致数据丢失的问题(内存被覆盖，错误ack掉了数据)。这个问题排查也是十分不易！！！所以可以说是血的教训。 并且也为公司推广 “服务器日志查询方案” 中，确定了以GPL（grafna+protomal+loki）的日志查询架构，统一了日志查询的入口，大大提高了日志查询的效率。 在 go服务流量录制和回放方案和实现 中，为了实现流量闭环，特意去调研查看了 滴滴的 《写轮眼》项目，并且知道了通过类似注入的方式，可以在用户层编解码上替换到原来的函数指针，从而实现在用户层替换go底层源码的方式，但是由于性价比问题，在公司中最后并未推广，也没有进行研发。该需求后面被搁置了。 在 配置服务方案 中，这个需求在以往其实已经试过才用consule来做配置服务中心，原因是早期我们想要对服务做一个服务注册和发现，在 php 这种fastcgi的方式来说，consule的主动发现服务，就比很多类似etcd被动发现注册服务好用。但是由于最终因为我们的服务目前来说比较零散，并且需求的任务不是着急，这件事后面也被搁置了，后面今天再次提了一个这样子的类似的需求，实现了通过 go语言写的的工具，类似于k8s的kubectl的工具，进行配置的同步和管理，分别分为了2个模式，一个是C端的工具，一个是S端的同步服务，中间的枢纽，最终选择了以etcd为配置分布式存储服务。我们服务的部署特点，利用jenkins的多阶段自定义编译的jenkins-shared-library实现了灵活编译，根据现有的服务灵活部署，从而达到非嵌入式的配置同步方式。 因为公司成立得比较早，代码仓库一直从未进行变更，所以其中一个需求就是 gitbucket 到 gitlab 的代码仓库迁移，写了一个小工具，从而实现了从 gitbucket到 gitlab一键自动化无缝迁移代码，包括项目组，项目的历史commit，tag，branch等等都自动化完成，大大减轻了项目迁移的负担。 对 jenkins 的shard-library 模块进行优化升级，编写了一个python的支持多凭据认证的脚本，并且支持自定义编译代码。 集成了一套，本地的大数据docker环境，我们都知道大数据环境十分的繁杂，并且还需要多台机器才能处理，这对我们本地开发来说十分的不友好，但是网上又没有那些很好的开箱即用的docker-compose环境，因此整理了一套本地的大数据docker环境(非CDH版本) 优化升级部分 mproxy 的代码，从而让测试人员更友好的在该项目中完成自动化测试的脚本功能。 推动flink的落地，由于我们早期的流计算，是单机的，并且存在严重的外部依赖属性，所以，我们推动了flink的落地，探究了几种开发方式，分别是用纯java 写的datastream-api方式，这个方式有一个好处就是，所有的上层的api都是基于datastream的，一些上层的api无法满足我们的需求，我们通过datastream可以很轻松的实现各类需求。在这个过程中，我踩了不少的坑，主要是来自于watermark和window的概念，咋一看其实都是一些比较好理解的概念，但是其实在配合大数据专用的kafka消息队列中间件，一切就变得复杂起来，由于kafka的partition只能有一个client去消费的原因，加上flink自身的概念并行度，这一切结合在一起，就会出现一些让你疑惑的点。经过了大量反复的摸索和钻研，最终才掌握了在多partition下flink的watermark和window-trigger机制方式，但是由于该方式不够直观，也不管灵活，所以我们最终推广了sql-api。好家伙，你以为这就完了吗，并没有，由于flink自身带有sql-client的原因，我一开始尝试了用sql-client来编写流计算的模型，但是发现这个工具有太多问题了，不同的版本有不用的调用方式，不同版本下，对于SET支持的粒度也是不一致的，这让我很头疼。所以最终选择了，基于flink编写了一个自研的flink-sql-client，通过flink run flink-sql-client.jar的方式，我们就可以轻松的提交sql任务或者做其他的需求（例如查看hive的cataglog等等）。然而到了这里还没有结束，由于sql的部分，我们没办法控制，是由flink-core自身标准化了流程，所以有一些bug我们没办法解决，例如在 flink-1.12.0 种，就会因为watermark在idle的情况下，无法推进watermark，从而导致窗口在少量数据情况下，根本不能及时的统计和计算（这和号称实时分析的流计算违背），所以我们只能想了一些版本做了一些迂回的操作。从而最终解决类似这种由于底层bug所带来的问题。 自己的学习上 对golang的protobuf服务有一些的了解，并且学会了protobuf的插件开发。了解了protobuf的协议。 对rust上的生态更为清晰了，利用了其中的一些actor模型，async-io等分别实现了一些基础的工具。 对flutter也有了一定的了解，利用dart编写了一个可以用于自定义通信的库。s 对linux 种的一些磁盘io，网络io，已经shell命令的灵活运用更加深刻。 未完待续！！ (悄咪咪的告诉大家，我买房了。嘿嘿)","categories":[{"name":"年终总结","slug":"年终总结","permalink":"http://blog.crazylaw.cn/categories/%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"年终总结","slug":"年终总结","permalink":"http://blog.crazylaw.cn/tags/%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93/"}]},{"title":"【大数据】- flink开发环境搭建","slug":"大数据/flink开发环境搭建","date":"2021-09-28T01:56:40.000Z","updated":"2021-09-28T09:10:18.184Z","comments":true,"path":"2021/09/28/大数据/flink开发环境搭建/","link":"","permalink":"http://blog.crazylaw.cn/2021/09/28/%E5%A4%A7%E6%95%B0%E6%8D%AE/flink%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/","excerpt":"前言由于最近要推动flink的流计算代替我们的内部的自行研发的mstream流计算服务，所以需要对flink进行开发。","text":"前言由于最近要推动flink的流计算代替我们的内部的自行研发的mstream流计算服务，所以需要对flink进行开发。 环境搭建我们的flink的版本上1.12.0 理论上只是jdk8和jdk11.所以我们需要安装jdk8和jdk8 因为我的是macOS，所以这里我说一下mac安装的过程。 首先，我们需要安装jdk。 123brew tap adoptopenjdk/openjdkbrew install adoptopenjdk8brew install adoptopenjdk11 置于要用jdk8还是jdk11，自行抉择，我这里2个都安装了。 但是这个时候你可能会找不到安装路径 12345➜ ~ /usr/libexec/java_home -VMatching Java Virtual Machines (3): 11.0.12 (x86_64) \"Oracle Corporation\" - \"Java SE 11.0.12\" /Library/Java/JavaVirtualMachines/jdk-11.0.12.jdk/Contents/Home 11.0.11 (x86_64) \"AdoptOpenJDK\" - \"AdoptOpenJDK 11\" /Library/Java/JavaVirtualMachines/adoptopenjdk-11.jdk/Contents/Home 1.8.0_292 (x86_64) \"AdoptOpenJDK\" - \"AdoptOpenJDK 8\" /Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home 通过/usr/libexec/java_home -V内置的一个程序，就可以找到所有相关的java_home，这里我们就可以看到对应的java_home，然后找到对应的java解析器。 IDE初始化项目我这里用的是IDEA，所以我这里列一下我的操作步骤。 New Project先把Maven包的路径确定下来。后面利用docker-maven工具的时候，指定挂载仓库有用。 开始创建项目 选择使用Maven来创建项目，并且选择刚才安装好的JDK8或者JDK11。 默认情况下，这是不带archetype的，这个是Maven模板的类型。我们需要勾选这个archetype， 接下来添加flink-quickstart-java的archetype。 GroupId = org.apache.flink AryofactId = flink-quickstart-java Version = 1.12.0 利用模版创建项目 可以根据自行的需要，填写项目的路径以及对应的GroupId, AryofactId, Version 然后就是Maven的相关配置，这里使用的默认的就行，直接点击Finish完成项目初始化，然后项目会自动根据Maven的配置加载对应的Jar包。 等待一切初始化完毕后，会看到如下图的模板 其中包含了2个Job，分别是BatchJob和StreamingJob。 BatchJob 代表批处理任务 StreamingJob 代表流处理任务 编写批处理代码并测试执行123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354/* * Licensed to the Apache Software Foundation (ASF) under one * or more contributor license agreements. See the NOTICE file * distributed with this work for additional information * regarding copyright ownership. The ASF licenses this file * to you under the Apache License, Version 2.0 (the * \"License\"); you may not use this file except in compliance * with the License. You may obtain a copy of the License at * * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an \"AS IS\" BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. */package my.flink;import org.apache.flink.api.java.ExecutionEnvironment;import org.apache.flink.api.java.operators.DataSource;import org.apache.flink.util.Collector;import java.util.Arrays;/** * Skeleton for a Flink Batch Job. * * &lt;p&gt;For a tutorial how to write a Flink batch application, check the * tutorials and examples on the &lt;a href=\"https://flink.apache.org/docs/stable/\"&gt;Flink Website&lt;/a&gt;. * * &lt;p&gt;To package your application into a JAR file for execution, * change the main class in the POM.xml file to this class (simply search for 'mainClass') * and run 'mvn clean package' on the command line. */public class BatchJob &#123; public static void main(String[] args) throws Exception &#123; // set up the batch execution environment final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); DataSource&lt;String&gt; el = env.fromElements(\"good good study\", \"day day up\"); el.flatMap( (String a, Collector&lt;String&gt; out) -&gt; Arrays.stream(a.split(\" \")).forEach(x -&gt; out.collect(x)) ).returns(String.class).print(); // 由于print()是调试模式，所以不能指定Jobname，print()内部会自动调用Execute() // 所以 env.execute() 将无法调用，需要注释掉，否则会有报错抛出，当然你也可以选择忽略这个异常 // execute program // env.execute(\"Flink Batch Java API Skeleton\"); &#125;&#125; 这里我们把BatchJob加入了具体的任务。我这里的写法是Java8的lamba的写法，使用lamba的写法记得需要在后面加上returns的函数，这是因为使用lamba的情况下，会导致部分信息无法自动推导，需要手动显式指定，从而导致我们需要调用多这个函数。 我们初始化了一个数据源集合，这个集合类型为String类型，我们指定这个集合的元素有2个，分别是good good study, day day up。 然后我们通过flatMap的方法进行一个归并的操作，把每个元素通过一个空格进行切分，切分之后，我们通过Collector的collect()进行收集起来。最终输出在终端。 \b并且这个Job的名字，我们定义为Flink Batch Java API Skeleton。 我们运行这个Job.默认情况下，会遇到如下报错： 1234567Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org&#x2F;apache&#x2F;flink&#x2F;api&#x2F;java&#x2F;ExecutionEnvironment at my.flink.BatchJob.main(BatchJob.java:41)Caused by: java.lang.ClassNotFoundException: org.apache.flink.api.java.ExecutionEnvironment at java.base&#x2F;jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581) at java.base&#x2F;jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178) at java.base&#x2F;java.lang.ClassLoader.loadClass(ClassLoader.java:522) ... 1 more 你可能会觉得很奇怪，明明IDEA已经把Flink的包加载进来，也能正常跳转，为什么在运行的时候却出现了这个，这是因为这是编译的行为，和IDEA加载包没有直接的关系。 打开你的pom.xml，找到dependencies下的&lt;groupId&gt;org.apache.flink&lt;/groupId&gt;的所有依赖包，你会发现每个依赖包下面都有一个&lt;scope /&gt;的定义，里面的value写的是provided，我们只需要把这一整行注释掉就好了。 123456789101112131415161718&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-java&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-java_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-clients_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; 注释后 123456789101112131415161718&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-java&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;!-- &lt;scope&gt;provided&lt;/scope&gt; --&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-java_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;!-- &lt;scope&gt;provided&lt;/scope&gt; --&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-clients_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;!-- &lt;scope&gt;provided&lt;/scope&gt; --&gt;&lt;/dependency&gt; 这样子，就等同于定义依赖包，使用默认的scope范围。 我们这里需要了解一下scope的一些细节。 对于scope=compile的情况（默认scope),也就是说这个项目在编译，测试，运行阶段``都需要这个artifact(模块)对应的jar包在classpath中。 而对于scope=provided的情况，则可以认为这个provided是目标容器已经provide这个artifact。换句话说，它只影响到编译，测试阶段。在编译测试阶段，我们需要这个artifact对应的jar包在classpath中，而在运行阶段，假定目标的容器（比如我们这里的liferay容器）已经提供了这个jar包，所以无需我们这个artifact对应的jar包了。 maven中三种classpath编译，测试，运行 compile：默认范围，编译测试运行都有效 provided：在编译和测试时有效 runtime：在测试和运行时有效 test：只在测试时有效 system：在编译和测试时有效，与本机系统关联，可移植性差 所以我们需要改变的就是这个scope的范围，让某情况下可以运行。例如，我们需要在本机上运行，那么我们就可以注释掉，然后就会使用默认的compile。 但是需要注意的是，当你改动了这个pom.xml之后，idea我不知道是不是bug，反正我的当前版本不会自动刷新，怎么理解这句话？ 通过File -&gt; Project Structure打开页面（因为我是mac），所以可以通过快捷键Command + [:;]打开。界面如下 我们可以看到，Flink相关的依赖包其实已经存在了，这里显示了我们的Maven包下的scope是Provided，那就代表IDEA并未自动识别我刚才的注释。因为如果成功识别出来了，应该是会变成Compile。当然我可以直接在这里进行修改，但是为了统一维护的问题，不建议在此处修改，虽然直接修改很方便，但是下次加载还是从pom.xml加载的，并且间接依赖包也特别多，你无法掌握那么多依赖包的关系。 所以注释后，我们需要利用pom.xml进行maven的reload project。 这个时候，我们就发现不管是直接还是间接的依赖包都变成了Compile。 接下来，我们在运行一次我们的任务。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697989910010110210310410510610710810911011111211311411511611711811912012112212312412512612712812913013113213313413513613713813914014114214314414514614714814915015115215315415515615715815916016116216316416516615:33:55,353 INFO org.apache.flink.api.java.utils.PlanGenerator [] - The job has 0 registered types and 0 default Kryo serializers15:33:55,523 INFO org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils [] - The configuration option taskmanager.cpu.cores required for local execution is not set, setting it to the maximal possible value.15:33:55,523 INFO org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils [] - The configuration option taskmanager.memory.task.heap.size required for local execution is not set, setting it to the maximal possible value.15:33:55,523 INFO org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils [] - The configuration option taskmanager.memory.task.off-heap.size required for local execution is not set, setting it to the maximal possible value.15:33:55,523 INFO org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils [] - The configuration option taskmanager.memory.network.min required for local execution is not set, setting it to its default value 64 mb.15:33:55,524 INFO org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils [] - The configuration option taskmanager.memory.network.max required for local execution is not set, setting it to its default value 64 mb.15:33:55,524 INFO org.apache.flink.runtime.taskexecutor.TaskExecutorResourceUtils [] - The configuration option taskmanager.memory.managed.size required for local execution is not set, setting it to its default value 128 mb.15:33:55,548 INFO org.apache.flink.runtime.minicluster.MiniCluster [] - Starting Flink Mini Cluster15:33:55,551 INFO org.apache.flink.runtime.minicluster.MiniCluster [] - Starting Metrics Registry15:33:55,627 INFO org.apache.flink.runtime.metrics.MetricRegistryImpl [] - No metrics reporter configured, no metrics will be exposed&#x2F;reported.15:33:55,627 INFO org.apache.flink.runtime.minicluster.MiniCluster [] - Starting RPC Service(s)15:33:55,780 INFO org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils [] - Trying to start local actor system15:33:56,203 INFO akka.event.slf4j.Slf4jLogger [] - Slf4jLogger started15:33:56,313 INFO org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils [] - Actor system started at akka:&#x2F;&#x2F;flink15:33:56,328 INFO org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils [] - Trying to start local actor system15:33:56,341 INFO akka.event.slf4j.Slf4jLogger [] - Slf4jLogger started15:33:56,356 INFO org.apache.flink.runtime.rpc.akka.AkkaRpcServiceUtils [] - Actor system started at akka:&#x2F;&#x2F;flink-metrics15:33:56,373 INFO org.apache.flink.runtime.rpc.akka.AkkaRpcService [] - Starting RPC endpoint for org.apache.flink.runtime.metrics.dump.MetricQueryService at akka:&#x2F;&#x2F;flink-metrics&#x2F;user&#x2F;rpc&#x2F;MetricQueryService .15:33:56,399 INFO org.apache.flink.runtime.minicluster.MiniCluster [] - Starting high-availability services15:33:56,418 INFO org.apache.flink.runtime.blob.BlobServer [] - Created BLOB server storage directory &#x2F;var&#x2F;folders&#x2F;zq&#x2F;2b48w4_x5vq89_jrz3yns13h0000gn&#x2F;T&#x2F;blobStore-4ec8c72e-36f6-4b8d-aba8-70bb3d443f9315:33:56,430 INFO org.apache.flink.runtime.blob.BlobServer [] - Started BLOB server at 0.0.0.0:58212 - max concurrent requests: 50 - max backlog: 100015:33:56,434 INFO org.apache.flink.runtime.blob.PermanentBlobCache [] - Created BLOB cache storage directory &#x2F;var&#x2F;folders&#x2F;zq&#x2F;2b48w4_x5vq89_jrz3yns13h0000gn&#x2F;T&#x2F;blobStore-3433044e-b47e-445c-9df2-ceb5d1e8da6f15:33:56,436 INFO org.apache.flink.runtime.blob.TransientBlobCache [] - Created BLOB cache storage directory &#x2F;var&#x2F;folders&#x2F;zq&#x2F;2b48w4_x5vq89_jrz3yns13h0000gn&#x2F;T&#x2F;blobStore-4d06675d-1b13-4fa2-87e9-0a1609934f0915:33:56,436 INFO org.apache.flink.runtime.minicluster.MiniCluster [] - Starting 1 TaskManger(s)15:33:56,441 INFO org.apache.flink.runtime.taskexecutor.TaskManagerRunner [] - Starting TaskManager with ResourceID: a7c681c7-48a2-4491-803a-535036a51fcb15:33:56,477 INFO org.apache.flink.runtime.taskexecutor.TaskManagerServices [] - Temporary file directory &#39;&#x2F;var&#x2F;folders&#x2F;zq&#x2F;2b48w4_x5vq89_jrz3yns13h0000gn&#x2F;T&#39;: total 233 GB, usable 25 GB (10.73% usable)15:33:56,482 INFO org.apache.flink.runtime.io.disk.FileChannelManagerImpl [] - FileChannelManager uses directory &#x2F;var&#x2F;folders&#x2F;zq&#x2F;2b48w4_x5vq89_jrz3yns13h0000gn&#x2F;T&#x2F;flink-io-dc01cff1-6b52-43ed-9d16-9085f49c732e for spill files.15:33:56,492 INFO org.apache.flink.runtime.io.disk.FileChannelManagerImpl [] - FileChannelManager uses directory &#x2F;var&#x2F;folders&#x2F;zq&#x2F;2b48w4_x5vq89_jrz3yns13h0000gn&#x2F;T&#x2F;flink-netty-shuffle-41571afb-b13e-494b-b937-0696d2c77ca1 for spill files.15:33:56,580 INFO org.apache.flink.runtime.io.network.buffer.NetworkBufferPool [] - Allocated 64 MB for network buffer pool (number of memory segments: 2048, bytes per segment: 32768).15:33:56,594 INFO org.apache.flink.runtime.io.network.NettyShuffleEnvironment [] - Starting the network environment and its components.15:33:56,596 INFO org.apache.flink.runtime.taskexecutor.KvStateService [] - Starting the kvState service and its components.15:33:56,631 INFO org.apache.flink.runtime.rpc.akka.AkkaRpcService [] - Starting RPC endpoint for org.apache.flink.runtime.taskexecutor.TaskExecutor at akka:&#x2F;&#x2F;flink&#x2F;user&#x2F;rpc&#x2F;taskmanager_0 .15:33:56,650 INFO org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Start job leader service.15:33:56,653 INFO org.apache.flink.runtime.filecache.FileCache [] - User file cache uses directory &#x2F;var&#x2F;folders&#x2F;zq&#x2F;2b48w4_x5vq89_jrz3yns13h0000gn&#x2F;T&#x2F;flink-dist-cache-fc4fd5a1-79fa-4a19-8d7d-f3072006c91e15:33:56,714 INFO org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint [] - Starting rest endpoint.15:33:56,717 INFO org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint [] - Failed to load web based job submission extension. Probable reason: flink-runtime-web is not in the classpath.15:33:57,089 WARN org.apache.flink.runtime.webmonitor.WebMonitorUtils [] - Log file environment variable &#39;log.file&#39; is not set.15:33:57,089 WARN org.apache.flink.runtime.webmonitor.WebMonitorUtils [] - JobManager log files are unavailable in the web dashboard. Log file location not found in environment variable &#39;log.file&#39; or configuration key &#39;web.log.path&#39;.15:33:57,300 INFO org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint [] - Rest endpoint listening at localhost:5822315:33:57,302 INFO org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService [] - Proposing leadership to contender http:&#x2F;&#x2F;localhost:5822315:33:57,305 INFO org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint [] - http:&#x2F;&#x2F;localhost:58223 was granted leadership with leaderSessionID&#x3D;22a043f5-f263-4e6c-87e9-6e61beef307515:33:57,306 INFO org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService [] - Received confirmation of leadership for leader http:&#x2F;&#x2F;localhost:58223 , session&#x3D;22a043f5-f263-4e6c-87e9-6e61beef307515:33:57,327 INFO org.apache.flink.runtime.rpc.akka.AkkaRpcService [] - Starting RPC endpoint for org.apache.flink.runtime.resourcemanager.StandaloneResourceManager at akka:&#x2F;&#x2F;flink&#x2F;user&#x2F;rpc&#x2F;resourcemanager_1 .15:33:57,344 INFO org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService [] - Proposing leadership to contender LeaderContender: DefaultDispatcherRunner15:33:57,345 INFO org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService [] - Proposing leadership to contender LeaderContender: StandaloneResourceManager15:33:57,347 INFO org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - ResourceManager akka:&#x2F;&#x2F;flink&#x2F;user&#x2F;rpc&#x2F;resourcemanager_1 was granted leadership with fencing token 99793e5c3d8a81ced62f8a03bd21494c15:33:57,350 INFO org.apache.flink.runtime.minicluster.MiniCluster [] - Flink Mini Cluster started successfully15:33:57,350 INFO org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerImpl [] - Starting the SlotManager.15:33:57,351 INFO org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Start SessionDispatcherLeaderProcess.15:33:57,353 INFO org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Recover all persisted job graphs.15:33:57,354 INFO org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Successfully recovered 0 persisted job graphs.15:33:57,355 INFO org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService [] - Received confirmation of leadership for leader akka:&#x2F;&#x2F;flink&#x2F;user&#x2F;rpc&#x2F;resourcemanager_1 , session&#x3D;d62f8a03-bd21-494c-9979-3e5c3d8a81ce15:33:57,357 INFO org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Connecting to ResourceManager akka:&#x2F;&#x2F;flink&#x2F;user&#x2F;rpc&#x2F;resourcemanager_1(99793e5c3d8a81ced62f8a03bd21494c).15:33:57,365 INFO org.apache.flink.runtime.rpc.akka.AkkaRpcService [] - Starting RPC endpoint for org.apache.flink.runtime.dispatcher.StandaloneDispatcher at akka:&#x2F;&#x2F;flink&#x2F;user&#x2F;rpc&#x2F;dispatcher_2 .15:33:57,378 INFO org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService [] - Received confirmation of leadership for leader akka:&#x2F;&#x2F;flink&#x2F;user&#x2F;rpc&#x2F;dispatcher_2 , session&#x3D;0a8eb324-f6f9-44d7-a452-87c855415b0e15:33:57,387 INFO org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Resolved ResourceManager address, beginning registration15:33:57,393 INFO org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registering TaskManager with ResourceID a7c681c7-48a2-4491-803a-535036a51fcb (akka:&#x2F;&#x2F;flink&#x2F;user&#x2F;rpc&#x2F;taskmanager_0) at ResourceManager15:33:57,395 INFO org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Successful registration at resource manager akka:&#x2F;&#x2F;flink&#x2F;user&#x2F;rpc&#x2F;resourcemanager_1 under registration id 3e9b649958365e1a080d0b1102807505.15:33:57,396 INFO org.apache.flink.runtime.dispatcher.StandaloneDispatcher [] - Received JobGraph submission c9c27c95a1e3b4a8bfd7250101fa1126 (Flink Java Job at Tue Sep 28 15:33:55 CST 2021).15:33:57,396 INFO org.apache.flink.runtime.dispatcher.StandaloneDispatcher [] - Submitting job c9c27c95a1e3b4a8bfd7250101fa1126 (Flink Java Job at Tue Sep 28 15:33:55 CST 2021).15:33:57,423 INFO org.apache.flink.runtime.rpc.akka.AkkaRpcService [] - Starting RPC endpoint for org.apache.flink.runtime.jobmaster.JobMaster at akka:&#x2F;&#x2F;flink&#x2F;user&#x2F;rpc&#x2F;jobmanager_3 .15:33:57,433 INFO org.apache.flink.runtime.jobmaster.JobMaster [] - Initializing job Flink Java Job at Tue Sep 28 15:33:55 CST 2021 (c9c27c95a1e3b4a8bfd7250101fa1126).15:33:57,452 INFO org.apache.flink.runtime.jobmaster.JobMaster [] - Using restart back off time strategy NoRestartBackoffTimeStrategy for Flink Java Job at Tue Sep 28 15:33:55 CST 2021 (c9c27c95a1e3b4a8bfd7250101fa1126).15:33:57,487 INFO org.apache.flink.runtime.jobmaster.JobMaster [] - Running initialization on master for job Flink Java Job at Tue Sep 28 15:33:55 CST 2021 (c9c27c95a1e3b4a8bfd7250101fa1126).15:33:57,490 INFO org.apache.flink.runtime.jobmaster.JobMaster [] - Successfully ran initialization on master in 3 ms.15:33:57,512 INFO org.apache.flink.runtime.scheduler.adapter.DefaultExecutionTopology [] - Built 1 pipelined regions in 3 ms15:33:57,518 INFO org.apache.flink.runtime.jobmaster.JobMaster [] - Using failover strategy org.apache.flink.runtime.executiongraph.failover.flip1.RestartPipelinedRegionFailoverStrategy@4fe83a40 for Flink Java Job at Tue Sep 28 15:33:55 CST 2021 (c9c27c95a1e3b4a8bfd7250101fa1126).15:33:57,527 INFO org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService [] - Proposing leadership to contender akka:&#x2F;&#x2F;flink&#x2F;user&#x2F;rpc&#x2F;jobmanager_315:33:57,528 INFO org.apache.flink.runtime.jobmaster.JobManagerRunnerImpl [] - JobManager runner for job Flink Java Job at Tue Sep 28 15:33:55 CST 2021 (c9c27c95a1e3b4a8bfd7250101fa1126) was granted leadership with session id 00c173d1-6a96-47ad-a2d9-da1ebc4d6a41 at akka:&#x2F;&#x2F;flink&#x2F;user&#x2F;rpc&#x2F;jobmanager_3.15:33:57,532 INFO org.apache.flink.runtime.jobmaster.JobMaster [] - Starting execution of job Flink Java Job at Tue Sep 28 15:33:55 CST 2021 (c9c27c95a1e3b4a8bfd7250101fa1126) under job master id a2d9da1ebc4d6a4100c173d16a9647ad.15:33:57,533 INFO org.apache.flink.runtime.jobmaster.JobMaster [] - Starting scheduling with scheduling strategy [org.apache.flink.runtime.scheduler.strategy.PipelinedRegionSchedulingStrategy]15:33:57,533 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Job Flink Java Job at Tue Sep 28 15:33:55 CST 2021 (c9c27c95a1e3b4a8bfd7250101fa1126) switched from state CREATED to RUNNING.15:33:57,537 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - CHAIN DataSource (at main(BatchJob.java:43) (org.apache.flink.api.java.io.CollectionInputFormat)) -&gt; FlatMap (FlatMap at main(BatchJob.java:45)) (1&#x2F;1) (2d0c18f32aaefecbd6f3d76a781d54b9) switched from CREATED to SCHEDULED.15:33:57,537 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - DataSink (collect()) (1&#x2F;1) (1feb48784b233306f550eda82cf1b5e9) switched from CREATED to SCHEDULED.15:33:57,546 INFO org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl [] - Cannot serve slot request, no ResourceManager connected. Adding as pending request [SlotRequestId&#123;1e28fd68790f78b4b48f557e8ba4d92f&#125;]15:33:57,552 INFO org.apache.flink.runtime.highavailability.nonha.embedded.EmbeddedLeaderService [] - Received confirmation of leadership for leader akka:&#x2F;&#x2F;flink&#x2F;user&#x2F;rpc&#x2F;jobmanager_3 , session&#x3D;00c173d1-6a96-47ad-a2d9-da1ebc4d6a4115:33:57,552 INFO org.apache.flink.runtime.jobmaster.JobMaster [] - Connecting to ResourceManager akka:&#x2F;&#x2F;flink&#x2F;user&#x2F;rpc&#x2F;resourcemanager_1(99793e5c3d8a81ced62f8a03bd21494c)15:33:57,554 INFO org.apache.flink.runtime.jobmaster.JobMaster [] - Resolved ResourceManager address, beginning registration15:33:57,555 INFO org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registering job manager a2d9da1ebc4d6a4100c173d16a9647ad@akka:&#x2F;&#x2F;flink&#x2F;user&#x2F;rpc&#x2F;jobmanager_3 for job c9c27c95a1e3b4a8bfd7250101fa1126.15:33:57,559 INFO org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Registered job manager a2d9da1ebc4d6a4100c173d16a9647ad@akka:&#x2F;&#x2F;flink&#x2F;user&#x2F;rpc&#x2F;jobmanager_3 for job c9c27c95a1e3b4a8bfd7250101fa1126.15:33:57,561 INFO org.apache.flink.runtime.jobmaster.JobMaster [] - JobManager successfully registered at ResourceManager, leader id: 99793e5c3d8a81ced62f8a03bd21494c.15:33:57,562 INFO org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl [] - Requesting new slot [SlotRequestId&#123;1e28fd68790f78b4b48f557e8ba4d92f&#125;] and profile ResourceProfile&#123;UNKNOWN&#125; with allocation id d73fe42189235dfaf22a937eb4556ee1 from resource manager.15:33:57,562 INFO org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Request slot with profile ResourceProfile&#123;UNKNOWN&#125; for job c9c27c95a1e3b4a8bfd7250101fa1126 with allocation id d73fe42189235dfaf22a937eb4556ee1.15:33:57,565 INFO org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Receive slot request d73fe42189235dfaf22a937eb4556ee1 for job c9c27c95a1e3b4a8bfd7250101fa1126 from resource manager with leader id 99793e5c3d8a81ced62f8a03bd21494c.15:33:57,570 INFO org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Allocated slot for d73fe42189235dfaf22a937eb4556ee1.15:33:57,571 INFO org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Add job c9c27c95a1e3b4a8bfd7250101fa1126 for job leader monitoring.15:33:57,573 INFO org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Try to register at job manager akka:&#x2F;&#x2F;flink&#x2F;user&#x2F;rpc&#x2F;jobmanager_3 with leader id 00c173d1-6a96-47ad-a2d9-da1ebc4d6a41.15:33:57,574 INFO org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Resolved JobManager address, beginning registration15:33:57,577 INFO org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Successful registration at job manager akka:&#x2F;&#x2F;flink&#x2F;user&#x2F;rpc&#x2F;jobmanager_3 for job c9c27c95a1e3b4a8bfd7250101fa1126.15:33:57,578 INFO org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Establish JobManager connection for job c9c27c95a1e3b4a8bfd7250101fa1126.15:33:57,580 INFO org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Offer reserved slots to the leader of job c9c27c95a1e3b4a8bfd7250101fa1126.15:33:57,588 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - CHAIN DataSource (at main(BatchJob.java:43) (org.apache.flink.api.java.io.CollectionInputFormat)) -&gt; FlatMap (FlatMap at main(BatchJob.java:45)) (1&#x2F;1) (2d0c18f32aaefecbd6f3d76a781d54b9) switched from SCHEDULED to DEPLOYING.15:33:57,590 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Deploying CHAIN DataSource (at main(BatchJob.java:43) (org.apache.flink.api.java.io.CollectionInputFormat)) -&gt; FlatMap (FlatMap at main(BatchJob.java:45)) (1&#x2F;1) (attempt #0) with attempt id 2d0c18f32aaefecbd6f3d76a781d54b9 to a7c681c7-48a2-4491-803a-535036a51fcb @ localhost (dataPort&#x3D;-1) with allocation id d73fe42189235dfaf22a937eb4556ee115:33:57,595 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - DataSink (collect()) (1&#x2F;1) (1feb48784b233306f550eda82cf1b5e9) switched from SCHEDULED to DEPLOYING.15:33:57,595 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Deploying DataSink (collect()) (1&#x2F;1) (attempt #0) with attempt id 1feb48784b233306f550eda82cf1b5e9 to a7c681c7-48a2-4491-803a-535036a51fcb @ localhost (dataPort&#x3D;-1) with allocation id d73fe42189235dfaf22a937eb4556ee115:33:57,595 INFO org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot d73fe42189235dfaf22a937eb4556ee1.15:33:57,627 INFO org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Received task CHAIN DataSource (at main(BatchJob.java:43) (org.apache.flink.api.java.io.CollectionInputFormat)) -&gt; FlatMap (FlatMap at main(BatchJob.java:45)) (1&#x2F;1)#0 (2d0c18f32aaefecbd6f3d76a781d54b9), deploy into slot with allocation id d73fe42189235dfaf22a937eb4556ee1.15:33:57,628 INFO org.apache.flink.runtime.taskmanager.Task [] - CHAIN DataSource (at main(BatchJob.java:43) (org.apache.flink.api.java.io.CollectionInputFormat)) -&gt; FlatMap (FlatMap at main(BatchJob.java:45)) (1&#x2F;1)#0 (2d0c18f32aaefecbd6f3d76a781d54b9) switched from CREATED to DEPLOYING.15:33:57,630 INFO org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot d73fe42189235dfaf22a937eb4556ee1.15:33:57,630 INFO org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Activate slot d73fe42189235dfaf22a937eb4556ee1.15:33:57,633 INFO org.apache.flink.runtime.taskmanager.Task [] - Loading JAR files for task CHAIN DataSource (at main(BatchJob.java:43) (org.apache.flink.api.java.io.CollectionInputFormat)) -&gt; FlatMap (FlatMap at main(BatchJob.java:45)) (1&#x2F;1)#0 (2d0c18f32aaefecbd6f3d76a781d54b9) [DEPLOYING].15:33:57,634 INFO org.apache.flink.runtime.taskmanager.Task [] - Registering task at network: CHAIN DataSource (at main(BatchJob.java:43) (org.apache.flink.api.java.io.CollectionInputFormat)) -&gt; FlatMap (FlatMap at main(BatchJob.java:45)) (1&#x2F;1)#0 (2d0c18f32aaefecbd6f3d76a781d54b9) [DEPLOYING].15:33:57,642 INFO org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Received task DataSink (collect()) (1&#x2F;1)#0 (1feb48784b233306f550eda82cf1b5e9), deploy into slot with allocation id d73fe42189235dfaf22a937eb4556ee1.15:33:57,642 INFO org.apache.flink.runtime.taskmanager.Task [] - DataSink (collect()) (1&#x2F;1)#0 (1feb48784b233306f550eda82cf1b5e9) switched from CREATED to DEPLOYING.15:33:57,643 INFO org.apache.flink.runtime.taskmanager.Task [] - Loading JAR files for task DataSink (collect()) (1&#x2F;1)#0 (1feb48784b233306f550eda82cf1b5e9) [DEPLOYING].15:33:57,644 INFO org.apache.flink.runtime.taskmanager.Task [] - Registering task at network: DataSink (collect()) (1&#x2F;1)#0 (1feb48784b233306f550eda82cf1b5e9) [DEPLOYING].15:33:57,647 INFO org.apache.flink.runtime.taskmanager.Task [] - CHAIN DataSource (at main(BatchJob.java:43) (org.apache.flink.api.java.io.CollectionInputFormat)) -&gt; FlatMap (FlatMap at main(BatchJob.java:45)) (1&#x2F;1)#0 (2d0c18f32aaefecbd6f3d76a781d54b9) switched from DEPLOYING to RUNNING.15:33:57,648 INFO org.apache.flink.runtime.taskmanager.Task [] - DataSink (collect()) (1&#x2F;1)#0 (1feb48784b233306f550eda82cf1b5e9) switched from DEPLOYING to RUNNING.15:33:57,648 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - CHAIN DataSource (at main(BatchJob.java:43) (org.apache.flink.api.java.io.CollectionInputFormat)) -&gt; FlatMap (FlatMap at main(BatchJob.java:45)) (1&#x2F;1) (2d0c18f32aaefecbd6f3d76a781d54b9) switched from DEPLOYING to RUNNING.15:33:57,649 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - DataSink (collect()) (1&#x2F;1) (1feb48784b233306f550eda82cf1b5e9) switched from DEPLOYING to RUNNING.15:33:57,659 WARN org.apache.flink.metrics.MetricGroup [] - The operator name DataSource (at main(BatchJob.java:43) (org.apache.flink.api.java.io.CollectionInputFormat)) exceeded the 80 characters length limit and was truncated.15:33:57,667 INFO org.apache.flink.runtime.taskmanager.Task [] - CHAIN DataSource (at main(BatchJob.java:43) (org.apache.flink.api.java.io.CollectionInputFormat)) -&gt; FlatMap (FlatMap at main(BatchJob.java:45)) (1&#x2F;1)#0 (2d0c18f32aaefecbd6f3d76a781d54b9) switched from RUNNING to FINISHED.15:33:57,667 INFO org.apache.flink.runtime.taskmanager.Task [] - Freeing task resources for CHAIN DataSource (at main(BatchJob.java:43) (org.apache.flink.api.java.io.CollectionInputFormat)) -&gt; FlatMap (FlatMap at main(BatchJob.java:45)) (1&#x2F;1)#0 (2d0c18f32aaefecbd6f3d76a781d54b9).15:33:57,670 INFO org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Un-registering task and sending final execution state FINISHED to JobManager for task CHAIN DataSource (at main(BatchJob.java:43) (org.apache.flink.api.java.io.CollectionInputFormat)) -&gt; FlatMap (FlatMap at main(BatchJob.java:45)) (1&#x2F;1)#0 2d0c18f32aaefecbd6f3d76a781d54b9.15:33:57,677 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - CHAIN DataSource (at main(BatchJob.java:43) (org.apache.flink.api.java.io.CollectionInputFormat)) -&gt; FlatMap (FlatMap at main(BatchJob.java:45)) (1&#x2F;1) (2d0c18f32aaefecbd6f3d76a781d54b9) switched from RUNNING to FINISHED.15:33:57,678 INFO org.apache.flink.runtime.taskmanager.Task [] - DataSink (collect()) (1&#x2F;1)#0 (1feb48784b233306f550eda82cf1b5e9) switched from RUNNING to FINISHED.15:33:57,678 INFO org.apache.flink.runtime.taskmanager.Task [] - Freeing task resources for DataSink (collect()) (1&#x2F;1)#0 (1feb48784b233306f550eda82cf1b5e9).15:33:57,679 INFO org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Un-registering task and sending final execution state FINISHED to JobManager for task DataSink (collect()) (1&#x2F;1)#0 1feb48784b233306f550eda82cf1b5e9.15:33:57,682 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - DataSink (collect()) (1&#x2F;1) (1feb48784b233306f550eda82cf1b5e9) switched from RUNNING to FINISHED.15:33:57,685 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Job Flink Java Job at Tue Sep 28 15:33:55 CST 2021 (c9c27c95a1e3b4a8bfd7250101fa1126) switched from state RUNNING to FINISHED.15:33:57,691 INFO org.apache.flink.runtime.dispatcher.StandaloneDispatcher [] - Job c9c27c95a1e3b4a8bfd7250101fa1126 reached globally terminal state FINISHED.15:33:57,691 INFO org.apache.flink.runtime.minicluster.MiniCluster [] - Shutting down Flink Mini Cluster15:33:57,691 INFO org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint [] - Shutting down rest endpoint.15:33:57,691 INFO org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Stopping TaskExecutor akka:&#x2F;&#x2F;flink&#x2F;user&#x2F;rpc&#x2F;taskmanager_0.15:33:57,692 INFO org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Close ResourceManager connection 01714233597d70de71bbfbda09ac665e.15:33:57,692 INFO org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Closing TaskExecutor connection a7c681c7-48a2-4491-803a-535036a51fcb because: The TaskExecutor is shutting down.15:33:57,693 INFO org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Close JobManager connection for job c9c27c95a1e3b4a8bfd7250101fa1126.15:33:57,694 INFO org.apache.flink.runtime.jobmaster.JobMaster [] - Stopping the JobMaster for job Flink Java Job at Tue Sep 28 15:33:55 CST 2021(c9c27c95a1e3b4a8bfd7250101fa1126).15:33:57,695 INFO org.apache.flink.runtime.taskexecutor.slot.TaskSlotTableImpl [] - Free slot TaskSlot(index:0, state:ALLOCATED, resource profile: ResourceProfile&#123;managedMemory&#x3D;128.000mb (134217728 bytes), networkMemory&#x3D;64.000mb (67108864 bytes)&#125;, allocationId: d73fe42189235dfaf22a937eb4556ee1, jobId: c9c27c95a1e3b4a8bfd7250101fa1126).15:33:57,697 INFO org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl [] - Suspending SlotPool.15:33:57,697 INFO org.apache.flink.runtime.jobmaster.JobMaster [] - Close ResourceManager connection 01714233597d70de71bbfbda09ac665e: Stopping JobMaster for job Flink Java Job at Tue Sep 28 15:33:55 CST 2021(c9c27c95a1e3b4a8bfd7250101fa1126)..15:33:57,697 INFO org.apache.flink.runtime.jobmaster.slotpool.SlotPoolImpl [] - Stopping SlotPool.15:33:57,697 INFO org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Disconnect job manager a2d9da1ebc4d6a4100c173d16a9647ad@akka:&#x2F;&#x2F;flink&#x2F;user&#x2F;rpc&#x2F;jobmanager_3 for job c9c27c95a1e3b4a8bfd7250101fa1126 from the resource manager.15:33:57,699 INFO org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Stop job leader service.15:33:57,699 INFO org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager [] - Shutting down TaskExecutorLocalStateStoresManager.goodgoodstudydaydayup15:33:57,725 INFO org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint [] - Removing cache directory &#x2F;var&#x2F;folders&#x2F;zq&#x2F;2b48w4_x5vq89_jrz3yns13h0000gn&#x2F;T&#x2F;flink-web-ui15:33:57,727 INFO org.apache.flink.runtime.dispatcher.DispatcherRestEndpoint [] - Shut down complete.15:33:57,729 INFO org.apache.flink.runtime.resourcemanager.StandaloneResourceManager [] - Shut down cluster because application is in CANCELED, diagnostics DispatcherResourceManagerComponent has been closed..15:33:57,729 INFO org.apache.flink.runtime.io.disk.FileChannelManagerImpl [] - FileChannelManager removed spill file directory &#x2F;var&#x2F;folders&#x2F;zq&#x2F;2b48w4_x5vq89_jrz3yns13h0000gn&#x2F;T&#x2F;flink-io-dc01cff1-6b52-43ed-9d16-9085f49c732e15:33:57,730 INFO org.apache.flink.runtime.io.network.NettyShuffleEnvironment [] - Shutting down the network environment and its components.15:33:57,730 INFO org.apache.flink.runtime.entrypoint.component.DispatcherResourceManagerComponent [] - Closing components.15:33:57,730 INFO org.apache.flink.runtime.dispatcher.runner.SessionDispatcherLeaderProcess [] - Stopping SessionDispatcherLeaderProcess.15:33:57,730 INFO org.apache.flink.runtime.dispatcher.StandaloneDispatcher [] - Stopping dispatcher akka:&#x2F;&#x2F;flink&#x2F;user&#x2F;rpc&#x2F;dispatcher_2.15:33:57,731 INFO org.apache.flink.runtime.dispatcher.StandaloneDispatcher [] - Stopping all currently running jobs of dispatcher akka:&#x2F;&#x2F;flink&#x2F;user&#x2F;rpc&#x2F;dispatcher_2.15:33:57,731 INFO org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerImpl [] - Closing the SlotManager.15:33:57,731 INFO org.apache.flink.runtime.resourcemanager.slotmanager.SlotManagerImpl [] - Suspending the SlotManager.15:33:57,731 INFO org.apache.flink.runtime.rest.handler.legacy.backpressure.BackPressureRequestCoordinator [] - Shutting down back pressure request coordinator.15:33:57,731 INFO org.apache.flink.runtime.io.disk.FileChannelManagerImpl [] - FileChannelManager removed spill file directory &#x2F;var&#x2F;folders&#x2F;zq&#x2F;2b48w4_x5vq89_jrz3yns13h0000gn&#x2F;T&#x2F;flink-netty-shuffle-41571afb-b13e-494b-b937-0696d2c77ca115:33:57,732 INFO org.apache.flink.runtime.taskexecutor.KvStateService [] - Shutting down the kvState service and its components.15:33:57,732 INFO org.apache.flink.runtime.dispatcher.StandaloneDispatcher [] - Stopped dispatcher akka:&#x2F;&#x2F;flink&#x2F;user&#x2F;rpc&#x2F;dispatcher_2.15:33:57,732 INFO org.apache.flink.runtime.taskexecutor.DefaultJobLeaderService [] - Stop job leader service.15:33:57,734 INFO org.apache.flink.runtime.filecache.FileCache [] - removed file cache directory &#x2F;var&#x2F;folders&#x2F;zq&#x2F;2b48w4_x5vq89_jrz3yns13h0000gn&#x2F;T&#x2F;flink-dist-cache-fc4fd5a1-79fa-4a19-8d7d-f3072006c91e15:33:57,735 INFO org.apache.flink.runtime.taskexecutor.TaskExecutor [] - Stopped TaskExecutor akka:&#x2F;&#x2F;flink&#x2F;user&#x2F;rpc&#x2F;taskmanager_0.15:33:57,735 INFO org.apache.flink.runtime.rpc.akka.AkkaRpcService [] - Stopping Akka RPC service.15:33:57,760 INFO org.apache.flink.runtime.rpc.akka.AkkaRpcService [] - Stopping Akka RPC service.15:33:57,760 INFO org.apache.flink.runtime.rpc.akka.AkkaRpcService [] - Stopped Akka RPC service.15:33:57,766 INFO org.apache.flink.runtime.blob.PermanentBlobCache [] - Shutting down BLOB cache15:33:57,768 INFO org.apache.flink.runtime.blob.TransientBlobCache [] - Shutting down BLOB cache15:33:57,772 INFO org.apache.flink.runtime.blob.BlobServer [] - Stopped BLOB server at 0.0.0.0:5821215:33:57,772 INFO org.apache.flink.runtime.rpc.akka.AkkaRpcService [] - Stopped Akka RPC service. 可以看到，我们的代码已经执行，并且生效。这样子我们的开发环境就搭建完毕了。其他的基本大同小异，如需记录，后面会额外的篇章进行记录。 项目打包并提交Flink集群执行直到刚才为止，我们都是本地开发的模式，但是如果要在生产环境运行，那么我们需要打包成jar，然后借助flink-client提交job图对象给flink-job-manager，然后再分发给各个的taskManager进行执行。 所以这里我们需要打包出jar包。 我们使用Maven的mvn clean package命令可以很方便地进行打包。 如果需要额外指定一些内容的话，可以使用mvn clean package -Dfile.encoding=UTF-8 -DskipTests=true，这样子可以忽略测试阶段。 利用docker 构架一个flink1.12的集群 构建一个maven工具，版本3.6.3（由于idea采用的是内置的maven，这是是一个独立的jar包，所以外部无法直接引用mvn命令） 12docker pull flink:1.12-scala_2.11-java8docker pull maven:3.6.3 在代码目录下打包出jar包 记得打包成生成环境的jar包的时候，把&lt;scope /&gt;改回 provided, 也就是把注释取消掉。 1234567891011121314151617181920212223242526272829303132333435363738394041424344➜ my-flink-jdk11 docker run --rm -it -v ~/.m2:/root/.m2 -v $(PWD):/www -w /www maven:3.6.3 mvn clean package[INFO] Scanning for projects...[INFO] [INFO] ----------------------&lt; my-flink:my-flink-jdk11 &gt;-----------------------[INFO] Building Flink Quickstart Job 1.0-SNAPSHOT[INFO] --------------------------------[ jar ]---------------------------------[INFO] [INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ my-flink-jdk11 ---[INFO] Deleting /www/target[INFO] [INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ my-flink-jdk11 ---[INFO] Using 'UTF-8' encoding to copy filtered resources.[INFO] Copying 1 resource[INFO] [INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ my-flink-jdk11 ---[INFO] Changes detected - recompiling the module![INFO] Compiling 2 source files to /www/target/classes[INFO] [INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ my-flink-jdk11 ---[INFO] Using 'UTF-8' encoding to copy filtered resources.[INFO] skip non existing resourceDirectory /www/src/test/resources[INFO] [INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ my-flink-jdk11 ---[INFO] No sources to compile[INFO] [INFO] --- maven-surefire-plugin:2.12.4:test (default-test) @ my-flink-jdk11 ---[INFO] No tests to run.[INFO] [INFO] --- maven-jar-plugin:2.4:jar (default-jar) @ my-flink-jdk11 ---[INFO] Building jar: /www/target/my-flink-jdk11-1.0-SNAPSHOT.jar[INFO] [INFO] --- maven-shade-plugin:3.1.1:shade (default) @ my-flink-jdk11 ---[INFO] Excluding org.slf4j:slf4j-api:jar:1.7.15 from the shaded jar.[INFO] Excluding org.apache.logging.log4j:log4j-slf4j-impl:jar:2.12.1 from the shaded jar.[INFO] Excluding org.apache.logging.log4j:log4j-api:jar:2.12.1 from the shaded jar.[INFO] Excluding org.apache.logging.log4j:log4j-core:jar:2.12.1 from the shaded jar.[INFO] Replacing original artifact with shaded artifact.[INFO] Replacing /www/target/my-flink-jdk11-1.0-SNAPSHOT.jar with /www/target/my-flink-jdk11-1.0-SNAPSHOT-shaded.jar[INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESS[INFO] ------------------------------------------------------------------------[INFO] Total time: 8.988 s[INFO] Finished at: 2021-09-28T08:30:35Z[INFO] ------------------------------------------------------------------------ 转移就构建成功了。 在target目录下查看jar包 12➜ my-flink-jdk11 ll target/my-flink-jdk11-1.0-SNAPSHOT.jar -rw-r--r-- 1 caiwenhui staff 6.6K Sep 28 16:30 target/my-flink-jdk11-1.0-SNAPSHOT.jar 同样把代码目录挂载进flink容器，然后构建flink容器（此步骤只要是拿到target目录下的jar包，如果你指定了其他路径换个挂载目录也可以） 1docker run -it --name flinkc --privileged -w /www -v$(PWD):/www -p 8081:8081 flink:1.12-scala_2.11-java8 bash 8081是flink webui的服务，具体的内容后面再说 进到容器后，启动单机版flink集群 1234flink@ed5e7ea28514:/www$ start-cluster.shStarting cluster.Starting standalonesession daemon on host ed5e7ea28514.Starting taskexecutor daemon on host ed5e7ea28514 123456789101112131415flink@ed5e7ea28514:&#x2F;www$ flink run --class my.flink.BatchJob .&#x2F;target&#x2F;my-flink-jdk11-1.0-SNAPSHOT.jarJob has been submitted with JobID ca99d6d7ef6f913ac334d7123d63658bProgram execution finishedJob with JobID ca99d6d7ef6f913ac334d7123d63658b has finished.Job Runtime: 187 msAccumulator Results:- 40a6a5d6af948dba01cbb7bee71f2d4e (java.util.ArrayList) [6 elements]goodgoodstudydaydayup 可以看到，可以这个结果和我们再IDEA执行的结果一致，所以开发环境搭建完毕。后面的篇章将会是具体的流计算内容。","categories":[{"name":"大数据","slug":"大数据","permalink":"http://blog.crazylaw.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"flink","slug":"flink","permalink":"http://blog.crazylaw.cn/tags/flink/"}]},{"title":"【Golang】- protobuf插件扩展开发2","slug":"Golang/protobuf插件扩展开发2","date":"2021-09-06T01:16:51.000Z","updated":"2021-09-06T01:57:29.825Z","comments":true,"path":"2021/09/06/Golang/protobuf插件扩展开发2/","link":"","permalink":"http://blog.crazylaw.cn/2021/09/06/Golang/protobuf%E6%8F%92%E4%BB%B6%E6%89%A9%E5%B1%95%E5%BC%80%E5%8F%912/","excerpt":"前言上一片，我们讲到了protobuf扩展开发的大体流程和思路，这一篇，我们来继续总结一下相关的API细节。","text":"前言上一片，我们讲到了protobuf扩展开发的大体流程和思路，这一篇，我们来继续总结一下相关的API细节。 API第一点1option go_package &#x3D; &quot;github.com&#x2F;xxxx&#x2F;pb&#x2F;go-pb&#x2F;api;pb&quot;; 我们经常可以看到这种定义，分好前面的github.com/xxxx/pb/go-pb/api，我们在写代码的时候会被加载到： 123456// 源码func (gen *Plugin) NewGeneratedFile(filename string, goImportPath GoImportPath) *GeneratedFile// 我们自己的代码filename := f.GeneratedFilenamePrefix + \".api.go\"g := plugin.NewGeneratedFile(strings.TrimPrefix(filename, strings.Trim(pbPkg.String(), \"\\\"\")), f.GoImportPath) 这里，我们可以看到Plugin.NewGeneratedFile有2个参数，第一个是filename,另外一个是goImportPath，分别的含义是： 生成的文件名和这个文件被import的时候，应该要怎么import。 这个文件名需要注意的是，这是一个全路径文件名，如果你的文件名种存在/，那么前面的都是目录，直到最后一个，才是文件名。 例如我这里的是github.com/xxxx/pb/go-pb/api，那么生成的文件被引用的时候，就会import &quot;github.com/xxxx/pb/go-pb/api&quot;。 看到这里，我们再来看看分号后面的pb，这个在外面写代码的时候到体现是： 12// 我们自己的代码g.P(\"package \" + f.GoPackageName) 看到其实这个pb会被加载进文件的GoPackageName种，所以分号前面的意义是不同的，前面的对应filename和GoImportPath,后面对应的就是GoPackageName 第二点123var ( pbPkg = protogen.GoImportPath(\"github.com/xxx/pb/go-pb\")) 在代码中，我们会定义一些这样子的xxxPkg的变量，他们由protogen.GoImportPath来包装起来，在使用上就是： 1g.P(\" \", method.GoName, \"Api = \", pbPkg.Ident(\"NewApi\"), \"(\\\"\", method.GoName, \"\\\", \\\"\", m, \"\\\", \\\"\", url, \"\\\")\") 看到这里的pbPkg.Ident(&quot;NewApi&quot;)，意思就是这个要调用pbPkg包下的NewApi的方法。 值得注意的是，这里用到的Ident()API，顾名思义就是.的意思。 第三点有时候，我们调用Plugin.NewGeneratedFile，创建了生成文件实例并且已经预写入了一些内容，但是实际代码中，并没有任何有意义有价值的代码，那么这个时候我希望这个文件不要生成，我就可以调用Skip()的API。 123// 我们自己的代码g := plugin.NewGeneratedFile(strings.TrimPrefix(filename, strings.Trim(pbPkg.String(), \"\\\"\")), f.GoImportPath)g.Skip() 那么这个文件在最终将不会被生成。","categories":[{"name":"Golang","slug":"Golang","permalink":"http://blog.crazylaw.cn/categories/Golang/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://blog.crazylaw.cn/tags/Golang/"}]},{"title":"【Golang】- protobuf插件扩展开发","slug":"Golang/protobuf插件扩展开发","date":"2021-09-04T06:16:51.000Z","updated":"2021-09-04T06:28:36.840Z","comments":true,"path":"2021/09/04/Golang/protobuf插件扩展开发/","link":"","permalink":"http://blog.crazylaw.cn/2021/09/04/Golang/protobuf%E6%8F%92%E4%BB%B6%E6%89%A9%E5%B1%95%E5%BC%80%E5%8F%91/","excerpt":"前言最近，项目需要用到protobuf来定义消息，但是我们需要一个更灵活的代码片段，如何通过proto文件来创建自定义的代码呢？可以通过proto的plugin对方式来自己是一个proto-gen。\b 在网上看了一些教程，发现有一些教程已经过时了，而且过于片面，没有把整套思想很好的说明。并且也有一些功能点并没有完全实现。这里总结一下相关的内容，并且说一下最近实现的一个插件。 对于已经了解大概proto的人来说，相对简单，但是如果是自定义option呢？你又了解吗？","text":"前言最近，项目需要用到protobuf来定义消息，但是我们需要一个更灵活的代码片段，如何通过proto文件来创建自定义的代码呢？可以通过proto的plugin对方式来自己是一个proto-gen。\b 在网上看了一些教程，发现有一些教程已经过时了，而且过于片面，没有把整套思想很好的说明。并且也有一些功能点并没有完全实现。这里总结一下相关的内容，并且说一下最近实现的一个插件。 对于已经了解大概proto的人来说，相对简单，但是如果是自定义option呢？你又了解吗？ 需求12345678910111213141516171819202122232425262728293031323334&#x2F;&#x2F; test.protosyntax &#x3D; &quot;proto3&quot;;package pb;option go_package &#x3D; &quot;&#x2F;;pb&quot;;import &quot;unknow.proto&quot;;service ApiIMService &#123; rpc RegisterDevice (ApiIMRegisterDeviceReq) returns (ApiIMRegisterDeviceResp) &#123; option (unknow.api.http) &#x3D; &#123; method: &quot;post&quot; url: &quot;&#x2F;v1&#x2F;im&#x2F;register_device&quot; &#125;; &#125;&#125;&#x2F;&#x2F; 设备类型enum ApiIMDeviceType &#123; API_IM_UNKNOWN &#x3D; 0; API_IM_Android &#x3D; 1; API_IM_IOS &#x3D; 2; API_IM_Window &#x3D; 3; API_IM_MacOS &#x3D; 4; API_IM_Web &#x3D; 5;&#125;message ApiIMRegisterDeviceReq &#123; ApiIMDeviceType type &#x3D; 1; &#x2F;&#x2F; 设备类型&#125;message ApiIMRegisterDeviceResp &#123; int64 device_id &#x3D; 1; &#x2F;&#x2F; 设备id&#125; 我们看到这一个proto文件，常规的我就不说了。主要是看到import &quot;unknow.proto&quot;, option (unknow.api.http) 可以看到，我这里引入一个unknow.proto的文件。 我希望最终生成一个api.unknow.go的文件。里面的内容期待如下： 123456789101112131415161718192021// Copyright (c) 2021, whiteCcinn Inc.// Code generated by protoc-gen-unknow. DO NOT EDIT.// source: test.protopackage pbtype Api struct &#123; Name string Method string Url string&#125;func newApi(name, method, url string) *Api &#123; return &amp;Api&#123; name, method, url, &#125;&#125;var ( RegisterDeviceApi = newApi(\"RegisterDevice\", \"post\", \"/v1/im/register_device\")) 生成的文件，我可以在项目通过pb.RegisterDeviceApi拿到在proto定义好的API内容。 12345678910111213141516171819&#x2F;&#x2F; unknow.protosyntax &#x3D; &quot;proto3&quot;;package unknow.api;option go_package &#x3D; &quot;&#x2F;;pb&quot;;import &quot;google&#x2F;protobuf&#x2F;descriptor.proto&quot;;extend google.protobuf.MethodOptions &#123; &#x2F;&#x2F; See &#96;HttpRule&#96;. HttpRule http &#x3D; 72295728;&#125;message HttpRule &#123; string url &#x3D; 1; string method &#x3D; 2;&#125; google/api/annotations.proto google/api/http.proto google/protobuf/descriptor.proto 如果有了解过google/api/annotations.proto 和 google/api/http.proto的人应该不会陌生，当你需要用到grpc-gateway或者proto-gen-swaager的时候，都会有介绍到option(goggle.api.http)的用法。 这里我们看到unknow.proto的结构体，这就是一个简化版本。用于实现自定义的option用的。 unknow.proto对于基础的语法规则来说，我们来看剖析一下unknow.proto，常规的就不说了。 1234567891011package unknow.api;extend google.protobuf.MethodOptions &#123; &#x2F;&#x2F; See &#96;HttpRule&#96;. HttpRule http &#x3D; 72295728;&#125;message HttpRule &#123; string url &#x3D; 1; string method &#x3D; 2;&#125; 对于extend的用法，我这里列一下官方的链接。 Custom Options 我们看到这里，extend google.protobuf.MethodOptions。 代表着，我这里自定义个作用于Method的option。所以在真正的proto文件中，我就可以使用unknow.api.option的标签。也就是option (unknow.api.http)。 接着我们看到，这个option我们定义了一个元素，类型是Message HttpRule，命名为http，并且给它定义一个唯一的Number。 接着我们看到HttpRule，内部存在2个元素，一个是string url 和 string method，这意味着我们可以使用独立行写法：option (unknow.api.http).url = &quot;/v1/im/register_device&quot;，然后再下一行使用 option (unknow.api.http).method = &quot;post&quot;，一个完整的例如如下： 刚才说到，这是一个method的option，也就是我们这里定义的rpc下的 option。 123456service ApiIMService &#123; rpc RegisterDevice (ApiIMRegisterDeviceReq) returns (ApiIMRegisterDeviceResp) &#123; option (unknow.api.http).method &#x3D; &quot;post&quot; option (unknow.api.http).url &#x3D; &quot;&#x2F;v1&#x2F;im&#x2F;register_device&quot; &#125;&#125; 除了这个写法，我更推荐如下更简洁的写法，用map的写法： 12345678service ApiIMService &#123; rpc RegisterDevice (ApiIMRegisterDeviceReq) returns (ApiIMRegisterDeviceResp) &#123; option (unknow.api.http) &#x3D; &#123; method: &quot;post&quot; url: &quot;&#x2F;v1&#x2F;im&#x2F;register_device&quot; &#125;; &#125;&#125; 这样子，我们就看懂了test.proto的内容了对吧。连贯起来，那么我们的这个unknow.proto就是用于实现option(unknow.api.http)。而option(unknow.api.http)的使用则在test.proto进行采用。 好了，有了定义的文件，那么接下来就是我们的重点了，如何编写一个实现自定义代码的protobuf插件扩展 扩展实现 protobuf解析旧版的流程图，便于我们理解。新版的后续我抽空再画画 不科学的例子第一个例子，以golang旧版proto-gen-go为例。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package mainimport ( &quot;io&#x2F;ioutil&quot; &quot;os&quot; &quot;github.com&#x2F;golang&#x2F;protobuf&#x2F;proto&quot; &quot;github.com&#x2F;golang&#x2F;protobuf&#x2F;protoc-gen-go&#x2F;generator&quot;)func main() &#123; &#x2F;&#x2F; Begin by allocating a generator. The request and response structures are stored there &#x2F;&#x2F; so we can do error handling easily - the response structure contains the field to &#x2F;&#x2F; report failure. g :&#x3D; generator.New() data, err :&#x3D; ioutil.ReadAll(os.Stdin) if err !&#x3D; nil &#123; g.Error(err, &quot;reading input&quot;) &#125; if err :&#x3D; proto.Unmarshal(data, g.Request); err !&#x3D; nil &#123; g.Error(err, &quot;parsing input proto&quot;) &#125; if len(g.Request.FileToGenerate) &#x3D;&#x3D; 0 &#123; g.Fail(&quot;no files to generate&quot;) &#125; g.CommandLineParameters(g.Request.GetParameter()) &#x2F;&#x2F; Create a wrapped version of the Descriptors and EnumDescriptors that &#x2F;&#x2F; point to the file that defines them. g.WrapTypes() g.SetPackageNames() g.BuildTypeNameMap() g.GenerateAllFiles() &#x2F;&#x2F; Send back the results. data, err &#x3D; proto.Marshal(g.Response) if err !&#x3D; nil &#123; g.Error(err, &quot;failed to marshal output proto&quot;) &#125; _, err &#x3D; os.Stdout.Write(data) if err !&#x3D; nil &#123; g.Error(err, &quot;failed to write output proto&quot;) &#125;&#125; \b我发现现在网上很多教程都会以旧版的为主，并且都是沿用参考了proto-gen-go的这个旧版的写法，所以导致，我们在学习写的时候，会出现一些问题。并且其实你用了旧版的这个写法，当你在用protoc去编译的情况下，protoc也会友好的提示你，该API已经被废弃，将在未来的版本下移除，请使用新的写法。虽然你还是能编译通过，但是你不敢保证未来哪一个版本就直接不向后兼容了。 第二个例子。 12345678910input, _ := ioutil.ReadAll(os.Stdin)var req pluginpb.CodeGeneratorRequestproto.Unmarshal(input, &amp;req)// 使用默认选项初始化我们的插件opts := protogen.Options&#123;&#125;plugin, err := opts.New(&amp;req)if err != nil &#123; panic(err)... 这种方式就是从头到尾都自己写，把整个pipeline的流程都自己处理。但是大可不必，因为其实有很多流程化的东西，在新版的genpro下已经封装成了一个Run传递回调函数即可。 正确的例子如果真的要参考的话，推荐参考最新版本的proto-gen-go 首先，我们需要知道一点，我们在采用protoc对proto文件进行编译的时候，经常是执行类似如下代码： 12protoc -I.:$&#123;GOGO_PROTOBUF&#125; \\--gofast_out=plugins=grpc:./go-pb \b由于这里我用的是gogoprotobuf，所以你会看到我这里的是--gofast_out=plugins=grpc:./go-pb，如果你是protobuf的官方的proto-gen的话，那么你这里应该是--go_out=plugins=grpc:./go-pb 这里我们需要注意的是什么呢，就是插件二进制文件名，这是有一定规则的，这是我之前在自定义git凭据文章中说明的一样，这二进制文件需要在你的环境变量中，否则你就需要通过-I来指定文件路径。然后命名规则为proto-gen-xxx，而这个xxx就是你的自定义的名字。在本例子中，我的扩展名字就是proto-gen-unknow。 因此，最终如果我要执行的话，那么就是执行如下命令： 12protoc -I.:$&#123;GOGO_PROTOBUF&#125; \\--unknow_out=./go-pb 其实如果有接触过thrift 或者 Rust的元编程 甚至是 Python的 lark-parser自定义抽象语法树，或者其他经由AST抽象语法树写代码的同学应该都知道，这其实抽象出来就是一个AST的解析处理而已。所以我首先需要了解他的部署。 一个简陋的AST定义如下（哈哈，略显简陋，但是了解AST是啥的应该多少能看懂一些）： 12345678910FileDescriptor -&gt; ServiceDescriptor -&gt; ServiceOptionDescriptor -&gt; MethodDescriptor -&gt; MethodOptionDescriptor -&gt; [MessageDescriptor] -&gt; MessageDescriptor -&gt; MessageOptionDescriptor -&gt; FieldDescriptor -&gt; FieldOptionDescriptor -&gt; FieldOptionDescriptor 知道怎么执行了，和AST, 我们就来看看怎么编写代码。 12345678910111213➜ protoc-gen-unknow git:(main) tree.├── README.md├── go.mod├── go.sum├── internal│ └── unknow.go├── main.go├── out│ └── unknow.pb.go└── proto ├── descriptor.proto └── unknow.proto 可以看到，这是我的一个代码结构目录 1234567891011package mainimport ( \"internal\" \"google.golang.org/protobuf/compiler/protogen\")func main() &#123; u := internal.Unknow&#123;&#125; protogen.Options&#123;&#125;.Run(u.Generate)&#125; \b这里，我们借助protobuf 编译包下的 protogen 工具来解析 proto 文件。我们接下来看一下 internal 包下具体的写法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135package internalimport ( pb \"out\" \"google.golang.org/protobuf/compiler/protogen\" \"google.golang.org/protobuf/proto\" \"google.golang.org/protobuf/types/descriptorpb\")type Unknow struct &#123;&#125;func (u Unknow) Generate(plugin *protogen.Plugin) error &#123; if len(plugin.Files) &lt; 1 &#123; return nil &#125; // 指定生成文件的文件名 filename := \"/api.unknow.go\" // 创建一个文件生成器对象 g := plugin.NewGeneratedFile(filename, plugin.Files[len(plugin.Files)-1].GoImportPath) // 调用g.P就是往文件开始写入自己期待的代码 g.P(`// Copyright (c) 2021, whiteCcinn Inc.`) g.P(\"// Code generated by protoc-gen-unknow. DO NOT EDIT.\") g.P(\"// source: all MethodOptions(unknow.api.http) in proto file\") g.P() g.P(\"package \", plugin.Files[len(plugin.Files)-1].GoPackageName) g.P(`type Api struct &#123; Name string Method string Url string&#125;func newApi(name, method, url string) *Api &#123; return &amp;Api&#123; name, method, url, &#125;&#125;`) g.P(\"var (\") //\b 通过plugin.Fiels，我们可以拿到所有的输入的proto文件 // 如果我们需要对这个文件生成代码的话，那么就进入到generateFile()逻辑 // 并且把g和f一起传递过去 for _, f := range plugin.Files &#123; if f.Generate &#123; if _, err := u.generateFile(g, f); err != nil &#123; return err &#125; &#125; &#125; g.P(\")\") return nil&#125;func (u Unknow) generateFile(g *protogen.GeneratedFile, file *protogen.File) (*protogen.GeneratedFile, error) &#123; // 这一段代码仅仅只是为了忽略包含proto文件中包含了streamClient和streamServer的代码 isGenerated := false for _, srv := range file.Services &#123; for _, method := range srv.Methods &#123; if method.Desc.IsStreamingClient() || method.Desc.IsStreamingServer() &#123; continue &#125; isGenerated = true &#125; &#125; if !isGenerated &#123; return nil, nil &#125; // file 的下一层级就是 services 层级 for _, srv := range file.Services &#123; if err := u.genService(g, srv); err != nil &#123; return nil, err &#125; &#125; return g, nil&#125;func (u Unknow) genService(g *protogen.GeneratedFile, srv *protogen.Service) error &#123; // service 内部有很多 rpc 关键字的方法 for _, method := range srv.Methods &#123; if method.Desc.IsStreamingClient() || method.Desc.IsStreamingServer() &#123; continue &#125; // 由于我们自定义的是就是MethodOptions，所以就来到了这里来进行判断 if err := u.genMethodHTTPRule(g, method); err != nil &#123; return err &#125; &#125; return nil&#125;func (u Unknow) genMethodHTTPRule(g *protogen.GeneratedFile, method *protogen.Method) error &#123; // 因为我们通过method.Desc.Options() 拿到的数据类型是`interface&#123;&#125;` 类型 // 所以这里我们需要对Options，明确指定转换为 *descriptorpb.MethodOptions 类型 // 这样子就能拿到我们的MethodOption对象 options, ok := method.Desc.Options().(*descriptorpb.MethodOptions) if !ok &#123; return nil &#125; // PS：重点 // 这里我们看到我们借助了一个非protogen下的包的内容 // 原因就是，protobuf编译器会把自定义的Option全部指定为Extension，由于并非内置的属性和值 // protobuf官方是没办法拿到和你对应的可读的内容的，只能通过拿到经过序列化之后的数据。 // 因此，我们这里通过 proto.GetExtension的方法，把刚才unknow.proto单独编译好的 unknow.pb.proto 文件下的 pb. E_HTTP 加载进来，指定了我需要在自定义扩展的MethodOptions中，拿到该Http下里面的value // 也因此，我们可以再经过一次类型转换，就可以拿到了具体的httpRule httpRule, ok := proto.GetExtension(options, pb.E_Http).(*pb.HttpRule) if !ok &#123; return nil &#125; // 接下来，我们就可以通过GetXxx的方式，来获取我们设置在其Message内部filed m := httpRule.GetMethod() url := httpRule.GetUrl() if len(m) == 0 &amp;&amp; len(url) == 0 &#123; return nil &#125; g.P(\" \", method.GoName, \"Api = \", \"newApi(\\\"\", method.GoName, \"\\\", \\\"\", m, \"\\\", \\\"\", url, \"\\\")\") return nil&#125; 请跟着说明中注释一步步查看详解 查看这段代码逻辑，也是非常简单，因为没有特别复杂的逻辑，尽量不要跳过，因为里面涉及到如何读取自定义的Option的问题。代码大致定位在 proto.GetExtension方法附近。 因为这里的demo生成的代码比较简单。最终，我们生成的代码就是： 123456789101112131415161718192021// Copyright (c) 2021, whiteCcinn Inc.// Code generated by protoc-gen-unknow. DO NOT EDIT.// source: source: all MethodOptions(unknow.api.http) in proto filepackage pbtype Api struct &#123; Name string Method string Url string&#125;func newApi(name, method, url string) *Api &#123; return &amp;Api&#123; name, method, url, &#125;&#125;var ( RegisterDeviceApi = newApi(\"RegisterDevice\", \"post\", \"/v1/im/register_device\")) 放一下完整的测试命令: 1go install . &amp;&amp; protoc --proto_path proto/ -I=. test.proto test2.proto --unknow_out=./out --go_out=./out 最后生成的文件目录结构如下： 123456789101112131415161718➜ protoc-gen-unknow git:(main) tree.├── README.md├── go.mod├── go.sum├── internal│ └── unknow.go├── main.go├── out│ ├── api.unknow.go│ ├── test.pb.go│ ├── test2.pb.go│ └── unknow.pb.go└── proto ├── descriptor.proto ├── test.proto ├── test2.proto └── unknow.protos 最后推荐几个扩展库写得不错的扩展插件: protoc-gen-grpc-gateway protoc-gen-grpc-openapiv2 protoc-gen-grpc-gohttp","categories":[{"name":"Golang","slug":"Golang","permalink":"http://blog.crazylaw.cn/categories/Golang/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://blog.crazylaw.cn/tags/Golang/"}]},{"title":"【Golang】- []byte在结构体的友好可读性处理","slug":"Golang/[]byte在结构体的友好可读性处理","date":"2021-09-03T03:16:51.000Z","updated":"2021-09-03T05:55:43.831Z","comments":true,"path":"2021/09/03/Golang/[]byte在结构体的友好可读性处理/","link":"","permalink":"http://blog.crazylaw.cn/2021/09/03/Golang/[]byte%E5%9C%A8%E7%BB%93%E6%9E%84%E4%BD%93%E7%9A%84%E5%8F%8B%E5%A5%BD%E5%8F%AF%E8%AF%BB%E6%80%A7%E5%A4%84%E7%90%86/","excerpt":"前言有些时候，我们会发现，[]byte 类型在 struct 中，是必不可少的结构体，因为用了[]byte\u001c代表可以存储字节数据，也可以叫做二进制安全的存储。代表可以存储任何数据。 如何才能做到在序列化json的情况下，可以Println出一个可读性的在struct的[]byte呢？","text":"前言有些时候，我们会发现，[]byte 类型在 struct 中，是必不可少的结构体，因为用了[]byte\u001c代表可以存储字节数据，也可以叫做二进制安全的存储。代表可以存储任何数据。 如何才能做到在序列化json的情况下，可以Println出一个可读性的在struct的[]byte呢？ 实现最近我在开发我们的部门的配置服务，需要提供一个配置工具。里面设计的一个struct，有一个[]byte类型，就是用来存储实际数据的。但是我们在这里的时候，我们有一个查看原始数据的需求，因为我们的数据经过了加密，和压缩，最终才会放到该结构体。 简化结构体，这里列举一下例子： 1234567type V []bytetype Value struct &#123; PublishTime int64 PublishDateTime string Value V&#125; 这里我们看到，我们这里的Value\b实际就是一个[]byte，我们把这个结构体经过json.Marshal之后推送到远端kv服务中，一切都正常。 但是当我们需要查看的时候，就需要从远端的kv拉回来，经过json.Unmarsha处理，这个时候，我们会发现： 1&#123;\"PublishTime\":1630636657,\"PublishDateTime\":\"2021-09-03 02:37:37.8693941 +0000 UTC m=+0.015759101\",\"Value\":\"MTIzCg==\"&#125; 这里，我们看到Value是一个经过base64加密过的数据，这是因为默认情况下[]byte将会把数据经过base64变成字符串来符合json数据类型。那么我们有什么版本让他显示出原来真是的数据呢？ 这里我使用了一个方案，借助多一个数据结构，对T V进行一个重组。 12345678910111213141516type VO []bytetype ValueReadable struct &#123; PublishTime int64 PublishDateTime string Value VO&#125;func (b *VO) MarshalJSON() ([]byte, error) &#123; return *b, nil&#125;func (b *VO) UnmarshalJSON(input []byte) error &#123; *b = input return nil&#125; 定义多一个大体上一致的结构体，注意此时的Value不再是V，而是VO，我们对VO自定义json序列化的行为，那就是把base64的行为给去掉。 这样子，我们得到的数据就会是 1&#123;\"PublishTime\":1630636657,\"PublishDateTime\":\"2021-09-03 02:37:37.8693941 +0000 UTC m=+0.015759101\",\"Value\":123&#125; 细心的朋友一定发现了问题所在，那就是Value和ValueReadable怎么进行转换。 因为你存的时候是通过Value进行marshal的，那么你的unmarsha行为一定要对应才能解到正确的数据。 所以这里，就是我们的一个重点，我们需要借助unsafe.Pointer 1234567891011 // because []byte in struct will be base64encode// so you will see such as \"Ik1USXpDZz09Ig==\"// we should base64decode, so we custom a struct do not base64encode// struct type transform use unsafe.Pointerp := unsafe.Pointer(&amp;persistenceValue)vr := (*config_sync.ValueReadable)(p)tv, err := json.Marshal(vr)if err != nil &#123; log.Fatal(err)&#125;fmt.Println(string(tv)) 我们利用unsafe的指针数据类型，进行一个强制转换，为什么会成功呢，因为在内存对齐的结构上，这2个对象的内存是一致的，所以我们就可以进行强制转换，而不用担心有panic的产生。这只是unsafe指针的一个灵活运用。但是可以达到我们的目的，十分的有效果。 1&#123;\"PublishTime\":1630636657,\"PublishDateTime\":\"2021-09-03 02:37:37.8693941 +0000 UTC m=+0.015759101\",\"Value\":123&#125; 转换后，就可以看到我原本的数据了 123.","categories":[{"name":"Golang","slug":"Golang","permalink":"http://blog.crazylaw.cn/categories/Golang/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://blog.crazylaw.cn/tags/Golang/"}]},{"title":"【kubernetes】k8s-docker-for-mac-磁盘挂载","slug":"k8s/k8s-docker-for-mac-磁盘挂载","date":"2021-08-18T09:47:30.000Z","updated":"2021-08-18T14:45:43.337Z","comments":true,"path":"2021/08/18/k8s/k8s-docker-for-mac-磁盘挂载/","link":"","permalink":"http://blog.crazylaw.cn/2021/08/18/k8s/k8s-docker-for-mac-%E7%A3%81%E7%9B%98%E6%8C%82%E8%BD%BD/","excerpt":"前言上一次，我们说到了docker-for-mac 已经内置了最小化的k8s。我们在本地开发的时候，或多或少希望yaml配置是最接近线上环境的配置。 因此，上一次，教会了大家，如何在mac上开启nfs,把我们的pv和本地mac的nfs进行一个通信。 这一次，我们来聊聊docker-for-mac磁盘挂在的相关内容。","text":"前言上一次，我们说到了docker-for-mac 已经内置了最小化的k8s。我们在本地开发的时候，或多或少希望yaml配置是最接近线上环境的配置。 因此，上一次，教会了大家，如何在mac上开启nfs,把我们的pv和本地mac的nfs进行一个通信。 这一次，我们来聊聊docker-for-mac磁盘挂在的相关内容。 虚拟机Docker for Mac是一个原生的苹果应用程序，被安装到 /Application 目录，安装时会创建 /usr/local/bin 目录下的 docker、docker-compose 符号链接。 Docker for Mac 使用通过 Hypervisor.framework 提供的轻量级的 xhyve 虚拟化技术Docker for Mac 不使用 docker-machine 管理虚拟机Docker for Mac 不通过 TCP 端口通信，反而使用 docker.sock 套接字文件通信（实际上是将 /var/tmp 目录挂载到了虚拟机中，虚拟机在其中生成套接字文件） 但是尽管如此，你可以理解为还是存在虚拟机的。这个虚拟机的作用就是允许在macos上运行docker。 docker for mac 这个就是我们的docker-for-mac的桌面版客户端。 根据上面这个图，我们可以发现，所有的镜像都存储在。 /Users/caiwenhui/Library/Containers/com.docker.docker/Data/vms/0/data (记得修改为自己的路径，就是我这里的caiwenhui) 123456789101112131415161718192021222324252627➜ ~ ll /Users/caiwenhui/Library/Containers/com.docker.docker/Data/vms/0/datatotal 103652496-rw-r--r-- 1 caiwenhui staff 96G 8 18 20:21 Docker.raw➜ ~ head -n 10 /Users/caiwenhui/Library/Containers/com.docker.docker/Data/vms/0/data/Docker.raw�D��?��� U�`�&amp;3�Ѻ`1O�� �ha�ha��S�J�#` &lt;�kD*�&lt;O5K�VS�~~�/var/lib��B�uK����J�1�@ ]�#`���p��q Z�]J, )NL������w)��%�z?fw G/�%|gCԬ?f�u ))�% �.?f �% |?f) �% ��?f)� (L � X %X i;�F!)D0 M %@�\")� �� ֑ @#)� � �f�H$)\" - :,�%)� �e LE��()\"h �a ��SnU&amp;43�%�C4k�?f �% &#123; �% �D WA T қ �� ��v&amp;)�\" t� FW�D') &#125;�1 &amp;� � � (A �� =�97 �; �&amp;N � _Z˭� � �� �% F?f �0 �V�� s �% ԇ �% X� �% s�?f �RiDX0�m�&#125;�u � �@$���w?� �% ��?f �% ��?f. � Շ� �% MS?f W s �w� � ! �o� �% E�?f �% n�?f 我们这里可以看到，这个文件，占用了96G，就是我所分配的磁盘大小。并且这是一个经过了字节压缩的二进制文件。 这张图，算得上是我们今天的重点。 使用文件共享允许 Mac 上的本地目录与 Linux 容器共享。默认情况下/Users，/Volume、/private、/tmp和/var/folders目录是共享的。如果您的项目在此目录之外，则必须将其添加到列表中。否则，您可能会在运行时得到Mounts denied或cannot start service出错。 因此，我们可以看到在默认情况下，有几个目录是已经被共享的了。我们重点需要关注的是/Users目录，因为我们常常把我们的所有个人用户相关的东西，都会放在对应用户下，就我而言，我会把所有的代码都在 /Users/caiwenhui/www 下，这也意味着，我的所有代码，都将会被虚拟机同步到linux系统中，也正是因此，当你在MAC系统下，执行docker run -v $(PWD):/www xxx之类的命令的时候，你可以成功的挂载到容器中。如果你把挂载的目录放在上述的几个目录之外，docker命令将会挂载失败。 也是因为这个原因，你会发现，当你的代码，在下载大量依赖，或者在构建一堆索引的时候，或者你的/Users目录下有很多文件在变动的时候，你会发现你的CPU变成异常的高，而且会特别卡，可能你会觉得怎么那么卡。如果你不去查看哪个进程占用那么多cpu的话，你永远不会知道，其实大多数，显示出来都是docker的虚拟机占用的cpu为大头就是因为这个原因。 获取虚拟机shell既然，我们知道了上述的共享文件了，那么我们就会想知道，我要怎么去看，怎么去调试，或者我有什么更深的理解呢？ 其实是有的，例如，我想看看，整个虚拟化技术，都挂载了什么数据卷构成我们的文件系统。 我们知道MacOS上的Docker Desktop for mac实际上是在Linux虚拟机中运行的Docker容器，这对于macOS主机上使用Docker多了一层虚拟化。有些情况下，我们需要能够访问这个Linux虚拟机，以便实现一些hack操作。 netcat12➜ ~ ll ~/Library/Containers/com.docker.docker/Data/debug-shell.socksrwxr-xr-x 1 caiwenhui staff 0B 8 16 21:31 /Users/caiwenhui/Library/Containers/com.docker.docker/Data/debug-shell.sock 我们可以看到，这里有一个名字叫debug-shell.sock的文件，这是一个可执行的sock文件。 使用 nc 命令连接Docker的debug-shell socket文件: 12➜ ~ nc -U ~/Library/Containers/com.docker.docker/Data/debug-shell.sock/ # ^[[49;5R 显示的提示符比较奇怪，不过不影响使用 我们使用 df -h 命令可以看到Docker虚拟机的存储挂载: 12345678910111213141516171819202122232425262728293031323334353637383940/ # ^[[49;5Rdf -hdf -hFilesystem Size Used Available Use% Mounted onoverlay 3.9G 4.0K 3.9G 0% /tmpfs 3.9G 8.0K 3.9G 0% /containers/onboot/000-dhcpcd/tmptmpfs 3.9G 0 3.9G 0% /containers/onboot/001-sysfs/tmptmpfs 3.9G 0 3.9G 0% /containers/onboot/002-sysctl/tmptmpfs 3.9G 0 3.9G 0% /containers/onboot/003-format/tmptmpfs 3.9G 0 3.9G 0% /containers/onboot/004-extend/tmptmpfs 3.9G 0 3.9G 0% /containers/onboot/005-mount/tmptmpfs 3.9G 0 3.9G 0% /containers/onboot/006-metadata/tmptmpfs 3.9G 0 3.9G 0% /containers/onboot/007-services0/tmptmpfs 3.9G 0 3.9G 0% /containers/onboot/008-services1/tmptmpfs 3.9G 0 3.9G 0% /containers/onboot/009-swap/tmptmpfs 3.9G 0 3.9G 0% /containers/onboot/010-mount-docker/tmp/dev/vda1 94.2G 48.2G 41.2G 54% /containers/services/dev/vda1 94.2G 48.2G 41.2G 54% /containers/services/dockertmpfs 3.9G 4.0K 3.9G 0% /containers/services/acpid/tmpoverlay 3.9G 4.0K 3.9G 0% /containers/services/acpid/rootfstmpfs 3.9G 0 3.9G 0% /containers/services/binfmt/tmpoverlay 3.9G 0 3.9G 0% /containers/services/binfmt/rootfstmpfs 3.9G 8.0K 3.9G 0% /containers/services/dhcpcd/tmpoverlay 3.9G 8.0K 3.9G 0% /containers/services/dhcpcd/rootfstmpfs 3.9G 4.0K 3.9G 0% /containers/services/diagnose/tmpoverlay 3.9G 4.0K 3.9G 0% /containers/services/diagnose/rootfsoverlay 3.9G 4.0K 3.9G 0% /containers/services/diagnose/rootfstmpfs 3.9G 0 3.9G 0% /containers/onboot/011-bridge/tmptmpfs 64.0M 0 64.0M 0% /devtmpfs 796.2M 528.0K 795.7M 0% /run/resolvconf/resolv.conftmpfs 796.2M 528.0K 795.7M 0% /run/configtmpfs 796.2M 528.0K 795.7M 0% /run/containerdtmpfs 796.2M 528.0K 795.7M 0% /run/guest-servicestmpfs 796.2M 528.0K 795.7M 0% /run/host-servicestmpfs 796.2M 528.0K 795.7M 0% /run/resolvconf/resolv.conftmpfs 3.9G 0 3.9G 0% /sys/fs/cgroup/dev/vda1 94.2G 48.2G 41.2G 54% /var/lib/containerd/dev/vda1 94.2G 48.2G 41.2G 54% /var/lib/dockertmpfs 796.2M 528.0K 795.7M 0% /var/runtmpfs 796.2M 528.0K 795.7M 0% /var/run/linuxkit-containerd/containerd.sock... 使用命令 exit 或者^C 可以退出这个shell 进入shell，可以执行 . /etc/profile 获得环境 nsenter使用nsenter从容器内部进入host主机的名字空间(namespace)，但是对文件系统是只读 另外一种巧妙的方法是运行一个debian容器，然后在这个debian容器中执行 nsenter 通过 pid=host 来实现进入到运行 Docker4Mac 的mini VM的进程空间，这样就相当于进入了macOS的Docker虚拟机 在这个运行的debian容器中通过 nsenter 进入到host主机，也就是Docker VM名字空间以后，就可以看到虚拟机的提示符: 12➜ ~ docker run -it --rm --privileged --pid=host debian nsenter -t 1 -m -u -n -i bashbash-5.0# 我们可以在这个Docker VM中执行网络检查 12345678910111213141516171819202122232425262728293031323334353637383940414243444546bash-5.0# ip addr1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 brd 127.255.255.255 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 02:50:00:00:00:01 brd ff:ff:ff:ff:ff:ff inet 192.168.65.3/24 brd 192.168.65.255 scope global dynamic noprefixroute eth0 valid_lft 1576sec preferred_lft 136sec inet6 fe80::50:ff:fe00:1/64 scope link valid_lft forever preferred_lft forever3: tunl0@NONE: &lt;NOARP&gt; mtu 1480 qdisc noop state DOWN group default qlen 1000 link/ipip 0.0.0.0 brd 0.0.0.04: ip6tnl0@NONE: &lt;NOARP&gt; mtu 1452 qdisc noop state DOWN group default qlen 1000 link/tunnel6 :: brd ::5: services1@if6: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default link/ether 42:2f:8d:45:a3:03 brd ff:ff:ff:ff:ff:ff link-netns services inet 192.168.65.4 peer 192.168.65.5/32 scope global services1 valid_lft forever preferred_lft forever inet6 fe80::402f:8dff:fe45:a303/64 scope link valid_lft forever preferred_lft forever7: br-3a4b2ff7a5cb: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default link/ether 02:42:11:9a:3a:8f brd ff:ff:ff:ff:ff:ff inet 192.168.7.1/24 brd 192.168.7.255 scope global br-3a4b2ff7a5cb valid_lft forever preferred_lft forever8: br-ef48d4809428: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default link/ether 02:42:10:cb:22:b6 brd ff:ff:ff:ff:ff:ff inet 172.18.0.1/16 brd 172.18.255.255 scope global br-ef48d4809428 valid_lft forever preferred_lft forever9: br-f6b05f844262: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default link/ether 02:42:f4:ec:b5:c4 brd ff:ff:ff:ff:ff:ff inet 172.19.0.1/16 brd 172.19.255.255 scope global br-f6b05f844262 valid_lft forever preferred_lft forever10: br-ff0c8e959ac0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default link/ether 02:42:48:7f:f8:bb brd ff:ff:ff:ff:ff:ff inet 192.168.110.1/24 brd 192.168.110.255 scope global br-ff0c8e959ac0 valid_lft forever preferred_lft forever11: docker0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default link/ether 02:42:b9:db:7e:59 brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever inet6 fe80::42:b9ff:fedb:7e59/64 scope link valid_lft forever preferred_lft forever... 因为信息比较多，我们重点关注几个网卡信息 1234567891011122: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 02:50:00:00:00:01 brd ff:ff:ff:ff:ff:ff inet 192.168.65.3/24 brd 192.168.65.255 scope global dynamic noprefixroute eth0 valid_lft 1576sec preferred_lft 136sec inet6 fe80::50:ff:fe00:1/64 scope link valid_lft forever preferred_lft forever11: docker0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default link/ether 02:42:b9:db:7e:59 brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever inet6 fe80::42:b9ff:fedb:7e59/64 scope link valid_lft forever preferred_lft forever 上面的信息，结合来看，我们在docker-for-mac客户端设置了我们的虚拟机的网段 192.168.65.0/24，结合eth0和docker0这2个网卡信息，我们会发现，eth0 的 192.168.65.3, 就是我们这是的网段的信息，所以，这个虚拟机和macOS物理主机上对应的IP地址 192.168.65.1 对应，也就是说，如果我们使用 NFS 方式挂载物理主机上的NFS卷，访问的NFS服务器端地址就是这样获得的。 这里还可以看到在Docker VM上运行的Docker网络是 172.17.xx.xx/16 ，是一个NAT网络，我们可以看到在Docker VM端分配的IP地址是 172.17.0.1 。这也验证了我们的Docker VM上实际上有2个网络。 192.168.65.x/24 =&gt; 和物理主机macOS连接的NAT网络，用于虚拟机 172.17.x.x/16 =&gt; 和Docker0连接的NAT网络，用于容器 在Docker容器中，通过两层NAT，依然可以访问外界Internet。不过，反过来，外部需要访问Docker容器就比较麻烦了，需要做端口映射。 挂载我们前面说到了，当你通过 docker run -v的时候，你可以指定在共享目录下的所有文件下，进行挂载进去。那么我们反过来想一下，既然是共享目录的话，那么容器是如何做到查找这些目录的呢？ 通过mount -l，我们可以看到挂载点。 12345678910111213141516171819202122232425262728293031bash-5.0# mount -lrootfs on / type tmpfs (ro,relatime)proc on /proc type proc (rw,nosuid,nodev,noexec,relatime)tmpfs on /run type tmpfs (rw,nosuid,nodev,noexec,relatime,size=815300k,mode=755)tmpfs on /tmp type tmpfs (rw,nosuid,nodev,noexec,relatime,size=815300k)tmpfs on /var type tmpfs (rw,nosuid,nodev,noexec,relatime,mode=755)dev on /dev type devtmpfs (rw,nosuid,noexec,relatime,size=4008488k,nr_inodes=1002122,mode=755)mqueue on /dev/mqueue type mqueue (rw,nosuid,nodev,noexec,relatime)shm on /dev/shm type tmpfs (rw,nosuid,nodev,noexec,relatime)devpts on /dev/pts type devpts (rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=000)sysfs on /sys type sysfs (rw,nosuid,nodev,noexec,relatime)securityfs on /sys/kernel/security type securityfs (rw,nosuid,nodev,noexec,relatime)debugfs on /sys/kernel/debug type debugfs (rw,nosuid,nodev,noexec,relatime)fusectl on /sys/fs/fuse/connections type fusectl (rw,nosuid,nodev,noexec,relatime)pstore on /sys/fs/pstore type pstore (rw,nosuid,nodev,noexec,relatime)none on /sys/fs/bpf type bpf (rw,nodev,relatime)binfmt_misc on /proc/sys/fs/binfmt_misc type binfmt_misc (rw,nosuid,nodev,noexec,relatime)cgroup_root on /sys/fs/cgroup type tmpfs (rw,nosuid,nodev,noexec,relatime,size=10240k,mode=755)cpuset on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset)cpu on /sys/fs/cgroup/cpu type cgroup (rw,nosuid,nodev,noexec,relatime,cpu)cpuacct on /sys/fs/cgroup/cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpuacct)blkio on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)memory on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory)devices on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices)freezer on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,freezer)net_cls on /sys/fs/cgroup/net_cls type cgroup (rw,nosuid,nodev,noexec,relatime,net_cls)perf_event on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,perf_event)net_prio on /sys/fs/cgroup/net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,net_prio)hugetlb on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb)pids on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,pids)... 可以看到，我们这里有大量的挂载点，这些挂载点，组合成了一个独立的文件系统，或者各种命名空间。 现在有一个需求。例如，我们知道在使用k8s的时候，当我们用pv的type为hostPath的时候，是可以做到持久化数据磁盘的，那么我能不能做到，数据存放在Docker VM 非共享目录中呢？ 就是我的物理主机，并不希望同步这些数据。这些数据仅仅只需要存在Docker VM中就好了。 当然，这个场景就是我们经常说，我们想要看一些容器的配置或者目录的时候，会发现输出的路径，在我们的宿主机上找不到，为什么会这样子？难道是出错了吗？其实只是这些文件都在Docker VM中，其实就是因为这个原因导致的。只要我们进入到 Docker VM就可以看到这些文件。 例如，以我目前的例子为例子，我需要采集我本地所有k8s-pods的输出在终端的日志信息，我想通过promtail来进行日志采集，然后通过loki作为存储和查询服务器，再通过grafna来进行展示。我们的服务以前台的方式进行启动，所以我们那么首先我需要解决的第一个问题，就是pods的标准输出到哪里？后来发现/var/log/pods/。 123456789101112131415bash-5.0# ls -l /var/log/pods/total 0drwxr-xr-x 3 root root 60 Aug 18 09:05 default_dev-grafana-7cd4c89fd4-wdkpb_cf869741-be0d-44d2-b776-3239dd276069drwxr-xr-x 3 root root 60 Aug 18 09:05 default_dev-loki-statefulset-0_1d34dcac-a763-478a-b17b-addb082462efdrwxr-xr-x 3 root root 60 Aug 18 09:05 default_dev-loki-statefulset-1_c82d61e9-c66f-4b6a-960a-a8f7a1d5a8e2drwxr-xr-x 3 root root 60 Aug 18 09:05 default_dev-promtail-n6jgs_51afda31-be9c-4377-8167-8e29e991d9b0drwxr-xr-x 3 root root 60 Aug 16 13:32 kube-system_coredns-558bd4d5db-g2m9h_67ec7c7f-2e48-4cd7-8298-86295073072bdrwxr-xr-x 3 root root 60 Aug 16 13:32 kube-system_coredns-558bd4d5db-jk9bp_b9d8b64f-dda8-426a-8f55-029298828333drwxr-xr-x 3 root root 60 Aug 16 13:32 kube-system_etcd-docker-desktop_5d9d97b8d8daed31d6fd5c6d386c29c5drwxr-xr-x 3 root root 60 Aug 16 13:32 kube-system_kube-apiserver-docker-desktop_6fcd2fd42808f86960df2a06a72d6dc0drwxr-xr-x 3 root root 60 Aug 16 13:32 kube-system_kube-controller-manager-docker-desktop_bed77ee1871d9eabd1710836ad671f32drwxr-xr-x 3 root root 60 Aug 16 13:32 kube-system_kube-proxy-4wbs6_8bece9c8-e5fb-41f5-8869-2d408e71ba31drwxr-xr-x 3 root root 60 Aug 16 13:32 kube-system_kube-scheduler-docker-desktop_a52842863dff28cb2f7d4171a9f614a0drwxr-xr-x 3 root root 60 Aug 16 13:32 kube-system_storage-provisioner_79a3e8e5-6a93-43c1-abf3-c916384a4018drwxr-xr-x 3 root root 60 Aug 16 13:32 kube-system_vpnkit-controller_3742d607-ea8f-43df-aecf-4f925accbc3e 我们随便找一个pods，去查看日志。 1234567891011bash-5.0# tail -n 10 /var/log/pods/default_dev-grafana-7cd4c89fd4-wdkpb_cf869741-be0d-44d2-b776-3239dd276069/grafana/0.log&#123;\"log\":\"&#123;\\\"@level\\\":\\\"debug\\\",\\\"@message\\\":\\\"datasource: registering query type handler\\\",\\\"@timestamp\\\":\\\"2021-08-18T09:05:05.385825Z\\\",\\\"queryType\\\":\\\"node_graph\\\"&#125;\\n\",\"stream\":\"stderr\",\"time\":\"2021-08-18T09:05:05.3860495Z\"&#125;&#123;\"log\":\"&#123;\\\"@level\\\":\\\"debug\\\",\\\"@message\\\":\\\"datasource: registering query type fallback handler\\\",\\\"@timestamp\\\":\\\"2021-08-18T09:05:05.385855Z\\\"&#125;\\n\",\"stream\":\"stderr\",\"time\":\"2021-08-18T09:05:05.3860803Z\"&#125;&#123;\"log\":\"t=2021-08-18T09:05:05+0000 lvl=info msg=\\\"HTTP Server Listen\\\" logger=http.server address=[::]:3000 protocol=http subUrl= socket=\\n\",\"stream\":\"stdout\",\"time\":\"2021-08-18T09:05:05.3939314Z\"&#125;&#123;\"log\":\"t=2021-08-18T09:07:52+0000 lvl=eror msg=\\\"Failed to look up user based on cookie\\\" logger=context error=\\\"user token not found\\\"\\n\",\"stream\":\"stdout\",\"time\":\"2021-08-18T09:07:52.5402974Z\"&#125;&#123;\"log\":\"t=2021-08-18T09:07:52+0000 lvl=info msg=\\\"Request Completed\\\" logger=context userId=0 orgId=0 uname= method=GET path=/dashboard/new status=302 remote_addr=192.168.65.3 time_ms=0 size=29 referer=\\n\",\"stream\":\"stdout\",\"time\":\"2021-08-18T09:07:52.5403468Z\"&#125;&#123;\"log\":\"t=2021-08-18T09:07:56+0000 lvl=info msg=\\\"Successful Login\\\" logger=http.server User=admin@localhost\\n\",\"stream\":\"stdout\",\"time\":\"2021-08-18T09:07:56.6167821Z\"&#125;&#123;\"log\":\"t=2021-08-18T09:08:00+0000 lvl=info msg=\\\"Request Completed\\\" logger=context userId=1 orgId=1 uname=admin method=GET path=/login status=302 remote_addr=192.168.65.3 time_ms=11 size=24 referer=\\n\",\"stream\":\"stdout\",\"time\":\"2021-08-18T09:08:00.7292107Z\"&#125;&#123;\"log\":\"t=2021-08-18T09:09:50+0000 lvl=info msg=\\\"Request Completed\\\" logger=context userId=1 orgId=1 uname=admin method=GET path=/api/datasources/proxy/1/loki/api/v1/query_range status=400 remote_addr=192.168.65.3 time_ms=2 size=57 referer=\\\"http://localhost:3000/dashboard/new?editPanel=2\\u0026orgId=1\\\"\\n\",\"stream\":\"stdout\",\"time\":\"2021-08-18T09:09:50.7530735Z\"&#125;&#123;\"log\":\"t=2021-08-18T09:14:53+0000 lvl=eror msg=\\\"Data proxy error\\\" logger=data-proxy-log userId=1 orgId=1 uname=admin path=/api/datasources/proxy/1/loki/api/v1/label/filename/values remote_addr=192.168.65.3 referer=\\\"http://localhost:3000/d/UWO8RT7nk/new-dashboard-copy?editPanel=2\\u0026viewPanel=2\\u0026orgId=1\\\" error=\\\"http: proxy error: EOF\\\"\\n\",\"stream\":\"stdout\",\"time\":\"2021-08-18T09:14:53.0497419Z\"&#125;&#123;\"log\":\"t=2021-08-18T09:14:53+0000 lvl=eror msg=\\\"Request Completed\\\" logger=context userId=1 orgId=1 uname=admin method=GET path=/api/datasources/proxy/1/loki/api/v1/label/filename/values status=502 remote_addr=192.168.65.3 time_ms=79695 size=0 referer=\\\"http://localhost:3000/d/UWO8RT7nk/new-dashboard-copy?editPanel=2\\u0026viewPanel=2\\u0026orgId=1\\\"\\n\",\"stream\":\"stdout\",\"time\":\"2021-08-18T09:14:53.0502414Z\"&#125; 现在，我们知道了日志是存储在这里了，那么我们就可以知道，在Docker VM中，我们只需要挂载/var/log/pods到我们的容器promtail中，然后进行采集再推送到loki，就可以通过grafna查询了。 这个思路是没问题的，那么我们再往深程度的角度想，那么这个时候，我们的k8s-pv-type应该填什么呢？刚才不是说了hostPath是可以持久化到本地吗？但是那是针对物理主机macos来说的，况且这个目录，我们并非在Docker VM之下，这就回到了我们上面说的，在Docker VM存在，在物理主机不存在的需求。 那么我们这个时候，其实还是可以使用hostPath的，理由很简单，因为k8s会识别路径，如果是在共享目录下的话，那么他会从共享目录的数据卷中找到对应的磁盘路径，如果不在共享目录下的，则从Docker VM中查找，因此，这就是解开了我们这个疑惑了。可以大胆的放心使用hostPath来创建pv资源。 最后，附上一张最终的效果图：","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://blog.crazylaw.cn/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://blog.crazylaw.cn/tags/kubernetes/"}]},{"title":"【kubernetes】k8s-glp","slug":"k8s/k8s-glp","date":"2021-08-16T14:45:30.000Z","updated":"2021-08-19T02:58:22.026Z","comments":true,"path":"2021/08/16/k8s/k8s-glp/","link":"","permalink":"http://blog.crazylaw.cn/2021/08/16/k8s/k8s-glp/","excerpt":"前言我们经常有一些日志采集的需求，采集完毕之后，希望有一个中心WebUi来方便的查看很多不同节点，不同服务的日志。 按照传统的方式，一般都是会采用ELK,就是elasticsearch+logstash+kibana，但是由于JVM对资源的消耗太大，加上ES是通过全文搜索的方式需要进行倒排索引的分词，所以这些功能几乎是用不上，我们查询日志一般都可以通过定制常规的label信息，然后搜索即可，大可不必进行分词的行为。尽管后续又由于logstash的资源占用过大问题，作者又利用go语言开发出了filebeat，来辅助日志采集体系，后续加入了某公司之后，被集成到了beats的项目中，因为也可以交efk/ebk，都可以。 鉴于这一点，随之而来的就是GLP，就是grafna+loki+promtail。这是一套完全基于go语言生态写的，更贴近云原生。一套体系都是经过grafna lab云原生孕育而生。资源占用少，效率高，能够解决痛点，天生支持k8s等等特性。都让他成为新的崛起之秀。","text":"前言我们经常有一些日志采集的需求，采集完毕之后，希望有一个中心WebUi来方便的查看很多不同节点，不同服务的日志。 按照传统的方式，一般都是会采用ELK,就是elasticsearch+logstash+kibana，但是由于JVM对资源的消耗太大，加上ES是通过全文搜索的方式需要进行倒排索引的分词，所以这些功能几乎是用不上，我们查询日志一般都可以通过定制常规的label信息，然后搜索即可，大可不必进行分词的行为。尽管后续又由于logstash的资源占用过大问题，作者又利用go语言开发出了filebeat，来辅助日志采集体系，后续加入了某公司之后，被集成到了beats的项目中，因为也可以交efk/ebk，都可以。 鉴于这一点，随之而来的就是GLP，就是grafna+loki+promtail。这是一套完全基于go语言生态写的，更贴近云原生。一套体系都是经过grafna lab云原生孕育而生。资源占用少，效率高，能够解决痛点，天生支持k8s等等特性。都让他成为新的崛起之秀。 Kubernetes Logs 默认情况下，容器日志会存储在 /var/log/pods 路径下。 每个文件夹对应一个 Pod，Pod 下级目录为容器名，再下级即为容器日志。 12345678tree kube-system_kube-flannel-ds-amd64-9x66j_28e71490-d614-4cd8-9ea7-af23cc7b9bff/kube-system_kube-flannel-ds-amd64-9x66j_28e71490-d614-4cd8-9ea7-af23cc7b9bff/├── install-cni│ └── 3.log -&gt; /data/docker/containers/6accaa2d6890df8ca05d1f40aaa9b8da69ea0a00a8e4b07a0949cdc067843e37/6accaa2d6890df8ca05d1f40aaa9b8da69ea0a00a8e4b07a0949cdc067843e37-json.log└── kube-flannel ├── 2.log -&gt; /data/docker/containers/9e8eea717cc3efd0804900a53244a32286d9e04767f76d9c8a8cc3701c83ece5/9e8eea717cc3efd0804900a53244a32286d9e04767f76d9c8a8cc3701c83ece5-json.log └── 3.log -&gt; /data/docker/containers/06389981d26cbe60328cd5a46af7b003c8d687d1c411704784aa12d4d82672b8/06389981d26cbe60328cd5a46af7b003c8d687d1c411704784aa12d4d82672b8-json.log 日志文件 kube-flannel/3.log 只是对 /var/lib/docker/containers/***/***.log 文件的软链接，本质上还是 Docker 维护日志， k8s 对其引用而已。 日志是 JSON 格式的，每一行包含如下三个信息： log：日志内容 stream：stderr(异常输出)、stdout(正常输出) time：时间 /var/lib/docker/containers 是通过 /etc/docker/daemon.json 配置的，并且也是默认路径。 grafna由于k8s的网络架构的原因，我们访问的时候都是通过访问service的名字的，和docker-compose下的访问方式不太一样。 例如. 123456➜ whiteccinn.github.io git:(master) ✗ kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEdev-grafana LoadBalancer 10.102.248.247 localhost 3000:32695/TCP 17hdev-loki ClusterIP 10.106.32.224 &lt;none&gt; 3100/TCP 17hdev-promtail ClusterIP 10.108.116.190 &lt;none&gt; 9080/TCP 17hkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 3d9h 那么，容器中的访问方式就是通过dev-loki, dev-promtail, dev-grafna来对pod进行访问，service的port再映射到对应的容器的port上 depployment1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556apiVersion: apps/v1kind: Deploymentmetadata: labels: app: grafana name: grafanaspec: selector: matchLabels: app: grafana template: metadata: labels: app: grafana spec: securityContext: fsGroup: 472 supplementalGroups: - 0 containers: - name: grafana image: grafana/grafana:7.5.2 imagePullPolicy: IfNotPresent ports: - containerPort: 3000 name: http-grafana protocol: TCP readinessProbe: failureThreshold: 3 httpGet: path: /robots.txt port: 3000 scheme: HTTP initialDelaySeconds: 10 periodSeconds: 30 successThreshold: 1 timeoutSeconds: 2 livenessProbe: failureThreshold: 3 initialDelaySeconds: 30 periodSeconds: 10 successThreshold: 1 tcpSocket: port: 3000 timeoutSeconds: 1 resources: requests: cpu: 250m memory: 750Mi volumeMounts: - mountPath: /var/lib/grafana name: grafana-pv volumes: - name: grafana-pv persistentVolumeClaim: claimName: grafana-pvc pvc12345678910apiVersion: v1kind: PersistentVolumeClaimmetadata: name: grafana-pvcspec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi service12345678910111213apiVersion: v1kind: Servicemetadata: name: grafanaspec: ports: - port: 3000 protocol: TCP targetPort: http-grafana selector: app: grafana sessionAffinity: None type: LoadBalancer lokiconfig-map12345678910111213141516171819202122232425262728293031323334353637383940414243apiVersion: v1kind: ConfigMapmetadata: name: loki-configdata: loki-config.yml: | auth_enabled: false server: http_listen_port: 3100 ingester: lifecycler: address: 127.0.0.1 ring: kvstore: store: inmemory replication_factor: 1 final_sleep: 0s chunk_idle_period: 5m chunk_retain_period: 30s schema_config: configs: - from: 2020-05-15 store: boltdb object_store: filesystem schema: v11 index: prefix: index_ period: 168h storage_config: boltdb: directory: /tmp/loki/index filesystem: directory: /tmp/loki/chunks limits_config: enforce_metric_name: false reject_old_samples: true reject_old_samples_max_age: 168h pvc12345678910apiVersion: v1kind: PersistentVolumeClaimmetadata: name: loki-pvcspec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi service12345678910kind: ServiceapiVersion: v1metadata: name: lokispec: ports: - port: 3100 targetPort: http-loki selector: app: loki statefulet123456789101112131415161718192021222324252627282930313233343536373839404142apiVersion: apps/v1kind: StatefulSetmetadata: name: loki-statefulsetspec: selector: matchLabels: app: loki replicas: 2 serviceName: loki template: metadata: labels: app: loki spec: containers: - name: loki image: grafana/loki:2.3.0 imagePullPolicy: IfNotPresent args: - -config.file=/mnt/config/loki-config.yml ports: - containerPort: 3100 name: http-loki volumeMounts: - mountPath: /tmp/loki name: storage-volume - mountPath: /mnt/config name: config-volume securityContext: runAsUser: 0 runAsGroup: 0 volumes: - name: storage-volume persistentVolumeClaim: claimName: loki-pvc - name: config-volume configMap: name: loki-config items: - key: loki-config.yml path: loki-config.yml promtailconfig-map1234567891011121314151617181920212223242526272829303132333435363738394041apiVersion: v1kind: ConfigMapmetadata: name: promtail-config namespace: defaultdata: promtail-config.yml: | server: http_listen_port: 9080 grpc_listen_port: 0 positions: filename: /tmp/positions.yaml # clients: # - url: http://dev-loki:3100/loki/api/v1/push scrape_configs: - job_name: containers static_configs: - targets: - localhost labels: log_from: static_pods __path__: /var/log/pods/*/*/*.log pipeline_stages: - docker: &#123;&#125; - match: selector: '&#123;log_from=\"static_pods\"&#125;' stages: - regex: source: filename expression: \"(?:pods)/(?P&lt;namespace&gt;\\\\S+?)_(?P&lt;pod&gt;\\\\S+)-\\\\S+?-\\\\S+?_\\\\S+?/(?P&lt;container&gt;\\\\S+?)/\" - labels: namespace: pod: container: - match: selector: '&#123;namespace!~\"(default|kube-system)\"&#125;' action: drop drop_counter_reason: no_use daemonest1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556apiVersion: apps/v1kind: DaemonSetmetadata: name: promtailspec: selector: matchLabels: app: promtail template: metadata: labels: app: promtail spec: containers: - name: promtail image: grafana/promtail:2.3.0 imagePullPolicy: IfNotPresent args: - -config.file=/mnt/config/promtail-config.yml - -client.url=http://dev-loki:3100/loki/api/v1/push - -client.external-labels=hostname=$(NODE_NAME) ports: - containerPort: 9080 name: http-promtail volumeMounts: - mountPath: /var/lib/docker/containers name: containers-volume - mountPath: /var/log/pods name: pods-volume - mountPath: /mnt/config name: config-volume env: - name: NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName securityContext: runAsUser: 0 runAsGroup: 0 volumes: - name: containers-volume hostPath: path: /var/lib/docker/containers - name: pods-volume hostPath: path: /var/log/pods - name: config-volume configMap: name: promtail-config items: - key: promtail-config.yml path: promtail-config.yml tolerations: - key: node-role.kubernetes.io/master operator: Exists effect: NoSchedule 注意：上述提到 /var/log/pods 下的日志只是对 /var/lib/docker/containers 下日志的软链接，所以 Promtail 部署时需要同时挂载这两个目录。 service12345678910kind: ServiceapiVersion: v1metadata: name: promtailspec: ports: - port: 9080 targetPort: http-promtail selector: app: promtail 这些详情的参数就不解释的，这就是一整套GLP\u001d的k8s的部署文件。由于我这里是采用kustomize来部署的。所以会有多层结构。 123456789101112131415161718192021222324252627➜ kustomize git:(main) tree.├── base│ ├── grafna│ │ ├── deployment.yaml│ │ ├── kustomization.yml│ │ ├── pvc.yaml│ │ └── service.yaml│ ├── kustomization.yml│ ├── loki│ │ ├── config-map.yaml│ │ ├── kustomization.yml│ │ ├── pvc.yaml│ │ ├── service.yaml│ │ └── statefulset.yaml│ └── promtail│ ├── config-map.yaml│ ├── daemonset.yaml│ ├── kustomization.yml│ └── service.yaml└── overlays ├── dev │ ├── kustomization.yml │ └── patch.yaml └── prod ├── kustomization.yml └── patch.yaml 一整套的运行就是: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383➜ kustomize git:(main) kustomize build overlays/devapiVersion: v1data: promtail-config.yml: | server: http_listen_port: 9080 grpc_listen_port: 0 positions: filename: /tmp/positions.yaml # clients: # - url: http://dev-loki:3100/loki/api/v1/push scrape_configs: - job_name: containers static_configs: - targets: - localhost labels: log_from: static_pods __path__: /var/log/pods/*/*/*.log pipeline_stages: - docker: &#123;&#125; - match: selector: '&#123;log_from=\"static_pods\"&#125;' stages: - regex: source: filename expression: \"(?:pods)/(?P&lt;namespace&gt;\\\\S+?)_(?P&lt;pod&gt;\\\\S+)-\\\\S+?-\\\\S+?_\\\\S+?/(?P&lt;container&gt;\\\\S+?)/\" - labels: namespace: pod: container: - match: selector: '&#123;namespace!~\"(default|kube-system)\"&#125;' action: drop drop_counter_reason: no_usekind: ConfigMapmetadata: annotations: note: Hello, I am dev! labels: app: amyris org: unknow-x variant: dev name: dev-promtail-config namespace: default---apiVersion: v1data: loki-config.yml: | auth_enabled: false server: http_listen_port: 3100 ingester: lifecycler: address: 127.0.0.1 ring: kvstore: store: inmemory replication_factor: 1 final_sleep: 0s chunk_idle_period: 5m chunk_retain_period: 30s schema_config: configs: - from: 2020-05-15 store: boltdb object_store: filesystem schema: v11 index: prefix: index_ period: 168h storage_config: boltdb: directory: /tmp/loki/index filesystem: directory: /tmp/loki/chunks limits_config: enforce_metric_name: false reject_old_samples: true reject_old_samples_max_age: 168hkind: ConfigMapmetadata: annotations: note: Hello, I am dev! labels: app: amyris org: unknow-x variant: dev name: dev-loki-config---apiVersion: v1kind: Servicemetadata: annotations: note: Hello, I am dev! labels: app: amyris org: unknow-x variant: dev name: dev-grafanaspec: ports: - port: 3000 protocol: TCP targetPort: http-grafana selector: app: amyris org: unknow-x variant: dev sessionAffinity: None type: LoadBalancer---apiVersion: v1kind: Servicemetadata: annotations: note: Hello, I am dev! labels: app: amyris org: unknow-x variant: dev name: dev-lokispec: ports: - port: 3100 targetPort: http-loki selector: app: amyris org: unknow-x variant: dev---apiVersion: v1kind: Servicemetadata: annotations: note: Hello, I am dev! labels: app: amyris org: unknow-x variant: dev name: dev-promtailspec: ports: - port: 9080 targetPort: http-promtail selector: app: amyris org: unknow-x variant: dev---apiVersion: v1kind: PersistentVolumeClaimmetadata: annotations: note: Hello, I am dev! labels: app: amyris org: unknow-x variant: dev name: dev-grafana-pvcspec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi---apiVersion: v1kind: PersistentVolumeClaimmetadata: annotations: note: Hello, I am dev! labels: app: amyris org: unknow-x variant: dev name: dev-loki-pvcspec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi---apiVersion: apps/v1kind: Deploymentmetadata: annotations: note: Hello, I am dev! labels: app: amyris org: unknow-x variant: dev name: dev-grafanaspec: selector: matchLabels: app: amyris org: unknow-x variant: dev template: metadata: annotations: note: Hello, I am dev! labels: app: amyris org: unknow-x variant: dev spec: containers: - image: grafana/grafana:7.5.2 imagePullPolicy: IfNotPresent livenessProbe: failureThreshold: 3 initialDelaySeconds: 30 periodSeconds: 10 successThreshold: 1 tcpSocket: port: 3000 timeoutSeconds: 1 name: grafana ports: - containerPort: 3000 name: http-grafana protocol: TCP readinessProbe: failureThreshold: 3 httpGet: path: /robots.txt port: 3000 scheme: HTTP initialDelaySeconds: 10 periodSeconds: 30 successThreshold: 1 timeoutSeconds: 2 resources: requests: cpu: 250m memory: 750Mi volumeMounts: - mountPath: /var/lib/grafana name: grafana-pv securityContext: fsGroup: 472 supplementalGroups: - 0 volumes: - name: grafana-pv persistentVolumeClaim: claimName: dev-grafana-pvc---apiVersion: apps/v1kind: StatefulSetmetadata: annotations: note: Hello, I am dev! labels: app: amyris org: unknow-x variant: dev name: dev-loki-statefulsetspec: replicas: 2 selector: matchLabels: app: amyris org: unknow-x variant: dev serviceName: dev-loki template: metadata: annotations: note: Hello, I am dev! labels: app: amyris org: unknow-x variant: dev spec: containers: - args: - -config.file=/mnt/config/loki-config.yml image: grafana/loki:2.3.0 imagePullPolicy: IfNotPresent name: loki ports: - containerPort: 3100 name: http-loki securityContext: runAsGroup: 0 runAsUser: 0 volumeMounts: - mountPath: /tmp/loki name: storage-volume - mountPath: /mnt/config name: config-volume volumes: - name: storage-volume persistentVolumeClaim: claimName: dev-loki-pvc - configMap: items: - key: loki-config.yml path: loki-config.yml name: dev-loki-config name: config-volume---apiVersion: apps/v1kind: DaemonSetmetadata: annotations: note: Hello, I am dev! labels: app: amyris org: unknow-x variant: dev name: dev-promtailspec: selector: matchLabels: app: amyris org: unknow-x variant: dev template: metadata: annotations: note: Hello, I am dev! labels: app: amyris org: unknow-x variant: dev spec: containers: - args: - -config.file=/mnt/config/promtail-config.yml - -client.url=http://dev-loki:3100/loki/api/v1/push - -client.external-labels=hostname=$(NODE_NAME) env: - name: NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName image: grafana/promtail:2.3.0 imagePullPolicy: IfNotPresent name: promtail ports: - containerPort: 9080 name: http-promtail securityContext: runAsGroup: 0 runAsUser: 0 volumeMounts: - mountPath: /var/lib/docker/containers name: containers-volume - mountPath: /var/log/pods name: pods-volume - mountPath: /mnt/config name: config-volume tolerations: - effect: NoSchedule key: node-role.kubernetes.io/master operator: Exists volumes: - hostPath: path: /var/lib/docker/containers name: containers-volume - hostPath: path: /var/log/pods name: pods-volume - configMap: items: - key: promtail-config.yml path: promtail-config.yml name: dev-promtail-config name: config-volume 采用 kustomize build overlays/dev | kubectl apply -f - 来运行我们的dev环境的k8s所有的服务。 通过kubectl port-forward deployment.apps/dev-grafana 3000:3000 来做端口的转发。 123➜ whiteccinn.github.io git:(master) ✗ kubectl port-forward deployment.apps/dev-grafana 3000:3000Forwarding from 127.0.0.1:3000 -&gt; 3000Forwarding from [::1]:3000 -&gt; 3000 通过 kubectl get svc 查看端口转发情况。 123456➜ whiteccinn.github.io git:(master) ✗ kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEdev-grafana LoadBalancer 10.102.248.247 localhost 3000:32695/TCP 17hdev-loki ClusterIP 10.106.32.224 &lt;none&gt; 3100/TCP 17hdev-promtail ClusterIP 10.108.116.190 &lt;none&gt; 9080/TCP 17hkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 3d10h 最终通过命令 kubectl get pods -o wide 看到所有的pods都在正常运作了。 123456➜ whiteccinn.github.io git:(master) ✗ kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESdev-grafana-7cd4c89fd4-wdkpb 1/1 Running 0 17h 10.1.0.16 docker-desktop &lt;none&gt; &lt;none&gt;dev-loki-statefulset-0 1/1 Running 0 17h 10.1.0.18 docker-desktop &lt;none&gt; &lt;none&gt;dev-loki-statefulset-1 1/1 Running 0 17h 10.1.0.19 docker-desktop &lt;none&gt; &lt;none&gt;dev-promtail-n6jgs 1/1 Running 0 17h 10.1.0.17 docker-desktop &lt;none&gt; &lt;none&gt; 然后在浏览器打开localhost:3000，即可访问到grafna了。 grafna默认的账号密码就是admin。 1.我们先来配置grafna的Dashboard。 2.对日志进行可视化配置。 3.配置搜索栏。 4.可以看到搜索栏了，并且需要更新一下查询的公式 这里就是我希望利用k8s的glp来采集我的所有的pods在标准输出的所有的日志信息，做一个汇总和日志中心查询的web-ui。","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://blog.crazylaw.cn/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://blog.crazylaw.cn/tags/kubernetes/"}]},{"title":"【kubernetes】k8s-pv-nfs-for-mac","slug":"k8s/k8s-mac-pv-nfs","date":"2021-08-16T06:18:30.000Z","updated":"2021-08-16T07:06:32.263Z","comments":true,"path":"2021/08/16/k8s/k8s-mac-pv-nfs/","link":"","permalink":"http://blog.crazylaw.cn/2021/08/16/k8s/k8s-mac-pv-nfs/","excerpt":"前言docker-for-mac 现在已经内置了k8s，我们可以轻松的开启这个功能，然后就可以通过kubectl来执行我们的k8s的命令来对容器资源进行管理。 但是有一个比较头疼的点，那就是我们的pv资源。我们平时用docker的时候，有一些信息例如，日志之类的，需要持久化下来，这个时候，这个日志文件持久化会和容器所在的物理机在同一个机器下，因此，并不能很好的做到“存”，“算”分离的目的。并且没办法利用了网络上的其他资源。 再者就是还有一种情况就是，我需要利用k8s部署一些基础服务，例如mysql，例如redis，这些基础数据都是需要持久化的，因此，和物理机器强行绑定在一起的话，下次数据在哪个数据卷都不清楚了，更没办法重新回复数据，这是一个很严重的问题。 所以我们这里的pv，不能简单的使用hostPath或者local类型，根据服务器上用的比较多的或许是自己搭建一个nfs 服务器，因此，我们也希望在本地开发的时候定义资源文件的yaml也是用nfs来作为我们的pv-type。 但是也因此，我们需要在mac上开启一个nfs-server。尝试过docker-nfs-server，但是由于mac系统的架构问题，无法顺利的运行起来，需要处理modpre模块，处理那么多内核的东西不太合理。","text":"前言docker-for-mac 现在已经内置了k8s，我们可以轻松的开启这个功能，然后就可以通过kubectl来执行我们的k8s的命令来对容器资源进行管理。 但是有一个比较头疼的点，那就是我们的pv资源。我们平时用docker的时候，有一些信息例如，日志之类的，需要持久化下来，这个时候，这个日志文件持久化会和容器所在的物理机在同一个机器下，因此，并不能很好的做到“存”，“算”分离的目的。并且没办法利用了网络上的其他资源。 再者就是还有一种情况就是，我需要利用k8s部署一些基础服务，例如mysql，例如redis，这些基础数据都是需要持久化的，因此，和物理机器强行绑定在一起的话，下次数据在哪个数据卷都不清楚了，更没办法重新回复数据，这是一个很严重的问题。 所以我们这里的pv，不能简单的使用hostPath或者local类型，根据服务器上用的比较多的或许是自己搭建一个nfs 服务器，因此，我们也希望在本地开发的时候定义资源文件的yaml也是用nfs来作为我们的pv-type。 但是也因此，我们需要在mac上开启一个nfs-server。尝试过docker-nfs-server，但是由于mac系统的架构问题，无法顺利的运行起来，需要处理modpre模块，处理那么多内核的东西不太合理。 built-in-nfs后来查阅资料，发现了原来我们的macos，内置了nfs，我们只需要添加对应的配置和开启服务即可，十分的方便。 配置文件这个文件默认是不存在的，需要我们手动去创建和添加里面的内容，不熟悉nfs的朋友需要去看一下nfs的配置文件。 123sudo vim /etc/exports## 追加以下文件到文件中/Users/caiwenhui/nas_a -alldirs -maproot=caiwenhui:staff 其中的含义是: /Users/caiwenhui/nas_a 指定共享目录 -alldirs 共享目录下的所有目录 -maproot 把client端的caiwenhui用户映射为MacOS上的root，client端的staff组映射为MacOS上的wheel (gid=0) 组 检查配置是否正确1sudo nfsd checkexports 如果都正确的话，默认什么都不会输出。如果存在问题的话，则会弹出错误配置信息。 /etc/nfs.conf如果k8s需要运用nfs，还需要添加一行： 1nfs.server.mount.require_resv_port &#x3D; 0 服务命令123456sudo nfsd start # 启动服务sudo nfsd stop # 停止服务sudo nfsd restart # 重启服务sudo nfsd status # 查看状态sudo nfsd enable # 开机自启sudo nfsd disable # 禁止开机自启 查看配置是否生效showmount -e 123➜ ~ showmount -eExports list on localhost:&#x2F;Users&#x2F;caiwenhui&#x2F;nas_a Everyone 这里，可以看到，我们挂在的数据卷已经生效，并且是everyone(任何网段，任何用户)都可以进行读写。因为是本地开发，所以这么设置最方便，如果是服务器上的，就需要针对特定的网段或者ip以及user了。详情查看nfs配置。 本地测试 这一步你大可不必测试，因为是第一次，所以我想先确保nfs的服务的正常。 12345## 创建client角色的目录，就是被挂载的目录mkdir /Users/caiwenhui/nas_a2## 模拟client挂载nfs数据卷命令sudo mount -t nfs -o nolock,nfsvers=3,vers=3 127.0.0.1:/Users/caiwenhui/nas_a /Users/caiwenhui/nas_a2 这里的意思是： 挂载类型：nfs 采用nfs-v3协议: nfsvers=3,vers=3 nfs-server的目录: 127.0.0.1:/Users/caiwenhui/nas_a （这个就是我们showmount -e查看到的路径） nfs-client的目录：/Users/caiwenhui/nas_a2 挂载成功之后，尝试在client目录添加文件 1touch /Users/caiwenhui/nas_a2/hello 查看server目录是否有文件同步 12ls /Users/caiwenhui/nas_ahello 这个时候会发现nas_a目录下，自动多了一个hello的文件 取消挂在，可以在finder中直接弹出挂载，或者使用如下命令即可。 1umount /Users/caiwenhui/nas_a2 这个时候你再去看 /Users/caiwenhui/nas_a2 目录下，hello文件不见了，但是/Users/caiwenhui/nas_a/hello依旧存在。 k8s中测试nfsNFS PersistentVolume前面我们已经在mac上启动了一个NFS服务，现在通过在k8s上创建一个PersistentVolume来使用NFS。 创建一个PV，编辑配置文件nfs-pv-1.yaml，内容如下： 123456789101112131415161718➜ ~ cat nfs-pv-1.yamlapiVersion: v1kind: PersistentVolumemetadata: name: nfspv1spec: mountOptions: - nfsvers=3 - nolock capacity: storage: 2Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle storageClassName: nfs nfs: path: /Users/caiwenhui/nas_a server: docker.for.mac.host.internal nfs相关的配置： nfsvers=3 使用v3协议 路径就是nfs上的目录 注意这里，我们的服务器ip，如果不能明确的目前现在的网络通信情况的下，请使用docker.for.mac.host.internal \b申请资源PV。 1kubectl apply -f nfs-pv-1.yaml 查看资源状态。 123➜ ~ kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEnfspv1 2Gi RWO Recycle Available nfs 13h STATUS为Available表示nfspv1就绪，可以被PVC申请。 PersistentVolumeClaim下面创建PVC，编辑nfs-pvc-1.yaml文件 123456789101112➜ ~ cat nfs-pvc-1.yamlapiVersion: v1kind: PersistentVolumeClaimmetadata: name: nfspvc1 spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: nfs 申请资源PVC。 1kubectl apply -f nfs-pvc-1.yaml 查看pvc状态/查看pv状态。 1234567➜ ~ kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEnfspvc1 Bound nfspv1 10Gi RWO nfs 85s➜ ~ kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEnfspv1 2Gi RWO Recycle Bound default/nfspvc1 nfs 13h 这里，我们可以看到 status = bound，代表此时pv和pvc已经绑定在一起了。这样子我们就可以把pod和pvc绑定。 Pod编辑pod1.yaml 1234567891011121314151617181920➜ ~ cat pod1.yamlapiVersion: v1kind: Podmetadata: name: mypod1spec: containers: - name: mypod1 image: busybox args: - /bin/sh - -c - sleep 30000 volumeMounts: - mountPath: \"/mydata\" name: mydata volumes: - name: mydata persistentVolumeClaim: claimName: nfspvc1 申请资源pod 1kubectl apply -f pod1.yaml 查看pod状态 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253➜ ~ kubectl describe podName: mypod1Namespace: defaultPriority: 0Node: docker-desktop&#x2F;192.168.65.4Start Time: Mon, 16 Aug 2021 01:07:01 +0800Labels: &lt;none&gt;Annotations: &lt;none&gt;Status: RunningIP: 10.1.0.6IPs: IP: 10.1.0.6Containers: mypod1: Container ID: docker:&#x2F;&#x2F;0351e0c89fb94a3a1c8888939c641fc7b9b48d1f915f3653ca0ac188c50ae242 Image: busybox Image ID: docker-pullable:&#x2F;&#x2F;busybox@sha256:0f354ec1728d9ff32edcd7d1b8bbdfc798277ad36120dc3dc683be44524c8b60 Port: &lt;none&gt; Host Port: &lt;none&gt; Args: &#x2F;bin&#x2F;sh -c sleep 30000 State: Running Started: Mon, 16 Aug 2021 01:07:08 +0800 Ready: True Restart Count: 0 Environment: &lt;none&gt; Mounts: &#x2F;mydata from mydata (rw) &#x2F;var&#x2F;run&#x2F;secrets&#x2F;kubernetes.io&#x2F;serviceaccount from kube-api-access-pftwg (ro)Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled TrueVolumes: mydata: Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace) ClaimName: nfspvc1 ReadOnly: false kube-api-access-pftwg: Type: Projected (a volume that contains injected data from multiple sources) TokenExpirationSeconds: 3607 ConfigMapName: kube-root-ca.crt ConfigMapOptional: &lt;nil&gt; DownwardAPI: trueQoS Class: BestEffortNode-Selectors: &lt;none&gt;Tolerations: node.kubernetes.io&#x2F;not-ready:NoExecute op&#x3D;Exists for 300s node.kubernetes.io&#x2F;unreachable:NoExecute op&#x3D;Exists for 300sEvents: &lt;none&gt; 我们看到Mounts相关的内容已经挂载正确，并且容器也正确在运行了。我们对挂载好的数据卷操作。 1kubectl exec mypod1 touch /mydata/hello2 这个时候我们在本地的nfs-server的目录/Users/caiwenhui/nas_a 查看一下是否多了hello2的文件 12ls /Users/caiwenhui/nas_ahello hello2 这个时候，我们看到，hello2被成功创建，pod能正确的访问的pv挂载链接本地nfs-server了。","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://blog.crazylaw.cn/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://blog.crazylaw.cn/tags/kubernetes/"}]},{"title":"【DevOps】自定义git凭据存取器","slug":"DevOps/自定义git凭据存取器","date":"2021-08-02T06:35:30.000Z","updated":"2021-08-02T07:09:57.437Z","comments":true,"path":"2021/08/02/DevOps/自定义git凭据存取器/","link":"","permalink":"http://blog.crazylaw.cn/2021/08/02/DevOps/%E8%87%AA%E5%AE%9A%E4%B9%89git%E5%87%AD%E6%8D%AE%E5%AD%98%E5%8F%96%E5%99%A8/","excerpt":"前言最近，在处理公司到代码仓库，公司由gitbucket迁移到gitlab后，业务项目拉取私有vcs的代码依赖包的方式发生了改变。 以前可能是一个“权限较大”的用户，拥有多个项目组的访问权限，所以可以访问私有的vcs的代码。 但是迁移到gitlab之后，每个group下，owner都可以针对这个group生成对应都deploy-key，所以变成了一个group下一个deploy-key。 这就意味着，我们都go/php/node项目，在拉取不同项目都依赖包都时候，需要把git-credential的账号信息切换。","text":"前言最近，在处理公司到代码仓库，公司由gitbucket迁移到gitlab后，业务项目拉取私有vcs的代码依赖包的方式发生了改变。 以前可能是一个“权限较大”的用户，拥有多个项目组的访问权限，所以可以访问私有的vcs的代码。 但是迁移到gitlab之后，每个group下，owner都可以针对这个group生成对应都deploy-key，所以变成了一个group下一个deploy-key。 这就意味着，我们都go/php/node项目，在拉取不同项目都依赖包都时候，需要把git-credential的账号信息切换。 git-build-inGit 有一个内部接口,用于存储和检索系统特定助手的证书,以及提示用户输入用户名和密码。git-credential 命令向脚本开放了这个接口,脚本可以像 Git 一样检索、存储或提示用户输入凭证。这个可脚本接口的设计与内部的 C API 一样,请参见 credential.h 以了解更多的概念背景。 git-credential在命令行上使用“操作”选项（ fill ， approve 或 reject 之一），并在stdin上读取凭据描述（请参阅INPUT / OUTPUT FORMAT）。 如果操作为 fill ，则git-credential将尝试通过读取配置文件，联系任何已配置的凭据帮助程序或提示用户来向描述中添加“用户名”和“密码”属性。然后将凭证描述的用户名和密码属性与已经提供的属性一起打印到stdout。 如果操作被 approve ，则git-credential会将描述发送给任何已配置的凭据帮助器，该帮助器可以存储该凭据以供以后使用。 如果该操作被 reject ，则git-credential会将描述发送给任何已配置的凭据帮助器，这些帮助器可能会删除所有与该描述匹配的存储凭据。 如果操作是 approve 或 reject ，则不应发出任何输出。 Git我们知道，在window和mac下，分别对应的多凭据管理器，分别是git credential for window 和 oschinakey，可以做到精确的记录下我们的所有凭据。但是在linux下就只有build-in(内置)的存取器，分别是 cache, store 。 经过实现，在.gitconfig默认的配置的情况下，每次都会只读取 .git-credential 的第一行数据，如果不正确，则触发 git 的 build-in 的 reject 方法，清理掉这一行的凭据，这使得我们在多凭据下无法正常的工作。我们需要.git-credentital能根据vcs的地址特点，例如根据group来识别凭据。为了实现这一点，我们就需要借助自定义存取器。 自定义存储器要实现自定义存储器，就需要知道他是什么东西，和怎么实现。 自定义存储器允许你用任何语言来编写，你用c/c++/python/php/java/go/rust/erlang等等的语言写都是可以的，只需要程序能直接运行，并且符合输入/输出格式和在PATH的系统环境下能找到的情况下，都是可行的。我记得git-scm的例子采用的是ruby的写法，但是由于考虑到python2一般是每个系统都自带的，我们也采用了python2的写法来实现一个根据组织来智能区分git账号和密码的自定义存取器 这里，我们可以记住，把自定义存储器想像成一个管道(pipe)的概念，有输入端，也有输出端，他们都有自己对应的规则（协议/格式）。 输入/输出格式git credential 在其标准输入/输出中读取或写入（取决于使用的操作）凭证信息。此信息可以对应于 git credential 将为其获取登录信息的密钥（例如主机，协议，路径），也可以对应于将要获取的实际凭证数据（用户名/密码）。 凭证分为一组命名属性，每行一个属性。每个属性均由键值对指定，并以 = （等号）分隔，后跟换行符。 密钥可以包含除 = ，换行符或NUL之外的任何字节。该值可以包含除换行符或NUL之外的任何字节。 在这两种情况下,所有的字节都按原样处理(即没有引号,也不能传输带有换行或NUL的值)。属性列表以空行或文件末尾结束。 Git了解以下属性。 protocol 将使用凭证的协议（例如 https ）。 host 网络凭证的远程主机名,包括指定的端口号(如 “example.com:8088”)。如果指定了端口号,则包括端口号(例如 “example.com:8088”)。 path 凭据将使用的路径。例如，对于访问远程https资源库，这将是服务器上资源库的路径。(只有开启了useHttpPath=true的情况下，输入格式才会携带这个参数) username 凭据的用户名（如果已经有）（例如，来自URL，配置，用户或先前运行的帮助程序）。 password 凭据的密码（如果我们要求将其存储）。 url 当 git credential 读取此特殊属性时，该值将解析为URL，并被视为已读取其组成部分（例如， url=https://example.com 的行为就好像 protocol=https 和 host=example.com 已提供）。这可以帮助呼叫者避免自己解析URL。 请注意，指定协议是强制性的，并且如果URL未指定主机名（例如，“ cert：/// path / to / file”），则凭证将包含其主机名属性，其值为空字符串。 实战了解了输入/输出格式后。我们通过实战例子来说明。 1234# git-credential.examplehttps://caiwenhui:testpass@gitlab.mingchao.com/group1https://caiwenhui:testpass2@gitlab.mingchao.com/group2https://caiwenhui:testpass3@gitlab.mingchao.com/group3 以下文件，命名为 git-credential-mc 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172#!/usr/bin/env python# -*- coding: utf-8 -*-# @Date : 2021-07-30 10:58:53# @Author : caiwenhui# @Version : 1.0import osimport reimport sysimport argparseparser = argparse.ArgumentParser(description=\"get credentials by https://gitlab.mimgchao.com/&#123;group&#125;\", prog='git-credential-mc', usage='%(prog)s [options] &lt;action&gt;')parser.add_argument('-f', '--file', help='Specify path for backing store', required=True, default=\"~/.git-credentials\", type=str)parser.add_argument('action', metavar=\"action\", help='just support &lt;get&gt;')class CredentialsHelper: def __init__(self, credential_file=''): self.inputs = dict() self.file = credential_file def _input(self): while True: line = sys.stdin.readline() if line.strip() == '': break k, v = line.strip().split('=', 2) self.inputs[k] = v # 需要开启useHttpPath = true if self.inputs['path'] is None: sys.exit(1) # 解析path参数 self.inputs['group'] = self.inputs['path'].split('/')[0] def _output(self): with open(self.file, 'r') as f: lines = f.readlines() for line in lines: m = re.match(r'^(?P&lt;protocol&gt;.*?)://(?P&lt;username&gt;.*?):(?P&lt;password&gt;.*?)@(?P&lt;host&gt;.*)/(?P&lt;group&gt;.*)$', line) gd = m.groupdict() if self.inputs['protocol'] == gd['protocol'] and self.inputs['host'] == gd['host'] and self.inputs[ 'group'] == gd['group']: sys.stdout.write('protocol=&#123;protocol&#125;\\n'.format(protocol=gd['protocol'])) sys.stdout.write('host=&#123;host&#125;\\n'.format(host=gd['host'])) sys.stdout.write('username=&#123;username&#125;\\n'.format(username=gd['username'])) sys.stdout.write('password=&#123;password&#125;\\n'.format(password=gd['password'])) break sys.stdout.flush() def execute(self): self._input() self._output()if __name__ == '__main__': if len(sys.argv) &lt;= 1: parser.print_help() sys.exit(0) args = parser.parse_args() if args.action != \"get\": sys.exit(0) if not os.path.exists(args.file): sys.exit(0) credentialsHelper = CredentialsHelper(credential_file=args.file) credentialsHelper.execute() 直接运行下，脚本输出如下： 1234567891011➜ python git:(master) ./git-credential-mcusage: git-credential-mc [options] &lt;action&gt;get credentials by https://gitlab.mimgchao.com/&#123;group&#125;positional arguments: action just support &lt;get&gt;optional arguments: -h, --help show this help message and exit -f FILE, --file FILE Specify path for backing store 我们来模拟git的一个输入过程，然后让自定义存储器输出正确的输出格式告诉git凭据要用的账号密码： 12345678910111213141516171819➜ python git-credential-mc -f git-credential.example getprotocol=httpshost=gitlab.mingchao.compath=group1/repo1.gitprotocol=httpshost=gitlab.mingchao.comusername=caiwenhuipassword=testpass➜ python git-credential-mc -f git-credential.example getprotocol=httpshost=gitlab.mingchao.compath=group2/repo1.gitprotocol=httpshost=gitlab.mingchao.comusername=caiwenhuipassword=testpass2 这里，我们看到，我们根据不同的group，已经返回了不同的账号密码了。达到这个效果，我们的自定义读取器就算是完成了。但是系统化的整理起来，还需要把这个脚本，放在PATH路径下，并且，并且记得必须以git-credential-*来命令程序的文件名。因为git源码的credential模块中的源码读取自定义规则存储器就是这样子调用外部程序的。 配置git12git config --global credential.https:&#x2F;&#x2F;gitlab.mingchao.com.useHttpPath truegit config --global credential.https:&#x2F;&#x2F;gitlab.mingchao.com.helper &quot;mc --file ~&#x2F;.git-credential.example&quot; 这样子，git就可以针对https://gitlab.mingchao.com的时候，采用git-credential-mc的程序来读取凭据。达到我们的多组/多凭据的情况下正确的读取账号和密码。 这里只是一个简单的用法，后续如果有复杂的用法，都可以扩展这个自定义存取器，十分的灵活。 参考资料： https://github.com/git/git https://git-scm.com/book/zh/v2/Git-%E5%B7%A5%E5%85%B7-%E5%87%AD%E8%AF%81%E5%AD%98%E5%82%A8 https://revs.runtime-revolution.com/extending-git-with-ruby-874fddffd069","categories":[{"name":"DevOps","slug":"DevOps","permalink":"http://blog.crazylaw.cn/categories/DevOps/"}],"tags":[{"name":"DevOps","slug":"DevOps","permalink":"http://blog.crazylaw.cn/tags/DevOps/"}]},{"title":"【rust】序列化框架serde","slug":"Rust语言/rust-序列化框架serde","date":"2021-04-13T02:43:43.000Z","updated":"2021-04-22T07:44:11.893Z","comments":true,"path":"2021/04/13/Rust语言/rust-序列化框架serde/","link":"","permalink":"http://blog.crazylaw.cn/2021/04/13/Rust%E8%AF%AD%E8%A8%80/rust-%E5%BA%8F%E5%88%97%E5%8C%96%E6%A1%86%E6%9E%B6serde/","excerpt":"前言Rust 中有一个 99%的程序员或许都会用到的组件，那就是序列化组件: serde。 众所周知，rust的静态语言，所以这让我们在序列化上繁琐了很多，但是有了serde，它帮助我们更好的序列化结构体，生产对应的数据。它实现了各种声明宏 以及 过程宏 来协助我们序列化。 围绕着serde，也有很多衍生的子组件，例如serde-json, serde-yaml, serde-qs 等等。 由于我目前在开发 gitlab-rs，在生成对应的query_string 以及form的数据的时候，就比较棘手。 所以我在gitlab-rs 生成的过程宏中，借助了 seder 出色的序列化生态来完成功能。","text":"前言Rust 中有一个 99%的程序员或许都会用到的组件，那就是序列化组件: serde。 众所周知，rust的静态语言，所以这让我们在序列化上繁琐了很多，但是有了serde，它帮助我们更好的序列化结构体，生产对应的数据。它实现了各种声明宏 以及 过程宏 来协助我们序列化。 围绕着serde，也有很多衍生的子组件，例如serde-json, serde-yaml, serde-qs 等等。 由于我目前在开发 gitlab-rs，在生成对应的query_string 以及form的数据的时候，就比较棘手。 所以我在gitlab-rs 生成的过程宏中，借助了 seder 出色的序列化生态来完成功能。 目前围绕serde支持的序列化的数据格式组件有： JSON，许多HTTP api都使用的JavaScript对象符号。 Bincode，一个紧凑的二进制格式，用于伺服渲染的IPC引擎。 CBOR，一种简洁的二进制对象表示，专为小消息大小设计而不需要版本协商。 YAML，一个自称对人类友好的配置语言，其实不是标记语言。 MessagePack，一个高效的二进制格式，类似于一个紧凑的JSON。 TOML， Cargo使用的最小配置格式。 Pickle， Python世界中常见的格式。 [罗恩]，一个生锈的符号。—BSON， MongoDB使用的数据存储和网络传输格式。 Avro，在Apache Hadoop中使用的二进制格式，支持schema定义。 JSON5，一个JSON的超集，包括一些来自ES5的产品。 [明信片]，一个不_std和嵌入式系统友好的紧凑二进制格式。—URL查询字符串，格式为x-www-form-urlencoded。 Envy，一种将环境变量反序列化为锈蚀结构的方法。 (反序列化) Envy Store，一种反序列化AWS Parameter Store参数到Rust的方法结构体。(反序列化) S-expressions， Lisp使用的代码和数据的文本表示形式语言的家庭。 D-Bus的二进制线格式。 FlexBuffers，谷歌的FlatBuffers 0 -copy的无模式表兄弟序列化格式。 Bencode，在BitTorrent协议中使用的一个简单的二进制格式。—DynamoDB Items， rusoto_dynamodb用来传输数据的格式和DynamoDB。 Hjson，一个JSON的语法扩展，围绕人类阅读和编辑而设计。(反序列化) 最简单的一个实例1234567891011121314151617181920212223use serde::&#123;Serialize, Deserialize&#125;;#[derive(Serialize, Deserialize, Debug)]struct Point &#123; x: i32, y: i32,&#125;fn main() &#123; let point = Point &#123; x: 1, y: 2 &#125;; // Convert the Point to a JSON string. let serialized = serde_json::to_string(&amp;point).unwrap(); // Prints serialized = &#123;\"x\":1,\"y\":2&#125; println!(\"serialized = &#123;&#125;\", serialized); // Convert the JSON string back to a Point. let deserialized: Point = serde_json::from_str(&amp;serialized).unwrap(); // Prints deserialized = Point &#123; x: 1, y: 2 &#125; println!(\"deserialized = &#123;:?&#125;\", deserialized);&#125; 12345/Users/caiwenhui/.cargo/bin/cargo run --color=always --package serde_test --bin serde_test Finished dev [unoptimized + debuginfo] target(s) in 0.01s Running `target/debug/serde_test`serialized = &#123;\"x\":1,\"y\":2&#125;deserialized = Point &#123; x: 1, y: 2 &#125; 属性serde-device种支持 3种 属性，分别是: 容器属性 用于结构体或者枚举 字体属性 用于枚举的变体 字段属性 用于结构体或者枚举种的字段 字体属性 仅 用于枚举 123456789101112131415161718192021222324252627282930use serde::&#123;Serialize, Deserialize&#125;;#[derive(Serialize, Deserialize, Debug)]#[serde(deny_unknown_fields)] // &lt;-- this is a container attributestruct S &#123; #[serde(default)] // &lt;-- this is a field attribute f: i32, s_e: E&#125;#[derive(Serialize, Deserialize, Debug)]#[serde(rename = \"e\")] // &lt;-- this is also a container attributeenum E &#123; #[serde(rename = \"a\")] // &lt;-- this is a variant attribute A(String),&#125;fn main() &#123; let point_s = S &#123; f: 1 , s_e: E::A(String::from(\"inner-enum\"))&#125;; // Convert the Point to a JSON string. let serialized_a = serde_json::to_string(&amp;point_s).unwrap(); println!(\"serialized_a: &#123;&#125;\", serialized_a); let point_e = E::A(String::from(\"my-enum\")); // Convert the Point to a JSON string. let serialized_e = serde_json::to_string(&amp;point_e).unwrap(); println!(\"serialized_e: &#123;&#125;\", serialized_e)&#125; 12345/Users/caiwenhui/.cargo/bin/cargo run --color=always --package serde_test --bin serde_test Finished dev [unoptimized + debuginfo] target(s) in 0.01s Running `target/debug/serde_test`serialized_a: &#123;\"f\":1,\"s_e\":&#123;\"a\":\"inner-enum\"&#125;&#125;serialized_e: &#123;\"a\":\"my-enum\"&#125; 既然我们知道了serde有种类型的属性，具体都有哪些属性，大家一定也感兴趣。 容器属性(Container attributes)#[serde(rename = “name”)]#[serde(rename = &quot;name&quot;)] 序列化和反序列化的时候用一个名字来代替Rust结构体的名字 当然，也允许独立设置和分别设置: #[serde(rename(serialize = &quot;ser_name&quot;))] #[serde(rename(deserialize = &quot;de_name&quot;))] #[serde(rename(serialize = &quot;ser_name&quot;, deserialize = &quot;de_name&quot;))] #[serde(rename_all = “…”)]#[serde(rename_all = &quot;...&quot;)] 根据给定的大小写约定重命名所有字段（如果这是一个结构）或变量（如果这是一个枚举）。可能的值是&quot;lowercase&quot;, &quot;UPPERCASE&quot;, &quot;PascalCase&quot;, &quot;camelCase&quot;, &quot;snake_case&quot;, &quot;SCREAMING_SNAKE_CASE&quot;, &quot;kebab-case&quot;, &quot;SCREAMING-KEBAB-CASE&quot;。 顾名思义，这个容器属性主要是允许你定义驼峰，蛇形等序列化以及反序列化的字段 1234567891011121314151617181920212223242526use serde::&#123;Serialize, Deserialize&#125;;#[derive(Serialize, Deserialize, Debug)]#[serde(deny_unknown_fields)] // &lt;-- this is a container attribute#[serde(rename_all = \"UPPERCASE\")]struct S &#123; #[serde(default)] // &lt;-- this is a field attribute f: i32, hello_world: E&#125;#[derive(Serialize, Deserialize, Debug)]#[serde(rename = \"e\")] // &lt;-- this is also a container attributeenum E &#123; #[serde(rename = \"a\")] // &lt;-- this is a variant attribute A(String),&#125;fn main() &#123; let point_s = S &#123; f: 1 , hello_world: E::A(String::from(\"inner-enum\"))&#125;; // Convert the Point to a JSON string. let serialized_a = serde_json::to_string(&amp;point_s).unwrap(); // normal output =&gt; serialized_a: &#123;\"f\":1,\"hello_world\":&#123;\"a\":\"inner-enum\"&#125;&#125; println!(\"serialized_a: &#123;&#125;\", serialized_a);&#125; 1234/Users/caiwenhui/.cargo/bin/cargo run --color=always --package serde_test --bin serde_test Finished dev [unoptimized + debuginfo] target(s) in 0.02s Running `target/debug/serde_test`serialized_a: &#123;\"F\":1,\"HELLO_WORLD\":&#123;\"a\":\"inner-enum\"&#125;&#125; #[serde(deny_unknown_fields)]遇到未知字段时，在反序列化期间始终会出错。当此属性不存在时，默认情况下，对于诸如JSON之类的自描述格式，未知字段将被忽略。 1234567891011121314151617use serde::&#123;Serialize, Deserialize&#125;;#[derive(Serialize, Deserialize, Debug)]struct S &#123; #[serde(default)] f: i32,&#125;fn main() &#123; let s = r#\" &#123;\"f\":1,\"s_e\":&#123;\"a\":\"inner-enum\"&#125;&#125; \"#; let uns: S = serde_json::from_str(&amp;s).unwrap(); println!(\"un: &#123;:?&#125;\", uns);&#125; 1234/Users/caiwenhui/.cargo/bin/cargo run --color=always --package serde_test --bin serde_test Finished dev [unoptimized + debuginfo] target(s) in 0.02s Running `target/debug/serde_test`un: S &#123; f: 1&#125; 可以看到成功输出。如果没有匹配到字段到话会自动忽略。但是如果你希望严格到匹配的话。那么就可以加上此容器属性. 123456789101112131415161718use serde::&#123;Serialize, Deserialize&#125;;#[derive(Serialize, Deserialize, Debug)]#[serde(deny_unknown_fields)]struct S &#123; #[serde(default)] f: i32,&#125;fn main() &#123; let s = r#\" &#123;\"f\":1,\"s_e\":&#123;\"a\":\"inner-enum\"&#125;&#125; \"#; let uns: S = serde_json::from_str(&amp;s).unwrap(); println!(\"un: &#123;:?&#125;\", uns);&#125; 123456789101112131415161718/Users/caiwenhui/.cargo/bin/cargo run --color=always --package serde_test --bin serde_test Finished dev [unoptimized + debuginfo] target(s) in 0.02s Running `target/debug/serde_test`thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error(\"unknown field `s_e`, expected `f` or `s_e`\", line: 2, column: 44)', src/main.rs:34:43stack backtrace: 0: rust_begin_unwind at /rustc/e1884a8e3c3e813aada8254edfa120e85bf5ffca/library/std/src/panicking.rs:495:5 1: core::panicking::panic_fmt at /rustc/e1884a8e3c3e813aada8254edfa120e85bf5ffca/library/core/src/panicking.rs:92:14 2: core::option::expect_none_failed at /rustc/e1884a8e3c3e813aada8254edfa120e85bf5ffca/library/core/src/option.rs:1268:5 3: core::result::Result&lt;T,E&gt;::unwrap at /Users/caiwenhui/.rustup/toolchains/stable-x86_64-apple-darwin/lib/rustlib/src/rust/library/core/src/result.rs:973:23 4: serde_test::main at ./src/main.rs:34:18 5: core::ops::function::FnOnce::call_once at /Users/caiwenhui/.rustup/toolchains/stable-x86_64-apple-darwin/lib/rustlib/src/rust/library/core/src/ops/function.rs:227:5note: Some details are omitted, run with `RUST_BACKTRACE=full` for a verbose backtrace. 可以看到这里会报错. #[serde(tag = “type”)]该属性只适用于枚举体，具体用法详见:[ enum representations ](\bhttps://github.com/serde-rs/serde-rs.github.io/blob/master/_src/enum-representations.md) #[serde(tag = “t”, content = “c”)]该属性只适用于枚举体，具体用法详见:[ enum representations ](\bhttps://github.com/serde-rs/serde-rs.github.io/blob/master/_src/enum-representations.md) #[serde(untagged)]该属性只适用于枚举体，具体用法详见:[ enum representations ](\bhttps://github.com/serde-rs/serde-rs.github.io/blob/master/_src/enum-representations.md) serdeserde-qs","categories":[{"name":"rust","slug":"rust","permalink":"http://blog.crazylaw.cn/categories/rust/"}],"tags":[{"name":"rust","slug":"rust","permalink":"http://blog.crazylaw.cn/tags/rust/"}]},{"title":"【数据库开发】msource","slug":"数据库开发知识/msource","date":"2021-03-26T17:16:43.000Z","updated":"2021-03-31T09:00:10.369Z","comments":true,"path":"2021/03/27/数据库开发知识/msource/","link":"","permalink":"http://blog.crazylaw.cn/2021/03/27/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%BC%80%E5%8F%91%E7%9F%A5%E8%AF%86/msource/","excerpt":"前言CAP原则又称CAP定理，指的是在一个分布式系统中，一致性（Consistency）、可用性（Availability）、分区容错性（Partition tolerance）。CAP 原则指的是，这三个要素最多只能同时实现两点，不可能三者兼顾。 msource 是我们的一个 数据源组件，我们所有的大数据ETL服务都构建在此之上，所以我们msource可以说是所有业务系统的核心。他维护着一个稳定，可靠，高性能的数据传输机制。让我们 业务层 中可以做各种操作，同步，异步等等。 msource 的角色我大体分为了2种： spout （数据推送组件) db （数据存储组件）","text":"前言CAP原则又称CAP定理，指的是在一个分布式系统中，一致性（Consistency）、可用性（Availability）、分区容错性（Partition tolerance）。CAP 原则指的是，这三个要素最多只能同时实现两点，不可能三者兼顾。 msource 是我们的一个 数据源组件，我们所有的大数据ETL服务都构建在此之上，所以我们msource可以说是所有业务系统的核心。他维护着一个稳定，可靠，高性能的数据传输机制。让我们 业务层 中可以做各种操作，同步，异步等等。 msource 的角色我大体分为了2种： spout （数据推送组件) db （数据存储组件） 在这个图中，我们可以看到，db可以独立出来应用，他不依赖于spout。spout默认的传输机制是我们golang中的channel模式，但是它可以选择使用db模式。 MSOURCE_DBRockesDB的基础知识rocksdb 我们知道他是支持WAL（Write Ahead Log）的，一般的log文件中通常包括 redo log 和 undo log。其实这不仅仅是rocksdb独有的，这是一种可靠性的保证，像mysql一样有这种机制，也是分为redo log, undo log, binlog，区别就在于 binlog 属于逻辑日志，redo log和undo log属于物理日志。 rocksdb是facebook开发的一个kv存储引擎。他的机构模式是基于LSM的。基于LSM的架构都需要经过一个叫 Compaction 过程，通常Compaction涉及到三个放大因子。 Compaction需要在三者之间做取舍。 写放大 （Write Amplification） 读放大（Read Amplification） 空间放大 （Space Amplification） 后台的 compaction 来减少读放大（减少 SST 文件数量）和空间放大（清理过期数据），但也因此带来了写放大（Write Amplification）的问题。 Compaction写放大假设每秒写入10MB的数据，但观察到硬盘的写入是30MB/s，那么写放大就是3。写分为立即写和延迟写，比如redo log是立即写，传统基于B-Tree数据库刷脏页和LSM Compaction是延迟写。redo log使用direct IO写时至少以512字节对齐，假如log记录为100字节，磁盘需要写入512字节，写放大为5。 DirectIO是直接操作IO，不经过BufferIO。BufferIO也称为标准IO，两个系统调用实现的：read() 和 write()。BufferIO用了操作系统内核的页缓存，保护了磁盘，减少读盘的次数，提高了读取速度。但是由于使用了页缓存，它是处于内核空间的，无法被用户直接操作，所以需要经历一次数据拷贝复制。DirectIO 数据均直接在用户地址空间的缓冲区和磁盘之间直接进行传输，中间少了页缓存的支持。读写数据的时候获得更好的性能。使用直接 I/O 读写数据必须要注意缓冲区对齐。 读放大对应于一个简单query需要读取硬盘的次数。比如一个简单query读取了5个页面，发生了5次IO，那么读放大就是 5。假如B-Tree的非叶子节点都缓存在内存中，point read-amp 为1，一次磁盘读取就可以获取到Leaf Block；short range read-amp 为12，12次磁盘读取可以获取到所需的Leaf Block。 操作需要从新到旧（从上到下）一层一层查找，直到找到想要的数据。这个过程可能需要不止一次 I/O。特别是 range query 的情况，影响很明显。 空间放大假设我需要存储10MB数据，但实际硬盘占用了30MB，那么空间放大就是3。有比较多的因素会影响空间放大，比如在Compaction过程中需要临时存储空间，空间碎片，Block中有效数据的比例小，旧版本数据未及时删除等等。 所有的写入都是顺序写 append-only 的，不是 in-place update，所以过期数据不会马上被清理掉。 LSM 树LSM 树的设计思想非常朴素, 它的原理是把一颗大树拆分成N棵小树， 它首先写入到内存中（内存没有寻道速度的问题，随机写的性能得到大幅提升），在内存中构建一颗有序小树，随着小树越来越大，内存的小树会flush到磁盘上。磁盘中的树定期可以做 merge 操作，合并成一棵大树，以优化读性能【读数据的过程可能需要从内存 memtable 到磁盘 sstfile 读取多次，称之为读放大】。RocksDB 的 LSM 体现在多 level 文件格式上，最热最新的数据尽在 L0 层，数据在内存中，最冷最老的数据尽在 LN 层，数据在磁盘或者固态盘上。 RocksdbRocksDB的三种基本文件格式是 memtable / sstfile / logfile，memtable 是一种内存文件数据系统，新写数据会被写进 memtable，部分请求内容会被写进 logfile。logfile 是一种有利于顺序写的文件系统。memtable 的内存空间被填满之后，会有一部分老数据被转移到 sstfile 里面，这些数据对应的 logfile 里的 log 就会被安全删除 单独的 Get/Put/Delete 是原子操作，要么成功要么失败，不存在中间状态。 如果需要进行批量的 Get/Put/Delete 操作且需要操作保持原子属性，则可以使用 WriteBatch。 L0 -&gt; L1 L1 -&gt; L2 L1 -&gt; L2 可以看到主要的三个组成部分，内存结构memtable，类似事务日志角色的WAL文件，持久化的SST文件。 数据会放到内存结构memtable，当memtable的数据大小超过阈值(write_buffer_size)后，会新生成一个memtable继续写，将前一个memtable保存为只读memtable。当只读memtable的数量超过阈值后，会将所有的只读memtable合并并flush到磁盘生成一个SST文件。 这里的SST属于level0， level0中的每个SST有序，可能会有交叉。写入WAL文件是可选的，用来恢复未写入到磁盘的memtable。 memtable如其名为一种内存的数据结构。通过设置memtable的大小、总大小来控制何时flush到SST文件。大部分格式的memtable不支持并发写入，并发调用依然会依次写入。目前仅支持skiplist。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245rocksdb: options: # 如果数据库不存在是否自动建立 # default: false create.if.missing: true # 如果数据库已经存在是否直接抛出异常 # default: false error.if.exists: false # default: false paranoid.checks: false # 日志等级 # Debug = 0/Info = 1/Warn = 2/Error = 3/Fatal = 4 info.log.level: 3 # 最佳值是内核（cpu）数 # 默认RocksDB只使用一个后台线程进行flush和compaction # default: 1 increase.parallelism: 4 # 是否允许并发写入memtabe，目前仅支持skiplist # default: false allow.concurrent.memtable.writes: false # 更大的值可以提高性能，特别是在批量加载时 # 此外，更大的写缓冲区在下次打开数据库时将导致更长的恢复时间 write.buffer.size: 64 * 1024 * 1024 # 最大写缓冲区数, 当一个写缓冲区被刷新到存储时，新的写操作可以继续到另一个写缓冲区 # default: 2 max.write.buffer.number: 4 min.write.buffer.number.to.merge: 1 max.open.files: 1000 max.file.opening.threads: 16 # 数据压缩方式 # No = 0/ Snappy = 1 / ZLib = 2 / Bz2 = 3 / LZ4 = 4 / LZ4HC = 5 / Xpress = 6 / ZSTD = 7 compression: 1 # 设置数据库的level的数量 # 默认值为7层 num.levels: 7 # level-0 触发合并的的文件数条件 level0.file.num.compaction.trigger: 4 # level-0 放慢写入速度的文件数条件 level0.slowdown.writes.trigger: 8 # level-0 停止写入的文件数条件 level0.stop.writes.trigger: 12 # 尝试从mem到sst的最大级别 max.mem.compaction.level: 2 # 目标文件基础大小 # 如果 target_file_size_base是2MB, # target_file_size_multiplier是10， # 那么第1级的每个文件都是2MB # 第2级的每个文件是20MB # 第3级的每个文件是200MB target.file.size.base: &amp;target_file_size_base 2 * 1024 * 1024 # 目标基础文件倍数 target.file.size.multiplier: 1 # 所在level所有文件总大小 # 例如，max_bytes_for_level_base为20MB, # max_bytes_for_level_multiplier为10，则第1级的总数据大小为20MB # 第2级的总文件大小为200MB # 第3级的总文件大小为2GB # default: 10M max.bytes.for.level.base: 10 * 1024 * 1024 # 目标所在level总文件大小 # default: 10 max.bytes.for.level.multiplier: 10.0 level.compaction.dynamic.level.bytes: false # 一次性最大打压缩字节 # Default: target.file.size.base * 25 max.compaction.bytes: *target_file_size_base * 25 # 软限制：当需要压缩的估计字节数超过这个阈值时，所有的写都会被减速到至少 delayed_write_rate # default: 64GB soft.pending.compaction.bytes.limit: 64 * 1024 * 1024 * 1024 # 硬限制：当需要压缩的估计字节数超过这个阈值时，所有的写都停止 # default: 256GB hard.pending.compaction.bytes.limit: 256 * 1024 * 1024 * 1024 # 是否使用fsync刷盘 # default: false use.fsync: false # 指定数据库的日志目录的绝对路径，如果为空则和数据放在同一个目录 # default: empty db.log.dir: \"\" # 指定数据库的WAL(预写入日志)的目录的绝对路径，如果为空则和数据放在同一个目录 wal.dir: \"\" # 设置过期文件被删除的周期 # 通过压缩过程超出作用域的文件在每次压缩时仍然会被自动删除，不管这个设置是什么 # default: 6 hours delete.obsolete.files.period.micros: 6 * 60 * 60 * 1000 * 1000 # 设置后台任务的最大并发数，作用与低优先级线程池 # default: 1 max.background.compactions: 2 # 高优先级线程池的后台 memtable 的 flush 任务的最大并发数 # 默认所有任务都在低优先级池 # default: 0 max.background.flushes: 0 # 设置日志文件的最大大小，如果日志文件大于这个值将会被创建一个新的日志文件 # 如果等于0，则日志只会写入一个日志文件 # default: 0 max.log.file.size: 0 # 日志文件滚动的时间(以秒为单位)，日志按一定时间轮转 # default: 0 (禁用状态) log.file.time.to.roll: 24 * 60 * 60 # 最多保留的日志文件数 # default: 1000 keep.log.file.num: 30 # 软速率限制 # 当任何level的压缩分数超过soft_rate_limit时，put被延迟0-1毫秒。当 等于 0.0时，此参数将被忽略 # soft_rate_limit &lt;= hard_rate_limit。如果此约束不存在，RocksDB将设置soft_rate_limit = hard_rate_limit # default: 0.0(禁用状态) soft.rate.limit: 0.0 # 硬速率限制 # 当任何level的压缩分数超过hard_rate_limit时，put每次延迟1ms。当 小于等于 1.0 时，此参数被忽略 # default: 0.0(禁用状态) hard.rate.limit: 0.0 # 设置当强制执行hard_rate_limit时，put被停止的最大时间, 0 = 没有限制 # default: 1000 rate.limit.delay.max.milliseconds: 1000 # 设置最大清单文件大小，直到滚动为止, 会删除旧的清单文件 # 默认值:MAX_INT，这样滚动就不会发生 max.manifest.file.size: 1&lt;&lt;64 - 1 # 设置表缓存使用的分片数量 # default: 4 table.cache.numshardbits: 4 # 设置扫描过程中的计数限制 # 在表的LRU缓存数据回收时，严格遵循LRU是低效的，因为这块内存不会真正被释放，除非它的refcount降到零。 # 相反，进行两次传递:第一次传递将释放refcount = 1的项，如果在扫描该参数指定的元素数量后没有足够的空间释放，将按LRU顺序删除项 # default: 16 table.cache.remove.scan.count.limit: 16 # default: 0 (自动计算一个合适的值) arena.block.size: 0 # 启用/禁用自动压缩 # default: false disable.auto.compactions: false # 设置 Wal 的恢复模式 # TolerateCorruptedTailRecordsRecovery = 0 / AbsoluteConsistencyRecovery = 1 # PointInTimeRecovery = 2 / SkipAnyCorruptedRecordsRecovery = 3 # default: 0 w.a.l.recovery.mode: 0 # 设置wal的ttl时间 # 有2个值影响 归档的 wal 是否会被删除 # 1。如果两者都设置为0，日志将被尽快删除，并且不会进入存档。 # 2。如果wal_ttl_seconds为0,wal_size_limit_mb不为0，则每10分钟检查一次WAL文件，如果总大小大于wal_size_limit_mb，则从最早的文件开始删除，直到满足size_limit。所有的空文件将被删除。 # 3。如果wal_ttl_seconds不为0,wall_size_limit_mb为0，那么每个wal_ttl_seconds / 2都会检查WAL文件，比wal_ttl_seconds老的文件会被删除。 # 4。如果两个都不是0，则每10分钟检查一次WAL文件，并且两个检查都将在ttl优先的情况下执行。 # default: 0 w.a.l.ttl.seconds: 0 # 设置WAL大小限制，单位为MB # 如果WAL文件的总大小大于wal_size_limit_mb，则从最早的文件开始删除，直到满足size_limit值为止 # default: 0 wal.size.limit.mb: 0 # 允许管道写入 # default: false enable.pipelined.write: false # 设置预分配(通过fallocate) manifest文件的字节数 # 默认值是4mb，这对于减少随机IO以及防止预分配大量数据的挂载(例如xfs的allocsize选项)过度分配是合理的 # default: 4mb manifest.preallocation.size: 1024 * 1024 * 4 # 当memtable被刷新到存储中时[启用|禁用] 清除 [重复\\被删除]的 键 # default: true purge.redundant.kvs.while.flush: true # 开启/关闭sst表的mmap读功能 # default: false allow.mmap.reads: false # 开启/关闭sst表的mmap写功能 # default: false allow.mmap.writes: false # 启用/禁用读操作的直接I/O模式(O_DIRECT) # default: false use.direct.reads: false # 启用/禁用后台flush和compaction的直接I/O模式(O_DIRECT) # 当为true时，new_table_reader_for_compaction_inputs被强制为true。 # default: false use.direct.i.o.for.flush.and.compaction: false # [开启|禁用] 子进程继承打开的文件 # default: true is.fd.close.on.exec: true # [启用|禁用]在恢复时跳过日志损坏错误(如果客户端可以丢失最近的更改) # default: false skip.log.error.on.recovery: false # 设置统计转储周期，以秒为单位 # default: 3600 (1 hour) stats.dump.period.sec: 3600 # 当打开sst文件时，是否会提示底层文件系统文件访问模式是随机的 # default: true advise.random.on.open: true # 设置所有列族写入磁盘之前在memtables中建立的数据量。 # 这与write_buffer_size不同，后者强制对单个memtable进行限制 # default: 0(禁用) db.write.buffer.size: 0 # 压缩启动后的文件访问模式 # NoneCompactionAccessPattern = 0, NormalCompactionAccessPattern = 1 # SequentialCompactionAccessPattern = 2, WillneedCompactionAccessPattern = 3 # default: NormalCompactionAccessPattern access.hint.on.compaction.start: 1 # 启用/禁用自适应互斥锁，它在求助于内核之前在用户空间旋转 # 当互斥锁不是严重竞争时，可以减少上下文切换。但是，如果互斥对象是热的，最终可能会浪费旋转时间 # default: false use.adaptive.mutex: false # 允许操作系统在后台异步写入文件时增量同步文件到磁盘 # 对每写一个bytes_per_sync发出一个请求。 # default: 0(禁用) bytes.per.sync: 0 # 设置压缩样式 # LevelCompactionStyle = 0 / UniversalCompactionStyle = 1 / FIFOCompactionStyle = 2 # default: LevelCompactionStyle compaction.style: 0 # 指定迭代-&gt;Next()是否按顺序跳过具有相同user-key的键 # 这个数字指定在重寻之前将被连续跳过的键数(与userkey相同) # default: 8 max.sequential.skip.in.iterations: 8 #[启用|禁用]线程安全的就地更新 # default: false inplace.update.support: false # 用于就地更新的锁的数量 # default: 0, ，如果 inplace_update_support = true ，则为 10000 inplace.update.num.locks: 0 # 设置memtable使用的arena的大页大小 # 如果&lt;=0，它不会从大页分配，而是从malloc分配。用户有责任为它预留巨大的页面以供分配。 # 例如:sysctl -w vm.nr_hugepages=20 # 参见linux doc Documentation/vm/hugetlbpage.txt # 如果没有足够的空闲大页，它会退回到malloc # 通过SetOptions() API动态更改 memtable.huge.page.size: 0 # 设置布隆过滤器的指针位置 # 控制bloom filter探针的位置，以提高缓存遗漏率。 # 该选项仅适用于memtable前缀bloom和plain前缀bloom。 # 它本质上限制了每个bloom filter检查可以触及的缓存线的最大数量。 # 设置为0时，此优化被关闭。这个数目不应该大于探测的数目。 # 这个选项可以提高内存工作负载的性能，但应该小心使用，因为它可能会导致更高的误报率。 # default: 0 bloom.locality: 0 # 设置memtable中一个键的最大连续合并操作数 # default: 0 (禁用状态) max.successive.merges: 0 # 开启统计 # default: 无参数，false代表不启动，true则会调用对应的api enable.statistics: false # 预加载数据库为了批量加载 # 所有数据将在0级没有任何自动压缩 # 建议在从数据库读取数据之前手动调用CompactRange(NULL, NULL)，否则读取速度会非常慢 # default: 无参数，false代表不启动，true则会调用对应的api prepare.for.bulk.load: false # 设置一个MemTableRep，它由一个向量支持 # 在迭代时，向量被排序，这对于迭代非常少且读操作开始后通常不执行写操作的工作负载非常有用 # default: 无参数，false代表不启动，true则会调用对应的api memtable.vector.rep: false # 如果不存在列族是否自动建立 # default: true create.if.missing.column.families: true SQL-AST的支持我们的db希望能做到与语言无关，不仅仅是我们的目前的golang，就算是php也可以用到本地持久化的方式的话，就需要借助RPC协议或者特定DSL来实现，但是既然是数据库，这里优先选择了以sql语法来管理数据。 那么我们就需要拿到sql的抽象语法树(sql-ast)，拿到sql-ast之后，我们就可以拿到我们所需要的信息去hit data。 Pingcap-parser 这里我们用到了pingcap公司的parser库，该库同样是TiDB的sql解析库，借助该库，我们可以很方便的拿到sql-ast 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071package mainimport ( \"fmt\" \"github.com/pingcap/parser\" _ \"github.com/pingcap/parser/test_driver\")type visitor struct &#123; table string fields []string&#125;func (v *visitor) Enter(in ast.Node) (out ast.Node, skipChildren bool) &#123; //fmt.Printf(\"Enter: %T\\n\", in) switch n := in.(type) &#123; case *ast.SelectStmt: case *ast.FieldList: case *ast.SelectField: case *ast.ColumnNameExpr: //fmt.Printf(\"Enter: %v\\n\", n.Name) case *ast.ColumnName: //v.fields = append(v.fields, n.Name.L) case *ast.TableName: //v.table = n.Name.L case *ast.BinaryOperationExpr: //fmt.Printf(\"Enter: %v\\n\", n.Op) case *ast.Join: //fmt.Printf(\"Enter: %v\\n\", n.Left) &#125; return in, false&#125;func (v *visitor) Leave(in ast.Node) (out ast.Node, ok bool) &#123; fmt.Printf(\"Leave: %T\\n\", in) switch n := in.(type) &#123; case *ast.SelectStmt: case *ast.FieldList: case *ast.SelectField: case *ast.ColumnNameExpr: case *ast.ColumnName: case *ast.TableName: v.table = n.Name.L case *ast.BinaryOperationExpr: //fmt.Printf(\"Leave: %v\\n\", n.L) &#125; return in, true&#125;func main() &#123; p := parser.New() sql := \"SELECT emp_no, first_name, last_name \" + \"FROM employees \" + \"where id='Aamodt' and (create_time &gt; 0 or last_name ='caiwenhui')\" stmtNodes, _, err := p.Parse(sql, \"\", \"\") if err != nil &#123; fmt.Printf(\"parse error:\\n%v\\n%s\", err, sql) return &#125; for _, stmtNode := range stmtNodes &#123; v := visitor&#123;&#125; stmtNode.Accept(&amp;v) fmt.Printf(\"%v\\n\", v) &#125;&#125; 这里用到了github.com/pingcap/parser/test_driver 的原因是因为该库和tidb的driver存在依赖关系，tidb在设计的时候，并未做到很好的分离，所以当其他项目需要使用该库的时候，需要引入这个驱动。 12345678910111213// Visitor visits a Node.type Visitor interface &#123; // Enter is called before children nodes are visited. // The returned node must be the same type as the input node n. // skipChildren returns true means children nodes should be skipped, // this is useful when work is done in Enter and there is no need to visit children. Enter(n Node) (node Node, skipChildren bool) // Leave is called after children nodes have been visited. // The returned node's type can be different from the input node if it is a ExprNode, // Non-expression node must be the same type as the input node n. // ok returns false to stop visiting. Leave(n Node) (node Node, ok bool)&#125; 并且这里，我们看到有一个结构体visitor，该结构体就是用来访问ast用的，因为 tidb的parser库 和阿里巴巴 的 druid sql 类似，都是采用 访问器的方式来遍历 ast的，所以我们只需要定义好我们的访问器，那么就可以访问对应的结构数据。至于访问器的接口如上图，只有2个API，一个是 Enter(n Node) (node Node, skipChildren bool)，另外一个是 Leave(n Node) (node Node, ok bool) 。2个接口返回的第二个参数分别定义为 是否跳过剩下的节点, 是否成功退出节点。 interface在这里的应用在parser中，大量运用了interface, 充分的给我们的展示了golang的组合特性。 例如： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778// Node is the basic element of the AST.// Interfaces embed Node should have 'Node' name suffix.type Node interface &#123; // Restore returns the sql text from ast tree Restore(ctx *format.RestoreCtx) error // Accept accepts Visitor to visit itself. // The returned node should replace original node. // ok returns false to stop visiting. // // Implementation of this method should first call visitor.Enter, // assign the returned node to its method receiver, if skipChildren returns true, // children should be skipped. Otherwise, call its children in particular order that // later elements depends on former elements. Finally, return visitor.Leave. Accept(v Visitor) (node Node, ok bool) // Text returns the original text of the element. Text() string // SetText sets original text to the Node. SetText(text string)&#125;// SelectStmt represents the select query node.// See https://dev.mysql.com/doc/refman/5.7/en/select.htmltype SelectStmt struct &#123; dmlNode resultSetNode // SelectStmtOpts wraps around select hints and switches. *SelectStmtOpts // Distinct represents whether the select has distinct option. Distinct bool // From is the from clause of the query. From *TableRefsClause // Where is the where clause in select statement. Where ExprNode // Fields is the select expression list. Fields *FieldList // GroupBy is the group by expression list. GroupBy *GroupByClause // Having is the having condition. Having *HavingClause // WindowSpecs is the window specification list. WindowSpecs []WindowSpec // OrderBy is the ordering expression list. OrderBy *OrderByClause // Limit is the limit clause. Limit *Limit // LockTp is the lock type LockTp SelectLockType // TableHints represents the table level Optimizer Hint for join type TableHints []*TableOptimizerHint // IsAfterUnionDistinct indicates whether it's a stmt after \"union distinct\". IsAfterUnionDistinct bool // IsInBraces indicates whether it's a stmt in brace. IsInBraces bool // QueryBlockOffset indicates the order of this SelectStmt if counted from left to right in the sql text. QueryBlockOffset int // SelectIntoOpt is the select-into option. SelectIntoOpt *SelectIntoOption&#125;func splitWhere(where ast.ExprNode) []ast.ExprNode &#123; var conditions []ast.ExprNode switch x := where.(type) &#123; case nil: case *ast.BinaryOperationExpr: if x.Op == opcode.LogicAnd &#123; conditions = append(conditions, splitWhere(x.L)...) conditions = append(conditions, splitWhere(x.R)...) &#125; else &#123; conditions = append(conditions, x) &#125; case *ast.ParenthesesExpr: conditions = append(conditions, splitWhere(x.Expr)...) default: conditions = append(conditions, where) &#125; return conditions&#125; ast.Node 是ast的基础接口，所有的节点都需要在此之上实现自己的功能。其他接口同理，一环扣一环，设计得十分巧妙。 KEY-VALUE的编码规则DB 对每个表分配一个 TableID，每一个索引都会分配一个 IndexID，每一行分配一个 RowID， 其中 DbId/TableID 在整个集群内唯一，IndexID/RowID 在表内唯一，这些 ID 都是 int64 类型。 其中细节如下： database 编码成 Key-Value pair： 12Key: metaPrefix(+)databasePrefix&#123;dbID&#125;Value: database struct json marshal database indexed 编码成 Key-Value pair： 12Key: metaPrefix(+)databasePrefix_indexPrefix&#123;database_name&#125;Value: dbID table 编码成 Key-Value pair： 12Key: metaPrefix(+)tablePrefix&#123;dbID&#125;_recordPrefixSep&#123;tableID&#125;Value: table struct json marshal table indexed 编码成 Key-Value pair： 12Key: metaPrefix(+)tablePrefix&#123;dbID&#125;_indexPrefix&#123;databaseId&#125;&#123;table_name&#125;Value: tableID 每行数据按照如下规则进行编码成 Key-Value pair： 12Key: databasePrefix&#123;dbID&#125;_tablePrefix&#123;tableID&#125;_recordPrefixSep&#123;rowID&#125;Value: [col1, col2, col3, col4] 对于 Unique Index 数据，会按照如下规则编码成 Key-Value pair： 12Key: databasePrefix&#123;dbID&#125;_tablePrefix&#123;tableID&#125;_indexPrefixSep&#123;indexID&#125;_indexedColumnsValueValue: rowID Index 数据还需要考虑 Unique Index 和非 Unique Index 两种情况，对于 Unique Index，可以按照上述编码规则。 但是对于非 Unique Index，通过这种编码并不能构造出唯一的 Key，因为同一个Index 的 databasePrefix{dbID}_tablePrefix{tableID}_indexPrefixSep{indexID}都一样，可能有多行数据的 ColumnsValue 是一样的. 对于 “非” Unique Index 的编码做了一点调整： 12Key: databasePrefix&#123;dbID&#125;_tablePrefix&#123;tableID&#125;_indexPrefixSep&#123;indexID&#125;_indexedColumnsValue_&#123;rowID&#125;Value: null 对应的标识符如下定义： 123456789101112131415161718192021222324252627282930313233var ( databasePrefix = []byte&#123;'d'&#125; tablePrefix = []byte&#123;'t'&#125; recordPrefixSep = []byte(\"_r\") indexPrefixSep = []byte(\"_i\") metaPrefix = []byte&#123;'m'&#125; sepPrefix = []byte&#123;'_'&#125; mdPrefix = append(metaPrefix, databasePrefix...) mdiPrefix = append(append(metaPrefix, databasePrefix...), indexPrefixSep...) mtPrefix = append(metaPrefix, tablePrefix...) mtiPrefix = append(append(metaPrefix, tablePrefix...), indexPrefixSep...))const ( idLen = 8 sepPrefixLen = 1 prefixLen = databasePrefixLength + idLen /*dbID*/ + sepPrefixLen + tablePrefixLength + idLen /*tableID*/ + recordPrefixSepLength uniqPrefixLen = databasePrefixLength + idLen /*dbID*/ + sepPrefixLen + tablePrefixLength + idLen /*tableID*/ + indexPrefixSepLength + idLen /*indexID*/ + sepPrefixLen /* +indexedColumnsValue */ indexPrefixLen = databasePrefixLength + idLen /*dbID*/ + sepPrefixLen + tablePrefixLength + idLen /*tableID*/ + indexPrefixSepLength + idLen /*indexID*/ + sepPrefixLen + sepPrefixLen indexPrefixLenWithID = databasePrefixLength + idLen /*dbID*/ + sepPrefixLen + tablePrefixLength + idLen /*tableID*/ + indexPrefixSepLength + idLen /*indexID*/ + sepPrefixLen + sepPrefixLen + idLen // RecordRowKeyLen is public for calculating avgerage row size. RecordRowKeyLen = prefixLen + idLen /*handle*/ tablePrefixLength = 1 databasePrefixLength = 1 recordPrefixSepLength = 2 indexPrefixSepLength = 2 metaPrefixLength = 1 mdPrefixLen = metaPrefixLength + databasePrefixLength mdiPrefixLen = mdPrefixLen + indexPrefixSepLength mtPrefixLen = metaPrefixLength + tablePrefixLength mtiPrefixLen = mtPrefixLen + indexPrefixSepLength) 我们把rowkey的编码规则来看 12345678910111213141516171819202122232425// EncodeRowKey encodes the table id and record handle into a kv.Key// EncodeRowKey databasePrefix&#123;dbID&#125;_tablePrefix&#123;tableID&#125;_recordPrefixSep&#123;rowID&#125;func EncodeRowKey(databaseId, tableID, rowId int64) kv.Key &#123; buf := make([]byte, 0, prefixLen+idLen /*rowId*/) buf = append(buf, databasePrefix...) buf = append(buf, EncodeIdBuf(databaseId)...) buf = append(buf, sepPrefix...) buf = append(buf, tablePrefix...) buf = append(buf, EncodeIdBuf(tableID)...) buf = append(buf, recordPrefixSep...) buf = append(buf, EncodeIdBuf(rowId)...) return buf&#125;func EncodeIdBuf(id int64) kv.Key &#123; var buf = make([]byte, 8) binary.BigEndian.PutUint64(buf[:], uint64(id)) return buf&#125;func DecodeIdBuf(b []byte) int64 &#123; return int64(binary.BigEndian.Uint64(b))&#125; 这里我们通过EncodeRowKey(databaseId, tableID, rowId int64) kv.Key 来生成数据的row-key，我们利用make([]byte,0, len) 的方式预申请内存的方式，后面再通过append的方式往 slice 中不断追加字节，当遇到int64的数据的时候，我们会调用EncodeIdBuf(id int64) kv.Key 来把int64转换为 大端(网络字节序) 的二进制字节。最后一个row-key就生成了。 database 编码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465type database struct &#123; Id int64 Name string&#125;// createDatabaseHandle Create databasefunc (s *Store) createDatabaseHandle(result *Result, stmt *ast.CreateDatabaseStmt) &#123; indexedKey := etccodec.EncodeDatabaseMetaIndexedKey([]byte(stmt.Name)) rdb := rocksdb.Load().(*Rocksdb) slice, err := rdb.Get(rdb.NewDefaultReadOptions(), indexedKey) if err != nil &#123; errorlog(UnexpectErrorCategory&#123;&#125;, UnknowRCode) result.Record(UnknowRCode, nil) return &#125; if slice.Exists() &#123; if !stmt.IfNotExists &#123; errorlog(UnexpectErrorCategory&#123;&#125;, DatabaseExistsRCode) result.Record(DatabaseExistsRCode, nil) &#125; else &#123; result.Success() &#125; return &#125; dbId, err := getDatabaseId() if err != nil &#123; msg := fmt.Sprintf(\"get database id failed, err: %s\", err) errorlog(UnexpectErrorCategory&#123;&#125;, msg) result.Record(CreateDatabaseFailedRCode, &amp;msg) return &#125; db := &amp;database&#123; Id: dbId, Name: stmt.Name, &#125; var buf = etccodec.EncodeIdBuf(dbId) key := etccodec.EncodeDatabaseMetaKey(dbId) value, err := json.Marshal(db) if err != nil &#123; msg := fmt.Sprintf(\"marshal database error, err: %s\", err) errorlog(UnexpectErrorCategory&#123;&#125;, msg) result.Record(CreateDatabaseFailedRCode, &amp;msg) return &#125; err = rdb.Put(key, value) if err != nil &#123; msg := fmt.Sprintf(\"rocksdb put metadata failed, err: %s\", err) errorlog(UnexpectErrorCategory&#123;&#125;, msg) result.Record(CreateDatabaseFailedRCode, &amp;msg) return &#125; err = rdb.Put(indexedKey, buf) if err != nil &#123; msg := fmt.Sprintf(\"rocksdb put indexed failed, err: %s\", err) errorlog(UnexpectErrorCategory&#123;&#125;, msg) result.Record(CreateDatabaseFailedRCode, &amp;msg) return &#125; debugf(NormalDebugCategory&#123;&#125;, \"create database [%s]\", db) result.Success()&#125; 这里，我们借助 create database stmt 来的处理方法来看看 db Key-Value pair 的处理逻辑。我们看到这里，我们通过一个stmt.Name 来拿到数据库的名，并且调用etccodec.EncodeDatabaseMetaIndexedKey([]byte(stmt.Name)) 方法来创建符合metaPrefix(+)databasePrefix_indexPrefix{database_name} 索引的key，然后判断是否存在所以索引来判断后续的逻辑。我们通过一个 getDatabaseId() 方法来获取一个全局的数据库id，并且初始化type database struct，然后我们调用了 etccodec.EncodeDatabaseMetaKey(dbid) 来对key进行生成，也就是上面所列出来的 metaPrefix(+)databasePrefix{dbID}, 接下来就是value的生成，这里的value比较直接，就是json.marshal来处理后的字节。然后我们把数据put 到rocksdb就结束了，索引数据也是如此，不过索引存储的dbid。 table的编码也类似 如果对其他的stmt，例如insert stmt/delete stmt具体的逻辑感兴趣的话，可以查阅源码，但是类似差不多。 COUNTER计数器-发号器这里我们需要讲一下counter，因为我们所有的数据都会有row_id，并且我们在create table的时候也有AUTO_INCREMENT的列，这个时候，我们也是需要一个ID发号器。 目前常见的发号器实现方案如下： UUID snowflake 数据库生成 美团的Leaf（基于数据库） UUIDUUID(Universally Unique Identifier)的标准型式包含32个16进制数字，以连字号分为五段，形式为8-4-4-4-12的36个字符，示例：550e8400-e29b-41d4-a716-446655440000，到目前为止业界一共有5种方式生成UUID，详情见IETF发布的UUID规范 A Universally Unique IDentifier (UUID) URN Namespace 由于他的无序性，不符合我们所期待的增长序列，所以抛弃 类snowflake方案这种方案大致来说是一种以划分命名空间（UUID也算，由于比较常见，所以单独分析）来生成ID的一种算法，这种方案把64-bit分别划分成多段，分开来标示机器、时间等，比如在snowflake中的64-bit分别表示如下图所示： 生成繁琐，由多个指令码组成，并且我们不需要用到分布式，这个还对本地的时间有强依赖，不够简洁 基于数据库的基于数据库的其实就是利用自增的自增机制+发号机制的组合，但是由于我们这里不基于数据库，所以给予数据库的基本也不考虑，但是其中的发号机制可以参考。例如：预分配机制。 自己的发号器123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168package msourceimport ( \"fmt\" \"gitlab.mingchao.com/basedev-deps/gorocksdb\" \"os\" \"os/signal\" \"reflect\" \"strconv\" \"syscall\" \"time\")type counter struct &#123; *gorocksdb.ReadOptions IdKey []byte GroupId string idChan chan int64 sig chan os.Signal&#125;func (c *counter) UnmarshalJSON(data []byte) (err error) &#123; c.IdKey = data[1 : len(data)-1] turboNew(c) return&#125;func (c *counter) MarshalJSON() ([]byte, error) &#123; b := make([]byte, 0, len(c.IdKey)+2) b = append(b, '\"') b = append(b, c.IdKey...) b = append(b, '\"') return b, nil&#125;func (c *counter) String() string &#123; t := reflect.TypeOf(c).Elem() v := reflect.ValueOf(c).Elem() p := fmt.Sprintf(\"%s &#123;\", t.Name()) for i := 0; i &lt; v.NumField(); i++ &#123; if v.Field(i).CanInterface() &#123; if v.Field(i).Kind() == reflect.Slice &#123; p += fmt.Sprintf(\"\\n\\t %s(%s): %s\", t.Field(i).Name, t.Field(i).Type, string(v.Field(i).Bytes())) &#125; else &#123; p += fmt.Sprintf(\"\\n\\t %s(%s): %v\", t.Field(i).Name, t.Field(i).Type, v.Field(i).Interface()) &#125; &#125; &#125; p += \"\\n&#125;\" return p&#125;func NewCounter(prefix string) *counter &#123; return newCounter(prefix)&#125;func newCounter(prefix string, args ...interface&#123;&#125;) *counter &#123; c := new(counter) if len(args) &gt; 0 &#123; c.GroupId = args[0].(string) &#125; c.IdKey = []byte(fmt.Sprintf(\"%s:%s\", prefix, c.GroupId)) turboNew(c) return c&#125;func turboNew(c *counter) &#123; ct := custom.Load().(*Custom) c.idChan = make(chan int64, ct.IdStep) c.sig = make(chan os.Signal, 1) c.ReadOptions = gorocksdb.NewDefaultReadOptions() signal.Notify(c.sig, syscall.SIGINT, syscall.SIGTERM) c.sender()&#125;func (c *counter) sender() &#123; go func() &#123; ct := custom.Load().(*Custom) for &#123; select &#123; case &lt;-c.sig: close(c.idChan) return default: if len(c.idChan) &lt; ct.IdStep/10 &#123; rdb := rocksdb.Load().(*Rocksdb) if c.ReadOptions == nil &#123; c.ReadOptions = gorocksdb.NewDefaultReadOptions() &#125; slice, err := rdb.Get(c.ReadOptions, c.getIdKey()) if err != nil &#123; fatal(UnexpectErrorCategory&#123;\"counter sender error\"&#125;, err) &#125; else &#123; var idStr string if slice.Exists() &amp;&amp; slice.Size() &gt; 0 &#123; idStr = string(slice.Data()) &#125; else &#123; idStr = \"0\" &#125; cid, err := strconv.ParseInt(idStr, 10, 64) if err != nil &#123; fatal(UnexpectErrorCategory&#123;\"counter sender error\"&#125;, err) &#125; else &#123; ct := custom.Load().(*Custom) // id回滚 if ((1&lt;&lt;63-1)/2)-ct.IdStep &lt; ct.IdStep &#123; cid = 0 &#125; nextId := cid + int64(ct.IdStep) err = c.ackId(nextId) if err != nil &#123; fatal(UnexpectErrorCategory&#123;\"counter sender error\"&#125;, err) &#125; for cid &lt; nextId &#123; cid++ c.idChan &lt;- cid &#125; &#125; &#125; &#125; else &#123; time.Sleep(50 * time.Millisecond) &#125; &#125; &#125; &#125;()&#125;// Rocksdb to get a globally unique self increment IDfunc (c *counter) getId() (int64, error) &#123; id := &lt;-c.idChan return id, nil //return -1, GetIdError&#123;bytes2string(c.IdKey)&#125;&#125;func (c *counter) GetId() (int64, error) &#123; return c.getId()&#125;func (c *counter) ackId(id int64) error &#123; rdb := rocksdb.Load().(*Rocksdb) err := rdb.Put(c.getIdKey(), []byte(strconv.FormatInt(id, 10))) if err != nil &#123; return err &#125; return nil&#125;func (c *counter) AckId(id int64) error &#123; return c.ackId(id)&#125;// Gets the key of the IDfunc (c *counter) getIdKey() []byte &#123; return c.IdKey&#125; 这里我们优先考虑可以通过内存直接通过++或者+1操作符分配的方式。我们重点看到： 123456nextId := cid + int64(ct.IdStep)err = c.ackId(nextId)for cid &lt; nextId &#123; cid++ c.idChan &lt;- cid&#125; 可以再这里看到，我们通过拿到当前cid的数值，通过idStep来增加固定的步长，然后先通过回写nextId的值到rocksdb进行持久化，再通过for循环来对cid进行叠加，每次都推送到有缓冲区的idChan中。 123456789// Rocksdb to get a globally unique self increment IDfunc (c *counter) getId() (int64, error) &#123; id := &lt;-c.idChan return id, nil&#125;func (c *counter) GetId() (int64, error) &#123; return c.getId()&#125; 通过 func (c *counter) getId() (int64, error) 来消费idChan中的id，达到一个获取id的效果。 1234// id回滚if ((1&lt;&lt;63-1)/2)-ct.IdStep &lt; ct.IdStep &#123; cid = 0&#125; 我们看到这里有一行代码，当int64的cid已经到达分配的极限了，那么cid就会进行回滚，基本保证了发号的可重复利用性。 扩展问题：id回溯了，怎么做递增判断？ 这个问题其实有点类似tcp的syn回溯的问题。因为syn一开始是随机生成的，并且这个过程了syn是会不断增加的。当syn到达分配的极限进行了回溯的时候，如何比较大小？ 我们查看到内核的tcp源码，可以看到提供的判断方式十分巧妙，如下： 123456789/** The next routines deal with comparing 32 bit unsigned ints* and worry about wraparound (automatic with unsigned arithmetic).*/static inline int before(__u32 seq1, __u32 seq2)&#123;return (__s32)(seq1-seq2) &lt; 0;&#125;#define after(seq2, seq1) before(seq1, seq2) 为什么（__s32）(seq1-seq2)&lt;0就可以判断seq1&lt;seq2呢？这里的__s32是有符号整型的意思，而__u32则是无符号整型。为了方便说明，我们以unsigned char和char为例来说明： 假设seq1=255， seq2=1（发生了回绕）seq1 = 1111 1111 seq2 = 0000 0001我们希望比较结果是 seq1&lt;seq2 12345 seq1 - seq2&#x3D; 1111 1111-0000 0001----------- 1111 1110 由于我们将结果转化成了有符号数，由于最高位是1，因此结果是一个负数，负数的绝对值为 0000 0001 + 1 = 0000 0010 = 2 (补码：取反+1) 因此 seq1 - seq2 &lt; 0 注意： 如果seq2=128的话，我们会发现： 12345 seq1 - seq2&#x3D; 1111 1111-1000 0000----------- 0111 1111 此时结果尤为正了，判断的结果是seq1&gt;seq2。因此，上述算法正确的前提是，回绕后的增量小于2^(n-1)-1。 由于tcp序列号用的32位无符号数，因此可以支持的回绕幅度是2^31-1，满足要求了。 但是由于我们这里不需要比较发号的先后次序，只需要保证其唯一性，所以这个回溯的大小比较问题，并不需要过多的关注 行级锁的实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495var ( // dbID:tblID // dbID:tblID:rowID rowLockLock sync.RWMutex rowLock rl ttlTime = int64(30 * 60))type ( rl map[string]*lock lock struct &#123; ttl int64 lock sync.RWMutex &#125;&#125;func init() &#123; // clean row lock, release memory go func() &#123; t := time.NewTicker(10 * time.Minute) for &#123; select &#123; case &lt;-t.C: ct := time.Now().Unix() rowLockLock.Lock() for key, lock := range rowLock &#123; if ct &gt; lock.ttl &#123; delete(rowLock, key) &#125; &#125; rowLockLock.Unlock() &#125; &#125; &#125;() rowLock = make(rl, 10)&#125;func rowLockKey(dbId, tblId, rowId int64) string &#123; return fmt.Sprintf(\"%d:%d:%d\", dbId, tblId, rowId)&#125;func (r *rl) Lock(lockKey string) &#123; rowLockLock.Lock() m, ok := (*r)[lockKey] if !ok &#123; m = new(lock) (*r)[lockKey] = m m.ttl = time.Now().Unix() + ttlTime &#125; rowLockLock.Unlock() m.lock.Lock()&#125;func (r *rl) UnLock(lockKey string) &#123; rowLockLock.Lock() m, ok := (*r)[lockKey] if !ok &#123; m = new(lock) (*r)[lockKey] = m m.ttl = time.Now().Unix() + ttlTime &#125; rowLockLock.Unlock() m.lock.Unlock()&#125;func (r *rl) RLock(lockKey string) &#123; rowLockLock.Lock() m, ok := (*r)[lockKey] if !ok &#123; m = new(lock) (*r)[lockKey] = m m.ttl = time.Now().Unix() + ttlTime &#125; rowLockLock.Unlock() m.lock.RLock()&#125;func (r *rl) RUnlock(lockKey string) &#123; rowLockLock.Lock() m, ok := (*r)[lockKey] if !ok &#123; m = new(lock) (*r)[lockKey] = m m.ttl = time.Now().Unix() + ttlTime &#125; rowLockLock.Unlock() m.lock.RUnlock()&#125; 以上是行级锁的实现方式，主要是利用sync.RWMutex来实现读写锁，并且带有ttl的机制，每次加锁的时候，都会更新ttl的时间。其中在init阶段，我们利用的ticker来实现对锁进行一个类似LRU的机制，对于不活跃的锁对象进行释放，防止在这里造成内存只增不减。 1234567891011121314151617func (s *Store) updateHandle(result *Result, stmt *ast.UpdateStmt) &#123; // 通过ast获取old数据 .... // 行级锁锁定 rowID, _ := item[0].(json2.Number).Int64() rowlockKey := rowLockKey(db.Id, tbl.Id, rowID) rowLock.Lock(rowlockKey) defer rowLock.UnLock(rowlockKey) // 更新索引数据 for _, index := range indexs &#123; ... &#125; // 更新为new数据 ...&#125; 逆波兰表达式 &amp;&amp; 波兰表达式这一块其实暂时还没实现，但是他的原理有必要和大家说一下，我们的db实现，都是基于sql 来实现的，我们知道 sql 中也有表达式计算，并且是有优先级之分的。 前/中/后序遍历，相信大家基本都听说过，但是实际运用中少之又少，这是因为大家可能在实际中没有找到合适的模式和套用这些树的遍历方式。 前序遍历：根结点 —&gt; 左子树 —&gt; 右子树 中序遍历：左子树—&gt; 根结点 —&gt; 右子树 后序遍历：左子树 —&gt; 右子树 —&gt; 根结点 例如： 1SELECT (count * price) AS sum FROM orders WHERE order_id &lt; 100 其中 order_id &lt; 10 就是一个表达式，它有一个列输入参数： order_id，输出：Bool RPN 表达式(逆波兰表示法)RPN 是树的后序遍历，后序遍历在每个节点知道自己有几个子节点的时候等价于原本的树结构。 所以你波澜是后序遍历：左右中 比如说我们有一个数学算式 2 *（3 + 4）+ 5： 由于数学上习惯写法是中序遍历，我们通常要加上括号消除歧义（比如加减和乘除的顺序）。通过把操作符后移 我们得到 RPN：2 3 4 + * 5 +，这样我们无需括号就能无歧义地遍历这个表达式： 中序表达式转后序表达式： 1234原式：a+b*(c+d&#x2F;e)补全括号：(a+(b*(c+(d&#x2F;e))))操作符右移：(a(b(c(de)&#x2F;)+)*)+去掉括号：abcde&#x2F;+*+ 所以波兰表达式是中序遍历：左右中 执行 RPN 的过程需要一个栈来缓存中间结果，比如说对于 2 3 4 + * 5 +，我们从左到右遍历表达式，遇到值就压入栈中。直到 + 操作符，栈中已经压入了 2 3 4。 因为 + 是二元操作符，需要从栈中弹出两个值 3 4，结果为 7，重新压入栈中： 此时栈中的值为 2 7。 下一个是 * 运算符，也需要弹出两个值 2 7，结果为 14 压入栈中。 接着压入 5 。 最后 + 运算符弹出 14 5，结果为 19 ，压入栈。 最后留在栈里的就是表达式的结果，因此，如果需要计算表达式优先级的话，可以采用RPN的方式来读取tree结构来进行顺序计算。 单独使用DB例子： 这里有一个类似于mysql-client 的一个 bin 程序 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071package mainimport ( \"flag\" \"github.com/pingcap/parser\" \"github.com/pingcap/parser/ast\" \"gitlab.mingchao.com/basedev-deps/logbdev\" \"gitlab.mingchao.com/basedev-deps/msource/v2\" \"os\")var sql = flag.String(\"sql\", \"\", \"Input Your Sql\")func init() &#123; flag.Parse()&#125;func main() &#123; if os.Getenv(\"DEBUG\") == \"true\" &#123; logbdev.SetLevel(logbdev.DebugLevel) &#125; msource.PreparePhase() store := msource.NewStore() p := parser.New() stmtNode, err := p.ParseOneStmt(*sql, \"\", \"\") if err != nil &#123; logbdev.Error(err) return &#125; _, ok := stmtNode.(*ast.SelectStmt) var r *msource.Result if ok &#123; r, err = store.Query(*sql) if err != nil &#123; logbdev.Error(err) return &#125; &#125; else &#123; r, err = store.Execute(*sql) if err != nil &#123; logbdev.Error(err) return &#125; &#125; if r != nil &#123; if r.Data != nil &#123; switch ar := r.Data.(type) &#123; case *msource.InsertResultData: logbdev.Info(ar.GetSliceInt64()) logbdev.Info(ar.Raw()) case *msource.ShowDatabasesResultData: logbdev.Info(ar.GetSliceString()) case *msource.ShowTablesResultData: logbdev.Info(ar.GetSliceString()) case *msource.SelectResultSetData: logbdev.Info(ar.GetFields()) logbdev.Info(ar.GetValues()) logbdev.Info(ar.Count()) case *msource.DeleteResultData: logbdev.Info(ar.GetAffected()) case *msource.UpdateResultData: logbdev.Info(ar.GetAffected()) &#125; &#125; logbdev.Info(r) &#125;&#125; 具体用法： 12345go run example&#x2F;msource_db&#x2F;customStmt&#x2F;main.go --sql &quot;INSERT INTO users(\\&#96;name\\&#96;,\\&#96;age\\&#96;,\\&#96;last_login\\&#96;) VALUES (\\&quot;caiwenhui\\&quot;, 18, 1614776101)&quot;go run example&#x2F;msource_db&#x2F;customStmt&#x2F;main.go --sql &quot;show databases;&quot;go run example&#x2F;msource_db&#x2F;customStmt&#x2F;main.go --sql &quot;show tables;&quot;go run example&#x2F;msource_db&#x2F;customStmt&#x2F;main.go --sql &quot;INSERT INTO mingchao.users2(\\&#96;name\\&#96;,\\&#96;age\\&#96;) VALUES (\\&quot;caiwenhui\\&quot;, 18),(\\&quot;caiwenhui\\&quot;, 19)&quot;go run example&#x2F;msource_db&#x2F;customStmt&#x2F;main.go --sql &quot;INSERT INTO mingchao.users2 VALUES (1000,\\&quot;caiwenhui\\&quot;, 18)&quot; 我们可以用对外暴露一个msource.NewStore()来创建一个存储器对象，然后通过API进行数据库的操作。 NewStore我们用了sync.Pool封装，对象可以做到尽可能的复用。 可以看到如果是SELECT STMT的话，我们调用的是QUERYAPI，如果是非SELECT STMT的话，我们调用的是EXECUTEAPI。 TODO基于目前尚未实现，所以暂时不再展开讲叙，后续可以升级处理的点为： 事务处理，例如前面所说的redolog和undolog可实现。 orderby， 数据排序。 全双工的通信获取数据，无需一次性读取所有数据。 Explain执行计划的实现，逻辑根据执行计划走。 SPOUT另外一篇文章中，记录了我们的spout的作用，在这里，再简单说一下，spout 是我们msource组件的核心角色，它是用于把数据推送到上层业务的所使用。上层业务通过spout角色提供的API，可以获取到从数据源拿到的数据。 spout 自身保持了一套 高可靠, 高性能, 可容错 的数据机制，主要用于区别出ACK, NACK，并且自带有 失败重传, 多阶段状态机的checkpoint等机制。 channel-mode 大体数据流程图 之前有一篇文章，专门讲解channel-mode下，是如何工作的，这里不做过多详细的说明。简单复述一下。 channel模式下，是直接把数据推送到我们的golang的channel 当中，上层业务直接用过channel拿到数据，拿到数据后根据自身业务处理数据来判断可以ack或者nack掉数据，同时保存offset。 这里的问题就在于： 由于我们是本地存储的offset，因为不信任 kafka-client的auto-commit机制，当程序在某个节点crash的时候，这会让我们的程序在下次启动的时候，重复消费到数据或者遗漏数据 缺点：每个partition的offset都需要顺序消费，在上层业务无法并发处理，这极大程度的降低了我们的消费效率 期望：如果我们提前把offset存储起来了，而不需要ACK之后再存储offset的话，那么我们就可以再上层业务并发的处理消息，而无需关注offset的问题 db-demo 大体数据流程图 鉴于channel-mode下的缺点，由此诞生了我们的db-mode，其原理是把数据先存储在本地的数据库，也就是我们上面所说的db，所以这里我们可以得出关系spout &lt;- db， db角色 可以被 spout角色所依赖。 我们创建了4个表来存储不同的数据： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455const ( DefaultDatabase = \"default\" DefaultDatabaseSql = \"CREATE DATABASE IF NOT EXISTS `\" + DefaultDatabase + \"`\" UseDefaultDatabase = \"USE `\" + DefaultDatabase + \"`\" // 推送消息的时候使用 // 因为正常消息过来的时候是没有row_id的，所以这个payload_marshal内的row_id没意义 // 该表仅仅遍历数据到Runner表，到了Runner表和Loser表，Row_id才有用 SpoutStoreStorageTable = \"storage\" SpoutStoreStorageBuildTableSql = \"create table if not exists \" + SpoutStoreStorageTable + \"(\" + \" `payload_marshal` varchar(255) not null comment \\\"序列化后的payload\\\",\" + \" `is_multi_phase` int(11) not null default 0 comment \\\"是否多阶段，0=否, 1=是\\\",\" + \" `cur_state` int(11) not null default 0 comment \\\"当前状态\\\",\" + \" `fin_state` int(11) not null default 0 comment \\\"最终状态\\\"\" + \")\" // Storage -&gt; Runner 使用 // 刚启动服务的时候Runner-&gt;Storage有用 // Ack的时候有用 SpoutStoreRunningTable = \"runner\" SpoutStoreRunningBuildTableSql = \"create table if not exists \" + SpoutStoreRunningTable + \"(\" + \" `payload_marshal` varchar(255) not null comment \\\"序列化后的payload\\\",\" + \" `is_multi_phase` int(11) not null default 0 comment \\\"是否多阶段，0=否, 1=是\\\",\" + \" `cur_state` int(11) not null default 0 comment \\\"当前状态\\\",\" + \" `fin_state` int(11) not null default 0 comment \\\"最终状态\\\"\" + \")\" // Runner -&gt; Loser 使用 // Nack的时候有用 // 失败重传的时候用/刚启动服务的时候Loser-&gt;Storage有用 SpoutStoreLoserTable = \"loser\" SpoutStoreLoserBuildTableSql = \"create table if not exists \" + SpoutStoreLoserTable + \"(\" + \" `payload_marshal` varchar(255) not null comment \\\"序列化后的payload\\\",\" + \" `is_multi_phase` int(11) not null default 0 comment \\\"是否多阶段，0=否, 1=是\\\",\" + \" `cur_state` int(11) not null default 0 comment \\\"当前状态\\\",\" + \" `fin_state` int(11) not null default 0 comment \\\"最终状态\\\"\" + \")\" SpoutStoreDefaultOffsetTable = \"offset\" SpoutStoreDefaultOffsetBuildTableSql = \"create table if not exists \" + SpoutStoreDefaultOffsetTable + \"(\" + \" `group_id` varchar(255) not null comment \\\"消费组\\\",\" + \" `topic` varchar(255) not null comment \\\"消费的topic\\\",\" + \" `partition` int(11) not null comment \\\"topic的partition\\\",\" + \" `offset` int(11) not null comment \\\"当前消费的offset\\\",\" + \" UNIQUE KEY `uniq_idx` (`group_id`,`topic`,`partition`)\" + \")\") DB模式下的用法例子： channel-mode下的例子和db模式的差不多，但是更加简单，这里就不列出来了。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768package commonimport ( \"context\" \"gitlab.mingchao.com/basedev-deps/logbdev\" \"gitlab.mingchao.com/basedev-deps/msource/v2\" \"os\" \"os/signal\" \"sync\" \"syscall\")func PreparePhase() &#123; logbdev.SetLevel(logbdev.DebugLevel) S = msource.PreparePhase()&#125;var S *msource.Spoutfunc CoreStart(function func(payload *msource.Payload)) &#123; wg := new(sync.WaitGroup) // 创建主协程上下文 ctx, cannel := context.WithCancel(context.Background()) // 启动msource，并且传递主协程上下文，用于任务间的通信控制 S.Start(ctx) // 注册信号量 sig := make(chan os.Signal, 1) signal.Notify(sig, syscall.SIGINT, syscall.SIGTERM) wg.Add(1) go func() &#123; defer wg.Done() for &#123; select &#123; case &lt;-sig: cannel() return &#125; &#125; &#125;() // 这里我们可以再创建更多的worker协助我们的消费数据 // 使用方式基本向后兼容 innerWorker := 3 logbdev.Infof(\"total chan: %d\\n\", S.ChanSize()) // 主协程中创建子协程（worker）工作 for i := 0; i &lt; S.ChanSize(); i++ &#123; for ii := 0; ii &lt; innerWorker; ii++ &#123; wg.Add(1) go func(idx, idx2 int) &#123; defer wg.Done() logbdev.Infof(\"start chan[%d-%d]\\n\", idx, idx2) payloadCh := S.GetPayloadChanById(idx) for payload := range payloadCh.GetCh() &#123; function(payload) &#125; &#125;(i, ii) &#125; &#125; // 等待子协程结束 wg.Wait() // 等待msource退出 S.Stop()&#125; ACK123456789101112131415package mainimport ( \"gitlab.mingchao.com/basedev-deps/logbdev\" \"gitlab.mingchao.com/basedev-deps/msource/v2\" \"gitlab.mingchao.com/basedev-deps/msource/v2/example/spout/db/common\")func main() &#123; common.CoreStart(func(payload *msource.Payload) &#123; if err := common.S.Ack(payload); err != nil &#123; logbdev.Error(err) &#125; &#125;)&#125; NACK12345678910111213141516package mainimport ( \"gitlab.mingchao.com/basedev-deps/logbdev\" \"gitlab.mingchao.com/basedev-deps/msource/v2\" \"gitlab.mingchao.com/basedev-deps/msource/v2/example/spout/db/common\")func main() &#123; common.PreparePhase() common.CoreStart(func(payload *msource.Payload) &#123; if err := common.S.MarkFailure(payload); err != nil &#123; logbdev.Error(err) &#125; &#125;)&#125; STATE-MACHINE123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121package mainimport ( \"context\" \"fmt\" \"gitlab.mingchao.com/basedev-deps/logbdev\" \"gitlab.mingchao.com/basedev-deps/msource/v2/example/spout/db/common\" \"os\" \"os/signal\" \"sync\" \"syscall\")type MyStateMachine struct &#123; phases []string&#125;func (msm *MyStateMachine) AddPhase(name string) error &#123; msm.phases = append(msm.phases, name) return nil&#125;// get all phasefunc (msm *MyStateMachine) GetPhases() []string &#123; return msm.phases&#125;func main() &#123; common.PreparePhase() wg := new(sync.WaitGroup) // 创建主协程上下文 ctx, cannel := context.WithCancel(context.Background()) // 启动msource，并且传递主协程上下文，用于任务间的通信控制 // 需要设置为多阶段的话，必须设置状态机，并且在msource服务Start之前设置 sms := &amp;MyStateMachine&#123;&#125; _ = sms.AddPhase(\"step1\") _ = sms.AddPhase(\"step2\") _ = sms.AddPhase(\"step3\") common.S.SetStateMachine(sms) common.S.Start(ctx) // 注册信号量 sig := make(chan os.Signal, 1) signal.Notify(sig, syscall.SIGINT, syscall.SIGTERM) wg.Add(1) go func() &#123; defer wg.Done() for &#123; select &#123; case &lt;-sig: cannel() return &#125; &#125; &#125;() logbdev.Infof(\"total chan: %d\\n\", common.S.ChanSize()) // 主协程中创建子协程（worker）工作 for i := 0; i &lt; common.S.ChanSize(); i++ &#123; wg.Add(1) chCtx, _ := context.WithCancel(ctx) go func(idx int, childCtx context.Context) &#123; defer wg.Done() logbdev.Infof(\"start chan[%d]\\n\", idx) payloadCh := common.S.GetPayloadChanById(idx) for &#123; select &#123; case payload := &lt;-payloadCh.GetCh(): // 不同阶段之间如果相互无依赖的话，则可以并发处理 // 否则请使用同步的方式 var wg sync.WaitGroup wg.Add(1) go func() &#123; defer wg.Done() phase := \"step1\" if common.S.CanTransition(payload, phase) &#123; fmt.Println(\"Do Step 1 something\") _ = common.S.Transition(payload, phase) &#125; &#125;() wg.Add(1) go func() &#123; defer wg.Done() phase := \"step2\" if common.S.CanTransition(payload, phase) &#123; fmt.Println(\"Do Step 2 something\") _ = common.S.Transition(payload, phase) &#125; &#125;() wg.Add(1) go func() &#123; defer wg.Done() phase := \"step3\" if common.S.CanTransition(payload, phase) &#123; fmt.Println(\"Do Step 3 something\") _ = common.S.Transition(payload, phase) &#125; &#125;() wg.Wait() // 多阶段的请看下，Ack非必须要，如果手动调用ack的话，那么等于一条条同步删除 // msource 在后台会定期检测runner中的状态机数据 //if err := common.S.Ack(payload); err != nil &#123; // logbdev.Error(err) //&#125; case &lt;-ctx.Done(): fmt.Println(\"Done\") return &#125; &#125; &#125;(i, chCtx) &#125; // 等待子协程结束 wg.Wait() // 等待msource退出 common.S.Stop()&#125; 小知识总结time组件在开发的过程中，time组件用得还是比较多的，因为有各种异步任务在后台运行，常规的用法就不记录讲述了，这里说一下一些注意的点。 123456789&#x2F;&#x2F; After waits for the duration to elapse and then sends the current time&#x2F;&#x2F; on the returned channel.&#x2F;&#x2F; It is equivalent to NewTimer(d).C.&#x2F;&#x2F; The underlying Timer is not recovered by the garbage collector&#x2F;&#x2F; until the timer fires. If efficiency is a concern, use NewTimer&#x2F;&#x2F; instead and call Timer.Stop if the timer is no longer needed.func After(d Duration) &lt;-chan Time &#123; return NewTimer(d).C&#125; 我们看到这个API，如果想要用 12345678for &#123; select &#123; case &lt;-time.After(1*time.Second)): fmt.Println(\"时间到了\") default: fmt.Println(\"go on\") &#125;&#125; 看到这个例子，如果我们这么用的话，每1秒都会重新创建一个Timer对象，不断在堆空间申请内存，然后gc-worker再大量回收没有再使用的对象内存。这就导致cpu做了额外的一些无效工作。 所以这种用法我是不推荐的。 1234567func (t *Timer) Reset(d Duration) bool &#123; if t.r.f == nil &#123; panic(\"time: Reset called on uninitialized Timer\") &#125; w := when(d) return resetTimer(&amp;t.r, w)&#125; 我们看到其实Timer 其实有一个Reset的API，我们可以对同一个timer进行Reset的操作，不断是重置时间即可。 12345678910d := 1*time.Secondt:= NewTimer(d)for &#123; select &#123; case &lt;-t.C: t.Reset(d) default: fmt.Println(\"go on\") &#125;&#125; make函数make函数是一个很强大的函数，我们会经常使用到，但是有一些细节，需要大家知道的。 make([]byte,0,10) 与 make([]byte,10) 这是2中不同的切片，对于可能刚学习golang的小伙伴来说，会有点疑惑，但是这是需要了解的，如果是三个参数的时候，一个是cap,一个是len，他们是有区别的。如果是三个参数的话，那代表当前大小cap = len 我们经常会用三个参数来进行预分配空间，第二个参数默认都是填写0来进行优化，特别是我们在写DB的时候，用到了大量[]byte类型，在组装编码字节的时候，我们就需要使用这种方式来处理，否则。 12345a := make([]byte, 0, 5)a = append(a, []byte&#123;'a'&#125;) // a = [97]a := make([]byte, 5)a = append(a, []byte&#123;'a'&#125;) // a = [0,0,0,0,0,97] 看到这里，大家就会明白区别。 once函数有些时候，我们想要保证只运行一次，这里，我们就需要借助 sync.Once，需要注意的是 一个sync.Once只能与一个函数绑定！ 1234567once := new(sync.Once)callback:= func() &#123; fmt.Println(\"我只想运行一次\")&#125;once.Do(callback) // 会 输出once.Do(callback) // 无 输出once.Do(callback) // 无 输出once.Do(callback) // 无 输出 自定义marshal和unmarshal有时候，我们想要自己定义json的marshal 和 unmarshal，这里我们的发号计数器 就用到了，用它的原因其实是因为，我们的发号计数器在发号的过程中，其实是后台跑着一个异步任务在发号，所以在被反编码的时候，我们需要启动这个异步任务。 1234567891011121314151617181920212223242526272829303132func (c *counter) UnmarshalJSON(data []byte) (err error) &#123; c.IdKey = data[1 : len(data)-1] // 重点关注这里 turboNew(c) return&#125;func (c *counter) MarshalJSON() ([]byte, error) &#123; b := make([]byte, 0, len(c.IdKey)+2) b = append(b, '\"') b = append(b, c.IdKey...) b = append(b, '\"') return b, nil&#125;func turboNew(c *counter) &#123; ct := custom.Load().(*Custom) c.idChan = make(chan int64, ct.IdStep) c.sig = make(chan os.Signal, 1) c.ReadOptions = gorocksdb.NewDefaultReadOptions() signal.Notify(c.sig, syscall.SIGINT, syscall.SIGTERM) // 这里会启动一个异步任务 c.sender()&#125;func (c *counter) sender() &#123; // 异步任务 ...&#125; lockfree-queue和lockfree-stack我们知道如果想要做到并发安全的话，普遍做法就是2种 无锁化结构的设计（需要针对特定的业务常用，并且不允许乱用） 有锁结构 无锁化(lock-free)的实现方式有很多种，在开发的过程中，我也有想过利用lock-free-stack以及lock-free-queue，分别想要运用在RPN的实现以及发号器当中，虽然后来发现用不到，但是可以拿到这里和大家分享一下。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990// inrInt64 Increasefunc inrInt64(i *int64) &#123; t := int64(+1) for &#123; value := atomic.LoadInt64(i) if atomic.CompareAndSwapInt64(i, value, value+t) &#123; return &#125; time.Sleep(time.Nanosecond) &#125;&#125;// dcrInt64 Decreasefunc dcrInt64(i *int64) &#123; t := int64(-1) for &#123; value := atomic.LoadInt64(i) if atomic.CompareAndSwapInt64(i, value, value+t) &#123; return &#125; time.Sleep(time.Nanosecond) &#125;&#125;// LKStack returns an empty queue.func NewLKStack() *LKStack &#123; n := unsafe.Pointer(&amp;node&#123;&#125;) return &amp;LKStack&#123;head: n&#125;&#125;// LKStack is a lock-free unbounded stack.type LKStack struct &#123; len int64 head unsafe.Pointer&#125;func (q *LKStack) IsEmpty() bool &#123; return q.Len() == 0&#125;func (q *LKStack) Len() int64 &#123; return q.len&#125;// LKStack puts the given value v at the tail of the stack.func (q *LKStack) Push(v interface&#123;&#125;) &#123; n := &amp;node&#123;value: v&#125; for &#123; head := load(&amp;q.head) next := load(&amp;n.next) cas(&amp;n.next, next, head) if cas(&amp;q.head, head, n) &#123; inrInt64(&amp;q.len) return &#125; time.Sleep(time.Nanosecond) &#125;&#125;// Pop removes and returns the value at the head of the stack.// It returns nil if the stack is empty.func (q *LKStack) Pop() interface&#123;&#125; &#123; for &#123; head := load(&amp;q.head) next := load(&amp;head.next) if next == nil &#123; // is stack empty? return nil &#125; else &#123; // read value before CAS otherwise another pop might free the next node v := head.value if cas(&amp;q.head, head, next) &#123; dcrInt64(&amp;q.len) return v // Pop is done. return &#125; &#125; time.Sleep(time.Nanosecond) &#125;&#125;// load from atomic load pointer nodefunc load(p *unsafe.Pointer) (n *node) &#123; return (*node)(atomic.LoadPointer(p))&#125;// cas swap setfunc cas(p *unsafe.Pointer, old, new *node) (ok bool) &#123; return atomic.CompareAndSwapPointer( p, unsafe.Pointer(old), unsafe.Pointer(new))&#125; 这里也不过多在这里描述了，我们大家查看源码吧，主要就是利用了atomic包中的原子性操作CompareAndSwapXxx, 因为这是一个原子性的指令，合理的运用即可做到无锁化并发安全的结构。 atomic包的CompareAndSwapXxx其实就是一个CAS的理念，用乐观锁(逻辑锁)来做数据处理。 unsafa包中的指针的作用：零拷贝string和byte的转换零拷贝(zero-copy)，传统较多的说法就是无需经过用户态到内核态到数据copy，即可做到想做的事情。 通俗一点就是不经过copy就能转换数据。 12345678910type StringHeader struct &#123; Data uintptr Len int&#125;type SliceHeader struct &#123; Data uintptr Len int Cap int&#125; 这是String和slice的底层数据结构，他们基本是一致的，区别其实就是在于一个有Cap，一个是固定的Len。 只需要共享底层 Data 和 Len 就可以实现 zero-copy。 1234567func string2bytes(s string) []byte &#123; return *(*[]byte)(unsafe.Pointer(&amp;s))&#125;func bytes2string(b []byte) string &#123; return *(*string)(unsafe.Pointer(&amp;b))&#125; context控制上下文也讲解一下我们这里用到了大量协程，他们之间有一些或许是有上下文关系的，因此，我们这里就需要用到context来对协程进行一个上下文的管理，做到协助的作用。 特别是我们在退出程序的时候，我们想要某一些异步任务优雅,可靠,安全的退出程序，那么我们就需要用到context来控制每个后台运行的程序。 我们这里用到比较多的其实就是 context.WithCancel(ctx)， 我们需要管理每个协程的退出需要做的事情，例如：我需要msource在退出的时候，保存一下当前在内存中最新的数据到rocksdb中，那么这个时候context的作用就十分有效了。 pprof的查看要利用pprof粗略查看性能，及时它不能准确的反馈出所有的问题，起码它能帮助我们在前面的大问题上更容易发现问题。 sync.Pool如何做到优化 对STW暂停时间做了优化, 避免大的sync.Pool严重影响STW时间 第二个优化是GC时入对sync.Pool进行回收，不会一次将池化对象全部回收，这就避免了sync.Pool释放对象和重建对象导致的性能尖刺，造福于sync.Pool重度用户。 第三个就是对性能的优化。 对以上的改进主要是两次提交：sync: use lock-free structure for Pool stealingsync: smooth out Pool behavior over GC with a victim cache 分别是用到了无锁化结构 以及程序GC的行为的优化","categories":[{"name":"数据结构","slug":"数据结构","permalink":"http://blog.crazylaw.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"数据库","slug":"数据结构/数据库","permalink":"http://blog.crazylaw.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://blog.crazylaw.cn/tags/Kafka/"},{"name":"TiDB","slug":"TiDB","permalink":"http://blog.crazylaw.cn/tags/TiDB/"},{"name":"RocketDB","slug":"RocketDB","permalink":"http://blog.crazylaw.cn/tags/RocketDB/"},{"name":"SQL","slug":"SQL","permalink":"http://blog.crazylaw.cn/tags/SQL/"}]},{"title":"【Golang】- unsafe Pointer","slug":"Golang/unsafe-Pointer","date":"2021-01-14T03:37:12.000Z","updated":"2021-03-20T16:25:01.799Z","comments":true,"path":"2021/01/14/Golang/unsafe-Pointer/","link":"","permalink":"http://blog.crazylaw.cn/2021/01/14/Golang/unsafe-Pointer/","excerpt":"前言由于目前在使用使用一些go写的odbc库，里面涉及到一些cgo的内容，那就避不开内存和指针等问题，在这边文章中记录一下unsafe Pointer 和 uintptr 的相关内容。 Go语言在设计的时候，为了编写方便、效率高以及降低复杂度，被设计成为一门强类型的静态语言。强类型意味着一旦定义了，它的类型就不能改变了；静态意味着类型检查在运行前就做了。 同时为了安全的考虑，Go语言是不允许两个指针类型进行转换的。","text":"前言由于目前在使用使用一些go写的odbc库，里面涉及到一些cgo的内容，那就避不开内存和指针等问题，在这边文章中记录一下unsafe Pointer 和 uintptr 的相关内容。 Go语言在设计的时候，为了编写方便、效率高以及降低复杂度，被设计成为一门强类型的静态语言。强类型意味着一旦定义了，它的类型就不能改变了；静态意味着类型检查在运行前就做了。 同时为了安全的考虑，Go语言是不允许两个指针类型进行转换的。 指针类型转换我们一般使用*T作为一个指针类型，表示一个指向类型T变量的指针。为了安全的考虑，两个不同的指针类型不能相互转换，比如*int不能转为*float64。 12345678func main() &#123; i:= 10 ip:=&amp;i var fp *float64 = (*float64)(ip) fmt.Println(fp)&#125; 以上代码我们在编译的时候，会提示 cannot convert ip (type *int) to type *float64，也就是不能进行强制转型。那如果我们还是需要进行转换怎么做呢？这就需要我们使用 unsafe包 里的 Pointer 了，下面我们先看看 unsafe.Pointer 是什么，然后再介绍如何转换。 unsafe.Pointerunsafe.Pointer 是一种特殊意义的指针，它可以包含任意类型的地址，有点类似于 C语言 里的 void* 指针，全能型的。 12345678910func main() &#123; i:= 10 ip:=&amp;i var fp *float64 = (*float64)(unsafe.Pointer(ip)) *fp = *fp * 3 fmt.Println(i)&#125; 以上示例，我们可以把 *int 转为 *float64 ,并且我们尝试了对新的 *float64 进行操作，打印输出i，就会发现i的址同样被改变。 以上这个例子没有任何实际的意义，但是我们说明了，通过 unsafe.Pointer 这个万能的指针，我们可以在 *T 之间做任何转换。 123type ArbitraryType inttype Pointer *ArbitraryType 可以看到 unsafe.Pointer 其实就是一个 *int,一个通用型的指针。 我们看下关于 unsafe.Pointer 的4个规则。 任何指针 都可以转换为 unsafe.Pointer unsafe.Pointer 可以转换为 任何指针 uintptr 可以转换为 unsafe.Pointer unsafe.Pointer 可以转换为 uintptr 前面两个规则我们刚刚已经演示了，主要用于 *T1 和 *T2 之间的转换，那么最后两个规则是做什么的呢？我们都知道 *T 是 不能计算偏移量 的，也不能进行计算，但是uintptr可以，所以我们可以把指针转为uintptr再进行偏移计算，这样我们就可以访问特定的内存了，达到对不同的内存读写的目的。 下面我们以通过指针偏移 修改Struct结构体内的字段为例，来演示 uintptr 的用法。 1234567891011121314151617func main() &#123; u:=new(user) fmt.Println(*u) pName:=(*string)(unsafe.Pointer(u)) *pName=\"张三\" pAge:=(*int)(unsafe.Pointer(uintptr(unsafe.Pointer(u))+unsafe.Offsetof(u.age))) *pAge = 20 fmt.Println(*u)&#125;type user struct &#123; name string age int&#125; 以上我们通过内存偏移的方式，定位到我们需要操作的字段，然后改变他们的值。 第一个修改user的name值的时候，因为name是第一个字段，所以不用偏移，我们获取user的指针，然后通过 unsafe.Pointer 转为*string 进行赋值操作即可。 第二个修改user的age值的时候，因为age不是第一个字段，所以我们需要内存偏移，内存偏移牵涉到的计算只能通过uintptr，所我们要先把user的指针地址转为uintptr，然后我们再通过unsafe.Offsetof(u.age)获取需要偏移的值，进行地址运算(+)偏移即可。 现在偏移后，地址已经是user的age字段了，如果要给它赋值，我们需要把uintptr转为*int才可以。所以我们通过把uintptr转为unsafe.Pointer,再转为*int就可以操作了。 这里我们可以看到，我们第二个偏移的表达式非常长，但是也千万不要把他们分段，不能像下面这样。 123temp:=uintptr(unsafe.Pointer(u))+unsafe.Offsetof(u.age)pAge:=(*int)(unsafe.Pointer(temp))*pAge = 20 栈内指针在栈扩容的时候，有了新的地址，因此uintptr只能作为指针计算的中间态，不允许使用变量保存uintptr的值。 小结unsafe是不安全的，所以我们应该尽可能少的使用它，比如内存的操纵，这是绕过Go本身设计的安全机制的，不当的操作，可能会破坏一块内存，而且这种问题非常不好定位。 当然必须的时候我们可以使用它，比如底层类型相同的数组之间的转换；比如使用sync/atomic包中的一些函数时；还有访问Struct的私有字段时，该用还是要用，不过一定要慎之又慎。 整个unsafe包都是用于Go编译器的，不用运行时，在我们编译的时候，Go编译器已经把他们都处理了。","categories":[{"name":"Golang","slug":"Golang","permalink":"http://blog.crazylaw.cn/categories/Golang/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://blog.crazylaw.cn/tags/Golang/"}]},{"title":"【大数据】- flume + kudu 操作指南","slug":"大数据/flume+kudu","date":"2021-01-05T01:56:40.000Z","updated":"2021-03-20T16:25:01.815Z","comments":true,"path":"2021/01/05/大数据/flume+kudu/","link":"","permalink":"http://blog.crazylaw.cn/2021/01/05/%E5%A4%A7%E6%95%B0%E6%8D%AE/flume+kudu/","excerpt":"前言由于我们要尝试使用kudu，架构是由flume-&gt;kudu","text":"前言由于我们要尝试使用kudu，架构是由flume-&gt;kudu [kudu-flume-sink(v1.9)] (https://github.com/apache/kudu/blob/1.9.0/java/kudu-flume-sink/src/main/java/org/apache/kudu/flume/sink/KuduSink.java) 1.9 Configuration of KuDu Sink: Property Name Default Required Description channel - Yes 要绑定读取的channel type - Yes 组件名，必须填写org.apache.kudu.flume.sink.KuduSink masterAddresses - Yes 逗号分隔kudu master地址，例子: host1:port1,host2:port2,其中端口是选填 tableName - Yes 要写入的kudu表名 batchSize 1000 No sink每批次处理最大数 ignoreDuplicateRows true No 是否忽略插入导致的重复主键错误。 timeoutMillis 10000 No Kudu写操作的超时时间，单位为毫秒 producer SimpleKuduOperationsProducer No 接收器应该使用实现了的KuduOperationsProducer 接口的的完全限定类名。 producer.* - (Varies by operations producer) 要传递给操作生产者实现的配置属性。 由于这种方式必须一个sink对应一个table，不符合我们的使用场景，我们该用了其他方案。文章待定。","categories":[{"name":"大数据","slug":"大数据","permalink":"http://blog.crazylaw.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"http://blog.crazylaw.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"flume","slug":"flume","permalink":"http://blog.crazylaw.cn/tags/flume/"},{"name":"kudu","slug":"kudu","permalink":"http://blog.crazylaw.cn/tags/kudu/"}]},{"title":"【大数据】- 记一次华为云大数据服务对接问题记录","slug":"大数据/记一次华为云大数据服务对接问题记录","date":"2021-01-05T01:56:40.000Z","updated":"2021-05-14T09:05:22.942Z","comments":true,"path":"2021/01/05/大数据/记一次华为云大数据服务对接问题记录/","link":"","permalink":"http://blog.crazylaw.cn/2021/01/05/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E8%AE%B0%E4%B8%80%E6%AC%A1%E5%8D%8E%E4%B8%BA%E4%BA%91%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%9C%8D%E5%8A%A1%E5%AF%B9%E6%8E%A5%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/","excerpt":"前言由于我们在调研是否让自建IDC大数据机房上云服务，所以华为云进行一下测试。","text":"前言由于我们在调研是否让自建IDC大数据机房上云服务，所以华为云进行一下测试。 问题一：impala的连接问题由于我们开启了Kerberos，所以我们在终端执行impala-shell的时候，默认情况下是连不上的。需要通过Kerberos进行身份校验和授权才可以访问。 我们创建了一个名字叫hiveuser的账号，目前所有的组件服务都通过该账号进行访问。 所以我们需要初始化和续期凭据 12345678910111213141516[root@node-str-coreoVpr ~]# impala-shellStarting Impala Shell without Kerberos authenticationError connecting: TTransportException, Could not connect to node-str-coreoVpr:21000Kerberos ticket found in the credentials cache, retrying the connection with a secure transport.Error connecting: TTransportException, Could not connect to node-str-coreoVpr:21000***********************************************************************************Welcome to the Impala shell.(Impala Shell v3.2.0 (f63543a) built on Wed Nov 6 11:46:33 CST 2019)Want to know what version of Impala you&#39;re connected to? Run the VERSION command tofind out!***********************************************************************************[Not connected] &gt; quit;Connection lost, reconnecting...Error connecting: TTransportException, Could not connect to node-str-coreoVpr:21000Goodbye root 可以看到，我们是连不上的。 123456789[root@node-str-coreoVpr ~]# kinit hiveuser@122451B2_394D_494B_9FCA_B045F596D6D4.COMPassword for hiveuser@122451B2_394D_494B_9FCA_B045F596D6D4.COM:[root@node-str-coreoVpr ~]# klistTicket cache: FILE:&#x2F;tmp&#x2F;krb5cc_0Default principal: hiveuser@122451B2_394D_494B_9FCA_B045F596D6D4.COMValid starting Expires Service principal01&#x2F;05&#x2F;2021 17:05:25 01&#x2F;06&#x2F;2021 17:05:21 krbtgt&#x2F;122451B2_394D_494B_9FCA_B045F596D6D4.COM@122451B2_394D_494B_9FCA_B045F596D6D4.COM 再次尝试连接 123456789101112131415[root@node-str-coreoVpr ~]# impala-shell -i node-ana-coretXnLStarting Impala Shell without Kerberos authenticationOpened TCP connection to node-ana-coretXnL:21000Error connecting: TTransportException, TSocket read 0 bytesKerberos ticket found in the credentials cache, retrying the connection with a secure transport.Opened TCP connection to node-ana-coretXnL:21000Connected to node-ana-coretXnL:21000Server version: impalad version 3.2.0 RELEASE (build 83150778f5d85f48878f611da47face9328e9e6a)***********************************************************************************Welcome to the Impala shell.(Impala Shell v3.2.0 (f63543a) built on Wed Nov 6 11:46:33 CST 2019)Press TAB twice to see a list of available commands.***********************************************************************************[node-ana-coretXnL:21000] default&gt; 可以看到，已经可以连接上了。 问题一：flume &amp;&amp; kudu目前，我们对基本配置如下： 服务 版本 flume 1.6 kudu 1.9 Kerberos 由于flume官方没有提供对应的sink。但是kudu有提供，所以我去找了kudu的sink的jar库下来。而版本对应嘛，一开始我们使用和kudu一样的1.9版本，发现api对应不上。查阅了资料之后，选择了1.4版本。 kudu-flume-sink-1.4.jar 需要把jar包放在 java运行程序的 classpath目录下，也就是 -cp 参数的目录下。由于不想通过修改GC_OPTS来指定目录，所以我选择了放在 /opt/Bigdata/MRS_2.1.0/install/FusionInsight-Flume-1.6.0/flume/lib/* 目录下。 可是谁曾想最后还要是动到 GC_OPTS到参数配置 还是类似的问题，目前遇到的问题几乎都是 Kerberos 引起的。因为以往我们并没有使用 Kerberos 作为身份认证。 123456789101112131415161718192021222324252627282930313233343536373839404142# &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;# &#x3D; 定义信息 &#x3D;# &#x3D; 华为云Agent必填: &#x3D;# &#x3D; - server &#x3D;# &#x3D; Agent中存在的sources: &#x3D;# &#x3D; - src_http_41600 &#x3D;# &#x3D; Agent中存在的channels: &#x3D;# &#x3D; - ch_kudu_table &#x3D;# &#x3D; Agent中存在的sinks: &#x3D;# &#x3D; - sink_kudu_table &#x3D;# &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;server.sources &#x3D; src_http_41600server.channels &#x3D; ch_kudu_tableserver.sinks &#x3D; sink_kudu_table# &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;# &#x3D; Http Source &#x3D;# &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;server.sources.src_http_41600.type &#x3D; httpserver.sources.src_http_41600.port &#x3D; 41600server.sources.src_http_41600.channels &#x3D; ch_kudu_table# &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;# &#x3D; Http-Kudu&#39;s Channel &#x3D;# &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;server.channels.ch_kudu_table.type &#x3D; memoryserver.channels.ch_kudu_table.capacity &#x3D; 1000server.channels.ch_kudu_table.transactionCapacity &#x3D; 100# server.sinks.sink_kudu_table.type &#x3D; logger# server.sinks.sink_kudu_table.channel &#x3D; ch_kudu_table# &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;# &#x3D; Kudu Sink &#x3D;# &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;# 组件名，必须填写&#96;org.apache.kudu.flume.sink.KuduSink&#96;server.sinks.sink_kudu_table.type &#x3D; org.apache.kudu.flume.sink.KuduSink# 要绑定读取的channelserver.sinks.sink_kudu_table.channel &#x3D; ch_kudu_tableserver.sinks.sink_kudu_table.masterAddresses &#x3D; node-master1rfFB,node-ana-corezDdi,node-master2RSDtserver.sinks.sink_kudu_table.tableName &#x3D; impala::kudu_test.my_first_table 测试的时候所采用的配置如上 1omm 17051 1 0 11:31 ? 00:00:09 &#x2F;opt&#x2F;Bigdata&#x2F;jdk1.8.0_212&#x2F;&#x2F;bin&#x2F;java -XX:OnOutOfMemoryError&#x3D;bash &#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;install&#x2F;FusionInsight-Flume-1.6.0&#x2F;flume&#x2F;bin&#x2F;out_memory_error.sh &#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;1_7_Flume&#x2F;etc %p -Xms2G -Xmx4G -XX:CMSFullGCsBeforeCompaction&#x3D;1 -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:+UseCMSCompactAtFullCollection -Djava.security.krb5.conf&#x3D;&#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;1_7_Flume&#x2F;etc&#x2F;krb5.conf -Djava.security.auth.login.config&#x3D;&#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;1_7_Flume&#x2F;etc&#x2F;jaas.conf -Dzookeeper.request.timeout&#x3D;120000 -Djavax.security.auth.useSubjectCredsOnly&#x3D;false -verbose:gc -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles&#x3D;10 -XX:GCLogFileSize&#x3D;1M -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:&#x2F;var&#x2F;log&#x2F;Bigdata&#x2F;flume&#x2F;&#x2F;flume&#x2F;flume-omm-20210106113125-%p-gc.log -Djava.security.krb5.conf&#x3D;&#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;1_5_KerberosClient&#x2F;etc&#x2F;kdc.conf -Djava.security.auth.login.config&#x3D;&#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;1_7_Flume&#x2F;etc&#x2F;jaas.conf -Dzookeeper.server.principal&#x3D;zookeeper&#x2F;hadoop.122451b2_394d_494b_9fca_b045f596d6d4.com -Dzookeeper.request.timeout&#x3D;120000 -Dsolrclient.token.enabled&#x3D;false -Dcom.amazonaws.sdk.disableCertChecking&#x3D;true -Dnet.sf.ehcache.skipUpdateCheck&#x3D;true -Dflume.instance.id&#x3D;1000 -Dflume.role&#x3D;server -Dlog4j.configuration.watch&#x3D;true -Dlog4j.configuration&#x3D;log4j.properties -Dflume_log_dir&#x3D;&#x2F;var&#x2F;log&#x2F;Bigdata&#x2F;flume&#x2F;&#x2F;flume&#x2F; -Dflume.monitoring.type&#x3D;http -Dflume.monitoring.port&#x3D;21150 -Dbeetle.application.home.path&#x3D;&#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;install&#x2F;FusionInsight-Flume-1.6.0&#x2F;flume&#x2F;conf&#x2F;service -Dflume.called.from.service -Dflume.conf.dir&#x3D;&#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;1_7_Flume&#x2F;etc -Dflume.metric.conf.dir&#x3D;&#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;install&#x2F;FusionInsight-Flume-1.6.0&#x2F;flume&#x2F;conf -Dflume.script.home&#x3D;&#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;install&#x2F;FusionInsight-Flume-1.6.0&#x2F;flume&#x2F;bin -cp &#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;1_7_Flume&#x2F;etc:&#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;install&#x2F;FusionInsight-Flume-1.6.0&#x2F;flume&#x2F;lib&#x2F;*:&#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;install&#x2F;FusionInsight-Flume-1.6.0&#x2F;flume&#x2F;conf&#x2F;service&#x2F; -Djava.library.path&#x3D;&#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;install&#x2F;FusionInsight-Flume-1.6.0&#x2F;flume&#x2F;plugins.d&#x2F;native&#x2F;native org.apache.flume.node.Application --conf-file &#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;1_7_Flume&#x2F;etc&#x2F;properties.properties --name server 启动的参数如上，其中有几个参数是我后面加的： -Djava.security.krb5.conf=/opt/Bigdata/MRS_2.1.0/1_7_Flume/etc/krb5.conf （指定krb5的配置文件路径） -Djava.security.auth.login.config=/opt/Bigdata/MRS_2.1.0/1_7_Flume/etc/jaas.conf (获取jass的认证配置文件路径) -Dzookeeper.request.timeout=120000 -Djavax.security.auth.useSubjectCredsOnly=false (通过底层获取凭据) 123456789101112131415161718192021222324252627282930[root@node-str-coreoVpr ~]# cat &#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;1_7_Flume&#x2F;etc&#x2F;jaas.confKafkaClient &#123;com.sun.security.auth.module.Krb5LoginModule requireduseKeyTab&#x3D;truekeyTab&#x3D;&quot;&#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;1_7_Flume&#x2F;etc&#x2F;user.keytab&quot;principal&#x3D;&quot;hiveuser@122451B2_394D_494B_9FCA_B045F596D6D4.COM&quot;storeKey&#x3D;trueuseTicketCache&#x3D;false;&#125;;Client &#123;com.sun.security.auth.module.Krb5LoginModule requiredstoreKey&#x3D;trueprincipal&#x3D;&quot;hiveuser@122451B2_394D_494B_9FCA_B045F596D6D4.COM&quot;useTicketCache&#x3D;falsekeyTab&#x3D;&quot;&#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;1_7_Flume&#x2F;etc&#x2F;user.keytab&quot;debug&#x3D;trueuseKeyTab&#x3D;true;&#125;;com.sun.security.jgss.initiate &#123;com.sun.security.auth.module.Krb5LoginModule requireduseKeyTab&#x3D;trueuseTicketCache&#x3D;falsedoNotPrompt&#x3D;truestoreKey&#x3D;trueprincipal&#x3D;&quot;hiveuser@122451B2_394D_494B_9FCA_B045F596D6D4.COM&quot;keyTab&#x3D;&quot;&#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;1_7_Flume&#x2F;etc&#x2F;user.keytab&quot;debug&#x3D;true;&#125;; 由于华为云自己修改过部分组件，所以不太确定为什么凭据一直失败，所以我加了 -Djavax.security.auth.useSubjectCredsOnly=false 参数，这是让我们从底层去拿凭据，而默认拿的section部分就是 com.sun.security.jgss.initiate详见源码,所以加上了 com.sun.security.jgss.initiate 之后，flume就可以连接上了kudu. 以下是我调试的时候遇到的问题。 GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos Ticket) Cause: This may occur if no valid Kerberos credentials are obtained. In particular, this occurs if you want the underlying mechanism to obtain credentials but you forgot to indicate this by setting the javax.security.auth.useSubjectCredsOnly system property value to false (for example via -Djavax.security.auth.useSubjectCredsOnly=false in your execution command). GSSException: No valid credentials provided (Mechanism level: Attempt to obtain new INITIATE credentials failed! (null)) . . . Caused by: javax.security.auth.login.LoginException: Clock skew too great Cause: Kerberos requires the time on the KDC and on the client to be loosely synchronized. (The default is within 5 minutes.) If that’s not the case, you will get this error. 附上 jgss 的异常尝试解决方案官方文档, PS：反正我根据文档没解决，不知道是不是华运自己修改的部分有什么潜规则。所以我才采用默认的jass配置。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748492021-01-06 11:31:26,235 | INFO | [lifecycleSupervisor-1-0] | Configuration provider starting | org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start(PollingPropertiesFileConfigurationProvider.java:61)2021-01-06 11:31:26,239 | INFO | [conf-file-poller-0] | Reloading configuration file:&#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;1_7_Flume&#x2F;etc&#x2F;properties.properties | org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run(PollingPropertiesFileConfigurationProvider.java:133)2021-01-06 11:31:26,236 | INFO | [main] | starting taskCounter | org.apache.flume.tools.FlumeMetricsMgr.start(FlumeMetricsMgr.java:230)2021-01-06 11:31:26,250 | INFO | [conf-file-poller-0] | Processing:sink_kudu_table | org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty(FlumeConfiguration.java:1017)2021-01-06 11:31:26,250 | INFO | [conf-file-poller-0] | Processing:sink_kudu_table | org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty(FlumeConfiguration.java:1017)2021-01-06 11:31:26,250 | INFO | [conf-file-poller-0] | Processing:sink_kudu_table | org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty(FlumeConfiguration.java:1017)2021-01-06 11:31:26,251 | INFO | [conf-file-poller-0] | Processing:sink_kudu_table | org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty(FlumeConfiguration.java:1017)2021-01-06 11:31:26,251 | INFO | [conf-file-poller-0] | Added sinks: sink_kudu_table Agent: server | org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty(FlumeConfiguration.java:931)2021-01-06 11:31:26,262 | INFO | [conf-file-poller-0] | Post-validation flume configuration contains configuration for agents: [server] | org.apache.flume.conf.FlumeConfiguration.validateConfiguration(FlumeConfiguration.java:141)2021-01-06 11:31:26,263 | INFO | [conf-file-poller-0] | Creating channels | org.apache.flume.node.AbstractConfigurationProvider.loadChannels(AbstractConfigurationProvider.java:155)2021-01-06 11:31:26,270 | INFO | [conf-file-poller-0] | Creating instance of channel ch_kudu_table type memory | org.apache.flume.channel.DefaultChannelFactory.create(DefaultChannelFactory.java:42)2021-01-06 11:31:26,274 | INFO | [conf-file-poller-0] | Created channel ch_kudu_table | org.apache.flume.node.AbstractConfigurationProvider.loadChannels(AbstractConfigurationProvider.java:210)2021-01-06 11:31:26,274 | INFO | [conf-file-poller-0] | Creating instance of source src_http_41600, type http | org.apache.flume.source.DefaultSourceFactory.create(DefaultSourceFactory.java:41)2021-01-06 11:31:26,289 | INFO | [main] | Monitored counter group for type: OTHER, name: taskcount: Successfully registered new MBean. | org.apache.flume.instrumentation.MonitoredCounterGroup.register(MonitoredCounterGroup.java:132)2021-01-06 11:31:26,290 | INFO | [main] | Component type: OTHER, name: taskcount started | org.apache.flume.instrumentation.MonitoredCounterGroup.start(MonitoredCounterGroup.java:105)2021-01-06 11:31:26,323 | INFO | [conf-file-poller-0] | Creating instance of sink: sink_kudu_table, type: org.apache.kudu.flume.sink.KuduSink | org.apache.flume.sink.DefaultSinkFactory.create(DefaultSinkFactory.java:42)2021-01-06 11:31:26,328 | WARN | [conf-file-poller-0] | No Kudu operations producer provided, using default | org.apache.kudu.flume.sink.KuduSink.configure(KuduSink.java:202)2021-01-06 11:31:26,330 | INFO | [conf-file-poller-0] | Channel ch_kudu_table connected to [src_http_41600, sink_kudu_table] | org.apache.flume.node.AbstractConfigurationProvider.getConfiguration(AbstractConfigurationProvider.java:124)2021-01-06 11:31:26,413 | INFO | [main] | ServiceServer started (at port[21151]) | org.wcc.framework.business.service.server.ServiceServer.start(ServiceServer.java:260)2021-01-06 11:31:26,413 | INFO | [main] | flume meric server startred ip:192.168.0.222,port:21151. | org.apache.flume.tools.FlumeMetricsMgr.initMetricsRpcServer(FlumeMetricsMgr.java:84)2021-01-06 11:31:26,413 | INFO | [main] | current role is server | org.apache.flume.tools.FlumeMetricsMgr.start(FlumeMetricsMgr.java:237)2021-01-06 11:31:26,429 | INFO | [main] | Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog | org.mortbay.log.Slf4jLog.info(Slf4jLog.java:67)2021-01-06 11:31:26,430 | INFO | [main] | jetty-6.1.26 | org.mortbay.log.Slf4jLog.info(Slf4jLog.java:67)2021-01-06 11:31:26,441 | INFO | [main] | Started SelectChannelConnector@localhost:21150 | org.mortbay.log.Slf4jLog.info(Slf4jLog.java:67)2021-01-06 11:31:26,442 | INFO | [main] | starting compment mon | org.apache.flume.node.Application.startCompMon(Application.java:543)2021-01-06 11:31:26,443 | INFO | [main] | started compment mon success | org.apache.flume.node.Application.startCompMon(Application.java:582)2021-01-06 11:31:26,443 | INFO | [main] | log4j dynamic load is start. | org.apache.flume.tools.LogDynamicLoad.start(LogDynamicLoad.java:59)2021-01-06 11:31:26,444 | INFO | [conf-file-poller-0] | stopping compment mon | org.apache.flume.node.Application.stopCompMon(Application.java:587)2021-01-06 11:31:26,444 | INFO | [conf-file-poller-0] | stopped compment mon success | org.apache.flume.node.Application.stopCompMon(Application.java:608)2021-01-06 11:31:26,444 | INFO | [conf-file-poller-0] | Starting new configuration:&#123; sourceRunners:&#123;src_http_41600&#x3D;EventDrivenSourceRunner: &#123; source:org.apache.flume.source.http.HTTPSource&#123;name:src_http_41600,state:IDLE&#125; &#125;&#125; sinkRunners:&#123;sink_kudu_table&#x3D;SinkRunner: &#123; policy:org.apache.flume.sink.DefaultSinkProcessor@1e1f84ee counterGroup:&#123; name:null counters:&#123;&#125; &#125; &#125;&#125; channels:&#123;ch_kudu_table&#x3D;org.apache.flume.channel.MemoryChannel&#123;name: ch_kudu_table&#125;&#125; &#125; | org.apache.flume.node.Application.startAllComponents(Application.java:206)2021-01-06 11:31:26,498 | INFO | [conf-file-poller-0] | current role is server | org.apache.flume.tools.FlumeSendAlarmMgr.start(FlumeSendAlarmMgr.java:173)2021-01-06 11:31:26,505 | INFO | [conf-file-poller-0] | Starting Channel ch_kudu_table | org.apache.flume.node.Application.startAllComponents(Application.java:217)2021-01-06 11:31:26,508 | INFO | [lifecycleSupervisor-1-0] | Monitored counter group for type: CHANNEL, name: ch_kudu_table: Successfully registered new MBean. | org.apache.flume.instrumentation.MonitoredCounterGroup.register(MonitoredCounterGroup.java:132)2021-01-06 11:31:26,508 | INFO | [lifecycleSupervisor-1-0] | Component type: CHANNEL, name: ch_kudu_table started | org.apache.flume.instrumentation.MonitoredCounterGroup.start(MonitoredCounterGroup.java:105)2021-01-06 11:31:26,508 | INFO | [conf-file-poller-0] | Starting Sink sink_kudu_table | org.apache.flume.node.Application.startAllComponents(Application.java:245)2021-01-06 11:31:26,508 | INFO | [conf-file-poller-0] | Starting Source src_http_41600 | org.apache.flume.node.Application.startAllComponents(Application.java:256)2021-01-06 11:31:26,510 | INFO | [conf-file-poller-0] | Begin start init plugins. | com.huawei.flume.PluginManager.PluginManager.&lt;init&gt;(PluginManager.java:39)2021-01-06 11:31:26,510 | INFO | [conf-file-poller-0] | Set plugins configuration file dir successful. | com.huawei.flume.PluginManager.PluginManager.&lt;init&gt;(PluginManager.java:46)2021-01-06 11:31:26,511 | INFO | [conf-file-poller-0] | Reading monitor server configuration from: &#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;1_7_Flume&#x2F;etc&#x2F;flume-check.properties | com.huawei.flume.configuration.AbstractPluginsConfiguration.loadConfig(AbstractPluginsConfiguration.java:75)2021-01-06 11:31:26,517 | WARN | [conf-file-poller-0] | Needn&#39;t to create PluginsManager, plugins is empty. | com.huawei.flume.PluginManager.PluginManager.&lt;init&gt;(PluginManager.java:55)2021-01-06 11:31:26,517 | WARN | [conf-file-poller-0] | Have not set any plugins | com.huawei.flume.PluginManager.PluginManager.start(PluginManager.java:98)2021-01-06 11:31:26,517 | INFO | [conf-file-poller-0] | starting compment mon | org.apache.flume.node.Application.startCompMon(Application.java:543)2021-01-06 11:31:26,517 | INFO | [conf-file-poller-0] | started compment mon success | org.apache.flume.node.Application.startCompMon(Application.java:582)2021-01-06 11:31:26,542 | INFO | [lifecycleSupervisor-1-0] | jetty-6.1.26 | org.mortbay.log.Slf4jLog.info(Slf4jLog.java:67)2021-01-06 11:31:26,577 | INFO | [lifecycleSupervisor-1-0] | Started SelectChannelConnector@0.0.0.0:41600 | org.mortbay.log.Slf4jLog.info(Slf4jLog.java:67)2021-01-06 11:31:26,578 | INFO | [lifecycleSupervisor-1-0] | Monitored counter group for type: SOURCE, name: src_http_41600: Successfully registered new MBean. | org.apache.flume.instrumentation.MonitoredCounterGroup.register(MonitoredCounterGroup.java:132)2021-01-06 11:31:26,578 | INFO | [lifecycleSupervisor-1-0] | Component type: SOURCE, name: src_http_41600 started | org.apache.flume.instrumentation.MonitoredCounterGroup.start(MonitoredCounterGroup.java:105)2021-01-06 11:31:27,187 | INFO | [lifecycleSupervisor-1-3] | Monitored counter group for type: SINK, name: sink_kudu_table: Successfully registered new MBean. | org.apache.flume.instrumentation.MonitoredCounterGroup.register(MonitoredCounterGroup.java:132) 可以看到，sink启动成功。 记录几个需要用到的命令： 同步kudu-flume-sink到各flume节点1str_nodes&#x3D;&quot;node-str-corelgoh node-str-corenydJ node-str-coreoVpr&quot;;for ip in $(echo $&#123;str_nodes&#125;);do scp &#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;install&#x2F;FusionInsight-Flume-1.6.0&#x2F;flume&#x2F;lib&#x2F;kudu-flume-sink-1.4.0.jar $&#123;ip&#125;:&#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;install&#x2F;FusionInsight-Flume-1.6.0&#x2F;flume&#x2F;lib&#x2F;;ssh $&#123;ip&#125; &#39;chmod 751 &#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;install&#x2F;FusionInsight-Flume-1.6.0&#x2F;flume&#x2F;lib&#x2F;kudu-flume-sink-1.4.0.jar &amp;&amp; chown omm:ficommon &#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;install&#x2F;FusionInsight-Flume-1.6.0&#x2F;flume&#x2F;lib&#x2F;kudu-flume-sink-1.4.0.jar&#39;;done 修改对应的jass配置 和 凭据信息同步1str_nodes&#x3D;&quot;node-str-corelgoh node-str-corenydJ node-str-coreoVpr&quot;;for ip in $(echo $&#123;str_nodes&#125;);do scp &#x2F;tmp&#x2F;krb5.conf $&#123;ip&#125;:&#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;1_7_Flume&#x2F;etc&#x2F;; scp &#x2F;tmp&#x2F;user.keytab $&#123;ip&#125;:&#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;1_7_Flume&#x2F;etc&#x2F;;ssh $&#123;ip&#125; &quot;sed -i -e &#39;&#x2F;principal&#x2F;s&#x2F;flume&#x2F;hiveuser&#x2F;g&#39; -e &#39;&#x2F;keyTab&#x2F;s&#x2F;\\&#x2F;opt\\&#x2F;Bigdata\\&#x2F;MRS_2.1.0\\&#x2F;install\\&#x2F;FusionInsight-Flume-1.6.0\\&#x2F;flume\\&#x2F;conf\\&#x2F;flume.keytab&#x2F;\\&#x2F;opt\\&#x2F;Bigdata\\&#x2F;MRS_2.1.0\\&#x2F;1_7_Flume\\&#x2F;etc\\&#x2F;user.keytab&#x2F;g&#39; -e &#39;&#x2F;^Client&#x2F;s&#x2F;Client&#x2F;com.sun.security.jgss.initiate&#x2F;g&#39; &#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;1_7_Flume&#x2F;etc&#x2F;jaas.conf;chown omm:wheel &#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;1_7_Flume&#x2F;etc&#x2F;jaas.conf;chown -R omm:wheel &#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;1_7_Flume&#x2F;etc&#x2F;&quot;;done 这个不是命令，但是是记得最新的GC_OPTS参数1-Xms2G -Xmx4G -XX:CMSFullGCsBeforeCompaction&#x3D;1 -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:+UseCMSCompactAtFullCollection -Djava.security.krb5.conf&#x3D;&#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;1_7_Flume&#x2F;etc&#x2F;krb5.conf -Djava.security.auth.login.config&#x3D;&#x2F;opt&#x2F;Bigdata&#x2F;MRS_2.1.0&#x2F;1_7_Flume&#x2F;etc&#x2F;jaas.conf -Dzookeeper.request.timeout&#x3D;120000","categories":[{"name":"大数据","slug":"大数据","permalink":"http://blog.crazylaw.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"http://blog.crazylaw.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"flume","slug":"flume","permalink":"http://blog.crazylaw.cn/tags/flume/"},{"name":"kudu","slug":"kudu","permalink":"http://blog.crazylaw.cn/tags/kudu/"}]},{"title":"【数据库开发知识】Rocksdb","slug":"数据库开发知识/Rocksdb","date":"2020-12-29T03:16:43.000Z","updated":"2021-03-20T16:25:01.817Z","comments":true,"path":"2020/12/29/数据库开发知识/Rocksdb/","link":"","permalink":"http://blog.crazylaw.cn/2020/12/29/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%BC%80%E5%8F%91%E7%9F%A5%E8%AF%86/Rocksdb/","excerpt":"前沿近日工作中使用了 RocksDB。RocksDB 的优点此处无需多说，它的一个 feature 是其有很多优化选项用于对 RocksDB 进行调优。欲熟悉这些参数，必须对其背后的原理有所了解，本文主要整理一些 RocksDB 的 wiki 文档，以备自己参考之用。","text":"前沿近日工作中使用了 RocksDB。RocksDB 的优点此处无需多说，它的一个 feature 是其有很多优化选项用于对 RocksDB 进行调优。欲熟悉这些参数，必须对其背后的原理有所了解，本文主要整理一些 RocksDB 的 wiki 文档，以备自己参考之用。 1 Basic Operations 先介绍一些 RocksDB 的基本操作和基本架构。 1.1 LSM 与 WriteBatch 参考文档5提到RocksDB 是一个快速存储系统，它会充分挖掘 Flash or RAM 硬件的读写特性，支持单个 KV 的读写以及批量读写。RocksDB 自身采用的一些数据结构如 LSM/SKIPLIST 等结构使得其有读放大、写放大和空间使用放大的问题。 LSM 大致结构如上图所示。LSM 树而且通过批量存储技术规避磁盘随机写入问题。 LSM 树的设计思想非常朴素, 它的原理是把一颗大树拆分成N棵小树， 它首先写入到内存中（内存没有寻道速度的问题，随机写的性能得到大幅提升），在内存中构建一颗有序小树，随着小树越来越大，内存的小树会flush到磁盘上。磁盘中的树定期可以做 merge 操作，合并成一棵大树，以优化读性能【读数据的过程可能需要从内存 memtable 到磁盘 sstfile 读取多次，称之为读放大】。RocksDB 的 LSM 体现在多 level 文件格式上，最热最新的数据尽在 L0 层，数据在内存中，最冷最老的数据尽在 LN 层，数据在磁盘或者固态盘上。RocksDB 还有一种日志文件叫做 manifest，用于记录对 sstfile 的更改，可以认为是 RocksDB 的 GIF，后面将会详述。 LSM-Tree(Log-Structured-Merge-Tree)LSM从命名上看，容易望文生义成一个具体的数据结构，一个tree。但LSM并不是一个具体的数据结构，也不是一个tree。LSM是一个数据结构的概念，是一个数据结构的设计思想。实际上，要是给LSM的命名断句，Log和Structured这两个词是合并在一起的，LSM-Tree应该断句成Log-Structured、Merge、Tree三个词汇，这三个词汇分别对应以下三点LSM的关键性质： 将数据形成Log-Structured：在将数据写入LSM内存结构之前，先记录log。这样LSM就可以将有易失性的内存看做永久性存储器。并且信任内存上的数据，等到内存容量达到threshold再集体写入磁盘。将数据形成Log-Structured，也是将整体存储结构转换成了“内存(in-memory)”存储结构。 将所有磁盘上数据不组织成一个整体索引结构，而组织成有序的文件集：因为磁盘随机读写比顺序读写慢3个数量级，LSM尽量将磁盘读写转换成顺序读写。将磁盘上的数据组织成B树这样的一个整体索引结构，虽然查找很高效，但是面对随机读写，由于大量寻道导致其性能不佳。而LSM用了一种很有趣的方法，将所有数据不组织成一个整体索引结构，而组织成有序的文件集。每次LSM面对磁盘写，将数据写入一个或几个新生成的文件，顺序写入且不能修改其他文件，这样就将随机读写转换成了顺序读写。LSM将一次性集体写入的文件作为一个level，磁盘上划分多level，level与level之间互相隔离。这就形成了，以写入数据时间线形成的逻辑上、而非物理上的层级结构，这也就是为什么LSM被命名为”tree“，但不是“tree”。 将数据按key排序，在合并不同file、level上的数据时类似merge-join：如果一直保持生成新的文件，不仅写入会造成冗余空间，而且也会大量降低读的性能。所以要高效的、周期性合并不同file、level。而如果数据是乱序的，根本做不到高效合并。所以LSM要将数据按key排序，在合并不同file、level上的数据时类似 merge-join。 很明显，LSM牺牲了一部分读的性能和增加了合并的开销，换取了高效的写性能。那LSM为什么要这么做？实际上，这就关系到对于磁盘写已经没有什么优化手段了，而对于磁盘读，不论硬件还是软件上都有优化的空间。通过多种优化后，读性能虽然仍是下降，但可以控制在可接受范围内。实际上，用于磁盘上的数据结构不同于用于内存上的数据结构，用于内存上的数据结构性能的瓶颈就在搜索复杂度，而用于磁盘上的数据结构性能的瓶颈在磁盘IO，甚至是磁盘IO的模式。 以上三段摘抄自参考文档20。个人以为，除了将随机写合并之后转化为顺写之外，LSM 的另外一个关键特性就在于其是一种自带数据 Garbage Collect 的有序数据集合，对外只提供了 Add/Get 接口，其内部的 Compaction 就是其 GC 的关键，通过 Compaction 实现了对数据的删除、附带了 TTL 的过期数据地淘汰、同一个 Key 的多个版本 Value 地合并。RocksDB 基于 LSM 对外提供了 Add/Delete/Get 三个接口，用户则基于 RocksDB 提供的 transaction 还可以实现 Update 语义。 RocksDB的三种基本文件格式是 memtable/sstfile/logfile，memtable 是一种内存文件数据系统，新写数据会被写进 memtable，部分请求内容会被写进 logfile。logfile 是一种有利于顺序写的文件系统。memtable 的内存空间被填满之后，会有一部分老数据被转移到 sstfile 里面，这些数据对应的 logfile 里的 log 就会被安全删除。sstfile 中的内容是有序的。 上图所示，所有 Column Family 共享一个 WAL 文件，但是每个 Column Family 有自己单独的 memtable &amp; ssttable(sstfile)，即 log 共享而数据分离。 一个进程对一个 DB 同时只能创建一个 rocksdb::DB 对象，所有线程共享之。这个对象内部有锁机制保证访问安全，多个线程同时进行 Get/Put/Fetch Iteration 都没有问题，但是如果直接 Iteration 或者 WriteBatch 则需要额外的锁同步机制保护 Iterator 或者 WriteBatch 对象。 基于 RocksDB 设计存储系统，要考虑到应用场景进行各种 tradeoff 设置相关参数。譬如，如果 RocksDB 进行 compaction 比较频繁，虽然有利于空间和读，但是会造成读放大；compaction 过低则会造成读放大和空间放大；增大每个 level 的 comparession 难度可以减小空间放大，但是会增加 cpu 负担，是运算时间增加换取使用空间减小；增大 SSTfile 的 data block size，则是增大内存使用量来加快读取数据的速度，减小读放大。 单独的 Get/Put/Delete 是原子操作，要么成功要么失败，不存在中间状态。 如果需要进行批量的 Get/Put/Delete 操作且需要操作保持原子属性，则可以使用 WriteBatch。 WriteBatch 还有一个好处是保持加快吞吐率。 1.2 同步写 与 异步写 默认情况下，RocksDB 的写是异步的：仅仅把数据写进了操作系统的缓存区就返回了，而这些数据被写进磁盘是一个异步的过程。如果为了数据安全，可以用如下代码把写过程改为同步写： 123rocksdb::WriteOptions write_options; write_options.sync = true; db-&gt;Put(write_options, …); 这个选项会启用 Posix 系统的 fsync(...) or fdatasync(...) or msync(..., MS_SYNC) 等函数。 异步写的吞吐率是同步写的一千多倍。异步写的缺点是机器或者操作系统崩溃时可能丢掉最近一批写请求发出的由操作系统缓存的数据，但是 RocksDB 自身崩溃并不会导致数据丢失。而机器或者操作系统崩溃的概率比较低，所以大部分情况下可以认为异步写是安全的。 RocksDB 由于有 WAL 机制保证，所以即使崩溃，其重启后会进行写重放保证数据一致性。如果不在乎数据安全性，可以把 write_option.disableWAL 设置为 true，加快写吞吐率。 RocksDB 调用 Posix API fdatasync() 对数据进行异步写。如果想用 fsync() 进行同步写，可以设置 Options::use_fsync 为 true。 1.3 Snapshots RocksDB 能够保存某个版本的所有数据（可称之为一个 Snapshot）以方便读取操作，创建并读取 Snapshot 方法如下： 1234567rocksdb::ReadOptions options; options.snapshot = db-&gt;GetSnapshot(); … apply some updates to db …. rocksdb::Iterator* iter = db-&gt;NewIterator(options); … read using iter to view the state when the snapshot was created …. delete iter; db-&gt;ReleaseSnapshot(options.snapshot); 如果 ReadOptions::snapshot 为 null，则读取的 snapshot 为 RocksDB 当前版本数据的 snapshot。 1.4 Slice &amp; PinnableSlice 不管是 it-&gt;key() 还是 it-&gt;value()，其值类型都是 rocksdb::Slice。 Slice 自身由一个长度字段[ size_t size_ ]以及一个指向外部一个内存区域的指针[ const char* data_ ]构成，返回 Slice 比返回一个 string 廉价，并不存在内存拷贝的问题。RocksDB 自身会给 key 和 value 添加一个 C-style 的 ‘\\0’，所以 slice 的指针指向的内存区域自身作为字符串输出没有问题。 Slice 与 string 之间的转换代码如下： 12345678rocksdb::Slice s1 = “hello”;std::string str(“world”);rocksdb::Slice s2 = str;OR: std::string str = s1.ToString(); assert(str == std::string(“hello”)); 但是请注意 Slice 的安全性，有代码如下： 123456rocksdb::Slice slice; if (…) &#123; std::string str = …; slice = str; &#125; Use(slice); 当退出 if 语句块后，slice 内部指针指向的内存区域已经不存在，此时再使用导致程序出问题。 Slice 自身虽然能够减少内存拷贝，但是在离开相应的 scope 之后，其值就会被释放，rocksdb v5.4.5 版本引入一个 PinnableSlice，其继承自 Slice，可替换之前 Get 接口的出参： 1234567Status Get(const ReadOptions&amp; options, ColumnFamilyHandle* column_family, const Slice&amp; key, std::string* value)virtual Status Get(const ReadOptions&amp; options, ColumnFamilyHandle* column_family, const Slice&amp; key, PinnableSlice* value) 这里的 PinnableSlice 如同 Slice 一样可以减少内存拷贝，提高读性能，但是 PinnableSlice 内部有一个引用计数功能，可以实现数据内存的延迟释放，延长相关数据的生命周期，相关详细分析详见 参考文档15。 参考文档16 提到 PinnableSlice, as its name suggests, has the data pinned in memory. The pinned data are released when PinnableSlice object is destructed or when ::Reset is invoked explicitly on it.。所谓的 pinned in memory 即为引用计数之意，文中提到内存数据释放是在 PinnableSlice 析构或者调用 ::Reset 之后。 用户也可以把一个 std::string 对象作为 PinnableSlice 构造函数的参数， 把这个 std::string 指定为 PinnableSlice 的初始内部 buffer [ rocksdb/slice.h:PinnableSlice::buf_ ]，使用方法可以参考 用新 Get 实现的旧版本的 Get： 123456789101112virtual inline Status Get(const ReadOptions&amp; options, ColumnFamilyHandle* column_family, const Slice&amp; key, std::string* value) &#123; assert(value != nullptr); PinnableSlice pinnable_val(value); assert(!pinnable_val.IsPinned()); auto s = Get(options, column_family, key, &amp;pinnable_val); if (s.ok() &amp;&amp; pinnable_val.IsPinned()) &#123; value-&gt;assign(pinnable_val.data(), pinnable_val.size()); &#125; // else value is already assigned return s; &#125; 通过 rocksdb/slice.h:PinnableSlice::PinSlice 实现代码可以看出，只有在这个函数里 PinnableSlice::pinned_ 被赋值为 true， 同时内存区域存放在 PinnableSlice::data_ 指向的内存区域，故而 PinnableSlice::IsPinned 为 true，则 内部 buffer [ rocksdb/slice.h:PinnableSlice::buf_ ] 必定为空。 具体的编程用例可参考 pinnalble_slice.cc。 1.5 Transactions 当使用 TransactionDB 或者 OptimisticTransactionDB 的时候，可以使用 RocksDB 的 BEGIN/COMMIT/ROLLBACK 等事务 API。RocksDB 支持活锁或者死等两种事务。 WriteBatch 默认使用了事务，确保批量写成功。 当打开一个 TransactionDB 的时候，如果 RocksDB 检测到某个 key 已经被别的事务锁住，则 RocksDB 会返回一个 error。如果打开成功，则所有相关 key 都会被 lock 住，直到事务结束。TransactionDB 的并发特性表现要比 OptimisticTransactionDB 好，但是 TransactionDB 的一个小问题就是不管写发生在事务里或者事务外，他都会进行写冲突检测。TransactionDB 使用示例代码如下： 12345678TransactionDB* txn_db;Status s = TransactionDB::Open(options, path, &amp;txn_db);Transaction* txn = txn_db-&gt;BeginTransaction(write_options, txn_options);s = txn-&gt;Put(“key”, “value”);s = txn-&gt;Delete(“key2”);s = txn-&gt;Merge(“key3”, “value”);s = txn-&gt;Commit();delete txn; OptimisticTransactionDB 提供了一个更轻量的事务实现，它在进行写之前不会进行写冲突检测，当对写操作进行 commit 的时候如果发生了 lock 冲突导致写操作失败，则 RocksDB 会返回一个 error。这种事务使用了活锁策略，适用于读多写少这种写冲突概率比较低的场景下，使用示例代码如下： 1234567891011DB* db;OptimisticTransactionDB* txn_db;Status s = OptimisticTransactionDB::Open(options, path, &amp;txn_db);db = txn_db-&gt;GetBaseDB();OptimisticTransaction* txn = txn_db-&gt;BeginTransaction(write_options, txn_options);txn-&gt;Put(“key”, “value”);txn-&gt;Delete(“key2”);txn-&gt;Merge(“key3”, “value”);s = txn-&gt;Commit();delete txn; 参考文档 6 详细描述了 RocksDB 的 Transactions。 1.6 Column Family CF 提供了对 DB 进行逻辑划分开来的方法，用户可以通过 CF 同时对多个 CF 的 KV 进行并行读写的方法，提高了并行度。 1.7 MemTable and Table factories RocksDB 内存中的数据格式是 skiplist，磁盘则是以 table 形式存储的 SST 文件格式。 table 格式有两种：继承自 leveldb 的文件格式【详见参考文档2】和 PlainTable 格式【详见参考文档3】。PlainTable 格式是针对 低查询延迟 或者低延迟存储媒介如 SSD 特别别优化的一种文件格式。 1.8 Block size RocksDB 把相邻的 key 放到同一个 block 中，block 是数据存储和传递的基本单元。默认 Block 的大小是 4096B，数据未经压缩。 经常进行 bulk scan 操作的用户可能希望增大 block size，而经常进行单 key 读写的用户则可能希望减小其值，官方建议这个值减小不要低于 1KB 的下限，变大也不要超过 a few megabytes。启用压缩也可以起到增大 block size 的好处。 修改 Block size 的方法是修改 Options::block_size。 1.9 Writer Buffer Options::write_buffer_size 指定了一个写内存 buffer 的大小，当这个 buffer 写满之后数据会被固化到磁盘上。这个值越大批量写入的性能越好。 RocksDB 控制写内存 buffer 数目的参数是 Options::max_write_buffer_number。这个值默认是 2，当一个 buffer 的数据被 flush 到磁盘上的时候，RocksDB 就用另一个 buffer 作为数据读写缓冲区。 ‘Options::min_write_buffer_number_to_merge’ 设定了把写 buffer 的数据固化到磁盘上时对多少个 buffer 的数据进行合并然后再固化到磁盘上。这个值如果为 1，则 L0 层文件只有一个，这会导致读放大，这个值太小会导致数据固化到磁盘上之前数据去重效果太差劲。 这两个值并不是越大越好，太大会延迟一个 DB 被重新打开时的数据加载时间。 1.10 Key Layout 在 1.8 章节里提到 “block 是数据存储和传递的基本单元”，RocksDB 的数据是一个 range 的 key-value 构成一个 Region，根据局部性原理每次访问一个 Region 的 key 的时候，有很多概率会访问其相邻的 key，每个 Region 的 keys 放在一个 block 里，多个 Region 的 keys 放在多个 block 里。 下面以文件系统作为类比，详细解释下 RocksDB 的文件系统：​​ filename -&gt; permission-bits, length, list of file_block_ids​ file_block_id -&gt; data 以多个维度组织 key 的时候，我们可能希望 filename 的前缀都是 ‘/‘， 而 file_block_id 的前缀都是 ‘0’，这样可以把他们分别放在不同的 block 里，以方便快速查询。 1.11 Checksums Rocksdb 对每个 kv 以及整体数据文件都分别计算了 checksum，以进行数据正确性校验。下面有两个选项对 checksum 的行为进行控制。 ReadOptions::verify_checksums 强制对每次从磁盘读取的数据进行校验，这个选项默认为 true。 Options::paranoid_checks 这个选项为 true 的时候，如果 RocksDB 打开一个数据检测到内部数据部分错乱，马上抛出一个错误。这个选择默认为 false。 如果 RocksDB 的数据错乱，RocksDB 会尽量把它隔离出来，保证大部分数据的可用性和正确性。 1.12 Approximate Sizes GetApproximateSizes 方法可以返回一个 key range 的磁盘占用空间大致使用量，示例代码如下： 12345rocksdb::Range ranges[2]; ranges[0] = rocksdb::Range(“a”, “c”); ranges[1] = rocksdb::Range(“x”, “z”); uint64_t sizes[2]; rocksdb::Status s = db-&gt;GetApproximateSizes(ranges, 2, sizes); 上面的 sizes[0] 返回 [a..c) key range 的磁盘使用量，而 sizes[1] 返回 [x..z) key range 的磁盘使用量。 1.13 Purging WAL files 一般情况下，RocksDB 会删除一些过时的 WAL 文件，所谓过时就是 WAL 文件里面对应的一些 key 的数据已经被固化到磁盘了。但是 RocksDB 提供了两个选项以实让用户控制 WAL 何时删除：Options::WAL_ttl_seconds 和 Options::WAL_size_limit_MB，这两个参数分别控制 WAL 文件的超时时间 和 最大文件 size。 如果这两个值都被设置为 0，则 log 不会被固化到文件系统上。 如果 Options::WAL_ttl_seconds 为 0 而 Options::WAL_size_limit_MB 不为 0， RocksDB 会每 10 分钟检测所有的 WAL 文件，如果其总体 size 超过 Options::WAL_size_limit_MB，则 RocksDB 会删除最早的日志直到满足这个值位置。一切空文件都会被删除。 如果 Options::WAL_ttl_seconds 不为 0 而 Options::WAL_size_limit_MB 为 0，RocksDB 会每 Options::WAL_ttl_seconds / 2 检测一次 WAL 文件， 所有 TTL 超过 Options::WAL_ttl_seconds 的 WAL 文件都会被删除。 如果两个值都不为 0，RocksDB 会每 10 分钟检测所有的 WAL 文件，所有不满足条件的 WAL 文件都会被删除，其中 ttl 参数优先。 1.14 Prefix Iterators 许多 LSM 引擎不支持高效的 RangeScan 操作，因为 Range 操作需要扫描所有的数据文件。一般情况下常规的技术手段是给 key 建立索引，只用遍历 key 就可以了。应用可以通过确认 prefix_extractor 指定一个可以的前缀，RocksDB 可以为这些 key prefix 建立 Bloom 索引，以加快查询速度。 1.15 Multi-Threaded Compactions 参考文档 5 的 Compaction Styles 一节提到，如果启用 Level Style Compaction, L0 存储着 RocksDB 最新的数据，Lmax 存储着比较老的数据，L0 里可能存着重复 keys，但是其他层文件则不可能存在重复 key。每个 compaction 任务都会选择 Ln 层的一个文件以及与其相邻的 Ln+1 层的多个文件进行合并，删除过期 或者 标记为删除 或者 重复 的 key，然后把合并后的文件放入 Ln+1 层。Compaction 过程会导致写放大【如写qps是10MB/s，但是实际磁盘io是50MB/s】效应，但是可以节省空间并减少读放大。 如果启用 Universal Style Compaction，则只压缩 L0 的所有文件，合并后再放入 L0 层里。 RocksDB 的 compaction 任务线程不宜过多，过多容易导致写请求被 hang 住。 1.16 Incremental Backups and Replication RocksDB 的 API GetUpdatesSince 可以让调用者从 transaction log 获知最近被更新的 key（原文意为用 tail 方式读取 transaction log），通过这个 API 可以进行数据的增量备份。 RocksDB 在进行数据备份时候，可以调用 API DisableFileDeletions 停止删除文件操作，调用 API GetLiveFiles/GetSortedWalFiles 以检索活跃文件列表，然后进行数据备份。备份工作完成以后在调用 API EnableFileDeletions 让 RocksDB 再启动过期文件淘汰工作。 1.17 Thread Pool RocksDB 会创建一个 thread pool 与 Env 对象进行关联，线程池中线程的数目可以通过 Env::SetBackgroundThreads() 设定。通过这个线程池可以执行 compaction 与 memtable flush 任务。 当 memtable flush 和 compaction 两个任务同时执行的时候，会导致写请求被 hang 住。RocksDB 建议创建两个线程池，分别指定 HIGH 和 LOW 两个优先级。默认情况下 HIGH 线程池执行 memtable flush 任务，LOW 线程池执行 compaction 任务。 相关代码示例如下： 12345678910111213#include “rocksdb/env.h”#include “rocksdb/db.h”auto env = rocksdb::Env::Default();env-&gt;SetBackgroundThreads(2, rocksdb::Env::LOW);env-&gt;SetBackgroundThreads(1, rocksdb::Env::HIGH);rocksdb::DB* db;rocksdb::Options options;options.env = env;options.max_background_compactions = 2;options.max_background_flushes = 1;rocksdb::Status status = rocksdb::DB::Open(options, “/tmp/testdb”, &amp;db);assert(status.ok()); 还有其他一些参数，可详细阅读参考文档4。​ 1.18 Bloom Filter RocksDB 的每个 SST 文件都包含一个 Bloom filter。Bloom Filter 只对特定的一组 keys 有效，所以只有新的 SST 文件创建的时候才会生成这个 filter。当两个 SST 文件合并的时候，会生成新的 filter 数据。 当 SST 文件加载进内存的时候，filter 也会被加载进内存，当关闭 SST 文件的时候，filter 也会被关闭。如果想让 filter 常驻内存，可以用如下代码设置： 1BlockBasedTableOptions::cache_index_and_filter_blocks=true 一般情况下不要修改 filter 相关参数。如果需要修改，相关设置上面已经说过，此处不再多谈，详细内容见参考文档 7。 1.19 Time to Live RocksDB 在进行 compact 的时候，会删除被标记为删除的数据，会删除重复 key 的老版本的数据，也会删除过期的数据。数据过期时间由 API DBWithTTL::Open(const Options&amp; options, const std::string&amp; name, StackableDB** dbptr, int32_t ttl = 0, bool read_only = false) 的 ttl 参数设定。 TTL 的使用有以下注意事项： 1 TTL 其值单位是秒，如果 TTL 值为 0 或者负数，则 TTL 值为 infinity，即永不过期； 2 每个 kv 被插入数据文件中的时候会带上创建时的机器 (int32_t)Timestamp 时间值； 3 compaction 时如果 kv 满足条件 Timestamp+ttl&lt;time_now，则会被淘汰掉； 4 Get/Iterator 的时候可能返回过期的 kv（compact 任务还未执行）； 5 不同的 DBWithTTL::Open 可能会带上不同的 TTL 值，此时 kv 以最大的 TTL 值为准； 6 如果 DBWithTTL::Open 的参数 read_only 为 true，则不会触发 compact 任务，不会有过期数据被删除。 2 RocksDB Memory RocksDB的内存大致有如下四个区： Block Cache Indexes and bloom filters Memtables Blocked pinned by iterators 2.1 Block Cache 第三节详述了 Block Cache，这里只给出总结性描述：它存储一些读缓存数据，它的下一层是操作系统的 Page Cache。 2.2 Indexes and bloom filters Index 由 key、offset 和 size 三部分构成，当 Block Cache 增大 Block Size 时，block 个数必会减小，index 个数也会随之降低，如果减小 key size，index 占用内存空间的量也会随之降低。 filter是 bloom filter 的实现，如果假阳率是 1%，每个key占用 10 bits，则总占用空间就是 num_of_keys * 10 bits，如果缩小 bloom 占用的空间，可以设置 options.optimize_filters_for_hits = true，则最后一个 level 的 filter 会被关闭，bloom 占用率只会用到原来的 10% 。 结合 block cache 所述，index &amp; filter 有如下优化选项： cache_index_and_filter_blocks 这个 option 如果为 true，则 index &amp; filter 会被存入 block cache，而 block cache 中的内容会随着 page cache 被交换到磁盘上，这就会大大降低 RocksDB的性能，把这个 option 设为 true 的同时也把 pin_l0_filter_and_index_blocks_in_cache 设为 true，以减小对性能的影响。 如果 cache_index_and_filter_blocks 被设置为 false （其值默认就是 false），index/filter 个数就会受 max_open_files 影响，官方建议把这个选项设置为 -1，以方便 RocksDB 加载所有的 index 和 filter 文件，最大化程序性能。 可以通过如下代码获取 index &amp; filter 内存量大小：​ 12std::string out;db-&gt;GetProperty(“rocksdb.estimate-table-readers-mem”, &amp;out); 2.3 Indexes and bloom filters block cache、index &amp; filter 都是读 buffer，而 memtable 则是写 buffer，所有 kv 首先都会被写进 memtable，其 size 是 write_buffer_size。 memtable 占用的空间越大，则写放大效应越小，因为数据在内存被整理好，磁盘上就越少的内容会被 compaction。如果 memtable 磁盘空间增大，则 L1 size 也就随之增大，L1 空间大小受 max_bytes_for_level_base option 控制。 可以通过如下代码获取 memtable 内存量大小： 12std::string out;db-&gt;GetProperty(“rocksdb.cur-size-all-mem-tables”, &amp;out); 2.4 Blocks pinned by iterators 这部分内存空间一般占用总量不多，但是如果有 100k 之多的transactions 发生，每个 iterator 与一个 data block 外加一个 L1 的 data block，所以内存使用量大约为 num_iterators * block_size * ((num_levels-1) + num_l0_files)。 可以通过如下代码获取 Pin Blocks 内存量大小：​ 1table_options.block_cache-&gt;GetPinnedUsage(); 2.5 读流程 RocksDB 的读流程分为逻辑读(logical read)和物理读(physical read)。逻辑读通常是对 cache【Block Cache &amp; Table Cache】进行读取，物理读就是直接读磁盘。 参考文档 12 详细描述了 LeveDB（RocksDB）的读流程，转述如下： 在MemTable中查找，无法命中转到下一流程； 在immutable_memtable中查找，查找不中转到下一流程； 在第0层SSTable中查找，无法命中转到下一流程； 对于L0 的文件，RocksDB 采用遍历的方法查找，所以为了查找效率 RocksDB 会控制 L0 的文件个数。 在剩余SSTable中查找。 对于 L1 层以及 L1 层以上层级的文件，每个 SSTable 没有交叠，可以使用二分查找快速找到 key 所在的 Level 以及 SSTfile。 至于写流程，请参阅 ### 5 Flush &amp; Compaction 章节内容。 2.6 memory pool 不管 RocksDB 有多少 column family，一个 DB 只有一个 WriteController，一旦 DB 中一个 column family 发生堵塞，那么就会阻塞其他 column family 的写。RocksDB 写入时间长了以后，可能会不定时出现较大的写毛刺，可能有两个地方导致 RocksDB 会出现较大的写延时：获取 mutex 时可能出现几十毫秒延迟 和 将数据写入 memtable 时候可能出现几百毫秒延时。 获取 mutex 出现的延迟是因为 flush/compact 线程与读写线程竞争导致的，可以通过调整线程数量降低毛刺时间。 至于写入 memtable 时候出现的写毛刺时间，解决方法一就是使用大的 page cache，禁用系统 swap 以及配置 min_free_kbytes、dirty_ratio、dirty_background_ratio 等参数来调整系统的内存回收策略，更基础的方法是使用内存池。 采用内存池时，memtable 的内存分配和回收流程图如下： 使用内存池时，RocksDB 的内容分配代码模块如下： 3 Block Cache Block Cache 是 RocksDB 的数据的缓存，这个缓存可以在多个 RocksDB 的实例下缓存。一般默认的Block Cache 中存储的值是未压缩的，而用户可以再指定一个 Block Cache，里面的数据可以是压缩的。用户访问数据先访问默认的 Block Cache，待无法获取后再访问用户 Cache，用户 Cache 的数据可以直接存入 page cache 中。 Cache 有两种：LRUCache 和 BlockCache。Block 分为很多 Shard，以减小竞争，所以 shard 大小均匀一致相等，默认 Cache 最多有 64 个 shards，每个 shard 的 最小 size 为 512k，总大小是 8M，类别是 LRU。 12345std::shared_ptr&lt;Cache&gt; cache = NewLRUCache(capacity); BlockedBasedTableOptions table_options; table_options.block_cache = cache; Options options; options.table_factory.reset(new BlockedBasedTableFactory(table_options)); 这个 Cache 是不压缩数据的，用户可以设置压缩数据 BlockCache，方法如下： 1table_options.block_cache_compressed = cache; 如果 Cache 为 nullptr，则RocksDB会创建一个，如果想禁用 Cache，可以设置如下 Option： 1table_options.no_block_cache = true; 默认情况下RocksDB用的是 LRUCache，大小是 8MB， 每个 shard 单独维护自己的 LRU list 和独立的 hash table，以及自己的 Mutex。 RocksDB还提供了一个 ClockCache，每个 shard 有自己的一个 circular list，有一个 clock handle 会轮询这个 circular list，寻找过时的 kv，如果 entry 中的 kv 已经被访问过则可以继续存留，相对于 LRU 好处是无 mutex lock，circular list 本质是 tbb::concurrent_hash_map，从 benchmark 来看，二者命中率相似，但吞吐率 Clock 比 LRU 稍高。 Block Cache初始化之时相关参数： capacity 总的内存使用量 num_shards_bits 把 key 的前 n bits 作为 shard id，则总 shard 的数目为 2 ^ num_shards_bits； strict_capacity_limit 在一些极端情况下 block cache 的总体使用量可能超过 capacity，如在对 block 进行读或者迭代读取的时候可能有插入数据的操作，此时可能因为加锁导致有些数据无法及时淘汰，使得总体capacity超标。如果这个选项设置为 true，则此时插入操作是被允许的，但有可能导致进程 OOM。如果设置为 false，则插入操作会被 refuse，同时读取以及遍历操作有可能失败。这个选项对每个 shard 都有效，这就意味着有的 shard 可能内存已满， 别的 shard 却有很多空闲。 high_pri_pool_ratio block中为高优先级的 block 保留多少比例的空间，这个选项只有 LRU Cache 有。 默认情况下 index 和filter block 与 block cache 是独立的，用户不能设定二者的内存空间使用量，但为了控制 RocksDB 的内存空间使用量，可以用如下代码把 index 和 filter 也放在 block cache 中： 12BlockBasedTableOptions table_options;table_options.cache_index_and_filter_blocks = true; index 与 filter 一般访问频次比 data 高，所以把他们放到一起会导致内存空间与 cpu 资源竞争，进而导致 cache 性能抖动厉害。有如下两个参数需要注意：cache_index_filter_blocks_with_high_priority 和 high_pri_pool_ratio 一样，这个参数只对 LRU Cache 有效，两者须同时生效。这个选项会把 LRU Cache 划分为高 prio 和低 prio 区，data 放在 low 区，index 和 filter 放在 high 区，如果高区占用的内存空间超过了 capacity * high_pri_pool_ratio，则会侵占 low 区的尾部数据空间。 pin_l0_filter_and_index_blocks_in_cache 把 level0 的 index 以及 filter block 放到 Block Cache 中，因为 l0 访问频次最高，一般内存容量不大，占用不了多大内存空间。 SimCache 用于评测 Cache 的命中率，它封装了一个真正的 Cache，然后用给定的 capacity 进行 LRU 测算，代码如下:​​```c++​// This cache is the actual cache use by the DB.​std::shared_ptr cache = NewLRUCache(capacity);​// This is the simulated cache.​std::shared_ptr sim_cache = NewSimCache(cache, sim_capacity, sim_num_shard_bits);​BlockBasedTableOptions table_options;​table_options.block_cache = sim_cache; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647大概只有容量的 2% 会被用于测算。### 4 [Column Families](https:&#x2F;&#x2F;github.com&#x2F;facebook&#x2F;rocksdb&#x2F;wiki&#x2F;Column-Families)---RocksDB 3.0 以后添加了一个 Column Family【后面简称 CF】 的feature，每个 kv 存储之时都必须指定其所在的 CF。RocksDB为了兼容以往版本，默认创建一个 “default” 的CF。存储 kv 时如果不指定 CF，RocksDB 会把其存入 “default” CF 中。#### 4.1 Option---RocksDB 的 Option 有 Options, ColumnFamilyOptions, DBOptions 三种。ColumnFamilyOptions 是 table 级的，而 Options 是 DB 级的，Options 继承自 ColumnFamilyOptions 和 DBOptions，它一般影响只有一个 CF 的 DB，如 “default”。每个 CF 都有一个 Handle：ColumnFamilyHandle，在 DB 指针被 delete 前，应该先 delete ColumnFamilyHandle。如果 ColumnFamilyHandle 指向的 CF 被别的使用者通过 DropColumnFamily 删除掉，这个 CF 仍然可以被访问，因为其引用计数不为 0.在以 Read&#x2F;Write 方式打开一个 DB 的时候，需要指定一个由所有将要用到的 CF string name 构成的 ColumnFamilyDescriptor array。不管 “default” CF 使用与否，都必须被带上。CF 存在的意义是所有 table 共享 WAL，但不共享 memtable 和 table 文件，通过 WAL 保证原子写，通过分离 table 可快读快写快删除。每次 flush 一个 CF 后，都会新建一个 WAL，都这并不意味着旧的 WAL 会被删除，因为别的 CF 数据可能还没有落盘，只有所有的 CF 数据都被 flush 且所有的 WAL 有关的 data 都落盘，相关的 WAL 才会被删除。RocksDB 会定时执行 CF flush 任务，可以通过 &#96;Options::max_total_wal_size&#96; 查看已有多少旧的 CF 文件已经被 flush 了。RocksDB 会在磁盘上依据 LSM 算法对多级磁盘文件进行 compaction，这会影响写性能，拖慢程序性能，可以通过 &#96;WriteOptions.low_pri &#x3D; true&#96; 降低 compaction 的优先级。#### 4.2 [Set Up Option](https:&#x2F;&#x2F;github.com&#x2F;facebook&#x2F;rocksdb&#x2F;wiki&#x2F;Set-Up-Options)---RocksDB 有很多选项以专门的目的进行设置，但是大部分情况下不需要进行特殊的优化。这里只列出一个常用的优化选项。* &#96;cf_options.write_buffer_size&#96;CF 的 write buffer 的最大 size。最差情况下 RocksDB 使用的内存量会翻倍，所以一般情况下不要轻易修改其值。* Set block cache size这个值一般设置为 RocksDB 想要使用的内存总量的 1&#x2F;3，其余的留给 OS 的 page cache。&#96;&#96;&#96;C++ BlockBasedTableOptions table_options; … \\\\ set options in table_options options.table_factory.reset(new std::shared_ptr&lt;Cache&gt; cache &#x3D; NewLRUCache(&lt;your_cache_size&gt;); table_options.block_cache &#x3D; cache; BlockBasedTableFactory(table_options)); 本进程的所有的 DB 所有的 CF 所有的 table_options 都必须使用同一个 cahce 对象，或者让所有的 DB 所有的 CF 使用同一个 table_options。 cf_options.compression, cf_options.bottonmost_compression 选择压缩方法跟你的机器、CPU 能力以及内存磁盘空间大小有关，官方推荐 cf_options.compression 使用 kLZ4Compression，cf_options.bottonmost_compression 使用 kZSTD，选用的时候要确认你的机器有这两个库，这两个选项也可以分别使用 Snappy 和 Zlib。 Bloom filter 官方真正建议修改的参数只有这个 filter 参数。如果大量使用迭代方法，不要修改这个参数，如果大量调用 Get() 接口，建议修改这个参数。修改方法如下： 1table_options.filter_policy.reset(NewBloomFilterPolicy(10, false)); 一个可能的优化设定如下： 1234567cf_options.level_compaction_dynamic_level_bytes &#x3D; true;options.max_background_compactions &#x3D; 4;options.max_background_flushes &#x3D; 2;options.bytes_per_sync &#x3D; 1048576;table_options.block_size &#x3D; 16 * 1024;table_options.cache_index_and_filter_blocks &#x3D; true;table_options.pin_l0_filter_and_index_blocks_in_cache &#x3D; true; 上面只是罗列了一些优化选项，这些选项也只能在进程启动的时候设定。更多的选项请详细阅读参考文档1。 4.3 WriteOption &amp; Persistence 参考文档 5 的 Persistence 一节提到，RocksDB 每次接收写请求的时候，请求内容会先被写入 WAL transaction log，然后再把数据写入 memfile 里面。 Put 函数的参数 WriteOptions 里有一个选项可以指明是否需要把写请求的内容写入 WAL log 里面。 RocksDB 内部有一个 batch-commit 机制，通过一次 commit 批量地在一次 sync 操作里把所有 transactions log 写入磁盘。 5 Flush &amp; Compaction &amp; Merge RocksDB 的内存数据在 memtable 中存着，有 active-memtable 和 immutable-memtable 两种。active-memtable 是当前被写操作使用的 memtable，当 active-memtable 空间写满之后( Options.write_buffer_size 控制其内存空间大小 )这个空间会被标记为 readonly 成为 immutable-memtable。memtable 实质上是一种有序 SkipList，所以写过程其实是写 WAL 日志和数据插入 SkipList 的过程。 RocksDB 的数据删除过程跟写过程相同，只不过 插入的数据是 “key:删除标记”。 immutable-memtable 被写入 L0 的过程被称为 flush 或者 minor compaction。flush 的触发条件是 immutable memtable数量超过 min_write_buffer_number_to_merge。flush 过程以 column family 为单位，一个 column family 会使用一个或者多个 immutable-memtable，flush 会一次把所有这些文件合并后写入磁盘的 L0 sstfile 中。 在 compaction 过程中如果某个被标记为删除的 key 在某个 snapshot 中存在，则会被一直保留，直到 snapshot 不存在才会被删除。 RocksDB 的 compaction 策略分为 Universal Compaction 和 Leveled Compaction 两种。两种策略分别有不同的使用场景，下面分两个章节详述。综述就是 Leveled Compaction 有利于减小空间放大却会增加读放大，Universal Compaction 有利于减少读放大却会增大空间放大。 5.1 Leveled Compaction compaction 的触发条件是文件个数和文件大小。L0 的触发条件是 sst 文件个数（level0_file_num_compaction_trigger 控制），触发 compaction score 是 L0 sst 文件个数与 level0_file_num_compaction_trigger 的比值或者所有文件的 size 超过 max_bytes_for_level_base。L1 ~ LN 的触发条件则是 sst 文件的大小。 如果 level_compaction_dynamic_level_bytes 为 false，L1 ~ LN 每个 level 的最大容量由 max_bytes_for_level_base 和 max_bytes_for_level_multiplier 决定，其 compaction score 就是当前总容量与设定的最大容量之比，如果某 level 满足 compaction 的条件则会被加入 compaction 队列。 如果 level_compaction_dynamic_level_bytes 为 true，则 Target_Size(Ln-1) = Target_Size(Ln) / max_bytes_for_level_multiplier，此时如果某 level 计算出来的 target 值小于 max_bytes_for_level_base / max_bytes_for_level_multiplier，则 RocksDB 不会再这个 level 存储任何 sst 数据。 5.1.1 Compaction Score compact 流程的 Compaction Score，不同 level 的计算方法不一样，下面先列出 L0 的计算方法。其中 num 代表未 compact 文件的数目。 Param Value Description Score level0_file_num_compaction_trigger 4 num 为 4 时，达到 compact 条件 num &lt; 20 时 Score = num / 4 level0_slowdown_writes_trigger 20 num 为 20 时，RocksDB 会减慢写入速度 20 &lt;= num &amp;&amp; num &lt; 24 时 Score = 10000 level0_stop_writes_trigger 24 num 为 24 时，RocksDB 停止写入文件，尽快对 L0 进行 compact 24 &lt;= num 时 Score = 1000000 对于 L1+ 层，score = Level_Bytes / Target_Size。 5.1.2 Level Max Bytes 每个 level 容量总大小的计算前文已经提过， Param Value Description max_bytes_for_level_base 10485760 L1 总大小 max_bytes_for_level_multiplier 10 最大乘法因子 max_bytes_for_level_multiplier_addtl[2…6] 1 L2 ~ L6 总大小调整参数 每个 level 的总大小计算公式为 Level_max_bytes[N] = Level_max_bytes[N-1] * max_bytes_for_level_multiplier^(N-1)*max_bytes_for_level_multiplier_additional[N-1]。 5.1.3 compact file 上面详述了 compact level 的选择，但是每个 level 具体的 compact 文件对象， L0 层所有文件会被选做 compact 对象，因为它们有很高的概率所有文件的 key range 发生重叠。 对于 L1+ 层的文件，先对所有文件的大小进行排序以选出最大文件。 LevelDB 的文件选取过程如下： LN 中每个文件都一个 seek 数值，其默认值非零，每次访问后这个数值减 1，其值越小说明访问越频繁。sst 文件的策略如下： 1 选择 seek 次数为 0 的文件进行 merge，如果没有 seek 次数为 0 的文件，则从第一个文件开始进行 compact； 2 一次 compact 后记录本次结束的 key，下次 compact 开始时以这个 key 为起始继续进行 compact。 5.1.4 compaction 大致的 compaction 流程大致为： 1 找到 score 最高的 level； 2 根据一定策略从 level 中选择一个 sst 文件进行 compact，L0 的各个 sst 文件之间 key range 【minkey， maxkey】 有重叠，所以可能一次选取多个； 3 获取 sst 文件的 minkey 和 maxkey; 4 从 level + 1 中选取出于 (minkey, maxkey) 用重叠的 sst 文件，有重叠的文件则把文件与 level 中的文件进行合并（merge - sort）作为目标文件，没有重叠文件则把原始文件作为目标文件； 5 对目标文件进行压缩后放入 level + 1 中。 5.1.5 并行 Compact 与 sub-compact 参数 max_background_compactions 大于 1 时，RocksDB 会进行并行 Compact，但 L0 和 L1 层的 Compaction 任务不能进行并行。 一次 compaction 只能 compact 一个或者多个文件，这会约束整体 compaction 速度。用户可以设置 max_subcompactions 参数大于 1，RocksDB 如上图一样尝试把一个文件拆为多个 sub，然后启动多个线程执行 sub-compact。 5.2 Universal Compaction Univesal Compaction 主要针对 L0。当 L0 中的文件个数多于 level0_file_num_compaction_trigger，则启动 compact。 L0 中所有的 sst 文件都可能存在重叠的 key range，假设所有的 sst 文件组成了文件队列 R1,R2,R3,…,Rn，R1 文件的数据是最新的，R2 其次，Rn 则包含了最老的数据，其 compact 流程如下： 1 如果空间放大超过 max_size_amplification_percent，则对所有的 sst 进行 compaction（就是所谓的 full compaction）； 2 如果前size(R1)小于size(R2)在一定比例，默认1%，则与R1与R2一起进行compaction，如果（R1+R2)*(100+ratio)%100&lt;R3，则将R3也加入到compaction任务中，依次顺序加入sst文件； 如果第1和第2种情况都没有compaction，则强制选择前N个文件进行合并。 Universal Compaction 主要针对低写放大场景，跟 Leveled Compaction 相比一次合并文件较多但因为一次只处理 L0 所以写放大整体较低，但是空间放大效应比较大。 RocksDB 还支持一种 FIFO 的 compaction。FIFO 顾名思义就是先进先出，这种模式周期性地删除旧数据。在 FIFO 模式下，所有文件都在 L0，当 sst 文件总大小超过阀值 max_table_files_size，则删除最老的 sst 文件。参考文档21中提到可以基于 FIFO compaction 机制把 RocksDB 当做一个时序数据库：对于 FIFO 来说，它的策略非常的简单，所有的 SST 都在 Level 0，如果超过了阈值，就从最老的 SST 开始删除，其实可以看到，这套机制非常适合于存储时序数据。 整个 compaction 是 LSM-tree 数据结构的核心，也是rocksDB的核心，详细内容请阅读 参考文档8 和 参考文档9。 5.4 Merge RocksDB 自身之提供了 Put/Delete/Get 等接口，若需要在现有值上进行修改操作【或者成为增量更新】，可以借助这三个操作进行以下操作实现之： 调用 Get 接口然后获取其值； 在内存中修改这个值； 调用 Put 接口写回 RocksDB。 如果希望整个过程是原子操作，就需要借助 RocksDB 的 Merge 接口了。参考文档14 给出了 RocksDB Merge 接口定义如下： 1 封装了read - modify - write语义，对外统一提供简单的抽象接口； 2 减少用户重复触发Get操作引入的性能损耗； 3 通过决定合并操作的时间和方式，来优化后端性能，并达到并不改变底层更新的语义； 4 渐进式的更新，来均摊更新带来带来的性能损耗，以得到渐进式的性能提升。 RocksDB 提供了一个 MergeOperator 作为 Merge 接口，其中一个子类 AssociativeMergeOperator 可在大部分场景下使用，其定义如下： 12345678910111213141516171819202122232425262728293031323334// The simpler, associative merge operator.class AssociativeMergeOperator : public MergeOperator &#123; public: virtual ~AssociativeMergeOperator() &#123;&#125; // Gives the client a way to express the read -&gt; modify -&gt; write semantics // key: (IN) 操作对象 KV 的 key // existing_value:(IN) 操作对象 KV 的 value，如果为 null 则意味着 KV 不存在 // value: (IN) 新值，用于替换/更新 @existing_value // new_value: (OUT) 客户端负责把 merge 后的新值填入这个变量 // logger: (IN) Client could use this to log errors during merge. // // Return true on success. // All values passed in will be client-specific values. So if this method // returns false, it is because client specified bad data or there was // internal corruption. The client should assume that this will be treated // as an error by the library. virtual bool Merge(const Slice&amp; key, const Slice* existing_value, const Slice&amp; value, std::string* new_value, Logger* logger) const = 0; private: // Default implementations of the MergeOperator functions virtual bool FullMergeV2(const MergeOperationInput&amp; merge_in, MergeOperationOutput* merge_out) const override; virtual bool PartialMerge(const Slice&amp; key, const Slice&amp; left_operand, const Slice&amp; right_operand, std::string* new_value, Logger* logger) const override;&#125;; RocksDB AssociativeMergeOperator 被称为关联性 Merge Operator，参考文档14 给出了关联性的定义： 调用Put接口写入RocksDB的数据的格式和Merge接口是相同的 用用户自定义的merge操作，可以将多个merge操作数合并成一个 **MergeOperator还可以用于非关联型数据类型的更新。** 例如，在RocksDB中保存json字符串，即Put接口写入data的格式为合法的json字符串。而Merge接口只希望更新json中的某个字段。所以代码可能是这样： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950 // Put/store the json string into to the database db_-&gt;Put(put_option_, \"json_obj_key\", \"&#123; employees: [ &#123;first_name: john, last_name: doe&#125;, &#123;first_name: adam, last_name: smith&#125;] &#125;\"); // Use a pre-defined \"merge operator\" to incrementally update the value of the json string db_-&gt;Merge(merge_option_, \"json_obj_key\", \"employees[1].first_name = lucy\"); db_-&gt;Merge(merge_option_, \"json_obj_key\", \"employees[0].last_name = dow\");`AssociativeMergeOperator无法处理这种场景，因为它假设Put和Merge的数据格式是关联的。我们需要区分Put和Merge的数据格式，也无法把多个merge操作数合并成一个。这时候就需要Generic MergeOperator。` // The Merge Operator // // Essentially, a MergeOperator specifies the SEMANTICS of a merge, which only // client knows. It could be numeric addition, list append, string // concatenation, edit data structure, ... , anything. // The library, on the other hand, is concerned with the exercise of this // interface, at the right time (during get, iteration, compaction...) class MergeOperator &#123; public: virtual ~MergeOperator() &#123;&#125; // Gives the client a way to express the read -&gt; modify -&gt; write semantics // key: (IN) The key that's associated with this merge operation. // existing: (IN) null indicates that the key does not exist before this op // operand_list:(IN) the sequence of merge operations to apply, front() first. // new_value: (OUT) Client is responsible for filling the merge result here // logger: (IN) Client could use this to log errors during merge. // // Return true on success. Return false failure / error / corruption. // 用于对已有的值做Put或Delete操作 virtual bool FullMerge(const Slice&amp; key, const Slice* existing_value, const std::deque&lt;std::string&gt;&amp; operand_list, std::string* new_value, Logger* logger) const = 0; // This function performs merge(left_op, right_op) // when both the operands are themselves merge operation types. // Save the result in *new_value and return true. If it is impossible // or infeasible to combine the two operations, return false instead. // 如果连续多次对一个 key 进行操作，则可以可以借助 PartialMerge 将两个操作数合并. virtual bool PartialMerge(const Slice&amp; key, const Slice&amp; left_operand, const Slice&amp; right_operand, std::string* new_value, Logger* logger) const = 0; // The name of the MergeOperator. Used to check for MergeOperator // mismatches (i.e., a DB created with one MergeOperator is // accessed using a different MergeOperator) virtual const char* Name() const = 0; &#125;; 工作原理 当调用DB::Put()和DB:Merge()接口时, 并不需要立刻计算最后的结果. RocksDB将计算的动作延后触发, 例如在下一次用户调用Get, 或者RocksDB决定做Compaction时. 所以, 当merge的动作真正开始做的时候, 可能积压(stack)了多个操作数需要处理. 这种情况就需要MergeOperator::FullMerge来对existing_value和一个操作数序列进行计算, 得到最终的值. PartialMerge 和 Stacking 有时候, 在调用FullMerge之前, 可以先对某些merge操作数进行合并处理, 而不是将它们保存起来, 这就是PartialMerge的作用: 将两个操作数合并为一个, 减少FullMerge的工作量.当遇到两个merge操作数时, RocksDB总是先会尝试调用用户的PartialMerge方法来做合并, 如果PartialMerge返回false才会保存操作数. 当遇到Put/Delete操作, 就会调用FullMerge将已存在的值和操作数序列传入, 计算出最终的值. 使用Associative Merge的场景 merge 操作数的格式和Put相同多个顺序的merge操作数可以合并成一个 使用Generic Merge的场景 merge 操作数的格式和Put不同当多个merge操作数可以合并时，PartialMerge()方法返回true *!!!: 本节文字摘抄自 参考文档14 。 6 磁盘文件 参考文档 12 列举了 RocksDB 磁盘上数据文件的种类： * db的操作日志 * 存储实际数据的 SSTable 文件 * DB的元信息 Manifest 文件 * 记录当前正在使用的 Manifest 文件，它的内容就是当前的 manifest 文件名 * 系统的运行日志，记录系统的运行信息或者错误日志。 * 临时数据库文件，repair 时临时生成的。manifest 文件记载了所有 SSTable 文件的 key 的范围、level 级别等数据。 上面是 leveldb 的架构图，可以作为参考，明白各种文件的作用。 6.1 log 文件 log 文件就是 WAL。 如上图，log 文件的逻辑单位是 Record，物理单位是 block，每个 Record 可以存在于一个 block 中，也可以占用多个 block。Record 的详细结构见上图文字部分，其 type 字段的意义见下图。 从上图可见 Record type的意义：如果某 KV 过长则可以用多 Record 存储。 6.2 Manifest 文件 RocksDB 整个 LSM 树的信息需要常驻内存，以让 RocksDB 快速进行 kv 查找或者进行 compaction 任务，RocksDB 会用文件把这些信息固化下来，这个文件就是 Manifest 文件。RocksDB 称 Manifest 文件记录了 DB 状态变化的事务性日志，也就是说它记录了所有改变 DB 状态的操作。主要内容有事务性日志和数据库状态的变化。 RocksDB 的函数 VersionSet::LogAndApply 是对 Manifest 文件的更新操作，所以可以通过定位这个函数出现的位置来跟踪 Manifest 的记录内容。 Manifest 文件作为事务性日志文件，只要数据库有变化，Manifest都会记录。其内容 size 超过设定值后会被 VersionSet::WriteSnapShot 重写。 RocksDB 进程 Crash 后 Reboot 的过程中，会首先读取 Manifest 文件在内存中重建 LSM 树，然后根据 WAL 日志文件恢复 memtable 内容。 上图是 leveldb 的 Manifest 文件结构，这个 Manifest 文件有以下文件内容： coparator名、log编号、前一个log编号、下一个文件编号、上一个序列号，这些都是日志、sstable文件使用到的重要信息，这些字段不一定必然存在； 其次是compact点，可能有多个，写入格式为{kCompactPointer, level, internal key} 其后是删除文件，可能有多个，格式为{kDeletedFile, level, file number}。 最后是新文件，可能有多个，格式为{kNewFile, level, file number, file size, min key, max key}。 RocksDB MANIFEST文件所保存的数据基本是来自于VersionEdit这个结构，MANIFEST包含了两个文件，一个log文件一个包含最新MANIFEST文件名的文件，Manifest的log文件名是这样 MANIFEST-(seqnumber)，这个seq会一直增长，只有当 超过了指定的大小之后，MANIFEST会刷新一个新的文件，当新的文件刷新到磁盘(并且文件名更新)之后，老的文件会被删除掉，这里可以认为每一次MANIFEST的更新都代表一次snapshot，其结构描述如下： MANIFEST = { CURRENT, MANIFEST-&lt;seq-no&gt;* } CURRENT = File pointer to the latest manifest log MANIFEST-&lt;seq no&gt; = Contains snapshot of RocksDB state and subsequent modifications在RocksDB中任意时间存储引擎的状态都会保存为一个Version(也就是SST的集合)，而每次对Version的修改都是一个VersionEdit,而最终这些VersionEdit就是 组成manifest-log文件的内容。 下面就是MANIFEST的log文件的基本构成: version-edit = Any RocksDB state change version = { version-edit* } manifest-log-file = { version, version-edit* } = { version-edit* }关于 VersionSet 相关代码分析见参考文档13。 6.3 SSTfile SSTfile 结构如下： &lt;beginning_of_file&gt; [data block 1] [data block 2] … [data block N] [meta block 1: filter block] [meta block 2: stats block] [meta block 3: compression dictionary block] … [meta block K: future extended block] [metaindex block] [index block] [Footer] &lt;end_of_file&gt;LevelDB 的 SSTfile 结构如下： 见参考文档12，SSTtable 大致分为几个部分： 数据块 Data Block，直接存储有序键值对，是 SSTfile 的数据实体； Meta Block，存储Filter相关信息； Meta Index Block，对Meta Block的索引，它只有一条记录，key是meta index的名字（也就是Filter的名字），value为指向meta index的位置； Index Block，是对Data Block的索引，对于其中的每个记录，其key &gt;=Data Block最后一条记录的key，同时&lt;其后Data Block的第一条记录的key；value是指向data index的位置信息； Footer，指向各个分区的位置和大小。 block 结构如下图： record 结构如下图： Footer 结构如下图： 6.4 memtable memtable 中存储了一些 metadata 和 data，data 在 skiplist 中存储。metadata 数据如下（源自参考文档 12）： 当前日志句柄； 版本管理器、当前的版本信息（对应 compaction）和对应的持久化文件标示； 当前的全部db配置信息比如 comparator 及其对应的 memtable 指针； 当前的状态信息以决定是否需要持久化 memtable 和合并 sstable； sstable 文件集合的信息。 6.5 VersionSet RocksDB 的 Version 表示一个版本的 metadata，其主要内容是 FileMetaData 指针的二维数组，分层记录了所有的SST文件信息。 FileMetaData 数据结构用来维护一个文件的元信息，包括文件大小，文件编号，最大最小值，引用计数等信息，其中引用计数记录了被不同的Version引用的个数，保证被引用中的文件不会被删除。 Version中还记录了触发 Compaction 相关的状态信息，这些信息会在读写请求或 Compaction 过程中被更新。在 CompactMemTable 和 BackgroundCompaction 过程中会导致新文件的产生和旧文件的删除，每当这个时候都会有一个新的对应的Version生成，并插入 VersionSet 链表头部，LevelDB 用 VersionEdit 来表示这种相邻 Version 的差值。 VersionSet 结构如上图所示，它是一个 Version 构成的双向链表，这些Version按时间顺序先后产生，记录了当时的元信息，链表头指向当前最新的Version，同时维护了每个Version的引用计数，被引用中的Version不会被删除，其对应的SST文件也因此得以保留，通过这种方式，使得LevelDB可以在一个稳定的快照视图上访问文件。 VersionSet中除了Version的双向链表外还会记录一些如LogNumber，Sequence，下一个SST文件编号的状态信息。 6.6 MetaData Restore 本节内容节选自参考文档 12。 为了避免进程崩溃或机器宕机导致的数据丢失，LevelDB 需要将元信息数据持久化到磁盘，承担这个任务的就是 Manifest 文件，每当有新的Version产生都需要更新 Manifest。 新增数据正好对应于VersionEdit内容，也就是说Manifest文件记录的是一组VersionEdit值，在Manifest中的一次增量内容称作一个Block。 Manifest Block 的详细结构如上图所示。 上图最上面的流程显示了恢复元信息的过程，也就是一次应用 VersionEdit 的过程，这个过程会有大量的临时 Version 产生，但这种方法显然太过于耗费资源，LevelDB 引入 VersionSet::Builder 来避免这种中间变量，方法是先将所有的VersoinEdit内容整理到VersionBuilder中，然后一次应用产生最终的Version，详细流程如上图下边流程所示。 数据恢复的详细流程如下： 依次读取Manifest文件中的每一个Block， 将从文件中读出的Record反序列化为VersionEdit； 将每一个的VersionEdit Apply到VersionSet::Builder中，之后从VersionSet::Builder的信息中生成Version； 计算compaction_level_、compaction_score_； 将新生成的Version挂到VersionSet中，并初始化VersionSet的manifest_file_number_， next_file_number_，last_sequence_，log_number_，prev_log_number_ 信息； 6.7 Snapshot RocksDB 每次进行更新操作就会把更新内容写入 Manifest 文件，同时它会更新版本号。 版本号是一个 8 字节的证书，每个 key 更新时，除了新数据被写入数据文件，同时记录下 RocksDB 的版本号。RocksDB 的 Snapshot 数据仅仅是逻辑数据，并没有对应的真实存在的物理数据，仅仅对应一下当前 RocksDB 的全局版本号而已，只要 Snapshot 存在，每个 key 对应版本号的数据在后面的更新、删除、合并时会一并存在，不会被删除，以保证数据一致性。 6.7.1 Checkpoints Checkpoints 是 RocksDB 提供的一种 snapshot，独立的存在一个单独的不同于 RocksDB 自身数据目录的目录中，既可以 ReadOnly 模式打开，也可以 Read-Write 模式打开。Checkpoints 被用于全量或者增量 Backup 机制中。 如果 Checkpoints 目录和 RocksDB 数据目录在同一个文件系统上，则 Checkpoints 目录下的 SST 是一个 hard link【SST 文件是 Read-Only的】，而 manifest 和 CURRENT 两个文件则会被拷贝出来。如果 DB 有多个 Column Family，wal 文件也会被复制，其时间范围足以覆盖 Checkpoints 的起始和结束，以保证数据一致性。 如果以 Read-Write 模式打开 Checkpoints 文件，则其中过时的 SST 文件会被删除掉。 6.8 Backup RocksDB 提供了 point-of-time 数据备份功能，可以调用 BackupEngine::CreateNewBackup(db, flush_before_backup = false) 接口进行数据备份， 其大致流程如下： 禁止删除文件（sst 文件和 log 文件）； 调用 GetLiveFiles() 获取当前的有效文件，如 table files, current, options and manifest file; 将 RocksDB 中的所有的 sst/Manifest/配置/CURRENT 等有效文件备份到指定目录； GetLiveFiles() 接口返回的 SST 文件如果已经被备份过，则这个文件不会被重新复制到目标备份目录，但是 BackupEngine 会对这个文件进行 checksum 校验，如果校验失败则会中止备份过程。 如果 flush_before_backup 为 false，则BackupEngine 会调用 GetSortedWalFiles() 接口把当前有效的 wal 文件也拷贝到备份目录； 重新允许删除文件。 sst 文件只有在 compact 时才会被删除，所以禁止删除就相当于禁止了 compaction。别的 RocksDB 在获取这些备份数据文件后会依据 Manifest 文件重构 LSM 结构的同时，也能恢复出 WAL 文件，进而重构出当时的 memtable 文件。 在进行 Backup 的过程中，写操作是不会被阻塞的，所以 WAL 文件内容会在 backup 过程中发生改变。RocksDB 的 flush_before_backup 选项用来控制在 backup 时是否也拷贝 WAL，其值为 true 则不拷贝。 6.8.1 Backup 编程接口 RocksDB 提供的 Backup 接口使用方法详见 参考文档17。include/rocksdb/utilities/backupable_db.h 主要提供了 BackupEngine 和 BackupEngineReadOnly，分别用于备份数据和恢复数据。 BackupEngine备份数据是增量式备份【设置选项 BackupableDBOptions::share_table_files 】，调用 BackupEngine::CreateNewBackup() 接口进行备份后，可以调用接口 BackupEngine::GetBackupInfo()获取备份文件的信息：ID、timestamp、size、file number 和 metadata【用户自定义数据】。 备份 DB 目录见上图，各个备份文件的 size 是其 private 目录下数据与 shared 目录下数据之和，shared 下面存储的数据是各个备份公共的数据，所以所有备份文件的 size 之和可能大于实际占用的磁盘空间大小。meta 目录下各个文件的格式详见 utilities/backupable/backupable_db.cc，上图中 meta/1内容如下： 12345671536821592 # checksum1 # backup ID4 # private file numberprivate/1/MANIFEST-000008 crc32 272357318private/1/OPTIONS-000011 crc32 3039312718private/1/CURRENT crc32 1581506767private/1/000009.log crc32 3494278128 Private 目录则包含一些非 SST 文件：options, current, manifest, WALs。如果 Options::share_table_files 为false，则 private 目录会存储 SST 文件。如果 Options::share_table_files 为 true 且 Options::share_files_with_checksum 为 false，shared 目录包含一些 SST 文件，SST 文件命名与原 RocksDB 目录下命名一致，所以在一个备份目录下只能备份一个 RocksDB 实例的数据。 接口 BackupEngine::VerifyBackups() 用于对备份数据进行校验，但是仅仅根据 meta 目录下各个 ID 文件记录的文件 size 与 相应的 private 目录下的文件的 size 是否相等，并不会进行 checksum 校验， 校验 checksum 需要读取数据文件，比较费时。另外需要注意的是，这个接口相应的 BackupEngine 句柄只能由BackupEngine::CreateNewBackup() 创建，也即只能在进行文件备份且句柄未失效前进行数据校验，因为校验时所依据的数据是在备份过程中产生的。 接口 BackupEngineReadOnly::RestoreDBFromBackup(backup_id, db_dir, wal_dir,restore_options) 用于备份数据恢复，参数 db_dir 和wal_dir大部分场景下都是同一个目录，但在 参考文档18 所提供的把 RocksDB 当做纯内存数据库的使用场景下， db_dir 在内存虚拟文件系统上，而 wal_dir 则是一个磁盘文件目录。进行数据恢复时，这个接口还会根据 meta 下相应 ID 记录的 备份数据 checksum 对 private 目录下的数据进行校验，发错错误时返回 Status::Corruption 错误。 6.8.2 Backup 性能优化 BackupEngine::Open() 启用时需要进行一些初始化工作，所以它会消耗一些时间。例如需要把本地 RocksDB 数据备份到远端的 HDFS 上，这个过程就可能消耗多个 network round-trip，所以在实际使用中不要频繁创建 BackupEngine 对象。 加快 BackupEngine 对象的方式之一是通过调用 PurgeOldBackups(N)来删除非必要的备份文件，接口 PurgeOldBackups(N) 本身之意就是只保留最近的 N 个备份，多余的会被删除掉。也可以通过调用 DeleteBackup(id) 接口根据备份 ID 删除某个确定的备份。 初始化 BackupEngine 对象过后，备份的速度就取决于本地与远端的媒介运行速度了。例如，如果本地媒介是 HDD，在其自身饱和运转之后就算是打开再多的线程也无济于事。如果媒介是一个小的 HDFS 集群，其表现也不会很好。如果本地是 SSD 而远端是一个大的 HDFS 集群，则相较于单线程， 16 个备份线程会被备份时间缩短 2/3。 6.8.3 高级编程接口 BackupEngine::CreateNewBackupWithMetadata() 用于设置 metadata，例如设置你能辨识的备份 ID，metadata 可以通过 BackupEngine::GetBackupInfo() 获取； rocksdb::LoadLatestOptions() or rocksdb:: LoadOptionsFromFile() RocksDB 现在也能对 RocksDB 的 options 进行备份，可以通过这两个接口获取相应备份的 Options； BackupableDBOptions::backup_env 用于设置备份目录的 ENV； BackupableDBOptions::backup_dir 用于设置备份文件的根目录； BackupableDBOptions::share_table_files 如果这个选项为 true，则 BackupEngine 会进行增量备份，把所有的 SST 文件存储到 “shared/“ 子目录，其危险是 SST 文件名字可能相同【在多个 RocksDB 对象共用同一备份目录的场景下】； BackupableDBOptions::share_files_with_checksum 在多个 RocksDB 对象共用同一备份目录的场景下，SST 文件名字可能相同，把这个选项设置为 true 可以处理这个冲突，SST 文件会被 BackupEngine 通过 checksum/size/seqnum 三个参数进行校验； BackupableDBOptions::max_background_operations 这个参数用于设置备份和恢复数据的线程数，在使用分布式文件系统如 HDFS 场景下，这个参数会大大提高备份和恢复的效率； BackupableDBOptions::info_log 用于设置 LOG 对象，可以在备份和恢复数据时进行日志输出； BackupableDBOptions::sync 如果设置为 true，BackupEngine 会调用 fsync 系统接口进行文件数据和 metadata 的数据写入，以防备系统重启或者崩溃时的数据不一致现象，大部分情况下如果为追求性能，这个参数可以设置为 false； BackupableDBOptions::destroy_old_data 如果这个选项为 true，新的 BackupEngine 被创建出来之后备份目录下旧的备份数据会被清空； BackupEngine::CreateNewBackup(db, flush_before_backup = false) flush_before_backup 被设置为 true 时，BackupEngine 首先 flush memtable，然后再进行数据复制，而 WAL log 文件不会被复制，因为 flush 时候它会被删掉，如果这个为 false 则相应的 WAL 日志文件也会被复制以保证备份数据与当前 RocksDB 状态一致； 7 FAQ 官方 wiki 【参考文档 11】提供了一份 FAQ，下面节选一些比较有意义的建议，其他内容请移步官方文档。 1 如果机器崩溃后重启，则 RocksDB 能够恢复的数据是同步写【WriteOptions.sync=true】调用 DB::SyncWAL() 之前的数据 或者已经被写入 L0 的 memtable 的数据都是安全的； 2 可以通过 GetIntProperty(cf_handle, “rocksdb.estimate-num-keys&quot;) 获取一个 column family 中大概的 key 的个数； 3 可以通过 GetAggregatedIntProperty(“rocksdb.estimate-num-keys&quot;, &amp;num_keys) 获取整个 RocksDB 中大概的 key 的总数，之所以只能获取一个大概数值是因为 RocksDB 的磁盘文件有重复 key，而且 compact 的时候会进行 key 的淘汰，所以无法精确获取； 4 Put()/Write()/Get()/NewIterator() 这几个 API 都是线程安全的； 5 多个进程可以同时打开同一个 RocksDB 文件，但是其中只能有一个写进程，其他的都只能通过 DB::OpenForReadOnly() 对 RocksDB 进行只读访问； 6 当进程中还有线程在对 RocksDB 进行 读、写或者手工 compaction 的时候，不能强行关闭它； 7 RocksDB 本身不建议使用大 key，但是它支持的 key 的最大长度是 8MB，value 的最大长度是 3GB； 8 RocksDB 最佳实践：一个线程进行写，且以顺序方式写；以 batch 方式把数据写入 RocksDB；使用 vector memtable；确保 options.max_background_flushes 最少为 4；插入数据之前设置关闭自动 compact，把 options.level0_file_num_compaction_trigger/options.level0_slowdown_writes_trigger/options.level0_stop_writes_trigger 三个值放大，数据插入后再启动调用 compact 函数进行 compaction 操作。 如果调用了Options::PrepareForBulkLoad()，后面三个方法会被自动启用； 9 关闭 RocksDB 对象时，如果是通过 DestroyDB() 去关闭时，这个 RocksDB 还正被另一个进程访问，则会造成不可预料的后果； 10 可以通过 DBOptions::db_paths/DBOptions::db_log_dir/DBOptions::wal_dir 三个参数分别存储 RocksDB 的数据，这种情况下如果要释放 RocksDB 的数据可以通过 DestroyDB() 这个 API 去执行删除任务； 11 当 BackupOptions::backup_log_files 或者 flush_before_backup 的值为 true 的时候，如果程序调用 CreateNewBackup() 则 RocksDB 会创建 point-in-time snapshot，RocksDB进行数据备份的时候不会影响正常的读写逻辑； 12 RocksDB 启动之后不能修改 prefix extractor； 13 SstFileWriter API 可以用来创建 SST 文件，如果这些 SST 文件被添加到别的 RocksDB 数据库发生 key range 重叠，则会导致数据错乱； 14 编译 RocksDB 的 gcc 版本 最低是 4.7，推荐 4.8 以上； 15 单个文件系统如 ext3 或者 xfs 可以使用多个磁盘，然后让 RocksDB 在这个文件系统上运行进而使用多个磁盘； 16 使用多磁盘时，RAID 的 stripe size 不能小于 64kb，推荐使用1MB； 17 RocksDB 可以针对每个 SST 文件通过 ColumnFamilyOptions::bottommost_compression 使用不同的压缩的方法； 18 当多个 Handle 指向同一个 Column Family 时，其中一个线程通过 DropColumnFamily() 删除一个 CF 的时候，其引用计数会减一，直至为 0 时整个 CF 会被删除； 19 RocksDB 接受一个写请求的时候，可能因为 compact 会导致 RocksDB 多次读取数据文件进行数据合并操作； 20 RocksDB 不直接支持数据的复制，但是提供了 API GetUpdatesSince() 供用户调用以获取某个时间点以后更新的 kv； 21 Options 的 block_size 是指 block 的内存空间大小，与数据是否压缩无关； 22 options.prefix_extractor 一旦启用，就无法继续使用 Prev() 和 SeekToLast() 两个 API，可以把 ReadOptions.total_order_seek 设置为 true，以禁用 prefix iterating； 23 当 BlockBaseTableOptions::cache_index_and_filter_blocks 的值为 true 时，在进行 Get() 调用的时候相应数据的 bloom filter 和 index block 会被放进 LRU cache 中，如果这个参数为 false 则只有 memtable 的 index 和 bloom filter 会被放进内存中； 24 当调用 Put() 或者 Write() 时参数 WriteOptions.sync 的值为 true，则本次写以前的所有 WriteOptions.disableWAL 为 false 的写的内容都会被固化到磁盘上； 25 禁用 WAL 时，DB::Flush() 只对单个 Column Family 的数据固化操作是原子的，对多个 Column Family 的数据操作则不是原子的，官方考虑将来会支持这个 feature； 26 当使用自定义的 comparators 或者 merge operators 时，ldb 工具就无法读取 sst 文件数据； 27 RocksDB 执行前台的 Get() 和 Write() 任务遇到错误时，它会返回 rocksdb::IOError 具体值； 28 RocksDB 执行后台任务遇到错误时 且 options.paranoid_checks 值为 true，则 RocksDB 会自动进入只读模式； 29 RocksDB 一个线程执行 compact 的时候，这个任务是不可取消的； 30 RocksDB 一个线程执行 compact 任务的时候，可以在另一个线程调用 CancelAllBackgroundWork(db, true) 以中断正在执行的 compact 任务； 31 当多个进程打开一个 RocksDB 时，如果指定的 compact 方式不一样，则后面的进程会打开失败； 32 Column Family 使用场景：(1) 不同的 Column Family 可以使用不同的 setting/comparators/compression types/merge operators/compaction filters；(2) 对数据进行逻辑隔离，方便分别删除；(3) 一个 Column Family 存储 metadata，另一个存储 data； 33 使用一个 RocksDB 就是使用一个物理存储系统，使用一个 Column Family 则是使用一个逻辑存储系统，二者主要区别体现在 数据备份、原子写以及写性能表现上。DB 是数据备份和复制以及 checkpoint 的基本单位，但是 Column Family 则利用 BatchWrite，因为这个操作是可以跨 Column Family 的，而且多个 Column Family 可以共享同一个 WAL，多个 DB 则无法做到这一点； 34 RocksDB 不支持多列； 35 RocksDB 的读并不是无锁的，有如下情况：(1) 访问 sharded block cache (2) 如果 table cache options.max_open_files 不等于 -1 (3) 如果 flush 或者 compaction 刚刚完成，RocksDB 此时会使用全局 mutex lock 以获取 LSM 树的最新 metadata (4) RocksDB 使用的内存分配器如 jemalloc 有时也会加锁，这四种情况总体很少发生，总体可以认为读是无锁的； 36 如果想高效的对所有数据进行 iterate，则可以创建 snapshot 然后再遍历； 37 如果一个 key space range (minkey, maxkey) 很大，则使用 Column Family 对其进行 sharding，如果这个 range 不大则不要单独使用一个 Column Family； 38 RocksDB 没有进行 compaction 的时候，可以通过 rocksdb.estimate-live-data-size 可以估算 RocksDB 使用的磁盘空间； 39 Snapshot 仅仅存在于逻辑概念上，其对应的实际物理文件可能正被 compaction 线程执行写任务；Checkpoint 则是实际物理文件的一个镜像，或者说是一个硬链接，而出处于同样的 Env 下【都是 RocksDB 数据文件格式】；而 backup 虽然也是物理数据的镜像，但是与原始数据处于不同的 Env 下【如 backup 可能在 HDFS 上】； 40 推荐使用压缩算法 LZ4，Snappy 次之，压缩之后如果为了更好的压缩效果可以使用 Zlib； 41 即使没有被标记为删除的 key，也没有数据过期，RocksDB 仍然会执行 compaction，以提高读性能； 42 RocksDB 的 key 和 value 是存在一起的，遍历一个 key 的时候，RocksDB 已经把其 value 读入 cache 中； 43 对于一个离线 DB 可以通过 “sst_dump –show_properties –command=none” 命令获取特定 sst 文件的 index &amp; filter 的 size，对于正在运行的 DB 可以通过读取 DB 的属性 “kAggregatedTableProperties” 或者调用 DB::GetPropertiesOfAllTables() 获取 DB 的 index &amp; filter 的 size。 参考文档 1 RocksDB Tuning Guide 2 Rocksdb BlockBasedTable Format 3 PlainTable Format 4 Thread Pool 5 RocksDB Basics 6 Transactions 7 Bloom Filter 8 Universal Compaction 9 Leveled Compaction 10 Time to Live 11 RocksDB-FAQ 12 设计思路和主要知识点 13 RocksDB · MANIFEST文件介绍 14 RocksDB. Merge Operator 15 使用PinnableSlice减少Get时的内存拷贝 16 PinnableSlice; less memcpy with point lookups 17 How to backup RocksDB? 18 RocksDB系列十三:How to persist in memory RocksDB database? 19 Checkpoints 20 LSM-Tree与RocksDB 21 自动调优 RocksDB","categories":[{"name":"数据结构","slug":"数据结构","permalink":"http://blog.crazylaw.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"数据库","slug":"数据结构/数据库","permalink":"http://blog.crazylaw.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"数据库开发知识","slug":"数据库开发知识","permalink":"http://blog.crazylaw.cn/tags/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%BC%80%E5%8F%91%E7%9F%A5%E8%AF%86/"}]},{"title":"【Golang】- 使用golang重写tga服务","slug":"Golang/使用golang重写tga服务","date":"2020-12-17T01:46:51.000Z","updated":"2021-09-06T02:03:31.268Z","comments":true,"path":"2020/12/17/Golang/使用golang重写tga服务/","link":"","permalink":"http://blog.crazylaw.cn/2020/12/17/Golang/%E4%BD%BF%E7%94%A8golang%E9%87%8D%E5%86%99tga%E6%9C%8D%E5%8A%A1/","excerpt":"前言早期，在我们的部门中后端的技术栈语言主要有三种语言，分别是php/python/erlang，其中用于做服务的是 php/erlang。 在我们的体系中，日志采集服务体系目前都是用erlang写的，而php写的服务多是基于swoole写的一些基础服务。 在tga服务中，我们需要从kafka -&gt; 服务 -&gt; 本地文件的模式。 业务据流图如下： 123456789101112131415161718192021222324252627282930313233343536+---------------------------------------------------------------+| || || +---------------------------------------------+ || | | || | | || | kafka服务 | || | | || | | || | | || +----------------------+----------------------+ || | || | || +----------------------v-----------------------+ | +----------------------------+| | | | | || | | | | || | | | | || | mthinkingdata服务 | &lt;---------------------+ logbus服务 || | | | | || | | | | || | | | | || +----------------------+-----------------------+ | +----------------------------+| | || | || +----------------------v------------------------+ || | | || | | || | | || | 本地文件 +-------+| | | || | | || | | || +-----------------------------------------------+ || |+---------------------------------------------------------------+ 于是我们试探性的自研基于swoole的kafka客户端，我们自己实现根据kafka协议的封包，解包，流程处理。(swoole-kafka)mthinkingdata服务就是我们基于kafka-swoole研发的业务服务。 1234567891011121314151617+------------+| || message1 +------------+| | |+------------+ | |+------------+ +------+--------+| | | || message2 +-----+ snappy压缩 || | | |+------------+ +------+--------+ |+---------------+ || | || message[3..n] +----------+| |+---------------+ 在这个过程中，我们发现php对于cpu密集型的处理存在瓶颈，因为我们在生产者一方如果发送多条协议的情况下，会经过 snappy 算法的压缩再推送以便减少network-io（增加cpu-io）。消费者在接受消息的时候也是被压缩过的数据，所以我们需要解压，在这个解压的过程中，是个十分消费cpu的过程，即使我们当时是基于swoole4.3的协程版本来处理，不行的是的抢占式协程当时并没有很好的完成，我们没办法达到快速的接受多个数据包的行为。消费速度也并不是特别理想。在这个大环境下，我们还需要借助redis来作为中间的存储。而redis是单线程的，我们在这个过程中，试过使用pipeline等手段减少tcp中的响应包的带来的性能损耗。但是由于redis只能利用单核的缘故，批量处理一批指令后，最高的cpu利用率接近100%下无法再增长。也因此，我们的服务注定无法达到很好的性能测试。 我们得出结论，当时服务的瓶颈在于： php语言本身的性能 swoole协程不支持抢占式调度 未实现动态伸缩扩展worker数量（感兴趣可以去看看kafka-swoole的架构分享） redis未能利用多核，cpu利用率达到峰值","text":"前言早期，在我们的部门中后端的技术栈语言主要有三种语言，分别是php/python/erlang，其中用于做服务的是 php/erlang。 在我们的体系中，日志采集服务体系目前都是用erlang写的，而php写的服务多是基于swoole写的一些基础服务。 在tga服务中，我们需要从kafka -&gt; 服务 -&gt; 本地文件的模式。 业务据流图如下： 123456789101112131415161718192021222324252627282930313233343536+---------------------------------------------------------------+| || || +---------------------------------------------+ || | | || | | || | kafka服务 | || | | || | | || | | || +----------------------+----------------------+ || | || | || +----------------------v-----------------------+ | +----------------------------+| | | | | || | | | | || | | | | || | mthinkingdata服务 | &lt;---------------------+ logbus服务 || | | | | || | | | | || | | | | || +----------------------+-----------------------+ | +----------------------------+| | || | || +----------------------v------------------------+ || | | || | | || | | || | 本地文件 +-------+| | | || | | || | | || +-----------------------------------------------+ || |+---------------------------------------------------------------+ 于是我们试探性的自研基于swoole的kafka客户端，我们自己实现根据kafka协议的封包，解包，流程处理。(swoole-kafka)mthinkingdata服务就是我们基于kafka-swoole研发的业务服务。 1234567891011121314151617+------------+| || message1 +------------+| | |+------------+ | |+------------+ +------+--------+| | | || message2 +-----+ snappy压缩 || | | |+------------+ +------+--------+ |+---------------+ || | || message[3..n] +----------+| |+---------------+ 在这个过程中，我们发现php对于cpu密集型的处理存在瓶颈，因为我们在生产者一方如果发送多条协议的情况下，会经过 snappy 算法的压缩再推送以便减少network-io（增加cpu-io）。消费者在接受消息的时候也是被压缩过的数据，所以我们需要解压，在这个解压的过程中，是个十分消费cpu的过程，即使我们当时是基于swoole4.3的协程版本来处理，不行的是的抢占式协程当时并没有很好的完成，我们没办法达到快速的接受多个数据包的行为。消费速度也并不是特别理想。在这个大环境下，我们还需要借助redis来作为中间的存储。而redis是单线程的，我们在这个过程中，试过使用pipeline等手段减少tcp中的响应包的带来的性能损耗。但是由于redis只能利用单核的缘故，批量处理一批指令后，最高的cpu利用率接近100%下无法再增长。也因此，我们的服务注定无法达到很好的性能测试。 我们得出结论，当时服务的瓶颈在于： php语言本身的性能 swoole协程不支持抢占式调度 未实现动态伸缩扩展worker数量（感兴趣可以去看看kafka-swoole的架构分享） redis未能利用多核，cpu利用率达到峰值 方案针对以上几个问题，我们逐一得出解决方案 使用golang语言（语言优先在公司内部博客有比较） 使用基于golang的嵌入式存储服务作为中间存储服务（badger）（rocksdb的使用在尝试中） 也因此催生出了使用golang重写tga服务（mtga）的需求。以其中一个项目开发为例子(项目编号：24) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172├── README.md├── _build # 部署的目录结构│ ├── README.md│ ├── bin │ │ └── mtga # 编译后的二进制文件│ ├── config│ │ ├── common.env│ │ ├── msource.yml│ │ ├── mtga.local.yml│ │ └── mtga.yml│ ├── mctl # 操作入口脚本│ ├── scripts│ │ └── init.sh # 第一次部署项目需要执行的初始化脚本│ ├── settings│ │ ├── mthinkingdata:filter.24.json│ │ └── mthinkingdata:metadata.24.json│ └── tmp├── _local_build # 本地开发部署的目录结构│ ├── README.md│ ├── bin│ │ └── mtga│ ├── config│ │ ├── common.env│ │ ├── msource.yml│ │ ├── mtga.local.yml│ │ └── mtga.yml│ ├── mctl│ ├── scripts│ │ └── init.sh│ ├── settings│ │ ├── mthinkingdata:filter.24.json│ │ └── mthinkingdata:metadata.24.json│ └── tmp├── build│ ├── Dockerfile│ ├── Jenkinsfile│ └── README.md└── src ├── Makefile # 便于构建服务 ├── app # 业务代码 │ ├── bussiness.go # 消费者的业务核心代码 │ ├── config │ │ ├── app.go │ │ ├── redis_keys.go │ │ ├── setting_tools.go │ │ └── settings.go │ ├── main.go │ └── reporter │ └── notify.go ├── cmd # cli终端命令 │ ├── clear_failure_queue.go # 清理失败队列 │ ├── failure_queue_count.go # 失败队列数量 │ ├── kafka_lag.go # 当前消费者阻塞总数据量 │ ├── offset_checker.go # topic中各个partition的当前offset以及阻塞情况 │ ├── restart.go # 重启服务 │ ├── root.go │ ├── start.go # 启动服务 │ ├── start_not_daemon.go # 以非守护进程的方式启动服务 │ └── stop.go # 暂停服务 ├── config # 配置目录 │ ├── msource.yml │ ├── mtga.local.yml │ └── mtga.yml ├── go.mod ├── go.sum ├── main.go ├── settings │ ├── mthinkingdata:filter.24.json │ └── mthinkingdata:metadata.24.json └── test ├── setting_test.go └── setting_tools_test.go 早期我们还没抛弃redis的时候，持续占用cpu100%的话就会出现这个。如果不用pipe模式的话，tps测试只有2500-3500之间。抛弃redis使用了badger之后， 结合业务逻辑，tps:1w/s左右 其中用到内部的组件包含如下： commentjson go-graceful-daemon logbdev metl-sdk msink msource mtga &amp;&amp; msource123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081// 转换前&#123; \"headers\":&#123; \"app_id\":24, \"log_name\":\"log_item\" &#125;, \"logs\":&#123; \"account_name\":\"4866014107\", \"action\":2146, \"agent_id\":36, \"amount\":1, \"bag_amount\":0, \"bag_type\":0, \"bind_type\":1, \"client_version\":\"\", \"device_id\":\"\", \"end_time\":0, \"idfa\":\"\", \"imei\":\"\", \"is_internal\":0, \"item_id\":31103003, \"mac\":\"\", \"mtime\":1608184243, \"pid\":1608184244000008, \"platform\":3100, \"quality\":0, \"regrow\":14, \"role_id\":6001310009300, \"role_level\":800, \"server_id\":5001, \"server_version\":\"\", \"sn\":\"205914E03BF640CB76\", \"star\":0, \"start_time\":1608184243, \"upf\":3100, \"via\":\"36|3100\", \"zero_dateline\":1608134400 &#125;&#125;// 转换后&#123; \"#account_id\":\"10289100005300x\", \"#distinct_id\":\"\", \"#event_name\":\"t_log_item\", \"#ip\":\"\", \"#time\":\"2020-12-17 13:52:39\", \"#type\":\"track\", \"properties\":&#123; \"account_name\":\"1608153237001005108\", \"action\":1005, \"agent_id\":11, \"amount\":-1, \"bag_amount\":1, \"bag_type\":0, \"bind_type\":1, \"client_version\":\"\", \"device_id\":\"\", \"end_time\":0, \"idfa\":\"\", \"imei\":\"\", \"is_internal\":0, \"item_id\":10203101, \"mac\":\"\", \"mtime\":1608184359, \"pid\":1608184359000031, \"platform\":101, \"quality\":0, \"regrow\":1, \"role_id\":110289100005300, \"role_level\":160, \"server_id\":289, \"server_version\":\"\", \"sn\":\"\", \"star\":0, \"start_time\":1608184359, \"upf\":101, \"via\":\"11|101\", \"zero_dateline\":1608134400 &#125;&#125; 数据在msource到mtga的交互 改版前： 12345678910111213141516171819202122232425262728293031323334 +-----------------------------------------+ | +---------+ | | | msource | | | +---------+ | | | | +--------------------------------+ | | | | | | | spoutKafka(channel) | | | | +-&gt;X+-------------------------+ | | | | | | +-------------^----------^-------+ | +-------------v----------------+ | | | | |-----------+ | | | | | || 业务服务 | | | | | | +-----------+ |+------------+ | | | | | || | | +--------------------------------+ | | XXXXXXXXXXXXXXX || kafka +---------&gt;X+ | +-------+ | | | | | X1.过滤的数据 X || | | | | | badge | | | | | | XXXXXXXXXXXXXXX |+------------+ | | | +-------+ | | | | | | | | | | | | | | | | | | +------+-------+ | | | | +---------------+ | | ^---------&gt; | | | ^--------------------+2.失败的数据 | | | | | 等待ack队列 &lt;-----^ | | | | +---------------+ | | | | | | | | | | | | | | +--------------+ | | | | | | | | | | | | | | | | | | +--------------+ | | | | | | +------------------+ | | | | +--^ &lt;--------------------------+3.正常处理完的数据 | | | | | 业务失败队列 | | | | | +------------------+ | | | | &lt;------------v | | | | | +--------------+ | | +------------------------------+ | +--------------------------------+ | +-----------------------------------------+ 改版后： 12345678910111213141516171819202122232425262728293031323334 +-----------------------------------------+ | +---------+ | +-----------------------+ | | msource | | | | | +---------+ | | 特定条件下提交offse &lt;----------+ | | | | | | +--------------------------------+ | +-----------------------+ | | | | | | | +--&gt; spoutKafka(channel) | | | | | | （有缓冲） +-&gt;X+-------------------------+ | | | | | | | | | | +------------------------^-------+ | +-------------v----------------+ | | | | | |-----------+ | | | | | | || 业务服务 | | | | | | | +-----------+ | |+------------+ | | | | | | || | | | +--------------------------------+ | | XXXXXXXXXXXXXXX | || kafka +---------&gt;X+ | +-------+ | | | | X1.过滤的数据 X | || | | | | badge | | | | | XXXXXXXXXXXXXXX | |+------------+ | | +-------+ ^--------&gt; | | | | | | | | | | | | | | | +-------+------+ | | | +---------------+ | | | | | &lt;-----------+X+------------------+2.失败的数据 | | | | | | 业务失败队列 | | | | +---------------+ | | | | | | | | | | | | | +--------------+ | | | | | | | | | | | | | | | | | +------------------+ | | | +--------------------------------+ | | |3.正常处理完的数据 +---------&gt; | | | +------------------+ | | | | | | | +------------------------------+ | | +-----------------------------------------+ 每10s记录一次本地offset 接收到信号量（SIGINT/SIGTERM）到时候也提交一次 1234567891011121314151617181920212223242526272829303132// 接收到信号量到时候也提交一次go func() &#123; ch := make(chan os.Signal, 1) signal.Notify(ch, syscall.SIGINT, syscall.SIGTERM) for _ = range ch &#123; t.Stop() logic.ackf.RLock() for _, partitionMessage := range logic.acks &#123; for _,msg := range partitionMessage &#123; logic.Sk.Ack(msg) &#125; &#125; logic.ackf.RUnlock() &#125;&#125;()// 每10s记录一次本地offsett := time.NewTicker(10 * time.Second)go func() &#123; for &#123; select &#123; case &lt;-t.C: logic.ackf.RLock() for _, partitionMessage := range logic.acks &#123; for _,msg := range partitionMessage &#123; logic.Sk.Ack(msg) &#125; &#125; logic.ackf.RUnlock() &#125; &#125;&#125;() 12345678910111213141516// 服务worker启动核心逻辑wg := sync.WaitGroup&#123;&#125;logic := new(CoreLogic)logic.Sk = sklogic.acks = map[string]map[int32]*kafka.Message&#123;&#125;logic.ackf = sync.RWMutex&#123;&#125;for i := 0; i &lt; config.AppConfig.Worker; &#123; wg.Add(1) go func() &#123; defer wg.Done() logic.Consume() &#125;() i++ &#125;wg.Wait() 123456789101112131415161718192021222324252627// bussiness.go 的核心代码type CoreLogic struct &#123; Sk *msource.SpoutKafka ackf sync.RWMutex acks map[string]map[int32]*kafka.Message&#125;func (business *CoreLogic) Consume() &#123; ... // 从正常队列来的数据 if msg.TopicPartition.Topic != nil &#123; business.ackf.Lock() if _, ok := business.acks[*msg.TopicPartition.Topic]; !ok &#123; business.acks[*msg.TopicPartition.Topic] = map[int32]*kafka.Message&#123;&#125; &#125; business.acks[*msg.TopicPartition.Topic][msg.TopicPartition.Partition] = msg business.ackf.Unlock() &#125; else &#123; // 从失败队列来的数据，独立写入，独立Ack err = p.Write(fileName, string(sinkBuffer)) if err != nil &#123; logbdev.Error(err) &#125; business.Sk.Ack(msg) &#125;&#125; 由于我们的消费者需要从msource中把消息拉出来，所以设置了一个Corelogic的结构体，其中包含了 sk,acks,ackf三个属性。 由于我们需要异步的提交offset，所以需要设置 ackf 的锁来确保数据的完整性 1234// 读取消息的方式for msg := range business.Sk.MessageChan() &#123; ...&#125; golang的格式化时间戳很奇葩，需要以&quot;2006-01-02 15:04:05&quot;为格式进行格式化。 1234// yyyy-MM-dd hh:mm:ss// yyyy-MM-dd H:i:sjsonArray[\"#time\"] = time.Unix(time.Now().Unix(), 0).Format(\"2006-01-02 15:04:05\") msourcemsource是kafka和本地存储的桥梁(通信服务)，间接得做着服务可靠性的保证。（数据不丢，不重，方便查看堵塞情况） 12345678910.&#x2F;mctlUsage: mtga [command]Available Commands: clear_failure_queue 清空失败队列 failure_queue_count 失败队列数量 kafka_lag 消费阻塞 offset_checker offset的情况 以下命令都是msource提供出来的api，再由业务服务封装成命令 例如：offset_checker 12345678910.---.----------------.-----------.------------.------------.------------.-----.| # | Topic | Partition | Low | High | Current | Lag |+---+----------------+-----------+------------+------------+------------+-----+| 0 | mulog_clean_24 | 0 | 6171720626 | 6197511755 | 6197511494 | 261 || 1 | mulog_clean_24 | 1 | 6169152656 | 6197196879 | 6197196555 | 324 || 2 | mulog_clean_24 | 2 | 6169656725 | 6195509715 | 6195509483 | 232 || 3 | mulog_clean_24 | 3 | 6172416843 | 6197496752 | 6197496518 | 234 || 4 | mulog_clean_24 | 4 | 6170706518 | 6197423974 | 6197423659 | 315 || 5 | mulog_clean_24 | 5 | 6168091223 | 6196757252 | 6196756991 | 261 |&#39;---&#39;----------------&#39;-----------&#39;------------&#39;------------&#39;------------&#39;-----&#39; 例如：kafka_lag/failure_queue_count 11234 msource如何做到由业务系统控制指定offset中消费呢？ 我们还是通过了badger来存储各个topic-partition的offset。服务在启动的时候会取各个partition的offset，如果存在的话，就设置需要从对应的offset开始读取数据。如果不存在对应的offset的话，那么就根据你的策略从最早,最近开始选择拉取数据。 12345678910111213141516171819+-------------------+|---------+ ||| badger | |+---------+ +---------------+| | || offset存储 | || | |+-------------------+ +---------v-------------+ | | +------------+ | 从partition拉取数据 +-----+ 。。。。。。| | | +------------+ +---------+-------------+ | |+-------------------+ || | || Kafka &lt;---------------+| |+-------------------+ msource的工作原理在介绍mtga的时候基本也差不多介绍完了。至于这些api的实现是基于Unix Socket实现的。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114func (sk *SpoutKafka) unixSocketListen() &#123;start: lis, err := net.Listen(\"unix\", UNIX_SOCKET_FILE) if err != nil &#123; logbdev.Info(\"UNIX Domain Socket 创建失败，正在尝试重新创建 -&gt; \", err) err = os.Remove(UNIX_SOCKET_FILE) if err != nil &#123; logbdev.Info(\"删除 sock 文件失败！程序退出 -&gt; \", err) &#125; goto start &#125; else &#123; logbdev.Info(\"创建 UNIX Domain Socket 成功\") &#125; sigs := make(chan os.Signal, 1) signal.Notify(sigs, syscall.SIGINT, syscall.SIGTERM) go func() &#123; &lt;-sigs if sk.SigTermCbNeed &#123; sk.SigTermCb(true) &#125; &#125;() defer func() &#123; lis.Close() os.Remove(UNIX_SOCKET_FILE) &#125;() invokeObjectMethod := func(conn net.Conn, object interface&#123;&#125;, methodName string, args ...interface&#123;&#125;) &#123; inputs := make([]reflect.Value, len(args)) for i, _ := range args &#123; inputs[i] = reflect.ValueOf(args[i]) &#125; intCb := func(v reflect.Value) &#123; n, err := conn.Write([]byte(fmt.Sprintf(\"%d\\n\", v.Int()))) if n &gt; 0 &#123; logbdev.Info(fmt.Sprintf(\"Cmd: %s ,成功响应结果: %d\", methodName, v.Int())) &#125; if err != nil &#123; logbdev.Error(fmt.Sprintf(\"Cmd: %s ,响应失败 %s\", methodName, err)) &#125; &#125; strCb := func(v reflect.Value) &#123; n, err := conn.Write([]byte(fmt.Sprintf(\"%s\\n\", v.String()))) if n &gt; 0 &#123; logbdev.Info(fmt.Sprintf(\"Cmd: %s ,成功响应结果: %s\", methodName, v.String())) &#125; if err != nil &#123; logbdev.Error(fmt.Sprintf(\"Cmd: %s ,响应失败 %s\", methodName, err)) &#125; &#125; for _, v := range reflect.ValueOf(object).MethodByName(methodName).Call(inputs) &#123; switch v.Kind() &#123; case reflect.Int: case reflect.Int64: intCb(v) break case reflect.String: strCb(v) break default: _, err := conn.Write([]byte(fmt.Sprintf(\"%s\\n\", \"不支持的响应数据类型\"))) if err != nil &#123; handleError(err.Error()) &#125; &#125; &#125; &#125; handle := func(conn net.Conn) &#123; defer conn.Close() for &#123; var buf = make([]byte, 1024) n, err := conn.Read(buf) // 如果已经没数据了，则结束 if err == io.EOF &#123; return &#125; if err != nil &#123; logbdev.Info(\"Socket conn read error:\", err) return &#125; var cmd Cmd if n &gt; 0 &#123; err := json.Unmarshal(buf[:n], &amp;cmd) if err != nil &#123; _, err := conn.Write([]byte(fmt.Sprintf(\"Rpc-Json解析失败, err: %s\", err) + \"\\n\")) if err != nil &#123; logbdev.Warn(err) continue &#125; &#125; invokeObjectMethod(conn, sk, cmd.RpcFuncName, cmd.Params...) &#125; &#125; &#125; for &#123; conn, err := lis.Accept() if err != nil &#123; logbdev.Info(\"请求接收错误 -&gt; \", err) continue // 一个连接错误，不会影响整体的稳定性，忽略就好 &#125; go handle(conn) //开始处理数据 &#125;&#125; 之前在mtga中出现过一个问题，那就是rpc超时问题的问题（socket超时）。这个问题导致了我们在执行各种命令的时候，如果出现问题会一直hang up的状态，得不到结果的同时一直卡住。影响到了我们对服务的监控。 1234567891011+----------------------------------------------------------+| +-------------+ || | msource | || +-------------+ || || +---------------------+ +---------------------+ || | | | | || | unix socket server | | unix socket client | || | | | | || +---------------------+ +---------------------+ |+----------------------------------------------------------+ 为了解决这个问题。我们在unix socket client 这里加了一个超时的机制。借助是context的机制，实现协程之间的超时通信。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758const ( TimeoutMsg = \"操作超时, 请检查服务状态!\" TimeoutInt = -1)rcn := make(chan bool)ctx, cancel := context.WithTimeout(context.Background(), time.Second*10)defer cancel()go func() &#123; if exists(UNIX_SOCKET_FILE) &#123; conn, err := net.Dial(\"unix\", UNIX_SOCKET_FILE) if err != nil &#123; handleError(err.Error()) &#125; defer conn.Close() rpc := Cmd&#123;RpcFuncName: \"FailureQueueCount\"&#125; data, err := json.Marshal(rpc) if err != nil &#123; handleError(fmt.Sprintf(\"encode-json失败\")) &#125; n, err := conn.Write(data) if n &gt; 0 &amp;&amp; err == nil &#123; reader := bufio.NewReader(conn) msg, err := reader.ReadString('\\n') if err != nil &#123; logbdev.Info(err) &#125; msg = msg[:len(msg)-len(\"\\n\")] lagInt, err := strconv.Atoi(msg) if err != nil &#123; handleError(err.Error()) &#125; lag = int64(lagInt) &#125; &#125; else &#123; sk := SourceToolRun(configOpts) lag = sk.FailureQueueCount() &#125; rcn &lt;- true&#125;()for &#123; select &#123; case &lt;-ctx.Done(): if ctx.Err() != nil &#123; logbdev.Warn(ctx.Err()) return TimeoutInt &#125; else &#123; return lag &#125; case &lt;- rcn: return lag &#125;&#125; 这里需要注意的是，这个context.WithTimeout 是不管你内部是否处理完毕，一定会在指定的时间内timeout，所以如果提前完成了必须通过自己的手段提前结束。 msink这个组件的作用主要是用于把指定的消息进行sink到各种终端，例如msink_file,msink_kafka等等。 目前我们是封装在同一个仓库中，以不同文件保存，后续，我们或许会考虑拆分开来便于维护发版。 1234567891011├── Dockerfile├── README.MD├── config.go├── config.yml├── config_test.go├── error.go├── go.mod├── go.sum├── msource_spout.go├── msource_spout_test.go└── rpc.go 这里主要和大家说以下msink_file吧，msink_kafka的实现原理差不多。 支持自定义回调函数，判断是否写入成功 支持批处理和流式处理 流式处理有独立的Api func (f *FileClient) WriteWithoutEol(filePath string, message string) error func (f *FileClient) Write(filePath string, message string) error (f *FileClient) BatchLineDataChannel() chan&lt;- map[Filename]ChannelMessage 批处理的Api (f *FileClient) BatchLineDataChannel() chan&lt;- map[Filename]ChannelMessage 可能细心的朋友已经发现了，流式处理的Api包含了批处理的Api。是的，被包含在一起了，为什么会这样子？ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102func NewFileClient(client afero.Fs, opts ...DialOption) *FileClient &#123; p := new(FileClient) p.client = client p.dopts = defaultOptions() p.batchLineDataChannel = make(chan map[Filename]ChannelMessage, 1000) p.events = make(chan FileMetaMessage, 100) //循环调用opts for _, opt := range opts &#123; opt.apply(&amp;p.dopts) &#125; var writer func(fileClient *FileClient) if p.dopts.batchWrite &#123; writer = channelBatchWriter &#125; else &#123; writer = channelWriter &#125; p.wg.Add(1) go func() &#123; writer(p) p.wg.Done() &#125;() p.wg.Add(1) go func() &#123; p.run() p.wg.Done() &#125;() return p&#125;// 流式func channelWriter(client *FileClient) &#123; for m := range client.batchLineDataChannel &#123; for fn, msg := range m &#123; err := client.WriteWithoutEol(string(fn), string(msg)) client.events &lt;- FileMetaMessage&#123;Message: string(msg), Error: err&#125; &#125; &#125;&#125;// 批量func channelBatchWriter(client *FileClient) &#123; var buffered = make(map[Filename][]ChannelMessage) bufferedCnt := 0 batchSize := client.dopts.batchLineDataChannelCount for m := range client.batchLineDataChannel &#123; for fn, msg := range m &#123; buffered[fn] = append(buffered[fn], msg) bufferedCnt++ &#125; loop: for true &#123; select &#123; case m, ok := &lt;-client.batchLineDataChannel: if !ok &#123; break loop &#125; if m == nil &#123; panic(\"nil message received on batchLineDataChannel\") &#125; for fn, msg := range m &#123; buffered[fn] = append(buffered[fn], msg) bufferedCnt++ if bufferedCnt &gt; batchSize &#123; break loop &#125; &#125; default: break loop &#125; &#125; var tmpLongMsg = make(map[Filename]string) for Filename, ChannelMessageList := range buffered &#123; tmpMsg := \"\" for _, cm := range ChannelMessageList &#123; tmpMsg += string(cm) &#125; tmpLongMsg[Filename] = tmpMsg &#125; for Filename, longMsg := range tmpLongMsg &#123; err := client.WriteWithoutEol(string(Filename), longMsg) for _, ChannelMessageList := range buffered &#123; for _, cm := range ChannelMessageList &#123; client.events &lt;- FileMetaMessage&#123;Message: string(cm), Error: err&#125; &#125; &#125; &#125; buffered = make(map[Filename][]ChannelMessage) bufferedCnt = 0 &#125;&#125; 看到这里大家应该也知道Api应该就可以理解为什么会被包含在一起了。 logdev这是我们的日志组件，在上面的代码中，多多少少已经有了这个组件的身影。 12345678910111213141516├── README.md├── exported.go├── go.mod├── go.sum├── hooks│ ├── reporter│ │ ├── metl.go│ │ ├── reporter.go│ │ └── reporter_test.go│ └── stacker│ ├── stacker.go│ └── stacker_test.go├── logbdev.go├── logbdev_test.go├── logger.go├── mc_formatter.go 常规设置，主要是我们在这个组件中添加了几个hook，分别是reporter,stacker reporter （设置需要上报日志的级别，用于如果发现了error级别的错误就推送日志到我们的告警服务中。） stacker （设置需要输出堆栈的级别日志级别） 日志存储的方式有几种 常规的单日志存储 日志按照大小轮转 日志按照日期轮转 commentjson这个是用来解析包含了换行符,空白行,行注释,段注释等等的json字符串 1234├── README.md├── hjson.go├── hjson_test.go└── test.json 这个原理也比较简单，就是一个个字符去匹配，重新构造出一个新的合法的json格式。里面的单元测试也做得比较完善了。 go-graceful-daemon这个组件是用来将服务变成守护进程用的。 1234567├── README.md├── daemon.go├── go.mod├── go.mod2├── signal.go└── test └── test.go 这里的核心主要是借助了syscall的ForkExec实现。 1234567891011121314func forkDaemon() error &#123; args := os.Args os.Setenv(\"__Daemon\", \"true\") procAttr := &amp;syscall.ProcAttr&#123; Env: os.Environ(), &#125; pid, err := syscall.ForkExec(os.Args[0], args, procAttr) if err != nil &#123; return err &#125; log.Printf(\"[%d] %s start daemon\\n\", pid, AppName) savePid(pid) return nil&#125; 并且支持自定实现信号量的捕捉处理逻辑 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859var ErrStop = errors.New(\"stop serve signals\")type SignalHandlerFunc func(sig os.Signal) (err error)func SetSigHandler(handler SignalHandlerFunc, signals ...os.Signal) &#123; for _, sig := range signals &#123; handlers[sig] = handler &#125;&#125;// ServeSignals calls handlers for system signals.func ServeSignals() (err error) &#123; signals := make([]os.Signal, 0, len(handlers)) for sig := range handlers &#123; signals = append(signals, sig) &#125; ch := make(chan os.Signal, 8) signal.Notify(ch, signals...) for sig := range ch &#123; err = handlers[sig](sig) if err != nil &#123; break &#125; &#125; signal.Stop(ch) if err == ErrStop &#123; err = nil &#125; return&#125;var handlers = make(map[os.Signal]SignalHandlerFunc)func init() &#123; handlers[syscall.SIGINT] = sigtermDefaultHandler handlers[syscall.SIGTERM] = sigtermDefaultHandler handlers[syscall.SIGHUP] = sighupDefaultHandler&#125;func sigtermDefaultHandler(sig os.Signal) error &#123; log.Printf(\"[%d] %s stop graceful\", os.Getpid(), AppName) log.Printf(\"[%d] %s stopped.\", os.Getpid(), AppName) os.Remove(PidFile) os.Exit(1) return ErrStop&#125;func sighupDefaultHandler(sig os.Signal) error &#123; //only deamon时不支持kill -HUP,因为可能监听地址会占用 log.Printf(\"[%d] %s stopped.\", os.Getpid(), AppName) os.Remove(PidFile) os.Exit(2) return ErrStop&#125; 目前内置了SIGINT,SIGTERM,SIGHUP的默认行为","categories":[{"name":"Golang","slug":"Golang","permalink":"http://blog.crazylaw.cn/categories/Golang/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://blog.crazylaw.cn/tags/Golang/"}]},{"title":"【DevOps】Jenkins的share-libraries明朝的运用","slug":"DevOps/Jenkins在明朝的运用","date":"2020-11-10T08:44:30.000Z","updated":"2021-03-20T16:25:01.798Z","comments":true,"path":"2020/11/10/DevOps/Jenkins在明朝的运用/","link":"","permalink":"http://blog.crazylaw.cn/2020/11/10/DevOps/Jenkins%E5%9C%A8%E6%98%8E%E6%9C%9D%E7%9A%84%E8%BF%90%E7%94%A8/","excerpt":"前言本次我们不从Jenkins的部署架构和具体配置说明，我们主要围绕 jenkinsfile 的内部来讲解。","text":"前言本次我们不从Jenkins的部署架构和具体配置说明，我们主要围绕 jenkinsfile 的内部来讲解。 Jenkinsfile目前我们的 jenkins 服务用过普通的pipeline和 多分支的pipeline，其主要区别就是多分支pipeline更加灵活一些，能够让我们少做一些逻辑处理。但是这引入了一个问题，那就是我们的jenkins，必须存在与对应的分支中，也因此，pipeline 的流程变成了代码管理，并且由于我们存在多个项目，那么将会有项目数 * 2(master/pre-develop) 那么多的 jenkins 需要管理，于是就导致了我们有很多这种冗余的写法，这还不是最糟糕的，最糟糕的是如果我们需要修改一些通用的内容的时候，就需要一个个去修改，这绝对是灾难级别的需求。 Jenkinsfile的实现分为2种，分别是声明式pipeline和脚本式pipeline，这2种各有优缺点 jenkins需要做的事情，可以概括为如下几种： 拉取代码 版本管理 单元测试 构建部署 通知结果 声明式pipeline可以为我们提供关键字 和 顺序结构来辅助我们完成如上的5个阶段。 但是他的缺点也是明显的，就是不能很灵活根据自己的想法去做处理逻辑。 还有一种情况是比较蛋疼的，例如项目A和项目B，独立维护一个jenkinsfile，但是其实项目A和项目B的jenkinsfile其实是一致的，如果某一条要优化这个jenkinsfile的部署逻辑了。那么项目B和项目A都需要一起处理，无法实现复用的概念，并没有提供抽象的概念，不利于维护。 对于一些相对简单，没有太复杂流程的构建过程来说，声明式一定是大家的首要选择。 脚本式pipeline可以给我们提供类似于写代码脚本的方式来组织部署逻辑，我把脚本式pipeline定义为升级版的声明式pipeline。 虽然脚本式的pipeline可以给我们提供灵活的部署逻辑，但是依旧无法解决我们的复用的问题。 我们需要把这种能复用的东西放在同一个地方进行统一管理，类似于依赖库的概念，独立与jenkinsfile之外的一种存在。于是乎，这便有了jenkins-shard-libaray。 Jenkins-shard-libaray这是一个jenkins中的共享类库，只需要在jenkinsfile中对其引入，就可以引用共享库的代码，从而实现我们的复用。 但是有一点是这个共享库，需要我们储备点groovy语言的知识，因为他是用groovy作为”外挂”的方式加载运行的。 我们在写共享库的时候，必须 依照要有2个基本目录，否则jenkins无法类库。 src (必要，核心代码) vars (非必要，声明自己的语法) resources (非必要，配置存放的目录) 123456789101112131415161718192021222324252627282930313233343536373839404142./├── Jenkinsfile.example jenkinsfile的DEMO样式├── README.MD├── jars│ ├── groovy-cps-1.1.jar│ └── pipeline-model-definition-1.7.1.jar├── resources 配置文件和基础库分开管理了，所以这里不会存在静态资源文件├── src│ └── com│ └── company│ └── jenkins│ ├── BasePipeline.groovy 基础管道│ ├── CommonPipeline.groovy 公共管道│ ├── Constant.groovy 常量类│ ├── Deploy.groovy 部署基类│ ├── Git.groovy git相关的操作类│ ├── GolangPipeline.groovy golang项目管道│ ├── LaravelPipeline.groovy laravel项目管道│ ├── MPipeline.groovy 初始化类│ ├── MetlNotify.groovy 用于发送通知的类│ ├── Repo.groovy 用于实例化代码仓库信息│ ├── SummaryNotify.groovy 用于结构化通知内容的类│ └── deployments 具体的部署类│ ├── AbstractDeployment.groovy 抽象部署类│ ├── BaseDeployment.groovy 基础部署类│ ├── DefaultDeployment.groovy 默认部署类│ └── GolangDeployment.groovy Go部署类├── tests 单元测试目录│ ├── RepoTest.groovy│ └── classfiles│ └── com│ └── company│ └── jenkins│ ├── Constant.class│ ├── Git.class│ ├── MPipeline.class│ ├── MetlNotify.class│ ├── Repo.class│ └── SummaryNotify.class└── vars 全局函数定义目录 ├── mpipeline.groovy 自定义入口语法糖 └── notify.groovy 定义通知全局调用函数 Jenkinsfile12345678910mpipeline&#123; script&#x3D;this&#125;import com.company.jenkins.LaravelPipelinedef laravelPipeline &#x3D; new LaravelPipeline(this)node &#123; laravelPipeline.preBuild().build().debug().reviews().production().execute()&#125; 这是一个最基本的jenkinsfile的demo。我们把他分为2个部分。 1234# 第一部分mpipeline&#123; script&#x3D;this&#125; 1234567# 第二部分import com.company.jenkins.LaravelPipelinedef laravelPipeline &#x3D; new LaravelPipeline(this)node &#123; laravelPipeline.preBuild().build().debug().reviews().production().execute()&#125; 这2个部分各司其职。 第一部分 mpipeline 的语法糖，来自于我们vars/mpipeline.groovy文件，这是我们自定义的语法糖。 1234567891011121314151617181920212223242526272829303132333435363738394041424344import com.company.jenkins.*&#x2F;** * * @param body * repo 代码仓库地址 default: null * autoCheckout 是否需要自动checkout代码，default: true * @return *&#x2F;def call(body) &#123; def config &#x3D; [:] body.resolveStrategy &#x3D; Closure.DELEGATE_FIRST body.delegate &#x3D; config body() config.repo &#x3D; config.repo ?: null config.script &#x3D; config.script ?: null config.autoCheckout &#x3D; config.autoCheckout ?: true if (config.script &#x3D;&#x3D; null) &#123; throw new Exception(&quot;&lt;script&#x3D;this&gt;是必填参数&quot;) &#125; if (config.repo &#x3D;&#x3D; null) &#123; config.repo &#x3D; config.script.scm.getUserRemoteConfigs()[0].getUrl() &#125; node &#123; def mpipe &#x3D; new MPipeline(config.script, config.repo) mpipe.initializeResources() def credentialsId &#x3D; config.credentialsId ?: config.script.gitCredentialsId if (config.autoCheckout) &#123; stage(&quot;Checkout&quot;) &#123; &#x2F;&#x2F; git clone repo git credentialsId: credentialsId, url: mpipe.repo.getGitLink(), branch: env.BRANCH_NAME &#125; &#125; mpipe.initializeEnvironment() &#125; return this&#125; 这个语法糖做的内容定义了 mpipeline 做的内容，其中有三个有效参数 repo, script, autoCheckout，分别的意思是代表该任务下的操作仓库地址，当前任务上下文, 是否自动拉取代码。其中 script参数为必填参数，其他2个都是选填参数，其都存在默认值。这个script必填的原因是因为一定要从jenkinsfile把对象传递进来，否则作用域有所不同。因为我们在第二部分要把这个对象继续传递下去，因为他携带了贯穿整个任务的上下文。 我们这里看到了 node 结构块，这是因为在脚本式pipeline中，他其实也是jenkins内置的一个自定义语法糖，所以他可以直接被嵌入在我们的代码中。因为我们这里实例化了共享库中的Mpipeline类，所以需要在 node 结构块中编写代码。 在这一块，我们可以看到，我们做了3件事 12345+--------------------------------------+ +--------------------------------+ +----------------------------------------------------+| | | | | || 1. Initialize the custom resource +---------------&gt;+ 2. Pull the warehouse code +-------------&gt;+ 3. Initialize the custom environment variable || | | | | |+--------------------------------------+ +--------------------------------+ +----------------------------------------------------+ 初始化自定义资源（接下来要说的resource-libaray共享库） 拉取仓库代码（这是jenkins自带的一个git插件，调用git函数，加上一些参数，就会拉取代码到一个目录，如果目录不存在，则会创建） 初始化自定义环境变量 初始化自定义资源的代码如下： 12345678910111213&#x2F;** * 初始化资源库 *&#x2F;def initializeResources() &#123; this.script.mresource_load([ script: this.script, groupRepo: this.getRepoGroupRepo(), shortJobName: this.getShortName(), host: this.getHost() ]) return this&#125; 这里，我们可以看到我们通过上下文变量，调用了 mresource_load 函数，这个是我们在 resource-libaray共享库 定义的函数，用于加载对应仓库的具体资源配置用，相关参数依旧会加载到上下文之中。 自定义环境变量部分代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&#x2F;** * 初始化环境变量 *&#x2F;def initializeEnvironment() &#123; this.initBaseEnv().initEnvDependGit().initEnvDependRepo() return this&#125;def initBaseEnv() &#123; &#x2F;&#x2F; SHORT_JOB_NAME this.script.env.SHORT_JOB_NAME &#x3D; this.getShortName() &#x2F;&#x2F; 是否仅仅显示stage节点而不执行内容 this.script.mStageShow &#x3D; false return this&#125;def initEnvDependGit() &#123; &#x2F;&#x2F; COMMIT_ID this.script.env.COMMIT_ID &#x3D; this.git.commitId() &#x2F;&#x2F; COMMIT_AUTHOR this.script.env.COMMIT_AUTHOR &#x3D; this.git.commitAuthor() &#x2F;&#x2F; COMMIT_TIME this.script.env.COMMIT_TIME &#x3D; this.git.commitTime() &#x2F;&#x2F; COMMIT_COMMENT this.script.env.COMMIT_COMMENT &#x3D; this.git.commitMessage() return this&#125;def initEnvDependRepo() &#123; &#x2F;&#x2F; REPO this.script.env.REPO &#x3D; this.repo.getRepo() &#x2F;&#x2F; REPOSITORY this.script.env.REPOSITORY &#x3D; this.repo.getFullRepoString() &#x2F;&#x2F; Notify this.script.env.COMMIT_ID_PATH &#x3D; this.repo.getCommitIdHostPath() &#x2F;&#x2F; REPOSITORY_LINK this.script.env.REPOSITORY_LINK &#x3D; this.repo.getGitLink() &#x2F;&#x2F; REPOSITORY_GROUP this.script.env.REPOSITORY_GROUP &#x3D; this.repo.getGroup() &#x2F;&#x2F; REPOSITORY_GROUP_REPO this.script.env.REPOSITORY_GROUP_REPO &#x3D; this.getRepoGroupRepo() return this&#125; 这个，我们把环境变量也拆分封装到对应的方法了，分别是git相关的环境变量和仓库信息的环境变量等等。 接下来，我们看下 第二部分 的代码。 1234567# 第二部分import com.company.jenkins.LaravelPipelinedef laravelPipeline &#x3D; new LaravelPipeline(this)node &#123; laravelPipeline.preBuild().build().testing().debug().reviews().production().execute()&#125; 这里，我们import进来了Laravel的具体实现管道，并且实例化这个对象，传入了当前上下文对象。 并且在下面的node结构中，调用laravelpipeline对象的相关封装方法。 经过我们对部署流程的抽象和统一，我们的部署流程大致分为 5个阶段。分别如下： 123456789101112131415161718192021+-----------------------------+| 1. Pre-construction phase |+------------+----------------+ | | +-----------------------+ +------------&gt;+2. Construction phase +------------------+ +-----------------------+ | | | +---------------------v-----------------------------+ +-------+3. Deployment development environment phase (Debug)| | +---------------------------------------------------+ | v +------------+---------------------------------------+ +-------------+4. Grayscale Environment Deployment Stage（ reviews） | | +----------------------------------------------------+ |+------------v----------------------------------------------+| 5. Deployment of production environment phase（ prodution） |+-----------------------------------------------------------+ 预构建阶段 构建阶段 部署开发环境阶段（debug） 部署灰度环境阶段（reviews） 部署生产环境阶段（production） 因此，我们把所有的仓库jenkinsfile都按照这样子的不是来写，这样子就可以做到，当我们需要修改部署的内容的时候，代码仓库的jenkinsfile不需要做任务变动，只需要修改我们的共享库即可。 部分代码如下： 1234567891011121314151617181920212223242526272829303132def preBuild() &#123; def callback &#x3D; &#123; this.startNotify() &#125; def apiName &#x3D; &#39;preBuild&#39; this.addCallback(apiName, callback) return this&#125;def build(isParallel &#x3D; true) &#123; def callback &#x3D; &#123; this.script.stage(&#39;Build&#39;) &#123; if (isParallel) &#123; def stages &#x3D; [failFast: true] stages[&quot;Composer Install&quot;] &#x3D; this.composerInstall() stages[&quot;Client Build&quot;] &#x3D; this.clientBuild() this.script.parallel stages &#125; else &#123; this.composerInstall()() this.clientBuild()() &#125; &#125; &#125; def apiName &#x3D; &#39;build&#39; this.addCallback(apiName, callback) return this&#125; 这里看到，我们几乎所有的代码都会通过回调函数来编写的，所以，我们在实际调用的时候，必须加上 execute() 方法来实际触发链式过程。 因为在我们这个例子中，laravel是php的项目，默认情况下，我们是有前端和后端的代码。分别是composer依赖，npm依赖。 由于这2个部分的依赖，他们是没有相关性的，所以，我们在设计上，默认是支持 并发 执行的，所以我们这里调用了jenkins的一个内置函数 parallel来实现并发。实际效果如下展示： 12345678910111213141516 +-----------------------+ | +------------------+ | | | Composer Install | | | +------------------+ | | | | |+----------------+ | || pre-build +-------&gt;+ build |+----------------+ | | | | | | | +------------------+ | | | Client Build | | | +------------------+ | | | +-----------------------+ 以npm依赖安装为例子： 1234567891011121314public npmInstall(def packageFile &#x3D; &#39;client&#x2F;package.json&#39;, def clientDir &#x3D; &#39;client&#39;, def version &#x3D; &#39;latest&#39;) &#123; def md5File &#x3D; &quot;$&#123;packageFile&#125;.md5&quot; def callback &#x3D; &#123; isChange -&gt; this.script.echo &quot;package.json是否有变化:$&#123;isChange&#125;&quot; this.script.echo &quot;是否强制编译前端资源:$&#123;this.script.params.isBuild&#125;&quot; if (isChange || this.script.params.isBuild) &#123; this.script.docker.image(&quot;jenkins_docker_node:$&#123;version&#125;&quot;).inside(&#39;--dns-opt&#x3D;ndots:5 -v $HOME&#x2F;.npm:&#x2F;root&#x2F;.npm&#39;) &#123; ... &#125; &#125; &#125; this.checkChangeFile(packageFile, md5File, callback)&#125; 另外在composer和npm依赖安装的过程中，我们做了一些细节上的优化，例如，我们会检测package.json以及composer.json，如果这2个文件没有变化的话，则不会更新依赖。避免了每次构建都需要去检查更新依赖包。另外由于我们的jenkisn环境本身也是一个jenkins的docker容器，所以我们的jenkins环境下是不会存在php以及node的环境，所以我们这个时候，在安装依赖的时候，借助了上下文中的 docker.image 关键字来构建一个容器来触发我们的依赖安装。 接下来就是部署相关的内容了，我们以部署 production 环境来举例子 12345678910111213141516171819202122def production(def map &#x3D; [ sshId : null, deployClass: null]) &#123; def callback &#x3D; &#123; if (map[&#39;sshId&#39;] &#x3D;&#x3D; null) &#123; map[&#39;sshId&#39;] &#x3D; this.getValue(&#39;deploySshId&#39;) &#125; if (map[&#39;deployClass&#39;] &#x3D;&#x3D; null) &#123; map[&#39;deployClass&#39;] &#x3D; this.getDeployClass() &#125; map[&#39;pipeline&#39;] &#x3D; this this.deploy.production(map) &#125; def apiName &#x3D; &#39;production&#39; this.addCallback(apiName, callback) return this&#125; 如果不指定 sshId 和 deployClass 的话，都会存在默认值。由于我们整个过程都是通过回调的方式调用的（其实类似于中间件），所以我们这里的多了一个 map[pipeline] = this 的代码，意义在于把当前上下文传递进具体的部署类。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134def production(def map &#x3D; [ sshId : null, deployClass: null, pipeline : null]) &#123; def keyword &#x3D; &#39;production&#39; this.script.stage(&#39;Production Deploy&#39;) &#123; if (this.git.isMasterBranch() &amp;&amp; this.script.currentResources[keyword].enabled !&#x3D; false) &#123; &#x2F;&#x2F; 注入参数 def tags &#x3D; this.git.tagsList() def buildParameters &#x3D; [ [ &#39;$class&#39; : &#39;ChoiceParameter&#39;, choiceType : &#39;PT_SINGLE_SELECT&#39;, description : &#39;&#39;&#39;选择构建类型：auto_deploy - 自动发布（直接发布最新代码） deploy - 指定版本发布rollback - 版本回滚发布&#39;&#39;&#39;, filterLength: 1, filterable : false, name : &#39;buildType&#39;, randomName : &#39;choice-parameter-69483043309720&#39;, script : [ &#39;$class&#39; : &#39;GroovyScript&#39;, fallbackScript: [ classpath: [], sandbox : true, script : &#39;return[&quot;unknow&quot;]&#39; ], script : [ classpath: [], sandbox : true, script : &#39;return[&quot;auto_deploy&quot;, &quot;deploy&quot;, &quot;rollback&quot;]&#39;, ] ] ], [ &#39;$class&#39; : &#39;ChoiceParameter&#39;, choiceType : &#39;PT_SINGLE_SELECT&#39;, description : &#39;&#39;&#39;选择Tag标签：auto_deploy时该参数无效&#39;&#39;&#39;, filterLength: 20, filterable : true, name : &#39;buildTag&#39;, randomName : &#39;choice-parameter-69483043309721&#39;, script : [ &#39;$class&#39; : &#39;GroovyScript&#39;, fallbackScript: [ classpath: [], sandbox : true, script : &#39;return[&quot;unknow&quot;]&#39; ], script : [ classpath: [], sandbox : true, script : &quot;return [$&#123;tags&#125;]&quot;, ] ] ] ] if (!this.script.mStageShow) &#123; this.script.notify &#39;请确认是否发布到生产环境？&#39; this.script.timeout(time: 10, unit: &#39;MINUTES&#39;) &#123; try &#123; this.script.input &#39;请确认是否要发布到生产环境？&#39; &#125; catch (e) &#123; map[&#39;pipeline&#39;].parameters +&#x3D; buildParameters throw e &#125; def updateHistoryFile &#x3D; &#39;UPDATE_HISTORY&#39; def currentTag if (this.script.params.buildType &#x3D;&#x3D; &#39;deploy&#39; || this.script.params.buildType &#x3D;&#x3D; &#39;rollback&#39;) &#123; &#x2F;&#x2F; 发布指定tag this.script.sh &quot;git checkout $&#123;this.script.params.buildTag&#125;&quot; this.script.currentBuild.description &#x3D; &quot;【指定发布】Tag: $&#123;this.script.params.buildTag&#125;&quot; if (this.script.params.buildType &#x3D;&#x3D; &#39;rollback&#39;) &#123; this.script.currentBuild.description &#x3D; &quot;【回滚】Tag: $&#123;this.script.params.buildTag&#125;&quot; &#125; currentTag &#x3D; this.script.params.buildTag &#125; else &#123; &#x2F;&#x2F; 开始打tag ... try &#123; def tagRes if (link.startsWith(&quot;http&quot;)) &#123; &#x2F;&#x2F; gitbucket ... &#125; &#125; else &#123; &#x2F;&#x2F; gitlab ... &#125; if (tagRes[&#39;exitCode&#39;] &gt; 0 &amp;&amp; !tagRes[&#39;stdout&#39;].contains(&#39;already exists&#39;)) &#123; throw new Exception(tagRes[&#39;stdout&#39;]) &#125; &#125; catch (e) &#123; throw e &#125; this.script.sh &quot;tee $&#123;preCommitIdFile&#125; &lt;&lt;&lt; $&#123;this.git.commitId()&#125;&quot; currentTag &#x3D; nextTag &#x2F;&#x2F; 最新tag加入下拉框 buildParameters[1][&#39;script&#39;][&#39;script&#39;][&#39;script&#39;] &#x3D; &quot;return [\\&quot;$&#123;currentTag&#125;\\&quot;,$&#123;tags&#125;]&quot; &#125; &#125; ... this.script.currentBuild.description &#x3D; &quot;【自动发布】Tag: $&#123;currentTag&#125;&quot; this.publishDeployment(keyword)(map[&#39;sshId&#39;], map[&#39;deployClass&#39;]) ... &#125; &#125; else &#123; this.script.echo &#39;Production Deploy Show&#39; &#125; map[&#39;pipeline&#39;].parameters +&#x3D; buildParameters &#125; else &#123; Utils.markStageSkippedForConditional(this.script.env.STAGE_NAME) &#125; &#125; return this&#125; 这里，我们再说明一下我们的自动化流程。 提交feature代码 合并到pre-develop分支，触发jenkins任务构建更新debug环境代码 最终合并到master分支，触发jenkins任务，如果代码有变化则对最新的master代码进行打tag，构建更新production环境代码 基于以上几点，我们就可以很直观的看到部署production类的时候，我们会检测master的代码，如果存在变化则进行打tag。然后再调用具体的部署逻辑。其中 this.script.currentResources[keyword].enabled 是来自于我们的 resource-libaray 的配置。 1234567891011121314151617abstract class AbstractDeployment &#123; &#x2F;** * 打包逻辑 * @return *&#x2F; abstract def parallelBefore(def packFiles, def packExclude) &#x2F;** * 并行逻辑 * @param ip * @param port * @param directory * @return *&#x2F; abstract def parallel(def ip, def port, def directory, def backupExclude)&#125; 我们的抽象具体部署类存在2个必须实现的抽象方法，分别是 parallelBefore, parallel。 parallelBefore：并发部署前做的事情 parallel：并发部署到多台目标机器上 resources-libraries资源共享库 对外提供一个全局的函数 mresource_load，内部提供给上下文关键字拿到当前项目，当前环境的对应的部署内容信息。 12345678910111213141516171819├── README.MD├── resources│ ├── git.xxxx.com│ │ ├── repo-group1│ │ │ ├── a.yaml│ │ │ └── b.yaml│ │ └── repo-group2│ │ └── c.yaml│ └── gitlab.xxx.com│ ├── repo-group3│ │ ├── d.yaml│ │ └── e.yaml│ └── repo-group4│ ├── f.yaml│ └── g.yaml├── tests│ └── test_mresource_load.groovy└── vars └── mresource_load.groovy 我们看到我们的基本结构和上一个 jenkins-shard-libraries 十分的类似，区别在于 resources-libraries 不存在 src 目录，但是 resources 全部都存在资源配置。 资源存放的目录，结构目录规则为 &lt;FQDN&gt;/&lt;group&gt;/&lt;repo&gt;，对应git仓库。 资源库采用yaml格式。目前支持的key有： type(normal|job_name) normal 常规项目，不区分项目 job_name 区分项目，一套仓库，多个项目区分部署 debug 内测环境 address ip:port directory 代码目录路径 deploy_ssh_id 部署的凭证 execute_user 执行的用户，目前仅仅在GolangPipline有效 ssh_username 部署代码的ssh的账号，目前在GolangPipline仅调试用 reviews 灰度环境 enabled: false 跳过reviews步骤 address ip:port directory 代码目录路径 deploy_ssh_id 部署的凭证 execute_user 执行的用户，目前仅仅在GolangPipline有效 ssh_username 部署代码的ssh的账号，目前在GolangPipline仅调试用【非必填，默认采用deploy_ssh_id的信息】 production 生产环境 address ip:port directory 代码目录路径 deploy_ssh_id 部署的凭证 branch 生产环境对应的分支 【非必填】 execute_user 执行的用户，目前仅仅在GolangPipline有效【非必填】 ssh_username 部署代码的ssh的账号，目前在GolangPipline仅调试用【非必填，默认采用deploy_ssh_id的信息】 附加在上下文的变量： script.currentResources script.gitCredentialsId script.buildCredentialsId 具体的yaml配置格式的demo 1234567891011121314151617181920212223242526type: &#39;normal&#39;git_credentials_id: &#39;git_credentials_id&#39;common: &amp;common deploy_ssh_id: &#39;deploy_ssh_id&#39;environment: debug: &lt;&lt;: *common address: - &#39;2.2.2.2:22&#39; - &#39;1.1.1.1:22&#39; directory: &#39;2.2.2.2:22&#39;: &#39;&#x2F;a&#39; &#39;1.1.1.1:22&#39;: &#39;&#x2F;b&#39; reviews: &lt;&lt;: *common address: &#39;3.3.3.3:22&#39; directory: &#39;&#x2F;c&#39; production: &lt;&lt;: *common address: &#39;4.4.4.4:22&#39; directory: &#39;&#x2F;d&#39; 这是基本的配置文件，我们借助了yaml的灵活性，配置出尽可能简洁，可复用的项。我们还有更加灵活的配置和格式，具体参考以下代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128@Grab(&#39;org.yaml:snakeyaml:1.26&#39;)import org.yaml.snakeyaml.*def call(def config &#x3D; [script: null, groupRepo: null, shortJobName: null, host: null]) &#123; def script &#x3D; config.script def groupRepo &#x3D; config.groupRepo def shortJobName &#x3D; config.shortJobName def host &#x3D; config.host if (groupRepo &#x3D;&#x3D; null) &#123; groupRepo &#x3D; script.env.REPOSITORY_GROUP_REPO &#125; if (shortJobName &#x3D;&#x3D; null) &#123; shortJobName &#x3D; script.env.SHORT_JOB_NAME &#125; if (host &#x3D;&#x3D; null) &#123; host &#x3D; &quot;git.xxxx.com&quot; &#125; configFile &#x3D; host + &#39;&#x2F;&#39; + groupRepo + &#39;.yaml&#39; Yaml yaml &#x3D; new Yaml() String resourceString &#x3D; libraryResource(configFile) Map&lt;String, Object&gt; obj &#x3D; yaml.load(resourceString) obj.type &#x3D; obj.type ?: null def resource &#x3D; [:] def git_credentials_id &#x3D; &#39;&#39; def build_credentials_id &#x3D; &#39;&#39; if (obj.type &#x3D;&#x3D; &#39;job_name&#39;) &#123; String pointJobName &#x3D; shortJobName for (item in obj) &#123; if (item.key &#x3D;&#x3D; pointJobName) &#123; def value &#x3D; item.value for (it1 in value) &#123; if (it1.key &#x3D;&#x3D; &#39;git_credentials_id&#39;) &#123; git_credentials_id &#x3D; it1.value continue &#125; if (it1.key &#x3D;&#x3D; &#39;build_credentials_id&#39;) &#123; build_credentials_id &#x3D; it1.value continue &#125; if (it1.key &#x3D;&#x3D; &#39;environment&#39;) &#123; &#x2F;&#x2F; 兼容 address，String &amp;&amp; List for (it in it1.value) &#123; for (values in it.value) &#123; if (values instanceof Map.Entry) &#123; if (values.key &#x3D;&#x3D; &#39;address&#39;) &#123; if (values.value instanceof String) &#123; values.value &#x3D; [values.value] &#125; &#125; &#125; &#125; &#125; &#125; if (it1.key &#x3D;&#x3D; &#39;environment&#39;) &#123; &#x2F;&#x2F; 兼容 directory for (it in it1.value) &#123; for (values in it.value) &#123; if (values instanceof Map.Entry) &#123; if (values.key &#x3D;&#x3D; &#39;directory&#39;) &#123; if (values.value instanceof String) &#123; def dir &#x3D; values.value values.value &#x3D; [:] for (def addr in it.value.address) &#123; values.value[addr] &#x3D; dir &#125; &#125; &#125; &#125; &#125; &#125; &#125; &#125; resource &#x3D; value break &#125; &#125; &#125; else &#123; &#x2F;&#x2F; job.type &#x3D;&#x3D; normal resource &#x3D; obj resource.remove(&#39;type&#39;) &#x2F;&#x2F; 兼容 address，String &amp;&amp; List for (item in resource.environment) &#123; for (values in item.value) &#123; if (values instanceof Map.Entry) &#123; if (values.key &#x3D;&#x3D; &#39;address&#39;) &#123; if (values.value instanceof String) &#123; values.value &#x3D; [values.value] &#125; &#125; &#125; &#125; &#125; &#x2F;&#x2F; 兼容 directory for (item in resource.environment) &#123; for (values in item.value) &#123; if (values instanceof Map.Entry) &#123; if (values.key &#x3D;&#x3D; &#39;directory&#39;) &#123; if (values.value instanceof String) &#123; def dir &#x3D; values.value values.value &#x3D; [:] for (def addr in item.value.address) &#123; values.value[addr] &#x3D; dir &#125; &#125; &#125; &#125; &#125; &#125; git_credentials_id &#x3D; resource.git_credentials_id build_credentials_id &#x3D; resource.build_credentials_id &#125; def environment &#x3D; resource.environment script.currentResources &#x3D; environment ?: [:] script.gitCredentialsId &#x3D; git_credentials_id ?: null script.buildCredentialsId &#x3D; build_credentials_id ?: null&#125; 以上就是我们的jenkins对共享库应用场景了，后续也会随着服务器对架构而升级优化改变。","categories":[{"name":"DevOps","slug":"DevOps","permalink":"http://blog.crazylaw.cn/categories/DevOps/"}],"tags":[{"name":"DevOps","slug":"DevOps","permalink":"http://blog.crazylaw.cn/tags/DevOps/"},{"name":"Jenkins","slug":"Jenkins","permalink":"http://blog.crazylaw.cn/tags/Jenkins/"}]},{"title":"【kubernetes】pause容器","slug":"k8s/pause容器","date":"2020-08-25T14:53:30.000Z","updated":"2021-03-20T16:25:01.807Z","comments":true,"path":"2020/08/25/k8s/pause容器/","link":"","permalink":"http://blog.crazylaw.cn/2020/08/25/k8s/pause%E5%AE%B9%E5%99%A8/","excerpt":"前言k8s中有 init容器的概念，其中一个就是pause容器，pause容器是一个十分重要的概念，贯穿整个Pod的通信。","text":"前言k8s中有 init容器的概念，其中一个就是pause容器，pause容器是一个十分重要的概念，贯穿整个Pod的通信。 Pause容器Pause容器，又叫Infra容器，本文将探究该容器的作用与原理。 我们知道在kubelet的配置中有这样一个参数： 1KUBELET_POD_INFRA_CONTAINER=--pod-infra-container-image=registry.access.redhat.com/rhel7/pod-infrastructure:latest 上面是openshift中的配置参数，kubernetes中默认的配置参数是： 1KUBELET_POD_INFRA_CONTAINER=--pod-infra-container-image=gcr.io/google_containers/pause-amd64:3.0 Pause容器，是可以自己来定义，官方使用的gcr.io/google_containers/pause-amd64:3.0容器的代码见Github，使用C语言编写。 Pause容器特点 镜像非常小，目前在700KB左右 永远处于Pause(暂停)状态 Pause容器背景像 Pod 这样一个东西，本身是一个逻辑概念。那在机器上，它究竟是怎么实现的呢？这就是我们要解释的一个问题。 既然说 Pod 要解决这个问题，核心就在于如何让一个 Pod 里的多个容器之间最高效的共享某些资源和数据。 因为容器之间原本是被 Linux Namespace 和 cgroups 隔开的，所以现在实际要解决的是怎么去打破这个隔离，然后共享某些事情和某些信息。这就是 Pod 的设计要解决的核心问题所在。 所以说具体的解法分为两个部分：网络和存储。 Pause容器就是为解决Pod中的网络问题而生的。 Pause容器实现Pod 里的多个容器怎么去共享网络？下面是个例子： 比如说现在有一个 Pod，其中包含了一个容器 A 和一个容器 B，它们两个就要共享 Network Namespace。在 Kubernetes 里的解法是这样的：它会在每个 Pod 里，额外起一个 Infra container 小容器来共享整个 Pod 的 Network Namespace。 Infra container 是一个非常小的镜像，大概 700KB 左右，是一个C语言写的、永远处于“暂停”状态的容器。由于有了这样一个 Infra container 之后，其他所有容器都会通过 Join Namespace 的方式加入到 Infra container 的 Network Namespace 中。 所以说一个 Pod 里面的所有容器，它们看到的网络视图是完全一样的。即：它们看到的网络设备、IP地址、Mac地址等等，跟网络相关的信息，其实全是一份，这一份都来自于 Pod 第一次创建的这个 Infra container。这就是 Pod 解决网络共享的一个解法。 在 Pod 里面，一定有一个 IP 地址，是这个 Pod 的 Network Namespace 对应的地址，也是这个 Infra container 的 IP 地址。所以大家看到的都是一份，而其他所有网络资源，都是一个 Pod 一份，并且被 Pod 中的所有容器共享。这就是 Pod 的网络实现方式。 由于需要有一个相当于说中间的容器存在，所以整个 Pod 里面，必然是 Infra container 第一个启动。并且整个 Pod 的生命周期是等同于 Infra container 的生命周期的，与容器 A 和 B 是无关的。这也是为什么在 Kubernetes 里面，它是允许去单独更新 Pod 里的某一个镜像的，即：做这个操作，整个 Pod 不会重建，也不会重启，这是非常重要的一个设计。 Pause容器的作用我们检查node节点的时候会发现每个node上都运行了很多的pause容器，例如如下。 1234567$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES2c7d50f1a7be docker.io/jimmysong/heapster-grafana-amd64@sha256:d663759b3de86cf62e64a43b021f133c383e8f7b0dc2bdd78115bc95db371c9a \"/run.sh\" 3 hours ago Up 3 hours k8s_grafana_monitoring-influxdb-grafana-v4-5697c6b59-76zqs_kube-system_5788a3c5-29c0-11e8-9e88-525400005732_05df93dea877a docker.io/jimmysong/heapster-influxdb-amd64@sha256:a217008b68cb49e8f038c4eeb6029261f02adca81d8eae8c5c01d030361274b8 \"influxd --config ...\" 3 hours ago Up 3 hours k8s_influxdb_monitoring-influxdb-grafana-v4-5697c6b59-76zqs_kube-system_5788a3c5-29c0-11e8-9e88-525400005732_09cec6c0ef583 jimmysong/pause-amd64:3.0 \"/pause\" 3 hours ago Up 3 hours k8s_POD_monitoring-influxdb-grafana-v4-5697c6b59-76zqs_kube-system_5788a3c5-29c0-11e8-9e88-525400005732_054d06e30a4c7 docker.io/jimmysong/kubernetes-dashboard-amd64@sha256:668710d034c4209f8fa9a342db6d8be72b6cb5f1f3f696cee2379b8512330be4 \"/dashboard --inse...\" 3 hours ago Up 3 hours k8s_kubernetes-dashboard_kubernetes-dashboard-65486f5fdf-lshl7_kube-system_27c414a1-29c0-11e8-9e88-525400005732_05a5ef33b0d58 jimmysong/pause-amd64:3.0 \"/pause\" 3 hours ago Up 3 hours k8s_POD_kubernetes-dashboard-65486f5fdf-lshl7_kube-system_27c414a1-29c0-11e8-9e88-525400005732_0 kubernetes中的pause容器主要为每个业务容器提供以下功能： 在pod中担任Linux命名空间共享的基础； 启用pid命名空间，开启init进程。 在The Almighty Pause Container这篇文章中做出了详细的说明，pause容器的作用可以从这个例子中看出，首先见下图： 我们首先在节点上运行一个pause容器。 1docker run -d --name pause -p 8880:80 jimmysong/pause-amd64:3.0 然后再运行一个nginx容器，nginx将为localhost:2368创建一个代理。 123456789101112131415$ cat &lt;&lt;EOF &gt;&gt; nginx.conferror_log stderr;events &#123; worker_connections 1024; &#125;http &#123; access_log /dev/stdout combined; server &#123; listen 80 default_server; server_name example.com www.example.com; location / &#123; proxy_pass http://127.0.0.1:2368; &#125; &#125;&#125;EOF$ docker run -d --name nginx -v `pwd`/nginx.conf:/etc/nginx/nginx.conf --net=container:pause --ipc=container:pause --pid=container:pause nginx 突然间发现报错： docker: Error response from daemon: can&#39;t join IPC of container 90208aca191fd04e86276bf29a1b1ff742f3d04c0c8c01f6cc61c3283909725b: non-shareable IPC (hint: use IpcMode:shareable for the donor container). 因为当前docker容器默认的&quot;default-ipc-mode&quot;: &quot;host&quot;，我们需要切换到&quot;default-ipc-mode&quot;: &quot;shareable&quot;, linux环境下写入到/etc/docker/daemon.json，docker-for-mac 写在配置中。然后重启即可。 然后再为ghost创建一个应用容器，这是一款博客软件。 1$ docker run -d --name ghost --net=container:pause --ipc=container:pause --pid=container:pause ghost 现在访问http://localhost:8880/就可以看到ghost博客的界面了。 解析 pause容器将内部的80端口映射到宿主机的8880端口，pause容器在宿主机上设置好了网络namespace后，nginx容器加入到该网络namespace中，我们看到nginx容器启动的时候指定了--net=container:pause，ghost容器同样加入到了该网络namespace中，这样三个容器就共享了网络，互相之间就可以使用localhost直接通信，--ipc=contianer:pause --pid=container:pause就是三个容器处于同一个namespace中，init进程为pause，这时我们进入到ghost容器中查看进程情况。 12345678# ps auxUSER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMANDroot 1 0.0 0.0 1024 4 ? Ss 13:49 0:00 /pauseroot 5 0.0 0.1 32432 5736 ? Ss 13:51 0:00 nginx: master psystemd+ 9 0.0 0.0 32980 3304 ? S 13:51 0:00 nginx: worker pnode 10 0.3 2.0 1254200 83788 ? Ssl 13:53 0:03 node current/inroot 79 0.1 0.0 4336 812 pts/0 Ss 14:09 0:00 shroot 87 0.0 0.0 17500 2080 pts/0 R+ 14:10 0:00 ps aux 在ghost容器中同时可以看到pause和nginx容器的进程，并且pause容器的PID是1。而在kubernetes中容器的PID=1的进程即为容器本身的业务进程。 12345➜ k8s docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES56b408835478 nginx &quot;&#x2F;docker-entrypoint.…&quot; About an hour ago Up About an hour nginxb4b5ecc6ddf4 ghost &quot;docker-entrypoint.s…&quot; 2 hours ago Up 2 hours ghost2be4a03fea85 k8s.gcr.io&#x2F;pause:3.2 &quot;&#x2F;pause&quot; 3 hours ago Up 3 hours 0.0.0.0:8880-&gt;80&#x2F;tcp pause 当关闭pause进程的时候，所有的容器都会停止 1docker stop pause 参考 The Almighty Pause Container Kubernetes之Pause容器 CNCF&amp;Aliyun云原生课程","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://blog.crazylaw.cn/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://blog.crazylaw.cn/tags/kubernetes/"}]},{"title":"【爬虫】- scrapy","slug":"爬虫/Tutorial","date":"2020-08-24T01:46:51.000Z","updated":"2021-03-20T16:25:01.820Z","comments":true,"path":"2020/08/24/爬虫/Tutorial/","link":"","permalink":"http://blog.crazylaw.cn/2020/08/24/%E7%88%AC%E8%99%AB/Tutorial/","excerpt":"前言最近在和朋友写一个二手房的项目，需要用到爬虫，这里借助了scrapy来写。顺便整理了scrapy的用法. fous_seas/sources","text":"前言最近在和朋友写一个二手房的项目，需要用到爬虫，这里借助了scrapy来写。顺便整理了scrapy的用法. fous_seas/sources 第一个 Spider12345678910111213141516171819import scrapyclass QuotesSpider(scrapy.Spider): name = \"quotes\" def start_requests(self): urls = [ 'http://quotes.toscrape.com/page/1/', 'http://quotes.toscrape.com/page/2/', ] for url in urls: yield scrapy.Request(url=url, callback=self.parse) def parse(self, response): page = response.url.split(\"/\")[-2] filename = 'quotes-%s.html' % page with open(filename, 'wb') as f: f.write(response.body) self.log('Saved file %s' % filename) 可以看到，我们新建的 QuotesSpider 类是继承自 scrapy.Spider 类的；下面看看其属性和方法的意义. name 是 Spider 的标识符，用于唯一标识该 Spider；它必须在整个项目中是全局唯一的. start_requests() 必须定义并返回一组可以被 Spider 爬取的 Requests，Request 对象由一个 URL 和一个回调函数构成. parse() 就是 Request 对象中的回调方法，用来解析每一个 Request 之后的 Response；所以，parse() 方法就是用来解析返回的内容，通过解析得到的 URL 同样可以创建对应的 Requests 进而继续爬取. 再来看看具体的实现。 start_request(self) 方法分别针对 http://quotes.toscrape.com/page/1/ 和 http://quotes.toscrape.com/page/2/ 创建了两个需要被爬取的 Requests 对象；并通过 yield 进行迭代返回；备注，yield 是迭代生成器，是一个 Generator； parse(self, response) 对 Request 的反馈的内容 Response 进行解析，这里的解析的逻辑很简单，就是分别创建两个本地文件，然后将 response.body 的内容放入这两个文件当中。 1scrapy crawl quotes 大致会输出如下内容 1234567891011...2016-12-16 21:24:05 [scrapy.core.engine] INFO: Spider opened2016-12-16 21:24:05 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)2016-12-16 21:24:05 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:60232016-12-16 21:24:05 [scrapy.core.engine] DEBUG: Crawled (404) &lt;GET http://quotes.toscrape.com/robots.txt&gt; (referer: None)2016-12-16 21:24:05 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://quotes.toscrape.com/page/1/&gt; (referer: None)2016-12-16 21:24:05 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://quotes.toscrape.com/page/2/&gt; (referer: None)2016-12-16 21:24:05 [quotes] DEBUG: Saved file quotes-1.html2016-12-16 21:24:05 [quotes] DEBUG: Saved file quotes-2.html2016-12-16 21:24:05 [scrapy.core.engine] INFO: Closing spider (finished)... 可以看到，通过爬取，我们在本地生成了两个 html 文件 quotes-1.html 和 quotes-2.html 如何提取通过命令行的方式提取Scrapy 提供了命令行的方式可以对需要被爬取的内容进行高效的调试，通过使用Scrapy shell进入命令行，然后在命令行中可以快速的对要爬取的内容进行提取； 一定要学会调试！！！这是不能跳过的步骤 我们试着通过 Scrapy shell 来提取下 “http://quotes.toscrape.com/page/1/&quot; 中的数据，通过执行如下命令，进入 shell 123456scrapy shell &quot;http:&#x2F;&#x2F;quotes.toscrape.com&#x2F;page&#x2F;1&#x2F;&quot;&#x2F;&#x2F; 或者scrapy shllfetch(&#39;http:&#x2F;&#x2F;quotes.toscrape.com&#x2F;page&#x2F;1&#x2F;&#39;) 输出 123456789101112131415[ ... Scrapy log here ... ]2016-09-19 12:09:27 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http:&#x2F;&#x2F;quotes.toscrape.com&#x2F;page&#x2F;1&#x2F;&gt; (referer: None)[s] Available Scrapy objects:[s] scrapy scrapy module (contains scrapy.Request, scrapy.Selector, etc)[s] crawler &lt;scrapy.crawler.Crawler object at 0x7fa91d888c90&gt;[s] item &#123;&#125;[s] request &lt;GET http:&#x2F;&#x2F;quotes.toscrape.com&#x2F;page&#x2F;1&#x2F;&gt;[s] response &lt;200 http:&#x2F;&#x2F;quotes.toscrape.com&#x2F;page&#x2F;1&#x2F;&gt;[s] settings &lt;scrapy.settings.Settings object at 0x7fa91d888c10&gt;[s] spider &lt;DefaultSpider &#39;default&#39; at 0x7fa91c8af990&gt;[s] Useful shortcuts:[s] shelp() Shell help (print this help)[s] fetch(req_or_url) Fetch request (or URL) and update local objects[s] view(response) View response in a browser&gt;&gt;&gt; 这样，我们就进入了 Scrapy shell 的环境，上面显示了连接请求和返回的相关信息，response 返回 status code 200 表示成功返回； 通过 CSS 标准进行提取这里主要是遵循 CSS 标准 https://www.w3.org/TR/selectors/ 来对网页的元素进行提取. 通过使用 css() 选择我们要提取的元素下面演示一下如何提取元素. 12&gt;&gt;&gt; response.css(&#39;title&#39;)[&lt;Selector xpath&#x3D;u&#39;descendant-or-self::title&#39; data&#x3D;u&#39;&lt;title&gt;Quotes to Scrape&lt;&#x2F;title&gt;&#39;&gt;] 可以看到，它通过返回一个类似 SelectorList 的对象成功的获取到了 http://quotes.toscrape.com/page/1/ 页面中的 的信息，该信息是封装在Selector对象中的 data 属性中的. 提取Selector元素的文本内容，一般有两种方式用来提取 通过使用 extract() 或者 extract_first() 方法来提取元素的内容；下面演示如何提取 #1 返回的元素 中的文本内容 text； 12&gt;&gt;&gt; response.css(&#39;title::text&#39;).extract_first()&#39;Quotes to Scrape&#39; extract_first() 表示提取返回队列中的第一个 Selector 对象；同样也可以使用如下的方式. 12&gt;&gt;&gt; response.css(&#39;title::text&#39;)[0].extract()&#39;Quotes to Scrape&#39; 不过 extract_first() 方法可以在当页面没有找到的情况下，避免出现IndexError的错误； 通过 re() 方法来使用正则表达式的方式来进行提取元素的文本内容 123456&gt;&gt;&gt; response.css(&#39;title::text&#39;).re(r&#39;Quotes.*&#39;)[&#39;Quotes to Scrape&#39;]&gt;&gt;&gt; response.css(&#39;title::text&#39;).re(r&#39;Q\\w+&#39;)[&#39;Quotes&#39;]&gt;&gt;&gt; response.css(&#39;title::text&#39;).re(r&#39;(\\w+) to (\\w+)&#39;)[&#39;Quotes&#39;, &#39;Scrape&#39;] 使用 XPath除了使用 CSS 标准 来提取元素意外，我们还可以使用 XPath 标准来提取元素，比如: 1234&gt;&gt;&gt; response.xpath(&#39;&#x2F;&#x2F;title&#39;)[&lt;Selector xpath&#x3D;&#39;&#x2F;&#x2F;title&#39; data&#x3D;&#39;&lt;title&gt;Quotes to Scrape&lt;&#x2F;title&gt;&#39;&gt;]&gt;&gt;&gt; response.xpath(&#39;&#x2F;&#x2F;title&#x2F;text()&#39;).extract_first()&#39;Quotes to Scrape&#39; XPath 比 CSS 的爬取方式更为强大，因为它不仅仅是根据 HTML 的结构元素去进行检索(Navigating)，并且它可以顺带的对文本(text)进行检索；所以它可以支持 CSS 标准不能做到的场景，比如，检索一个 包含文本内容”Next Page”的 link 元素；这就使得通过 XPath 去构建爬虫更为简单. 数据流设计图 ItemsScrapy 的核心目的就是从非结构化的网页中提取出结构化的数据；默认的，Scrapy 爬虫以 dicts 的形式返回格式化的数据；但是，这里有一个问题，就是 dicts 并不能很好的表示这种结构化数据的结构，而且经常容易出错，转换也麻烦。 因此，Item 诞生了，它提供了这样一个简单的容器来收集爬取到的数据，并提供非常简便的 API 来声明它的 fields。 声明 Items通过一个简单的 class 和多个 Field 对象来声明 Items 对象；看一个 Product Item 的例子。 1234567import scrapyclass Product(scrapy.Item): name &#x3D; scrapy.Field() price &#x3D; scrapy.Field() stock &#x3D; scrapy.Field() last_updated &#x3D; scrapy.Field(serializer&#x3D;str) 需要注意的是，Product 继承自 scrapy.Item 父类.","categories":[{"name":"爬虫","slug":"爬虫","permalink":"http://blog.crazylaw.cn/categories/%E7%88%AC%E8%99%AB/"}],"tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://blog.crazylaw.cn/tags/%E7%88%AC%E8%99%AB/"}]},{"title":"【Golang】- pprof性能分析","slug":"Golang/pprof性能分析","date":"2020-08-14T08:35:51.000Z","updated":"2021-03-20T16:25:01.799Z","comments":true,"path":"2020/08/14/Golang/pprof性能分析/","link":"","permalink":"http://blog.crazylaw.cn/2020/08/14/Golang/pprof%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/","excerpt":"前言最近在开发一个golang的大数据服务，发现只要到了业务层性能就急剧下降，为了排查这个原因，使用pprof进行性能分析","text":"前言最近在开发一个golang的大数据服务，发现只要到了业务层性能就急剧下降，为了排查这个原因，使用pprof进行性能分析 使用在main函数中加入以下代码 123456import \"runtime/pprof\"// ...cpuProfile, _ := os.Create(\"cpu_profile\")pprof.StartCPUProfile(cpuProfile)defer pprof.StopCPUProfile()// ... 这里 os.Create(&quot;cpu_profile&quot;) 指定生成的数据文件, 然后 pprof.StartCPUProfile 看名字就知道是开始对 CPU 的使用进行监控. 有开始就有结束, 一般直接跟着 defer pprof.StopCPUProfile() 省的后面忘了. 编译执行一次以后会在目录下生成监控数据并记录到 cpu_profile. 接着就可以使用 pprof 来解读分析这些监控生成的数据. CPU Profiling1234567root@60b1d42d6330:&#x2F;www# go tool pprof cpu_profileFile: mainBuild ID: 3d9d75a7fe2c5c4917c59acabfd743a6512d91faType: cpuTime: Aug 14, 2020 at 8:34am (UTC)Duration: 205.73ms, Total samples &#x3D; 30ms (14.58%)Entering interactive mode (type &quot;help&quot; for commands, &quot;o&quot; for options)","categories":[{"name":"Golang","slug":"Golang","permalink":"http://blog.crazylaw.cn/categories/Golang/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://blog.crazylaw.cn/tags/Golang/"}]},{"title":"【Golang】- devle","slug":"Golang/dlv在线调试","date":"2020-08-14T01:46:51.000Z","updated":"2021-03-20T16:25:01.799Z","comments":true,"path":"2020/08/14/Golang/dlv在线调试/","link":"","permalink":"http://blog.crazylaw.cn/2020/08/14/Golang/dlv%E5%9C%A8%E7%BA%BF%E8%B0%83%E8%AF%95/","excerpt":"前言由于我们可能断点调试程序，所以我们需要debug工具，对于golang来说，delve比gdb对go程序更友好。所以，我们今天来看看devle怎么debug程序","text":"前言由于我们可能断点调试程序，所以我们需要debug工具，对于golang来说，delve比gdb对go程序更友好。所以，我们今天来看看devle怎么debug程序 安装1go get github.com&#x2F;go-delve&#x2F;delve&#x2F;cmd&#x2F;dlv 用法a）dlv debug 使用dlv debug可以在main函数文件所在目录直接对main函数进行调试，也可以在根目录以指定包路径的方式对main函数进行调试。 b）dlv test 使用dlv test可以对test包进行调试。 c）dlv attach 使用dlv attach可以附加到一个已在运行的进程进行调试。 d）dlv connect 使用dlv connect可以连接到调试服务器进行调试。 e）dlv trace 使用dlv trace可以追踪程序。 f）dlv exec 使用dlv exec可以对编译好的二进制进行调试 创建main.go文件，main函数先通过循初始化一个切片，然后输出切片的内容： 12345678910111213package mainimport ( \"fmt\")func main() &#123; nums := make([]int, 5) for i := 0; i &lt; len(nums); i++ &#123; nums[i] = i * i &#125; fmt.Println(nums)&#125; 命令行进入包所在目录，然后输入dlv debug命令进入调试： 123$ dlv debugType &#39;help&#39; for list of commands.(dlv) 输入help命令可以查看到Delve提供的调试命令列表： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960(dlv) helpThe following commands are available:Running the program: call ------------------------ Resumes process, injecting a function call (EXPERIMENTAL!!!) continue (alias: c) --------- Run until breakpoint or program termination. next (alias: n) ------------- Step over to next source line. rebuild --------------------- Rebuild the target executable and restarts it. It does not work if the executable was not built by delve. restart (alias: r) ---------- Restart process. step (alias: s) ------------- Single step through program. step-instruction (alias: si) Single step a single cpu instruction. stepout (alias: so) --------- Step out of the current function.Manipulating breakpoints: break (alias: b) ------- Sets a breakpoint. breakpoints (alias: bp) Print out info for active breakpoints. clear ------------------ Deletes breakpoint. clearall --------------- Deletes multiple breakpoints. condition (alias: cond) Set breakpoint condition. on --------------------- Executes a command when a breakpoint is hit. trace (alias: t) ------- Set tracepoint.Viewing program variables and memory: args ----------------- Print function arguments. display -------------- Print value of an expression every time the program stops. examinemem (alias: x) Examine memory: locals --------------- Print local variables. print (alias: p) ----- Evaluate an expression. regs ----------------- Print contents of CPU registers. set ------------------ Changes the value of a variable. vars ----------------- Print package variables. whatis --------------- Prints type of an expression.Listing and switching between threads and goroutines: goroutine (alias: gr) -- Shows or changes current goroutine goroutines (alias: grs) List program goroutines. thread (alias: tr) ----- Switch to the specified thread. threads ---------------- Print out info for every traced thread.Viewing the call stack and selecting frames: deferred --------- Executes command in the context of a deferred call. down ------------- Move the current frame down. frame ------------ Set the current frame, or execute command on a different frame. stack (alias: bt) Print stack trace. up --------------- Move the current frame up.Other commands: config --------------------- Changes configuration parameters. disassemble (alias: disass) Disassembler. edit (alias: ed) ----------- Open where you are in $DELVE_EDITOR or $EDITOR exit (alias: quit | q) ----- Exit the debugger. funcs ---------------------- Print list of functions. help (alias: h) ------------ Prints the help message. libraries ------------------ List loaded dynamic libraries list (alias: ls | l) ------- Show source code. source --------------------- Executes a file containing a list of delve commands sources -------------------- Print list of source files. types ---------------------- Print list of typesType help followed by a command for full documentation. 每个Go程序的入口是main.main函数，我们可以用break(b)在此设置一个断点： 12(dlv) break main.mainBreakpoint 1 set at 0x10ae9b8 for main.main() .&#x2F;main.go:7 然后通过breakpoints(bp)查看已经设置的所有断点： 12345(dlv) breakpointsBreakpoint unrecovered-panic at 0x102a380 for runtime.startpanic() &#x2F;usr&#x2F;local&#x2F;go&#x2F;src&#x2F;runtime&#x2F;panic.go:588 (0) print runtime.curg._panic.argBreakpoint 1 at 0x10ae9b8 for main.main() .&#x2F;main.go:7 (0) 我们发现除了我们自己设置的main.main函数断点外，Delve内部已经为panic异常函数设置了一个断点。 通过vars命令可以查看全部包级的变量。因为最终的目标程序可能含有大量的全局变量，我们可以通过一个正则参数选择想查看的全局变量： 12345(dlv) vars mainmain.initdone· &#x3D; 2runtime.main_init_done &#x3D; chan bool 0&#x2F;0runtime.mainStarted &#x3D; true(dlv) 然后就可以通过continue(c)命令让程序运行到下一个断点处： 1234567891011121314(dlv) continue&gt; main.main() .&#x2F;main.go:7 (hits goroutine(1):1 total:1) (PC: 0x10ae9b8) 2: 3: import ( 4: &quot;fmt&quot; 5: ) 6:&#x3D;&gt; 7: func main() &#123; 8: nums :&#x3D; make([]int, 5) 9: for i :&#x3D; 0; i &lt; len(nums); i++ &#123; 10: nums[i] &#x3D; i * i 11: &#125; 12: fmt.Println(nums)(dlv) 输入next(n)命令单步执行进入main函数内部： 1234567891011121314(dlv) next&gt; main.main() .&#x2F;main.go:8 (PC: 0x10ae9cf) 3: import ( 4: &quot;fmt&quot; 5: ) 6: 7: func main() &#123;&#x3D;&gt; 8: nums :&#x3D; make([]int, 5) 9: for i :&#x3D; 0; i &lt; len(nums); i++ &#123; 10: nums[i] &#x3D; i * i 11: &#125; 12: fmt.Println(nums) 13: &#125;(dlv) 进入函数之后可以通过args和locals命令查看函数的参数和局部变量： 1234(dlv) args(no args)(dlv) localsnums &#x3D; []int len: 842350763880, cap: 17491881, nil 因为main函数没有参数，因此args命令没有任何输出。而locals命令则输出了局部变量nums切片的值：此时切片还未完成初始化，切片的底层指针为nil，长度和容量都是一个随机数值。 再次输入next命令单步执行后就可以查看到nums切片初始化之后的结果了： 12345678910111213141516(dlv) next&gt; main.main() .&#x2F;main.go:9 (PC: 0x10aea12) 4: &quot;fmt&quot; 5: ) 6: 7: func main() &#123; 8: nums :&#x3D; make([]int, 5)&#x3D;&gt; 9: for i :&#x3D; 0; i &lt; len(nums); i++ &#123; 10: nums[i] &#x3D; i * i 11: &#125; 12: fmt.Println(nums) 13: &#125;(dlv) localsnums &#x3D; []int len: 5, cap: 5, [...]i &#x3D; 17601536(dlv) 此时因为调试器已经到了for语句行，因此局部变量出现了还未初始化的循环迭代变量i。 下面我们通过组合使用break(b)和condition(cond)命令，在循环内部设置一个条件断点，当循环变量i等于3时断点生效： 1234(dlv) break main.go:10Breakpoint 2 set at 0x10aea33 for main.main() .&#x2F;main.go:10(dlv) condition 2 i&#x3D;&#x3D;3(dlv) 然后通过continue执行到刚设置的条件断点，并且输出局部变量： 1234567891011121314151617(dlv) continue&gt; main.main() .&#x2F;main.go:10 (hits goroutine(1):1 total:1) (PC: 0x10aea33) 5: ) 6: 7: func main() &#123; 8: nums :&#x3D; make([]int, 5) 9: for i :&#x3D; 0; i &lt; len(nums); i++ &#123;&#x3D;&gt; 10: nums[i] &#x3D; i * i 11: &#125; 12: fmt.Println(nums) 13: &#125;(dlv) localsnums &#x3D; []int len: 5, cap: 5, [...]i &#x3D; 3(dlv) print nums[]int len: 5, cap: 5, [0,1,4,0,0](dlv) 我们发现当循环变量i等于3时，nums切片的前3个元素已经正确初始化。 我们还可以通过stack查看当前执行函数的栈帧信息： 12345678(dlv) stack0 0x00000000010aea33 in main.main at .&#x2F;main.go:101 0x000000000102bd60 in runtime.main at &#x2F;usr&#x2F;local&#x2F;go&#x2F;src&#x2F;runtime&#x2F;proc.go:1982 0x0000000001053bd1 in runtime.goexit at &#x2F;usr&#x2F;local&#x2F;go&#x2F;src&#x2F;runtime&#x2F;asm_amd64.s:2361(dlv) 或者通过goroutine和goroutines命令查看当前Goroutine相关的信息： 1234567891011121314151617(dlv) goroutineThread 101686 at .&#x2F;main.go:10Goroutine 1: Runtime: .&#x2F;main.go:10 main.main (0x10aea33) User: .&#x2F;main.go:10 main.main (0x10aea33) Go: &#x2F;usr&#x2F;local&#x2F;go&#x2F;src&#x2F;runtime&#x2F;asm_amd64.s:258 runtime.rt0_go (0x1051643) Start: &#x2F;usr&#x2F;local&#x2F;go&#x2F;src&#x2F;runtime&#x2F;proc.go:109 runtime.main (0x102bb90)(dlv) goroutines[4 goroutines]* Goroutine 1 - User: .&#x2F;main.go:10 main.main (0x10aea33) (thread 101686) Goroutine 2 - User: &#x2F;usr&#x2F;local&#x2F;go&#x2F;src&#x2F;runtime&#x2F;proc.go:292 \\ runtime.gopark (0x102c189) Goroutine 3 - User: &#x2F;usr&#x2F;local&#x2F;go&#x2F;src&#x2F;runtime&#x2F;proc.go:292 \\ runtime.gopark (0x102c189) Goroutine 4 - User: &#x2F;usr&#x2F;local&#x2F;go&#x2F;src&#x2F;runtime&#x2F;proc.go:292 \\ runtime.gopark (0x102c189)(dlv) 最后完成调试工作后输入quit命令退出调试器。至此我们已经掌握了Delve调试器器的简单用法。","categories":[{"name":"Golang","slug":"Golang","permalink":"http://blog.crazylaw.cn/categories/Golang/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://blog.crazylaw.cn/tags/Golang/"}]},{"title":"【DevOps】Jenkins-shared-library 实战","slug":"DevOps/Jenkins-shared-library","date":"2020-07-10T09:35:30.000Z","updated":"2021-03-20T16:25:01.798Z","comments":true,"path":"2020/07/10/DevOps/Jenkins-shared-library/","link":"","permalink":"http://blog.crazylaw.cn/2020/07/10/DevOps/Jenkins-shared-library/","excerpt":"前言最近公司用了 jenkins 做为 CI/CD 的工具，jenkins是什么，我就不和大家详细说明了。 今天主要讲的是 jenkins 的 pipeline，旧版的 jenkins 我不太了解，但是听说这个 pipeline 是新出的。 今天要讲的也不单单是pipeline，更是有强力干货share-library相关的内容。","text":"前言最近公司用了 jenkins 做为 CI/CD 的工具，jenkins是什么，我就不和大家详细说明了。 今天主要讲的是 jenkins 的 pipeline，旧版的 jenkins 我不太了解，但是听说这个 pipeline 是新出的。 今天要讲的也不单单是pipeline，更是有强力干货share-library相关的内容。 Jenkinsfile目前我们的 jenkins 服务用过普通的pipeline和 多分支的pipeline，其主要区别就是多分支pipeline更加灵活一些，能够让我们少做一些逻辑处理。但是这引入了一个问题，那就是我们的jenkins，必须存在与对应的分支中，也因此，pipeline 的流程变成了代码管理，并且由于我们存在多个项目，那么将会有项目数 * 2(master/pre-develop) 那么多的 jenkins 需要管理，于是就导致了我们有很多这种冗余的写法，这还不是最糟糕的，最糟糕的是如果我们需要修改一些通用的内容的时候，就需要一个个去修改，这绝对是灾难级别的需求。 Shared-library幸好，或许 jenkins 已经考虑到了这种情况的出现，给我们提供了shared-library，在共享库的存在下，我们可以写一些自定义的定制功能。但是这不是没有代价的，代价就是你需要了解groovy这门语言，这是java的派生语言，提供了弱类型的语法，让写 groovy 脚本的变得更加轻松简单，但是也因此，groovy 和 java 的语法常常可以混在一起写，显得有点杂乱。 但是有总比没有好，我们把共享库分成了 2 个库，分别是resource-libraries，jenkins-shard-library。 resource-libraries 这个资源共享库定义了我们需要部署不同的配置资源信息，主要是对应libraryResource这个 api 来加载资源 jenkins-shard-library 这个共享库定义了我们需要的通用方法，例如部署流程已经需要统一对外的 api resource-libraries目前，我们的资源库目录结构如下，基本不存在 src 目录，这也意味者这个库不存在独特的类代码，只有统一对外开放的vars，整个库对外开放的 api 只有一个mresource_load方法，核心作用就是加载resources目录中的个项目的配置。由于可能会存在不同组之间的相同项目名，所以我们以组/项目为目录切分资源配置文件，文件以yaml格式为统一标准。 1234567891011121314➜ resource-libraries git:(master) tree.├── README.MD├── resources│ ├── GamePlatform│ │ └── mcfx-admin.yaml│ ├── mc-game-admin│ │ └── om.yaml│ └── mc-webapp│ └── testing-tools.yaml├── tests│ └── test_mresource_load.groovy└── vars └── mresource_load.groovy 基本结构如下： 123456789101112131415➜ resource-libraries git:(master) cat resources&#x2F;mc-webapp&#x2F;testing-tools.yamltype: &#39;normal&#39;git_credentials_id: &#39;basedev-git-account&#39;common: &amp;common directory: &#39;&#x2F;data&#x2F;webapp&#x2F;testing-tools&#39; deploy_ssh_id: &#39;basedev_tp_normal&#39;debug: address: &#39;192.168.8.29:61618&#39; &lt;&lt;: *commonproduction: address: &#39;192.168.8.93:61618&#39; &lt;&lt;: *common 1234567891011121314151617➜ resource-libraries git:(master) cat resources&#x2F;mc-game-admin&#x2F;om.yamltype: job_namegit_credentials_id: &#39;basedev_24_om&#39;debug_address: &amp;debug_address &#39;192.168.8.47:61618&#39;release_address: &amp;release_address &#39;192.168.8.47:61618&#39;deploy_ssh_id: &amp;deploy_ssh_id &#39;basedev_tp_normal&#39;basedev-m24-om: debug: address: *debug_address directory: &#39;&#x2F;data&#x2F;m24&#x2F;web&#x2F;om_debug&#39; deploy_ssh_id: *deploy_ssh_id production: address: *release_address directory: &#39;&#x2F;data&#x2F;m24&#x2F;web&#x2F;om&#39; deploy_ssh_id: *deploy_ssh_id (*)type normal 普通模式 job_name job_name 模式。对于一个代码仓库，需要分项目部署的我们定义为 job_name 模式，这个的好处就是不同的项目可以以同一份配置文件进行不同的配置管理 (*)git_credentials_id 这是 checkout 下来的 git 唯一凭证 id common 这个是推荐但是非必须存在的结构，这里存放一些我们在下面定义的一些通用配置来减少冗余 支持的环境 debug reviews production 环境必填的属性如下 address (ip+port) 支持 string 支持 list directory 代码部署目录 支持 string 支持 list deploy_ssh_id 部署代码到服务的 ssh 的唯一凭证 id 由以上构成我们的配置文件的格式。 jenkins-shard-library这个共享库直接影响到我们的 jenkinsfile 的写法，这是直接作用与 jenkinsfile 的共享库，先来一个目前 jenkinsfile 的写法： 12345678910111213def link &#x3D; &#39;https:&#x2F;&#x2F;xxx.com&#x2F;mc-webapp&#x2F;testing-tools&#39;mpipeline&#123; repo&#x3D;link script&#x3D;this&#125;import xxx.jenkins.LaravelPipelinedef larvaelPipeline &#x3D; new LaravelPipeline(this)node &#123; larvaelPipeline.preBuild().build().debug().reviews().production().execute()&#125; 这就是目前的 jenkinsfile 的写法，可以看到这个 jenkinsfile 没有定制的东西，除了一个link仓库链接之外，其他都是公用的内容。接下来说一下jenkinsfile-dsl 这个东西也就是我们在 jenkinsfile 中看到的mpipeline的结构体，这个是由于我们这个库提供的一个dsl结构，并非原始 jenkinsfile 提供的所有的 jenkinsfile 必须存在这个dsl，并且需要写在流程的头部，以确保 jenkinsfile 的初始化对应的事情（checkout+环境变量+关键字加载） 目前，封装了一个基于laravel项目的 pipeline 操作类，用这个类来进行流程的统一创建管理。最后我们在 node结构中进行流程的 api 调用。 12345678910111213141516171819202122232425262728293031323334➜ jenkins-shard-library git:(master) tree.├── Jenkinsfile.example├── README.MD├── jars│ └── groovy-cps-1.1.jar├── resources├── src│ └── xxx│ └── xxx│ └── jenkins│ ├── Constant.groovy│ ├── Deploy.groovy│ ├── Git.groovy│ ├── LaravelPipeline.groovy│ ├── MPipeline.groovy│ ├── MetlNotify.groovy│ ├── Repo.groovy│ └── SummaryNotify.groovy├── tests│ ├── RepoTest.groovy│ └── classfiles│ └── xxx│ └── xxx│ └── jenkins│ ├── Constant.class│ ├── Git.class│ ├── MPipeline.class│ ├── MetlNotify.class│ ├── Repo.class│ └── SummaryNotify.class└── vars ├── mpipeline.groovy └── notify.groovy 详细的就不多做讲解，只说一些重要的，LaravelPipeline 对外提供的 api 有： preBuild() 构建前需要做的事情，例如发送通知任务开始 build(isParallel = true) 构建过程，参数isParallel = true是否并行构建 deployment(def sshId = null) 部署 api，主要是把 debug，reviews，production 写在了一起 customDeploy(def closure = {}) 自定义部署流程，参数是一个回调函数，需要参考共享库的写法 debug(def sshId) 如果是对应的分支，那么将会部署到 debug 环境 reviews(def sshId) 如果是对应的分支，那么将会部署到 reviews 环境 production(def sshId) 如果是对应的分支，那么将会部署到 production 环境 show() 这是一个用于查看构建流程的方法，并不会真正运行各个结构的 stage 里面的内容。但是可以看到节点连线图 execute(def config = [show: false]) 这是一个启动执行的方法，如果传入了参数 show=true，那么该方法等于 show 方法 对于全局关键字的提供有： mpipeline 用于初始化整个部署流程，其中包含了 checkout 和环境变量等的初始化 notify 用于发送通知 最终的效果如下：","categories":[{"name":"DevOps","slug":"DevOps","permalink":"http://blog.crazylaw.cn/categories/DevOps/"}],"tags":[{"name":"DevOps","slug":"DevOps","permalink":"http://blog.crazylaw.cn/tags/DevOps/"}]},{"title":"【Docker】docker-apline的问题","slug":"docker/docker-apline的问题","date":"2020-06-18T12:28:30.000Z","updated":"2021-03-20T16:25:01.805Z","comments":true,"path":"2020/06/18/docker/docker-apline的问题/","link":"","permalink":"http://blog.crazylaw.cn/2020/06/18/docker/docker-apline%E7%9A%84%E9%97%AE%E9%A2%98/","excerpt":"前言今天领导发现了一个docker的问题，他在用apline作为基础镜像的时候，在公司的服务器上无法ping baidu.com 但是我们却是能在nslookup中解析到域名。","text":"前言今天领导发现了一个docker的问题，他在用apline作为基础镜像的时候，在公司的服务器上无法ping baidu.com 但是我们却是能在nslookup中解析到域名。 问题1. ping baidu.com在用apline作为基础镜像的时候，发现无法ping的情况，在我翻阅了一些文献的。说到了docker-apline是以8.8.8.8Google的dns服务作为默认网关。 又由于我国具有“墙”的特性，所以我们需要指定国内的dns服务器，例如114.114.114.114，但是这些并不能解决我们的问题。 又有一些网友说是因为我们自己的dns网关服务不正常导致的，但是确实我们的dns服务一直很正常。 基于了这些都不能解决我们问题点。基于感觉到了没有任何希望。 但是后来还是找到了一些另类的解决思路。 例如有一批人提到了/etc/resolve.conf的ndots的参数。 于是好奇的搜索了一些ndots相关的内容，发现这个是用于优化dns解析速度的参数。 ndotsoptions ndots:0 稍微有点麻烦，我查了一些资料。如果需要查找域名是相对域名，且该域名中包含的. 的数目大于或等于 option ndots:${n}命令指定的数，则查询的仅是该域名。否则会依次往传入的域名后追加 search 列表（search 列表在这里没有，详见search）中的后缀，直到解析出ip地址，或者解析完列表中所有后缀才会停止。这里设置成 0，意思就是对于传入的相对域名，只查询该域名。 一般ndots都是5即可。最高就是15。 默认：1 12ndots:n Sets a threshold for the number of dots which must appear in a name given to res_query(3) (see resolver(3)) before an initial absolute query will be made. The default for n is 1, meaning that if there are any dots in a name, the name will be tried first as an absolute name before any search list elements are appended to it. The value for this option is silently capped to 15. 于是抱着尝试的态度。 没有加入参数前 12[root@webapp_public_S1_192.168.8.135_61618_A ~]# docker run -it --rm composer:2.0 ping baidu.comping: bad address 'baidu.com' 加入参数后 1234[root@webapp_public_S1_192.168.8.135_61618_A ~]# docker run -it --dns-opt&#x3D;ndots:5 --rm composer:2.0 ping baidu.comPING baidu.com (39.156.69.79): 56 data bytes64 bytes from 39.156.69.79: seq&#x3D;0 ttl&#x3D;43 time&#x3D;37.941 ms64 bytes from 39.156.69.79: seq&#x3D;1 ttl&#x3D;43 time&#x3D;37.891 ms 神奇的一幕出现了，dns服务正常解析了。于是可以确定ndots在解决apline域名解析上有重要的作用。","categories":[{"name":"Docker","slug":"Docker","permalink":"http://blog.crazylaw.cn/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://blog.crazylaw.cn/tags/Docker/"}]},{"title":"【大数据】- docker构建基于Hadoop的大数据生态","slug":"大数据/docker构建基于Hadoop的大数据生态","date":"2020-06-16T10:07:40.000Z","updated":"2021-03-20T16:25:01.815Z","comments":true,"path":"2020/06/16/大数据/docker构建基于Hadoop的大数据生态/","link":"","permalink":"http://blog.crazylaw.cn/2020/06/16/%E5%A4%A7%E6%95%B0%E6%8D%AE/docker%E6%9E%84%E5%BB%BA%E5%9F%BA%E4%BA%8EHadoop%E7%9A%84%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%94%9F%E6%80%81/","excerpt":"前言众所周知，hadoop的大数据生态的组件版本对依赖十分的繁杂，对此我们在如果需要用apache开源库来一点点堆起积木，有点繁琐我们这个项目的目的只是为了更快更方便的构建大数据生态环境。所以我们这里选择采用cdh的方式来构建大数据生态，但是由于我们希望在终端就部署好服务，而不需要客户端，所以这里就不选择用cmf","text":"前言众所周知，hadoop的大数据生态的组件版本对依赖十分的繁杂，对此我们在如果需要用apache开源库来一点点堆起积木，有点繁琐我们这个项目的目的只是为了更快更方便的构建大数据生态环境。所以我们这里选择采用cdh的方式来构建大数据生态，但是由于我们希望在终端就部署好服务，而不需要客户端，所以这里就不选择用cmf 基于docker的cdh6的大数据生态 在这里，你可以找到你需要的东西 Hadoop Impala Hive Kudu 接下来，我会记录一下我在这个过程中所遇到的一些问题。 选择底层镜像 底层操作系统镜像 123456789101112131415161718192021222324FROM centos:7LABEL maintainer=\"Caiwenhui &lt;471113744@qq.com&gt;\"ENV JAVA_HOME /usr/lib/jvm/java-openjdk# 换阿里云源RUN yum install -y wget;\\ wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo;\\ # 移除阿里已经不用的域名 sed -i '/mirrors.aliyuncs.com/d' /etc/yum.repos.d/CentOS-Base.repo;\\ sed -i '/mirrors.cloud.aliyuncs.com/d' /etc/yum.repos.d/CentOS-Base.repo;\\ yum clean all; \\ yum makecache# # 安装基础必备软件RUN yum update -y; \\ yum intall -y deltarpm;\\ yum install -y java-1.8.0-openjdk-devel unzip curl vim python-setuptools sudo; \\ yum clean all;\\ yum makecacheCMD [\"/bin/bash\"] 在这里，我们基于的是centos7做操作系统镜像，并且在基础上，替换成了阿里云的yum的数据源，并且安装好了jdk8，方便后续部署服务 选择基础镜像 基于cdh6的基础镜像 123456789101112131415FROM ccinn/centos-openjdk:latestLABEL maintainer=\"Caiwenhui &lt;471113744@qq.com&gt;\"USER rootENV CDH_VERSION 6.3.2ADD cloudera-cdh6.repo /etc/yum.repos.d/RUN rpm --import https://archive.cloudera.com/cdh6/$CDH_VERSION/redhat7/yum/RPM-GPG-KEY-cloudera;\\ yum makecacheWORKDIR /CMD [\"/bin/bash\"] 在这里，我们的目录很简单，就是把cloudera-cdh6.repo放在yum的源中，方便我们通过yum安装服务，减少不必要的依赖问题 注意，我们这里选择的hadoop3的版本 构建hadoop镜像 基于基础镜像的hadoop服务 1234567891011121314FROM ccinn/cdh6:latestLABEL maintainer=\"Caiwenhui &lt;471113744@qq.com&gt;\"USER rootADD support.sh /support.shRUN source /support.sh;\\ loop_exec 'yum install -y hadoop'WORKDIR /CMD [\"/bin/bash\"] 问题11234567891011Install 1 Package (+52 Dependent packages)Total download size: 712 MInstalled size: 856 MDownloading packages:https:&#x2F;&#x2F;archive.cloudera.com&#x2F;cdh6&#x2F;6.3.2&#x2F;redhat7&#x2F;yum&#x2F;RPMS&#x2F;x86_64&#x2F;hadoop-hdfs-3.0.0%2Bcdh6.3.2-1605554.el7.x86_64.rpm: [Errno 14] curl#18 - &quot;transfer closed with 23278968 bytes remaining to read&quot;Trying other mirror.https:&#x2F;&#x2F;archive.cloudera.com&#x2F;cdh6&#x2F;6.3.2&#x2F;redhat7&#x2F;yum&#x2F;RPMS&#x2F;x86_64&#x2F;hadoop-3.0.0%2Bcdh6.3.2-1605554.el7.x86_64.rpm: [Errno 14] curl#18 - &quot;transfer closed with 40971220 bytes remaining to read&quot;Trying other mirror.https:&#x2F;&#x2F;archive.cloudera.com&#x2F;cdh6&#x2F;6.3.2&#x2F;redhat7&#x2F;yum&#x2F;RPMS&#x2F;noarch&#x2F;hive-2.1.1%2Bcdh6.3.2-1605554.el7.noarch.rpm: [Errno 14] curl#18 - &quot;transfer closed with 123435828 bytes remaining to read&quot;Trying other mirror. 我这里引入了一个loop_exec的函数，原因是因为国内的网络过慢，以及软件包“过大”，导致yum在安装包的时候，会导致传输中断，使得我们无法“一步”安装到位 所以这里我写了一个循环调用的函数，因为yum安装过程中是可以断点续传的，所以我利用这个函数来执行多几次这个命令即可，直到安装成功为止 一般我是3次安装完毕 问题2我发现原来hadoop包是基础包而已，我们还要安装hadoop-namenode,hadoop-datanode，因此。我又手动在容器中调试了起来。 1/usr/bin/hdfs --config /etc/hadoop/conf namenode 12345678910111213141516171819202122232425262728293031323334[root@25b603a089cc &#x2F;]# &#x2F;usr&#x2F;bin&#x2F;hdfs --config &#x2F;etc&#x2F;hadoop&#x2F;conf namenode2020-06-16 10:18:34,126 INFO namenode.NameNode: STARTUP_MSG:&#x2F;************************************************************STARTUP_MSG: Starting NameNodeSTARTUP_MSG: host &#x3D; 25b603a089cc&#x2F;172.17.0.3STARTUP_MSG: args &#x3D; []STARTUP_MSG: version &#x3D; 3.0.0-cdh6.3.2STARTUP_MSG: classpath &#x3D; &#x2F;etc&#x2F;hadoop&#x2F;conf:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-lang3-3.7.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-beanutils-1.9.4.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;log4j-1.2.17.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-logging-1.1.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;re2j-1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jackson-core-asl-1.9.13.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jettison-1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;guava-11.0.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;javax.activation-api-1.2.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;paranamer-2.8.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jetty-util-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;slf4j-log4j12.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jsr311-api-1.1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;azure-data-lake-store-sdk-2.2.9.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jersey-servlet-1.19.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jaxb-api-2.2.11.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;metrics-core-3.0.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;nimbus-jose-jwt-4.41.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;javax.servlet-api-3.1.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;protobuf-java-2.5.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-math3-3.1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jersey-core-1.19.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-compress-1.18.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;accessors-smart-1.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;woodstox-core-5.0.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jetty-io-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jackson-xc-1.9.13.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jetty-xml-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;curator-recipes-2.12.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jetty-servlet-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-lang-2.6.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;audience-annotations-0.5.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerby-config-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jetty-security-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jetty-server-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerb-util-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;netty-3.10.6.Final.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerb-admin-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jersey-json-1.19.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jackson-databind-2.9.9.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerb-core-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jackson-jaxrs-1.9.13.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerby-asn1-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-configuration2-2.1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerb-crypto-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;httpcore-4.4.6.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;log4j-core-2.8.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;json-smart-2.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;snappy-java-1.1.4.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jackson-mapper-asl-1.9.13-cloudera.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerb-simplekdc-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;logredactor-2.0.7.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;avro.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jackson-annotations-2.9.9.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerb-client-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jersey-server-1.19.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;curator-client-2.12.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-cli-1.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jcip-annotations-1.0-1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerb-server-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerby-xdr-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerb-common-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;htrace-core4-4.1.0-incubating.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;aws-java-sdk-bundle-1.11.271.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerby-util-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jetty-webapp-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;stax2-api-3.1.4.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;slf4j-api-1.7.25.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;log4j-api-2.8.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-io-2.6.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerby-pkix-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;curator-framework-2.12.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;gson-2.2.4.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jsp-api-2.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;xz-1.6.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;zookeeper.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jetty-http-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;wildfly-openssl-1.0.4.Final.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jackson-core-2.9.9.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerb-identity-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jaxb-impl-2.2.3-1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-net-3.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jul-to-slf4j-1.7.25.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;asm-5.0.4.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jsch-0.1.54.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jsr305-3.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-collections-3.2.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;httpclient-4.5.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-codec-1.11.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-format-javadoc.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-auth-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-jackson.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-azure-datalake.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-format.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-aws-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-azure-datalake-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-protobuf.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-annotations-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-auth.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-aws.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-azure.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-scala_2.11.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-nfs.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-annotations.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-pig.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-pig-bundle.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-common.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-hadoop-bundle.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-kms.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-kms-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-cascading.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-common-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-generator.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-azure-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-hadoop.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-common.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-common-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-avro.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-common-3.0.0-cdh6.3.2-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-format-sources.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-column.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-thrift.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-encoding.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-nfs-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-cascading3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-lang3-3.7.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-beanutils-1.9.4.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;log4j-1.2.17.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-logging-1.1.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;re2j-1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jackson-core-asl-1.9.13.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jettison-1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;guava-11.0.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;javax.activation-api-1.2.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;paranamer-2.8.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jetty-util-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jsr311-api-1.1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jersey-servlet-1.19.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jaxb-api-2.2.11.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;nimbus-jose-jwt-4.41.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;javax.servlet-api-3.1.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;protobuf-java-2.5.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;okio-1.6.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-math3-3.1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jersey-core-1.19.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-compress-1.18.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;accessors-smart-1.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;woodstox-core-5.0.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jetty-io-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jackson-xc-1.9.13.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jetty-xml-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;curator-recipes-2.12.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jetty-servlet-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-lang-2.6.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;json-simple-1.1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;avro-1.8.2-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;audience-annotations-0.5.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerby-config-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jetty-security-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jetty-server-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerb-util-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;zookeeper-3.4.5-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;netty-3.10.6.Final.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerb-admin-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jersey-json-1.19.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jackson-databind-2.9.9.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerb-core-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jackson-jaxrs-1.9.13.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerby-asn1-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-configuration2-2.1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerb-crypto-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;httpcore-4.4.6.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;json-smart-2.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;snappy-java-1.1.4.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jackson-mapper-asl-1.9.13-cloudera.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerb-simplekdc-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jackson-annotations-2.9.9.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerb-client-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jersey-server-1.19.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;curator-client-2.12.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-cli-1.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jcip-annotations-1.0-1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerb-server-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerby-xdr-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerb-common-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;htrace-core4-4.1.0-incubating.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerby-util-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jetty-webapp-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;stax2-api-3.1.4.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-io-2.6.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerby-pkix-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;curator-framework-2.12.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-daemon-1.0.13.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;gson-2.2.4.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;leveldbjni-all-1.8.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;xz-1.6.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jetty-http-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;okhttp-2.7.5.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jetty-util-ajax-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jackson-core-2.9.9.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerb-identity-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jaxb-impl-2.2.3-1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-net-3.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;asm-5.0.4.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jsch-0.1.54.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jsr305-3.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-collections-3.2.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;httpclient-4.5.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-codec-1.11.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-native-client.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-client-3.0.0-cdh6.3.2-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-nfs-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-native-client-3.0.0-cdh6.3.2-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-nfs.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-httpfs-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-client-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-3.0.0-cdh6.3.2-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-native-client-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-client-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-httpfs.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-native-client-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-client.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-examples-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-openstack.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-aliyun.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-core.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-extras-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-uploader.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-openstack-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-distcp.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-hs-plugins-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-sls.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-uploader-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-kafka.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-datajoin-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;netty-buffer-4.1.17.Final.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;netty-common-4.1.17.Final.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-jobclient-3.0.0-cdh6.3.2-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-gridmix-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;jdom-1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-jobclient-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-examples.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-kafka-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;netty-codec-http-4.1.17.Final.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-rumen-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-app.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-extras.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-streaming-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;lz4-java-1.5.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;aliyun-sdk-oss-2.8.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-app-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-datajoin.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;azure-storage-5.4.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-hs.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-nativetask-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-jobclient.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-distcp-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-hs-plugins.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-shuffle-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-aliyun-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-common.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;kafka-clients-2.2.1-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;ojalgo-43.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-shuffle.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-gridmix.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-hs-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-jobclient-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;netty-resolver-4.1.17.Final.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-resourceestimator-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-rumen.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;azure-keyvault-core-0.8.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;netty-codec-4.1.17.Final.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-nativetask.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;netty-transport-4.1.17.Final.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-sls-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-resourceestimator.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-archives.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-archives-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-common-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;netty-handler-4.1.17.Final.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-core-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-archive-logs.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;zstd-jni-1.3.8-1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-streaming.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-archive-logs-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;jackson-jaxrs-json-provider-2.9.9.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;guice-servlet-4.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;jackson-module-jaxb-annotations-2.9.9.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;jersey-client-1.19.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;objenesis-1.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;guice-4.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;jackson-jaxrs-base-2.9.9.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;metrics-core-3.0.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;java-util-1.9.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;fst-2.50.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;javax.inject-1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;geronimo-jcache_1.0_spec-1.0-alpha-1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;json-io-2.5.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;HikariCP-java7-2.4.12.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;bcprov-jdk15on-1.60.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;ehcache-3.3.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;bcpkix-jdk15on-1.60.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;aopalliance-1.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;jersey-guice-1.19.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;mssql-jdbc-6.2.1.jre7.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-applicationhistoryservice-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-registry-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-tests-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-common.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-sharedcachemanager-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-client-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-applicationhistoryservice.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-applications-unmanaged-am-launcher-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-resourcemanager.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-common.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-resourcemanager-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-timeline-pluginstorage.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-common-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-applications-distributedshell.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-router-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-client.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-common-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-web-proxy-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-sharedcachemanager.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-web-proxy.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-router.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-api-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-nodemanager-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-registry.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-applications-distributedshell-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-timeline-pluginstorage-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-api.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-applications-unmanaged-am-launcher.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-nodemanager.jarSTARTUP_MSG: build &#x3D; http:&#x2F;&#x2F;github.com&#x2F;cloudera&#x2F;hadoop -r 9aff20de3b5ecccf3c19d57f71b214fb4d37ee89; compiled by &#39;jenkins&#39; on 2019-11-08T13:49ZSTARTUP_MSG: java &#x3D; 1.8.0_252************************************************************&#x2F;2020-06-16 10:18:34,143 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]2020-06-16 10:18:34,296 INFO namenode.NameNode: createNameNode []2020-06-16 10:18:34,497 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties2020-06-16 10:18:34,709 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).2020-06-16 10:18:34,709 INFO impl.MetricsSystemImpl: NameNode metrics system started2020-06-16 10:18:34,764 INFO namenode.NameNode: fs.defaultFS is file:&#x2F;&#x2F;&#x2F;2020-06-16 10:18:34,972 ERROR namenode.NameNode: Failed to start namenode.java.lang.IllegalArgumentException: Invalid URI for NameNode address (check fs.defaultFS): file:&#x2F;&#x2F;&#x2F; has no authority. at org.apache.hadoop.hdfs.DFSUtilClient.getNNAddress(DFSUtilClient.java:646) at org.apache.hadoop.hdfs.DFSUtilClient.getNNAddressCheckLogical(DFSUtilClient.java:675) at org.apache.hadoop.hdfs.DFSUtilClient.getNNAddress(DFSUtilClient.java:637) at org.apache.hadoop.hdfs.server.namenode.NameNode.getRpcServerAddress(NameNode.java:562) at org.apache.hadoop.hdfs.server.namenode.NameNode.loginAsNameNodeUser(NameNode.java:693) at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:713) at org.apache.hadoop.hdfs.server.namenode.NameNode.&lt;init&gt;(NameNode.java:950) at org.apache.hadoop.hdfs.server.namenode.NameNode.&lt;init&gt;(NameNode.java:929) at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1653) at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1720)2020-06-16 10:18:34,983 INFO util.ExitUtil: Exiting with status 1: java.lang.IllegalArgumentException: Invalid URI for NameNode address (check fs.defaultFS): file:&#x2F;&#x2F;&#x2F; has no authority.2020-06-16 10:18:34,988 INFO namenode.NameNode: SHUTDOWN_MSG:&#x2F;************************************************************SHUTDOWN_MSG: Shutting down NameNode at 25b603a089cc&#x2F;172.17.0.3************************************************************&#x2F; 我以前台的方式启动服务，发现服务并没有启动成功，他告诉我检查fs.defaultFS参数 12345678910111213141516171819202122[root@25b603a089cc &#x2F;]# cat &#x2F;etc&#x2F;hadoop&#x2F;conf&#x2F;core-site.xml&lt;?xml version&#x3D;&quot;1.0&quot;?&gt;&lt;!-- Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the NOTICE file distributed with this work for additional information regarding copyright ownership. The ASF licenses this file to You under the Apache License, Version 2.0 (the &quot;License&quot;); you may not use this file except in compliance with the License. You may obtain a copy of the License at http:&#x2F;&#x2F;www.apache.org&#x2F;licenses&#x2F;LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.--&gt;&lt;?xml-stylesheet type&#x3D;&quot;text&#x2F;xsl&quot; href&#x3D;&quot;configuration.xsl&quot;?&gt;&lt;configuration&gt;&lt;&#x2F;configuration&gt; 于是我发现原来这个core-site文件默认是什么配置都没有的。于是我需要手动加上一些配置。 在这里顺带说明一下配置信息。 具体的参数配置请参见 &gt;&gt; hadoop3-默认配置参数 &lt;&lt;&lt; 在hadoop集群中，需要配置的文件主要包括四个，分别是core-site.xml、hdfs-site.xml、mapred-site.xml和yarn-site.xml，这四个文件分别是对不同组件的配置参数，主要内容如下表所示： 配置文件名 配置对象 主要内容 core-site.xml 集群全局参数 用于定义系统级别的参数，如HDFS URL、Hadoop的临时目录等 hdfs-site.xml HDFS参数 如名称节点和数据节点的存放位置、文件副本的个数、文件读取权限等 mapred-site.xml Mapreduce参数 包括JobHistory Server和应用程序参数两部分，如reduce任务的默认个数、任务所能够使用内存的默认上下限等 yarn-site.xml 集群资源管理系统参数 配置 ResourceManager，NodeManager 的通信端口，web监控端口等 搭建集群配置时重要参数： core-site.xml 参数名 默认值 参数解释 fs.defaultFS file:/// 文件系统主机和端口 io.file.buffer.size 4096 流文件的缓冲区大小 hadoop.tmp.dir /tmp/hadoop-${user.name} 临时文件夹 hdfs-site.xml 参数名 默认值 参数解释 dfs.namenode.secondary.http-address 0.0.0.0:50090 定义HDFS对应的HTTP服务器地址和端口 dfs.namenode.name.dir file://${hadoop.tmp.dir}/dfs/name 定义DFS的名称节点在本地文件系统的位置 dfs.datanode.data.dir file://${hadoop.tmp.dir}/dfs/data 定义DFS数据节点存储数据块时存储在本地文件系统的位置 dfs.replication 3 缺省的块复制数量 dfs.webhdfs.enabled true 是否通过http协议读取hdfs文件，如果选是，则集群安全性较差 mapred-site.xml 参数名 默认值 参数解释 mapreduce.framework.name local 取值local、classic或yarn其中之一，如果不是yarn，则不会使用YARN集群来实现资源的分配 mapreduce.jobhistory.address 0.0.0.0:10020 定义历史服务器的地址和端口，通过历史服务器查看已经运行完的Mapreduce作业记录 mapreduce.jobhistory.webapp.address 0.0.0.0:19888 定义历史服务器web应用访问的地址和端口 yarn-site.xml 参数名 默认值 参数解释 yarn.resourcemanager.address 0.0.0.0:8032 ResourceManager 提供给客户端访问的地址。客户端通过该地址向RM提交应用程序，杀死应用程序等 yarn.resourcemanager.scheduler.address 0.0.0.0:8030 ResourceManager提供给ApplicationMaster的访问地址。ApplicationMaster通过该地址向RM申请资源、释放资源等 yarn.resourcemanager.resource-tracker.address 0.0.0.0:8031 ResourceManager 提供给NodeManager的地址。NodeManager通过该地址向RM汇报心跳，领取任务等 yarn.resourcemanager.webapp.address 0.0.0.0:8088 ResourceManager对web 服务提供地址。用户可通过该地址在浏览器中查看集群各类信息 yarn.nodemanager.aux-services 通过该配置项，用户可以自定义一些服务，例如Map-Reduce的shuffle功能就是采用这种方式实现的，这样就可以在NodeManager上扩展自己的服务。 基于以上信息。 整理一份配置如下： core-site.xml 123456789101112131415161718&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://0.0.0.0:8020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.hue.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.hue.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;io.compression.codecs&lt;/name&gt; &lt;value&gt;org.apache.hadoop.io.compress.SnappyCodec&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hdfs-site.xml 1234567891011&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:///var/lib/hadoop-hdfs/cache/hdfs/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;!-- CLOUDERA-BUILD: CDH-64745. --&gt; &lt;property&gt; &lt;name&gt;cloudera.erasure_coding.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 接着，我们尝试启动服务。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135[root@25b603a089cc &#x2F;]# sudo &#x2F;usr&#x2F;bin&#x2F;hdfs --config &#x2F;etc&#x2F;hadoop&#x2F;conf namenode2020-06-16 15:55:52,251 INFO namenode.NameNode: STARTUP_MSG:&#x2F;************************************************************STARTUP_MSG: Starting NameNodeSTARTUP_MSG: host &#x3D; 25b603a089cc&#x2F;172.17.0.3STARTUP_MSG: args &#x3D; []STARTUP_MSG: version &#x3D; 3.0.0-cdh6.3.2STARTUP_MSG: classpath &#x3D; &#x2F;etc&#x2F;hadoop&#x2F;conf:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-lang3-3.7.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-beanutils-1.9.4.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;log4j-1.2.17.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-logging-1.1.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;re2j-1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jackson-core-asl-1.9.13.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jettison-1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;guava-11.0.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;javax.activation-api-1.2.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;paranamer-2.8.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jetty-util-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;slf4j-log4j12.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jsr311-api-1.1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;azure-data-lake-store-sdk-2.2.9.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jersey-servlet-1.19.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jaxb-api-2.2.11.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;metrics-core-3.0.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;nimbus-jose-jwt-4.41.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;javax.servlet-api-3.1.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;protobuf-java-2.5.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-math3-3.1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jersey-core-1.19.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-compress-1.18.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;accessors-smart-1.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;woodstox-core-5.0.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jetty-io-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jackson-xc-1.9.13.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jetty-xml-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;curator-recipes-2.12.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jetty-servlet-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-lang-2.6.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;audience-annotations-0.5.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerby-config-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jetty-security-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jetty-server-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerb-util-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;netty-3.10.6.Final.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerb-admin-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jersey-json-1.19.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jackson-databind-2.9.9.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerb-core-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jackson-jaxrs-1.9.13.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerby-asn1-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-configuration2-2.1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerb-crypto-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;httpcore-4.4.6.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;log4j-core-2.8.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;json-smart-2.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;snappy-java-1.1.4.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jackson-mapper-asl-1.9.13-cloudera.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerb-simplekdc-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;logredactor-2.0.7.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;avro.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jackson-annotations-2.9.9.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerb-client-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jersey-server-1.19.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;curator-client-2.12.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-cli-1.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jcip-annotations-1.0-1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerb-server-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerby-xdr-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerb-common-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;htrace-core4-4.1.0-incubating.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;aws-java-sdk-bundle-1.11.271.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerby-util-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jetty-webapp-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;stax2-api-3.1.4.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;slf4j-api-1.7.25.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;log4j-api-2.8.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-io-2.6.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerby-pkix-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;curator-framework-2.12.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;gson-2.2.4.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jsp-api-2.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;xz-1.6.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;zookeeper.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jetty-http-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;wildfly-openssl-1.0.4.Final.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jackson-core-2.9.9.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerb-identity-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jaxb-impl-2.2.3-1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-net-3.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jul-to-slf4j-1.7.25.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;asm-5.0.4.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jsch-0.1.54.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jsr305-3.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-collections-3.2.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;httpclient-4.5.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-codec-1.11.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-format-javadoc.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-auth-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-jackson.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-azure-datalake.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-format.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-aws-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-azure-datalake-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-protobuf.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-annotations-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-auth.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-aws.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-azure.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-scala_2.11.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-nfs.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-annotations.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-pig.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-pig-bundle.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-common.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-hadoop-bundle.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-kms.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-kms-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-cascading.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-common-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-generator.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-azure-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-hadoop.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-common.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-common-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-avro.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-common-3.0.0-cdh6.3.2-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-format-sources.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-column.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-thrift.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-encoding.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-nfs-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-cascading3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-lang3-3.7.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-beanutils-1.9.4.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;log4j-1.2.17.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-logging-1.1.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;re2j-1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jackson-core-asl-1.9.13.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jettison-1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;guava-11.0.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;javax.activation-api-1.2.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;paranamer-2.8.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jetty-util-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jsr311-api-1.1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jersey-servlet-1.19.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jaxb-api-2.2.11.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;nimbus-jose-jwt-4.41.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;javax.servlet-api-3.1.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;protobuf-java-2.5.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;okio-1.6.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-math3-3.1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jersey-core-1.19.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-compress-1.18.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;accessors-smart-1.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;woodstox-core-5.0.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jetty-io-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jackson-xc-1.9.13.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jetty-xml-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;curator-recipes-2.12.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jetty-servlet-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-lang-2.6.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;json-simple-1.1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;avro-1.8.2-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;audience-annotations-0.5.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerby-config-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jetty-security-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jetty-server-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerb-util-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;zookeeper-3.4.5-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;netty-3.10.6.Final.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerb-admin-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jersey-json-1.19.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jackson-databind-2.9.9.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerb-core-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jackson-jaxrs-1.9.13.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerby-asn1-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-configuration2-2.1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerb-crypto-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;httpcore-4.4.6.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;json-smart-2.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;snappy-java-1.1.4.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jackson-mapper-asl-1.9.13-cloudera.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerb-simplekdc-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jackson-annotations-2.9.9.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerb-client-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jersey-server-1.19.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;curator-client-2.12.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-cli-1.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jcip-annotations-1.0-1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerb-server-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerby-xdr-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerb-common-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;htrace-core4-4.1.0-incubating.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerby-util-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jetty-webapp-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;stax2-api-3.1.4.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-io-2.6.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerby-pkix-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;curator-framework-2.12.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-daemon-1.0.13.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;gson-2.2.4.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;leveldbjni-all-1.8.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;xz-1.6.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jetty-http-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;okhttp-2.7.5.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jetty-util-ajax-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jackson-core-2.9.9.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerb-identity-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jaxb-impl-2.2.3-1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-net-3.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;asm-5.0.4.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jsch-0.1.54.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jsr305-3.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-collections-3.2.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;httpclient-4.5.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-codec-1.11.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-native-client.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-client-3.0.0-cdh6.3.2-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-nfs-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-native-client-3.0.0-cdh6.3.2-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-nfs.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-httpfs-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-client-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-3.0.0-cdh6.3.2-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-native-client-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-client-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-httpfs.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-native-client-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-client.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-examples-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-openstack.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-aliyun.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-core.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-extras-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-uploader.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-openstack-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-distcp.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-hs-plugins-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-sls.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-uploader-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-kafka.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-datajoin-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;netty-buffer-4.1.17.Final.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;netty-common-4.1.17.Final.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-jobclient-3.0.0-cdh6.3.2-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-gridmix-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;jdom-1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-jobclient-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-examples.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-kafka-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;netty-codec-http-4.1.17.Final.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-rumen-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-app.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-extras.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-streaming-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;lz4-java-1.5.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;aliyun-sdk-oss-2.8.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-app-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-datajoin.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;azure-storage-5.4.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-hs.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-nativetask-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-jobclient.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-distcp-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-hs-plugins.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-shuffle-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-aliyun-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-common.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;kafka-clients-2.2.1-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;ojalgo-43.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-shuffle.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-gridmix.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-hs-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-jobclient-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;netty-resolver-4.1.17.Final.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-resourceestimator-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-rumen.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;azure-keyvault-core-0.8.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;netty-codec-4.1.17.Final.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-nativetask.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;netty-transport-4.1.17.Final.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-sls-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-resourceestimator.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-archives.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-archives-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-common-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;netty-handler-4.1.17.Final.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-core-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-archive-logs.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;zstd-jni-1.3.8-1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-streaming.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-archive-logs-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;jackson-jaxrs-json-provider-2.9.9.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;guice-servlet-4.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;jackson-module-jaxb-annotations-2.9.9.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;jersey-client-1.19.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;objenesis-1.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;guice-4.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;jackson-jaxrs-base-2.9.9.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;metrics-core-3.0.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;java-util-1.9.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;fst-2.50.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;javax.inject-1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;geronimo-jcache_1.0_spec-1.0-alpha-1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;json-io-2.5.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;HikariCP-java7-2.4.12.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;bcprov-jdk15on-1.60.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;ehcache-3.3.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;bcpkix-jdk15on-1.60.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;aopalliance-1.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;jersey-guice-1.19.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;mssql-jdbc-6.2.1.jre7.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-applicationhistoryservice-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-registry-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-tests-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-common.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-sharedcachemanager-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-client-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-applicationhistoryservice.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-applications-unmanaged-am-launcher-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-resourcemanager.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-common.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-resourcemanager-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-timeline-pluginstorage.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-common-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-applications-distributedshell.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-router-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-client.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-common-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-web-proxy-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-sharedcachemanager.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-web-proxy.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-router.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-api-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-nodemanager-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-registry.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-applications-distributedshell-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-timeline-pluginstorage-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-api.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-applications-unmanaged-am-launcher.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-nodemanager.jarSTARTUP_MSG: build &#x3D; http:&#x2F;&#x2F;github.com&#x2F;cloudera&#x2F;hadoop -r 9aff20de3b5ecccf3c19d57f71b214fb4d37ee89; compiled by &#39;jenkins&#39; on 2019-11-08T13:49ZSTARTUP_MSG: java &#x3D; 1.8.0_252************************************************************&#x2F;2020-06-16 15:55:52,271 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]2020-06-16 15:55:52,438 INFO namenode.NameNode: createNameNode []2020-06-16 15:55:52,671 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties2020-06-16 15:55:52,908 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).2020-06-16 15:55:52,909 INFO impl.MetricsSystemImpl: NameNode metrics system started2020-06-16 15:55:52,970 INFO namenode.NameNode: fs.defaultFS is hdfs:&#x2F;&#x2F;0.0.0.0:80202020-06-16 15:55:52,977 INFO namenode.NameNode: Clients are to use 0.0.0.0:8020 to access this namenode&#x2F;service.2020-06-16 15:55:53,268 INFO util.JvmPauseMonitor: Starting JVM pause monitor2020-06-16 15:55:53,317 INFO hdfs.DFSUtil: Starting Web-server for hdfs at: http:&#x2F;&#x2F;0.0.0.0:98702020-06-16 15:55:53,363 INFO util.log: Logging initialized @2085ms2020-06-16 15:55:53,573 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.2020-06-16 15:55:53,606 INFO http.HttpRequestLog: Http request log for http.requests.namenode is not defined2020-06-16 15:55:53,629 INFO http.HttpServer2: Added global filter &#39;safety&#39; (class&#x3D;org.apache.hadoop.http.HttpServer2$QuotingInputFilter)2020-06-16 15:55:53,635 INFO http.HttpServer2: Added filter static_user_filter (class&#x3D;org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs2020-06-16 15:55:53,635 INFO http.HttpServer2: Added filter static_user_filter (class&#x3D;org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static2020-06-16 15:55:53,635 INFO http.HttpServer2: Added filter static_user_filter (class&#x3D;org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs2020-06-16 15:55:53,697 INFO http.HttpServer2: Added filter &#39;org.apache.hadoop.hdfs.web.AuthFilter&#39; (class&#x3D;org.apache.hadoop.hdfs.web.AuthFilter)2020-06-16 15:55:53,700 INFO http.HttpServer2: addJerseyResourcePackage: packageName&#x3D;org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec&#x3D;&#x2F;webhdfs&#x2F;v1&#x2F;*2020-06-16 15:55:53,738 INFO http.HttpServer2: Jetty bound to port 98702020-06-16 15:55:53,747 INFO server.Server: jetty-9.3.25.v20180904, build timestamp: 2018-09-04T21:11:46Z, git hash: 3ce520221d0240229c862b122d2b06c12a6257322020-06-16 15:55:53,858 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7bba5817&#123;&#x2F;logs,file:&#x2F;&#x2F;&#x2F;var&#x2F;log&#x2F;hadoop-hdfs&#x2F;,AVAILABLE&#125;2020-06-16 15:55:53,859 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@75437611&#123;&#x2F;static,file:&#x2F;&#x2F;&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;webapps&#x2F;static&#x2F;,AVAILABLE&#125;2020-06-16 15:55:54,026 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@20bd8be5&#123;&#x2F;,file:&#x2F;&#x2F;&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;webapps&#x2F;hdfs&#x2F;,AVAILABLE&#125;&#123;&#x2F;hdfs&#125;2020-06-16 15:55:54,041 INFO server.AbstractConnector: Started ServerConnector@24105dc5&#123;HTTP&#x2F;1.1,[http&#x2F;1.1]&#125;&#123;0.0.0.0:9870&#125;2020-06-16 15:55:54,041 INFO server.Server: Started @2764ms2020-06-16 15:55:54,394 WARN namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!2020-06-16 15:55:54,395 WARN namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!2020-06-16 15:55:54,533 INFO namenode.FSEditLog: Edit logging is async:true2020-06-16 15:55:54,560 INFO namenode.FSNamesystem: KeyProvider: null2020-06-16 15:55:54,563 INFO namenode.FSNamesystem: fsLock is fair: true2020-06-16 15:55:54,564 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false2020-06-16 15:55:54,583 INFO namenode.FSNamesystem: fsOwner &#x3D; root (auth:SIMPLE)2020-06-16 15:55:54,583 INFO namenode.FSNamesystem: supergroup &#x3D; supergroup2020-06-16 15:55:54,583 INFO namenode.FSNamesystem: isPermissionEnabled &#x3D; true2020-06-16 15:55:54,584 INFO namenode.FSNamesystem: HA Enabled: false2020-06-16 15:55:54,683 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling2020-06-16 15:55:54,712 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured&#x3D;1000, counted&#x3D;60, effected&#x3D;10002020-06-16 15:55:54,713 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check&#x3D;true2020-06-16 15:55:54,722 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.0002020-06-16 15:55:54,723 INFO blockmanagement.BlockManager: The block deletion will start around 2020 Jun 16 15:55:542020-06-16 15:55:54,727 INFO util.GSet: Computing capacity for map BlocksMap2020-06-16 15:55:54,727 INFO util.GSet: VM type &#x3D; 64-bit2020-06-16 15:55:54,732 INFO util.GSet: 2.0% max memory 876.5 MB &#x3D; 17.5 MB2020-06-16 15:55:54,732 INFO util.GSet: capacity &#x3D; 2^21 &#x3D; 2097152 entries2020-06-16 15:55:54,760 INFO blockmanagement.BlockManager: dfs.block.access.token.enable &#x3D; false2020-06-16 15:55:54,771 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS2020-06-16 15:55:54,772 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct &#x3D; 0.99900001287460332020-06-16 15:55:54,772 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes &#x3D; 02020-06-16 15:55:54,772 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension &#x3D; 300002020-06-16 15:55:54,772 INFO blockmanagement.BlockManager: defaultReplication &#x3D; 32020-06-16 15:55:54,772 INFO blockmanagement.BlockManager: maxReplication &#x3D; 5122020-06-16 15:55:54,773 INFO blockmanagement.BlockManager: minReplication &#x3D; 12020-06-16 15:55:54,773 INFO blockmanagement.BlockManager: maxReplicationStreams &#x3D; 22020-06-16 15:55:54,773 INFO blockmanagement.BlockManager: redundancyRecheckInterval &#x3D; 3000ms2020-06-16 15:55:54,773 INFO blockmanagement.BlockManager: encryptDataTransfer &#x3D; false2020-06-16 15:55:54,773 INFO blockmanagement.BlockManager: maxNumBlocksToLog &#x3D; 10002020-06-16 15:55:54,841 INFO namenode.FSDirectory: GLOBAL serial map: bits&#x3D;24 maxEntries&#x3D;167772152020-06-16 15:55:54,878 INFO util.GSet: Computing capacity for map INodeMap2020-06-16 15:55:54,878 INFO util.GSet: VM type &#x3D; 64-bit2020-06-16 15:55:54,879 INFO util.GSet: 1.0% max memory 876.5 MB &#x3D; 8.8 MB2020-06-16 15:55:54,879 INFO util.GSet: capacity &#x3D; 2^20 &#x3D; 1048576 entries2020-06-16 15:55:54,880 INFO namenode.FSDirectory: ACLs enabled? false2020-06-16 15:55:54,880 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true2020-06-16 15:55:54,880 INFO namenode.FSDirectory: XAttrs enabled? true2020-06-16 15:55:54,881 INFO namenode.NameNode: Caching file names occurring more than 10 times2020-06-16 15:55:54,891 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: true, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true2020-06-16 15:55:54,903 INFO util.GSet: Computing capacity for map cachedBlocks2020-06-16 15:55:54,903 INFO util.GSet: VM type &#x3D; 64-bit2020-06-16 15:55:54,905 INFO util.GSet: 0.25% max memory 876.5 MB &#x3D; 2.2 MB2020-06-16 15:55:54,905 INFO util.GSet: capacity &#x3D; 2^18 &#x3D; 262144 entries2020-06-16 15:55:54,925 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets &#x3D; 102020-06-16 15:55:54,925 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users &#x3D; 102020-06-16 15:55:54,925 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes &#x3D; 1,5,252020-06-16 15:55:54,938 INFO namenode.FSNamesystem: Retry cache on namenode is enabled2020-06-16 15:55:54,938 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis2020-06-16 15:55:54,943 INFO util.GSet: Computing capacity for map NameNodeRetryCache2020-06-16 15:55:54,944 INFO util.GSet: VM type &#x3D; 64-bit2020-06-16 15:55:54,944 INFO util.GSet: 0.029999999329447746% max memory 876.5 MB &#x3D; 269.3 KB2020-06-16 15:55:54,944 INFO util.GSet: capacity &#x3D; 2^15 &#x3D; 32768 entries2020-06-16 15:55:54,978 INFO common.Storage: Lock on &#x2F;var&#x2F;lib&#x2F;hadoop-hdfs&#x2F;cache&#x2F;hdfs&#x2F;dfs&#x2F;name&#x2F;in_use.lock acquired by nodename 2029@25b603a089cc2020-06-16 15:55:55,028 INFO namenode.FileJournalManager: Recovering unfinalized segments in &#x2F;var&#x2F;lib&#x2F;hadoop-hdfs&#x2F;cache&#x2F;hdfs&#x2F;dfs&#x2F;name&#x2F;current2020-06-16 15:55:55,084 INFO namenode.FileJournalManager: Finalizing edits file &#x2F;var&#x2F;lib&#x2F;hadoop-hdfs&#x2F;cache&#x2F;hdfs&#x2F;dfs&#x2F;name&#x2F;current&#x2F;edits_inprogress_0000000000000000001 -&gt; &#x2F;var&#x2F;lib&#x2F;hadoop-hdfs&#x2F;cache&#x2F;hdfs&#x2F;dfs&#x2F;name&#x2F;current&#x2F;edits_0000000000000000001-00000000000000000012020-06-16 15:55:55,121 INFO namenode.FSImage: Planning to load image: FSImageFile(file&#x3D;&#x2F;var&#x2F;lib&#x2F;hadoop-hdfs&#x2F;cache&#x2F;hdfs&#x2F;dfs&#x2F;name&#x2F;current&#x2F;fsimage_0000000000000000000, cpktTxId&#x3D;0000000000000000000)2020-06-16 15:55:55,282 INFO namenode.FSImageFormatPBINode: Loading 1 INodes.2020-06-16 15:55:55,349 INFO namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.2020-06-16 15:55:55,350 INFO namenode.FSImage: Loaded image for txid 0 from &#x2F;var&#x2F;lib&#x2F;hadoop-hdfs&#x2F;cache&#x2F;hdfs&#x2F;dfs&#x2F;name&#x2F;current&#x2F;fsimage_00000000000000000002020-06-16 15:55:55,350 INFO namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@5674e1f2 expecting start txid #12020-06-16 15:55:55,351 INFO namenode.FSImage: Start loading edits file &#x2F;var&#x2F;lib&#x2F;hadoop-hdfs&#x2F;cache&#x2F;hdfs&#x2F;dfs&#x2F;name&#x2F;current&#x2F;edits_0000000000000000001-0000000000000000001 maxTxnsToRead &#x3D; 92233720368547758072020-06-16 15:55:55,355 INFO namenode.RedundantEditLogInputStream: Fast-forwarding stream &#39;&#x2F;var&#x2F;lib&#x2F;hadoop-hdfs&#x2F;cache&#x2F;hdfs&#x2F;dfs&#x2F;name&#x2F;current&#x2F;edits_0000000000000000001-0000000000000000001&#39; to transaction ID 12020-06-16 15:55:55,390 INFO namenode.FSImage: Edits file &#x2F;var&#x2F;lib&#x2F;hadoop-hdfs&#x2F;cache&#x2F;hdfs&#x2F;dfs&#x2F;name&#x2F;current&#x2F;edits_0000000000000000001-0000000000000000001 of size 1048576 edits # 1 loaded in 0 seconds2020-06-16 15:55:55,390 INFO namenode.FSNamesystem: Need to save fs image? false (staleImage&#x3D;false, haEnabled&#x3D;false, isRollingUpgrade&#x3D;false)2020-06-16 15:55:55,392 INFO namenode.FSEditLog: Starting log segment at 22020-06-16 15:55:55,519 INFO namenode.NameCache: initialized with 0 entries 0 lookups2020-06-16 15:55:55,519 INFO namenode.FSNamesystem: Finished loading FSImage in 569 msecs2020-06-16 15:55:55,917 INFO namenode.NameNode: RPC server is binding to 0.0.0.0:80202020-06-16 15:55:55,939 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler2020-06-16 15:55:55,965 INFO ipc.Server: Starting Socket Reader #1 for port 80202020-06-16 15:55:56,404 INFO namenode.FSNamesystem: Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.2020-06-16 15:55:56,423 INFO namenode.LeaseManager: Number of blocks under construction: 02020-06-16 15:55:56,448 INFO blockmanagement.BlockManager: initializing replication queues2020-06-16 15:55:56,449 INFO hdfs.StateChange: STATE* Leaving safe mode after 0 secs2020-06-16 15:55:56,449 INFO hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes2020-06-16 15:55:56,449 INFO hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks2020-06-16 15:55:56,469 INFO blockmanagement.BlockManager: Total number of blocks &#x3D; 02020-06-16 15:55:56,469 INFO blockmanagement.BlockManager: Number of invalid blocks &#x3D; 02020-06-16 15:55:56,469 INFO blockmanagement.BlockManager: Number of under-replicated blocks &#x3D; 02020-06-16 15:55:56,469 INFO blockmanagement.BlockManager: Number of over-replicated blocks &#x3D; 02020-06-16 15:55:56,470 INFO blockmanagement.BlockManager: Number of blocks being written &#x3D; 02020-06-16 15:55:56,471 INFO hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 22 msec2020-06-16 15:55:56,536 INFO ipc.Server: IPC Server Responder: starting2020-06-16 15:55:56,543 INFO ipc.Server: IPC Server listener on 8020: starting2020-06-16 15:55:56,554 INFO namenode.NameNode: NameNode RPC up at: 0.0.0.0&#x2F;0.0.0.0:80202020-06-16 15:55:56,561 INFO namenode.FSNamesystem: Starting services required for active state2020-06-16 15:55:56,561 INFO namenode.FSDirectory: Initializing quota with 4 thread(s)2020-06-16 15:55:56,581 INFO namenode.FSDirectory: Quota initialization completed in 19 millisecondsname space&#x3D;1storage space&#x3D;0storage types&#x3D;RAM_DISK&#x3D;0, SSD&#x3D;0, DISK&#x3D;0, ARCHIVE&#x3D;02020-06-16 15:55:56,595 INFO blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds^C2020-06-16 15:56:01,491 ERROR namenode.NameNode: RECEIVED SIGNAL 2: SIGINT2020-06-16 15:56:01,496 INFO namenode.NameNode: SHUTDOWN_MSG:&#x2F;************************************************************SHUTDOWN_MSG: Shutting down NameNode at 25b603a089cc&#x2F;172.17.0.3************************************************************&#x2F; 发现namenode启动成功。 12345678[root@25b603a089cc &#x2F;]# netstat -anpActive Internet connections (servers and established)Proto Recv-Q Send-Q Local Address Foreign Address State PID&#x2F;Program nametcp 0 0 0.0.0.0:9870 0.0.0.0:* LISTEN 1872&#x2F;javatcp 0 0 0.0.0.0:8020 0.0.0.0:* LISTEN 1872&#x2F;javatcp 0 0 172.17.0.3:36568 151.101.108.167:443 TIME_WAIT -tcp 0 0 172.17.0.3:43778 202.104.186.227:80 TIME_WAIT -tcp 0 0 172.17.0.3:43774 202.104.186.227:80 TIME_WAIT - 并且看到了8020他的rpc端口已经启动，9870就是web的端口。 接着，我们就可以在浏览器看到服务启动完毕了。 浏览器输入：http://localhost:9870/ 至此，大数据服务namenode容器安装完毕. 接着，我们继续部署我们的datanode服务。为了调试方便，我在同一个容器中部署datanode 1yum install -y hadoop-hdfs-datanode 安装完毕之后，再启动服务 1234567891011121314[root@9ba32201bb25 &#x2F;]# netstat -anpActive Internet connections (servers and established)Proto Recv-Q Send-Q Local Address Foreign Address State PID&#x2F;Program nametcp 0 0 0.0.0.0:8020 0.0.0.0:* LISTEN 917&#x2F;javatcp 0 0 127.0.0.11:39643 0.0.0.0:* LISTEN -tcp 0 0 0.0.0.0:9864 0.0.0.0:* LISTEN 990&#x2F;javatcp 0 0 0.0.0.0:9866 0.0.0.0:* LISTEN 990&#x2F;javatcp 0 0 0.0.0.0:9867 0.0.0.0:* LISTEN 990&#x2F;javatcp 0 0 127.0.0.1:38411 0.0.0.0:* LISTEN 990&#x2F;javatcp 0 0 0.0.0.0:9870 0.0.0.0:* LISTEN 917&#x2F;javatcp 0 0 172.24.0.2:42042 113.96.181.216:80 TIME_WAIT -tcp 0 0 172.24.0.2:8020 172.24.0.2:58574 ESTABLISHED 917&#x2F;javatcp 0 0 172.24.0.2:58574 172.24.0.2:8020 ESTABLISHED 990&#x2F;javaudp 0 0 127.0.0.11:54498 0.0.0.0:* 这里我们可以看到除了8020/9870这2个是namenode的端口之外，其他端口都是datanode节点的端口。 我们在hdfs-site.xml中添加了如下配置 1234&lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;&#x2F;name&gt; &lt;value&gt;file:&#x2F;&#x2F;&#x2F;var&#x2F;lib&#x2F;hadoop-hdfs&#x2F;cache&#x2F;hdfs&#x2F;dfs&#x2F;data&lt;&#x2F;value&gt;&lt;&#x2F;property&gt; 再调整dockerfile。最终如下。 1234567891011121314151617181920212223242526FROM ccinn&#x2F;cdh6:latestLABEL maintainer&#x3D;&quot;Caiwenhui &lt;471113744@qq.com&gt;&quot;USER rootADD support.sh &#x2F;support.shADD conf&#x2F;core-site.xml &#x2F;etc&#x2F;hadoop&#x2F;conf&#x2F;ADD conf&#x2F;hdfs-site.xml &#x2F;etc&#x2F;hadoop&#x2F;conf&#x2F;ADD conf&#x2F;mapred-site.xml &#x2F;etc&#x2F;hadoop&#x2F;conf&#x2F;ADD bin&#x2F;run.sh &#x2F;bin&#x2F;RUN source &#x2F;support.sh;\\ loop_exec &#39;yum install -y hadoop-hdfs-namenode hadoop-hdfs-datanode&#39;RUN mkdir -p var&#x2F;lib&#x2F;hadoop-hdfs&#x2F;cache&#x2F;hdfs&#x2F;dfs&#x2F;name;\\ mkdir -p var&#x2F;lib&#x2F;hadoop-hdfs&#x2F;cache&#x2F;hdfs&#x2F;dfs&#x2F;data;WORKDIR &#x2F;# 9870 namenode&#39;s http# 9864 datanode&#39;s httpEXPOSE 9870 9864CMD [&quot;&#x2F;bin&#x2F;run.sh&quot;] 浏览器输入：http://localhost:9864/ 构建hive镜像 基于基础镜像的hive服务 前面，我们说到hadoop的生态组件是个繁杂的依赖关系，版本搞不对，经常会出现服务起不来的问题。 当我们不想用cdh给我们选择好的组件的时候，记得需要自己梳理好版本关系。 hive下载前的版本依赖说明-查看hadoop和hive版本的关系 由于我们用的cdh6的源，所以我直接yum安装，这里hive的版本为2.1.1 12345678910111213141516171819FROM ccinn/cdh6:latestLABEL maintainer=\"Caiwenhui &lt;471113744@qq.com&gt;\"USER rootADD bin/support.sh /bin/ADD bin/run.sh /bin/# 安装元数据存储服务 postgresRUN source /bin/support.sh;\\ loop_exec 'yum install -y hive hive-metastore postgresql-jdbc' ;\\ ln -s /usr/share/java/postgresql-jdbc.jar /usr/lib/hive/lib/postgresql-jdbc.jarADD conf/hive-site.xml /etc/hive/conf/WORKDIR /CMD [\"/bin/run.sh\"] 这里我们使用postgresql作为我们的元数据存储服务，所以我们安装了一个postgresql-jdbc，并且需要把jar包放在hive服务加载的环境变量下。 构建用于hive镜像的postgres镜像 构建用于hive镜像的postgres镜像 12345678910FROM postgres:9LABEL maintainer=\"Caiwenhui &lt;471113744@qq.com&gt;\"COPY cdh6-hive-postgres /cdh6-hive-postgresCOPY init-hive-db.sh /docker-entrypoint-initdb.d/init-user-db.sh# 因为schema内部用了相对地址加载关联的sql，所以这里的工作目录需要指定为脚本当前目录WORKDIR /cdh6-hive-postgres 整个dockerfile很简单，我们的用户和密码都是使用了了hive。 构建impala镜像 构建impala镜像 123456789101112131415FROM ccinn/cdh6:latestLABEL maintainer=\"Caiwenhui &lt;471113744@qq.com&gt;\"USER rootADD bin/support.sh /bin/ADD bin/run.sh /bin/RUN source /bin/support.sh;\\ loop_exec 'yum install -y impala impala-server impala-shell impala-catalog impala-state-store' ;WORKDIR /CMD [\"/bin/bash\"] 在这里，需要注意的是，我们的impala服务启动是有关联顺序问题的。 1⃣️ impala-state-store 2⃣️ impala-catalog 3⃣️ impala-server(impalad) 到此，我们整套大数据OLAP的体系设施，算是基本完成了。其实到hive已经算ok了。但是大家其实可以看到，我这里到计算引擎并没有使用hiveserver2。这里到hive只是用了metastore。 也因此，这个hive到metastore，目前来说仅仅只是为了服务impala用到。","categories":[{"name":"大数据","slug":"大数据","permalink":"http://blog.crazylaw.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"大数据，Hadoop","slug":"大数据，Hadoop","permalink":"http://blog.crazylaw.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%EF%BC%8CHadoop/"}]},{"title":"【大数据】- Hadoop 基本操作","slug":"大数据/hadoop基本操作","date":"2020-06-12T01:56:40.000Z","updated":"2021-03-20T16:25:01.815Z","comments":true,"path":"2020/06/12/大数据/hadoop基本操作/","link":"","permalink":"http://blog.crazylaw.cn/2020/06/12/%E5%A4%A7%E6%95%B0%E6%8D%AE/hadoop%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/","excerpt":"前言由于之前一直没有整理过hadoop的内容，或者说整理得比较少，所以觉得有必要","text":"前言由于之前一直没有整理过hadoop的内容，或者说整理得比较少，所以觉得有必要 Hadoop 存储 - HDFSHadoop 的存储系统是 HDFS(Hadoop Distributed File System)分布式文件系统，对外部客户端而言，HDFS 就像一个传统的分级文件系统，可以进行创建、删除、移动或重命名文件或文件夹等操作，与 Linux 文件系统类似。 但是，Hadoop HDFS 的架构是基于一组特定的节点构建的，名称节点（NameNode，仅一个)，它在 HDFS 内部提供元数据服务；第二名称节点(Secondary NameNode)，名称节点的帮助节点，主要是为了整合元数据操作(注意不是名称节点的备份)；数据节点(DataNode)，它为 HDFS 提供存储块。由于仅有一个 NameNode，因此这是 HDFS 的一个缺点(单点失败，在 Hadoop2.x 后有较大改善)。 NameNode它是一个通常在 HDFS 架构中单独机器上运行的组件，负责管理文件系统名称空间和控制外部客户机的访问。NameNode 决定是否将文件映射到 DataNode 上的复制块上。对于最常见的 3 个复制块，第一个复制块存储在同一机架的不同节点上，最后一个复制块存储在不同机架的某个节点上。 Secondary NameNode第二名称节点的作用在于为 HDFS 中的名称节点提供一个 Checkpoint，它只是名称节点的一个助手节点，这也是它在社区内被认为是 Checkpoint Node 的原因。 DatabNode数据节点也是一个通常在 HDFS 架构中的单独机器上运行的组件。Hadoop 集群包含一个 NameNode 和大量 DataNode。数据节点通常以机架的形式组织，机架通过一个交换机将所有系统连接起来。 数据节点响应来自 HDFS 客户机的读写请求。它们还响应来自 NameNode 的创建、删除和复制块的命令。名称节点依赖来自每个数据节点的定期心跳（heartbeat）消息。每条消息都包含一个块报告，名称节点可以根据这个报告验证块映射和其他文件系统元数据。如果数据节点不能发送心跳消息，名称节点将采取修复措施，重新复制在该节点上丢失的块。","categories":[{"name":"大数据","slug":"大数据","permalink":"http://blog.crazylaw.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"大数据，Hadoop","slug":"大数据，Hadoop","permalink":"http://blog.crazylaw.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%EF%BC%8CHadoop/"}]},{"title":"【Docker】零碎知识整理","slug":"docker/docker-零碎知识整理","date":"2020-06-10T02:28:30.000Z","updated":"2021-03-20T16:25:01.805Z","comments":true,"path":"2020/06/10/docker/docker-零碎知识整理/","link":"","permalink":"http://blog.crazylaw.cn/2020/06/10/docker/docker-%E9%9B%B6%E7%A2%8E%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86/","excerpt":"前言虽然自己对docker有了一定了解了，但是有一些比较零碎对知识点，一直没整理，所以现在用一篇文章来记录下一些零碎点知识点，方便自己查阅","text":"前言虽然自己对docker有了一定了解了，但是有一些比较零碎对知识点，一直没整理，所以现在用一篇文章来记录下一些零碎点知识点，方便自己查阅 DOCKER 给运行中的容器添加映射端口1234➜ ~ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES8eed84e6b307 golang:1.14.2 \"bash\" 3 weeks ago Up 4 days msource241a499c7061 garethflowers/svn-server:latest \"/usr/bin/svnserve -…\" 5 weeks ago Up 6 days (healthy) 0.0.0.0:3690-&gt;3690/tcp svn 方法1 把现有的容器commit成一个新的images 再基于这个images run一个新的容器，这个时候带上端口映射规则即可 优点就是不需要涉及太多底层的东西，遵循docker的默认提供的API操作即可 方法2修改底层配置文件 1docker inspect `svn` | grep IPAddress 实例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243[ &#123; \"Id\": \"241a499c7061a70ce20216d6e2310d851e8dcc41b4072727a16c752c4463a8a5\", \"Created\": \"2020-05-05T01:40:40.8904321Z\", \"Path\": \"/usr/bin/svnserve\", \"Args\": [ \"--daemon\", \"--foreground\", \"--root\", \"/var/opt/svn\" ], \"State\": &#123; \"Status\": \"running\", \"Running\": true, \"Paused\": false, \"Restarting\": false, \"OOMKilled\": false, \"Dead\": false, \"Pid\": 1893, \"ExitCode\": 0, \"Error\": \"\", \"StartedAt\": \"2020-06-04T01:01:15.928217933Z\", \"FinishedAt\": \"2020-06-04T01:01:13.970497593Z\", \"Health\": &#123; \"Status\": \"healthy\", \"FailingStreak\": 0, \"Log\": [ &#123; \"Start\": \"2020-06-10T03:19:51.4613637Z\", \"End\": \"2020-06-10T03:19:51.5609883Z\", \"ExitCode\": 0, \"Output\": \"tcp 0 0 0.0.0.0:3690 0.0.0.0:* LISTEN \\n\" &#125;, &#123; \"Start\": \"2020-06-10T03:20:21.5325319Z\", \"End\": \"2020-06-10T03:20:21.6352731Z\", \"ExitCode\": 0, \"Output\": \"tcp 0 0 0.0.0.0:3690 0.0.0.0:* LISTEN \\n\" &#125;, &#123; \"Start\": \"2020-06-10T03:20:51.6086611Z\", \"End\": \"2020-06-10T03:20:51.7427852Z\", \"ExitCode\": 0, \"Output\": \"tcp 0 0 0.0.0.0:3690 0.0.0.0:* LISTEN \\n\" &#125;, &#123; \"Start\": \"2020-06-10T03:21:21.7182145Z\", \"End\": \"2020-06-10T03:21:21.8185606Z\", \"ExitCode\": 0, \"Output\": \"tcp 0 0 0.0.0.0:3690 0.0.0.0:* LISTEN \\n\" &#125;, &#123; \"Start\": \"2020-06-10T03:21:51.7906908Z\", \"End\": \"2020-06-10T03:21:51.8788176Z\", \"ExitCode\": 0, \"Output\": \"tcp 0 0 0.0.0.0:3690 0.0.0.0:* LISTEN \\n\" &#125; ] &#125; &#125;, \"Image\": \"sha256:cc28899d5b9077f49f22494074e1197ef4af59d0a1ddca636f57c3b1dc3289d3\", \"ResolvConfPath\": \"/var/lib/docker/containers/241a499c7061a70ce20216d6e2310d851e8dcc41b4072727a16c752c4463a8a5/resolv.conf\", \"HostnamePath\": \"/var/lib/docker/containers/241a499c7061a70ce20216d6e2310d851e8dcc41b4072727a16c752c4463a8a5/hostname\", \"HostsPath\": \"/var/lib/docker/containers/241a499c7061a70ce20216d6e2310d851e8dcc41b4072727a16c752c4463a8a5/hosts\", \"LogPath\": \"/var/lib/docker/containers/241a499c7061a70ce20216d6e2310d851e8dcc41b4072727a16c752c4463a8a5/241a499c7061a70ce20216d6e2310d851e8dcc41b4072727a16c752c4463a8a5-json.log\", \"Name\": \"/svn\", \"RestartCount\": 0, \"Driver\": \"overlay2\", \"Platform\": \"linux\", \"MountLabel\": \"\", \"ProcessLabel\": \"\", \"AppArmorProfile\": \"\", \"ExecIDs\": null, \"HostConfig\": &#123; \"Binds\": null, \"ContainerIDFile\": \"\", \"LogConfig\": &#123; \"Type\": \"json-file\", \"Config\": &#123;&#125; &#125;, \"NetworkMode\": \"default\", \"PortBindings\": &#123; \"3690/tcp\": [ &#123; \"HostIp\": \"\", \"HostPort\": \"3690\" &#125; ] &#125;, \"RestartPolicy\": &#123; \"Name\": \"always\", \"MaximumRetryCount\": 0 &#125;, \"AutoRemove\": false, \"VolumeDriver\": \"\", \"VolumesFrom\": null, \"CapAdd\": null, \"CapDrop\": null, \"Capabilities\": null, \"Dns\": [], \"DnsOptions\": [], \"DnsSearch\": [], \"ExtraHosts\": null, \"GroupAdd\": null, \"IpcMode\": \"private\", \"Cgroup\": \"\", \"Links\": null, \"OomScoreAdj\": 0, \"PidMode\": \"\", \"Privileged\": false, \"PublishAllPorts\": false, \"ReadonlyRootfs\": false, \"SecurityOpt\": null, \"UTSMode\": \"\", \"UsernsMode\": \"\", \"ShmSize\": 67108864, \"Runtime\": \"runc\", \"ConsoleSize\": [ 0, 0 ], \"Isolation\": \"\", \"CpuShares\": 0, \"Memory\": 0, \"NanoCpus\": 0, \"CgroupParent\": \"\", \"BlkioWeight\": 0, \"BlkioWeightDevice\": [], \"BlkioDeviceReadBps\": null, \"BlkioDeviceWriteBps\": null, \"BlkioDeviceReadIOps\": null, \"BlkioDeviceWriteIOps\": null, \"CpuPeriod\": 0, \"CpuQuota\": 0, \"CpuRealtimePeriod\": 0, \"CpuRealtimeRuntime\": 0, \"CpusetCpus\": \"\", \"CpusetMems\": \"\", \"Devices\": [], \"DeviceCgroupRules\": null, \"DeviceRequests\": null, \"KernelMemory\": 0, \"KernelMemoryTCP\": 0, \"MemoryReservation\": 0, \"MemorySwap\": 0, \"MemorySwappiness\": null, \"OomKillDisable\": false, \"PidsLimit\": null, \"Ulimits\": null, \"CpuCount\": 0, \"CpuPercent\": 0, \"IOMaximumIOps\": 0, \"IOMaximumBandwidth\": 0, \"MaskedPaths\": [ \"/proc/asound\", \"/proc/acpi\", \"/proc/kcore\", \"/proc/keys\", \"/proc/latency_stats\", \"/proc/timer_list\", \"/proc/timer_stats\", \"/proc/sched_debug\", \"/proc/scsi\", \"/sys/firmware\" ], \"ReadonlyPaths\": [ \"/proc/bus\", \"/proc/fs\", \"/proc/irq\", \"/proc/sys\", \"/proc/sysrq-trigger\" ] &#125;, \"GraphDriver\": &#123; \"Data\": &#123; \"LowerDir\": \"/var/lib/docker/overlay2/1e550f38444220f2853beac87489c7d717e24ac60cd9c64fe67e5571d01d7aee-init/diff:/var/lib/docker/overlay2/c9d13c35fbce9cce4b87c4c9f8e3a1fa335c495806c9aa28e9c85b01a7425f6f/diff:/var/lib/docker/overlay2/f0d6ab7fcc03bb7f98dd26452fd413e0db8b5b4fdc491fdef13aa8064f6d2b44/diff:/var/lib/docker/overlay2/9b3dba47a78414d42c011724a52ea1b25df7939c1db9746d8582c9c48ce80643/diff\", \"MergedDir\": \"/var/lib/docker/overlay2/1e550f38444220f2853beac87489c7d717e24ac60cd9c64fe67e5571d01d7aee/merged\", \"UpperDir\": \"/var/lib/docker/overlay2/1e550f38444220f2853beac87489c7d717e24ac60cd9c64fe67e5571d01d7aee/diff\", \"WorkDir\": \"/var/lib/docker/overlay2/1e550f38444220f2853beac87489c7d717e24ac60cd9c64fe67e5571d01d7aee/work\" &#125;, \"Name\": \"overlay2\" &#125;, \"Mounts\": [ &#123; \"Type\": \"volume\", \"Name\": \"c3a499500757570063073552298c2c6a2973351692eb91d9e3437944aeb79e6a\", \"Source\": \"/var/lib/docker/volumes/c3a499500757570063073552298c2c6a2973351692eb91d9e3437944aeb79e6a/_data\", \"Destination\": \"/var/opt/svn\", \"Driver\": \"local\", \"Mode\": \"\", \"RW\": true, \"Propagation\": \"\" &#125; ], \"Config\": &#123; \"Hostname\": \"241a499c7061\", \"Domainname\": \"\", \"User\": \"\", \"AttachStdin\": false, \"AttachStdout\": false, \"AttachStderr\": false, \"ExposedPorts\": &#123; \"3690/tcp\": &#123;&#125; &#125;, \"Tty\": false, \"OpenStdin\": false, \"StdinOnce\": false, \"Env\": [ \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\" ], \"Cmd\": [ \"/usr/bin/svnserve\", \"--daemon\", \"--foreground\", \"--root\", \"/var/opt/svn\" ], \"Healthcheck\": &#123; \"Test\": [ \"CMD-SHELL\", \"netstat -ln | grep 3690 || exit 1\" ] &#125;, \"Image\": \"garethflowers/svn-server:latest\", \"Volumes\": &#123; \"/var/opt/svn\": &#123;&#125; &#125;, \"WorkingDir\": \"/var/opt/svn\", \"Entrypoint\": null, \"OnBuild\": null, \"Labels\": &#123; \"org.label-schema.build-date\": \"2020-02-13T21:46:23Z\", \"org.label-schema.description\": \"SVN Server\", \"org.label-schema.docker.cmd\": \"docker run --detach --publish 3690:3690 --volume :/var/opt/svn garethflowers/svn-server\", \"org.label-schema.name\": \"svn-server\", \"org.label-schema.schema-version\": \"1.0\", \"org.label-schema.url\": \"https://subversion.apache.org\", \"org.label-schema.vcs-ref\": \"d622ac5\", \"org.label-schema.vcs-url\": \"https://github.com/garethflowers/docker-svn-server\", \"org.label-schema.vendor\": \"garethflowers\", \"org.label-schema.version\": \"1.3.3\" &#125; ... 我们关注一下，这个容器的id。 如果是linux系统下的，我们可以这样子操作。 1234cd /var/lib/docker/241a499c7061a70ce20216d6e2310d851e8dcc41b4072727a16c752c4463a8a5 #这里是CONTAINER IDvi hostconfig.json如果之前没有端口映射, 应该有这样的一段: “PortBindings”:{“3690/tcp”:[{“HostIp”:””,”HostPort”:”3690”}]} 格式化后“PortBindings”: { “3690/tcp”: [ { “HostIp”: “”, “HostPort”: “3690” } ]}, 12增加一个映射, 这样写: “PortBindings”:{“3690/tcp”:[{“HostIp”:””,”HostPort”:”3690”}],”3306/tcp”:[{“HostIp”:””,”HostPort”:”3307”}]} 12345前一个数字是容器端口, 后一个是宿主机端口.而修改现有端口映射更简单, 把端口号改掉就行.记得要重启docker服务，否则配置会被覆盖回去 如果是再mac系统下操作的话，由于docker for mac的本质是通过一个小型虚拟机操作的，所以需要先进入到虚拟机中 docker for mac 的内容默认在 ~/Library/Containers/com.docker.docker/ 我们找到 ~/Library/Containers/com.docker.docker/Data/vms/0/tty 通过 screen tty 进入到虚拟机，这个时候你就可以看到/var/lib/docker/241a499c7061a70ce20216d6e2310d851e8dcc41b4072727a16c752c4463a8a5 这里路径了 退出到时候记得用control+a k到组合方式退出 123docker-desktop:~# cat &#x2F;var&#x2F;lib&#x2F;docker&#x2F;containers&#x2F;241a499c7061a70ce20216d6e2310d851e8dcc41b4072727a16c752c4463a8a5&#x2F;hostconfig.json&#123;&quot;Binds&quot;:null,&quot;ContainerIDFile&quot;:&quot;&quot;,&quot;LogConfig&quot;:&#123;&quot;Type&quot;:&quot;json-file&quot;,&quot;Config&quot;:&#123;&#125;&#125;,&quot;NetworkMode&quot;:&quot;default&quot;,&quot;PortBindings&quot;:&#123;&quot;3690&#x2F;tcp&quot;:[&#123;&quot;HostIp&quot;:&quot;&quot;,&quot;HostPort&quot;:&quot;3690&quot;&#125;]&#125;,&quot;RestartPolicy&quot;:&#123;&quot;Name&quot;:&quot;always&quot;,&quot;MaximumRetryCount&quot;:0&#125;,&quot;AutoRemove&quot;:false,&quot;VolumeDriver&quot;:&quot;&quot;,&quot;VolumesFrom&quot;:null,&quot;CapAdd&quot;:null,&quot;CapDrop&quot;:null,&quot;Capabilities&quot;:null,&quot;Dns&quot;:[],&quot;DnsOptions&quot;:[],&quot;DnsSearch&quot;:[],&quot;ExtraHosts&quot;:null,&quot;GroupAdd&quot;:null,&quot;IpcMode&quot;:&quot;private&quot;,&quot;Cgroup&quot;:&quot;&quot;,&quot;Links&quot;:null,&quot;OomScoreAdj&quot;:0,&quot;PidMode&quot;:&quot;&quot;,&quot;Privileged&quot;:false,&quot;PublishAllPorts&quot;:false,&quot;ReadonlyRootfs&quot;:false,&quot;SecurityOpt&quot;:null,&quot;UTSMode&quot;:&quot;&quot;,&quot;UsernsMode&quot;:&quot;&quot;,&quot;ShmSize&quot;:67108864,&quot;Runtime&quot;:&quot;runc&quot;,&quot;ConsoleSize&quot;:[0,0],&quot;Isolation&quot;:&quot;&quot;,&quot;CpuShares&quot;:0,&quot;Memory&quot;:0,&quot;NanoCpus&quot;:0,&quot;CgroupParent&quot;:&quot;&quot;,&quot;BlkioWeight&quot;:0,&quot;BlkioWeightDevice&quot;:[],&quot;BlkioDeviceReadBps&quot;:null,&quot;BlkioDeviceWriteBps&quot;:null,&quot;BlkioDeviceReadIOps&quot;:null,&quot;BlkioDeviceWriteIOps&quot;:null,&quot;CpuPeriod&quot;:0,&quot;CpuQuota&quot;:0,&quot;CpuRealtimePeriod&quot;:0,&quot;CpuRealtimeRuntime&quot;:0,&quot;CpusetCpus&quot;:&quot;&quot;,&quot;CpusetMems&quot;:&quot;&quot;,&quot;Devices&quot;:[],&quot;DeviceCgroupRules&quot;:null,&quot;DeviceRequests&quot;:null,&quot;KernelMemory&quot;:0,&quot;KernelMemoryTCP&quot;:0,&quot;MemoryReservation&quot;:0,&quot;MemorySwap&quot;:0,&quot;MemorySwappiness&quot;:null,&quot;OomKillDisable&quot;:false,&quot;PidsLimit&quot;:null,&quot;Ulimits&quot;:null,&quot;CpuCount&quot;:0,&quot;CpuPercent&quot;:0,&quot;IOMaximumIOps&quot;:0,&quot;IOMaximumBandwidth&quot;:0,&quot;MaskedPaths&quot;:[&quot;&#x2F;proc&#x2F;asound&quot;,&quot;&#x2F;proc&#x2F;acpi&quot;,&quot;&#x2F;proc&#x2F;kcore&quot;,&quot;&#x2F;proc&#x2F;keys&quot;,&quot;&#x2F;proc&#x2F;latency_stats&quot;,&quot;&#x2F;proc&#x2F;timer_list&quot;,&quot;&#x2F;proc&#x2F;timer_stats&quot;,&quot;&#x2F;proc&#x2F;sched_debug&quot;,&quot;&#x2F;proc&#x2F;scsi&quot;,&quot;&#x2F;sys&#x2F;firmware&quot;],&quot;ReadonlyPaths&quot;:[&quot;&#x2F;proc&#x2F;bus&quot;,&quot;&#x2F;proc&#x2F;fs&quot;,&quot;&#x2F;proc&#x2F;irq&quot;,&quot;&#x2F;proc&#x2F;sys&quot;,&quot;&#x2F;proc&#x2F;sysrq-trigger&quot;]&#125;","categories":[{"name":"Docker","slug":"Docker","permalink":"http://blog.crazylaw.cn/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://blog.crazylaw.cn/tags/Docker/"}]},{"title":"【Golang】- sync.map","slug":"Golang/sync-map","date":"2020-06-02T05:59:51.000Z","updated":"2022-03-12T06:45:42.407Z","comments":true,"path":"2020/06/02/Golang/sync-map/","link":"","permalink":"http://blog.crazylaw.cn/2020/06/02/Golang/sync-map/","excerpt":"前言由于要造一些轮子，例如 像 laravel 一样的 Event组件，我参考了网上的几个库 go-trigger go-events 这 2 个库，大同小异，只有一小部分差别，新打组件将会在这 2 个库的基础上再封装 这 2 个库都是比较早期的库，所以在实现上用到了 map，但是由于考虑到 map 的线程安全性问题，所以他们都使用了 go1.9 之前实现的方式，就是在结构体嵌入一个读写锁来避免线程安全问题。 在 Go 1.6 之前， 内置的 map 类型是部分 goroutine 安全的，并发的读没有问题，并发的写可能有问题。自 go 1.6 之后， 并发地读写 map 会报错，这在一些知名的开源库中都存在这个问题，所以 go 1.9 之前的解决方案是额外绑定一个锁，封装成一个新的 struct 或者单独使用锁都可以。 本文带你深入到 sync.Map 的具体实现中，看看为了增加一个功能，代码是如何变的复杂的,以及作者在实现 sync.Map 的一些思想。","text":"前言由于要造一些轮子，例如 像 laravel 一样的 Event组件，我参考了网上的几个库 go-trigger go-events 这 2 个库，大同小异，只有一小部分差别，新打组件将会在这 2 个库的基础上再封装 这 2 个库都是比较早期的库，所以在实现上用到了 map，但是由于考虑到 map 的线程安全性问题，所以他们都使用了 go1.9 之前实现的方式，就是在结构体嵌入一个读写锁来避免线程安全问题。 在 Go 1.6 之前， 内置的 map 类型是部分 goroutine 安全的，并发的读没有问题，并发的写可能有问题。自 go 1.6 之后， 并发地读写 map 会报错，这在一些知名的开源库中都存在这个问题，所以 go 1.9 之前的解决方案是额外绑定一个锁，封装成一个新的 struct 或者单独使用锁都可以。 本文带你深入到 sync.Map 的具体实现中，看看为了增加一个功能，代码是如何变的复杂的,以及作者在实现 sync.Map 的一些思想。 有并发问题的 map官方的 faq 已经提到内建的 map 不是线程(goroutine)安全的。 首先，让我们看一段并发读写的代码,下列程序中一个 goroutine 一直读，一个 goroutine 一只写同一个键值，即即使读写的键不相同，而且 map 也没有”扩容”等操作，代码还是会报错。 123456789101112131415package mainfunc main() &#123; m := make(map[int]int) go func() &#123; for &#123; _ = m[1] &#125; &#125;() go func() &#123; for &#123; m[2] = 2 &#125; &#125;() select &#123;&#125;&#125; 报错信息为：fatal error: concurrent map read and map write 在开发 msource 组件的时候，同样遇到了这个问题 如果你查看 Go 的源代码: hashmap_fast.go#L118,会看到读的时候会检查 hashWriting 标志， 如果有这个标志，就会报并发错误。 写的时候会设置这个标志: hashmap.go#L542 1h.flags |= hashWriting hashmap.go#L628 设置完之后会取消这个标记。 当然，代码中还有好几处并发读写的检查， 比如写的时候也会检查是不是有并发的写，删除键的时候类似写，遍历的时候并发读写问题等。 有时候，map 的并发问题不是那么容易被发现, 你可以利用 -race 参数来检查。 Go 1.9 之前的解决方案但是，很多时候，我们会并发地使用 map 对象，尤其是在一定规模的项目中，map 总会保存 goroutine 共享的数据。在 Go 官方 blog 的 Go maps in action 一文中，提供了一种简便的解决方案。 1234var counter = struct&#123; sync.RWMutex m map[string]int&#125;&#123;m: make(map[string]int)&#125; 它使用嵌入 struct 为 map 增加一个读写锁。 读数据的时候很方便的加锁： 1234counter.RLock()n := counter.m[\"some_key\"]counter.RUnlock()fmt.Println(\"some_key:\", n) 写数据的时候: 123counter.Lock()counter.m[\"some_key\"]++counter.Unlock() sync.Map可以说，上面的解决方案相当简洁，并且利用读写锁而不是 Mutex 可以进一步减少读写的时候因为锁带来的性能。 但是，它在一些场景下也有问题，如果熟悉 Java 的同学，可以对比一下 java 的 ConcurrentHashMap 的实现，在 map 的数据非常大的情况下，一把锁会导致大并发的客户端共争一把锁，Java 的解决方案是 shard, 内部使用多个锁，每个区间共享一把锁，这样减少了数据共享一把锁带来的性能影响，orcaman 提供了这个思路的一个实现： concurrent-map，他也询问了 Go 相关的开发人员是否在 Go 中也实现这种方案，由于实现的复杂性，答案是 Yes, we considered it.,但是除非有特别的性能提升和应用场景，否则没有进一步的开发消息。 那么，在 Go 1.9 中 sync.Map 是怎么实现的呢？它是如何解决并发提升性能的呢？ sync.Map 的实现有几个优化点，这里先列出来，我们后面慢慢分析。 空间换时间。 通过冗余的两个数据结构(read、dirty),实现加锁对性能的影响。使用只读数据(read)，避免读写冲突。动态调整，miss 次数多了之后，将 dirty 数据提升为 read。double-checking。延迟删除。 删除一个键值只是打标记，只有在提升 dirty 的时候才清理删除的数据。优先从 read 读取、更新、删除，因为对 read 的读取不需要锁。下面我们介绍 sync.Map 的重点代码，以便理解它的实现思想。 首先，我们看一下 sync.Map 的数据结构： 123456789101112131415type Map struct &#123; // 当涉及到dirty数据的操作的时候，需要使用这个锁 mu Mutex // 一个只读的数据结构，因为只读，所以不会有读写冲突。 // 所以从这个数据中读取总是安全的。 // 实际上，实际也会更新这个数据的entries,如果entry是未删除的(unexpunged), 并不需要加锁。如果entry已经被删除了，需要加锁，以便更新dirty数据。 read atomic.Value // readOnly // dirty数据包含当前的map包含的entries,它包含最新的entries(包括read中未删除的数据,虽有冗余，但是提升dirty字段为read的时候非常快，不用一个一个的复制，而是直接将这个数据结构作为read字段的一部分),有些数据还可能没有移动到read字段中。 // 对于dirty的操作需要加锁，因为对它的操作可能会有读写竞争。 // 当dirty为空的时候， 比如初始化或者刚提升完，下一次的写操作会复制read字段中未删除的数据到这个数据中。 dirty map[interface&#123;&#125;]*entry // 当从Map中读取entry的时候，如果read中不包含这个entry,会尝试从dirty中读取，这个时候会将misses加一， // 当misses累积到 dirty的长度的时候， 就会将dirty提升为read,避免从dirty中miss太多次。因为操作dirty需要加锁。 misses int&#125; 它的数据结构很简单，值包含四个字段：read、mu、dirty、misses。 它使用了冗余的数据结构 read、dirty。dirty 中会包含 read 中未删除的 entries，新增加的 entries 会加入到 dirty 中。 read 的数据结构是： 1234type readOnly struct &#123; m map[interface&#123;&#125;]*entry amended bool // 如果Map.dirty有些数据不在中的时候，这个值为true&#125; amended 指明 Map.dirty 中有 readOnly.m 未包含的数据，所以如果从 Map.read 找不到数据的话，还要进一步到 Map.dirty 中查找。 对 Map.read 的修改是通过原子操作进行的。 虽然 read 和 dirty 有冗余数据，但这些数据是通过指针指向同一个数据，所以尽管 Map 的 value 会很大，但是冗余的空间占用还是有限的。 readOnly.m 和 Map.dirty 存储的值类型是\\*entry,它包含一个指针 p, 指向用户存储的 value 值。 123type entry struct &#123; p unsafe.Pointer // *interface&#123;&#125;&#125; p 有三种值： nil: entry 已被删除了，并且 m.dirty 为 nil expunged: entry 已被删除了，并且 m.dirty 不为 nil，而且这个 entry 不存在于 m.dirty 中 其它： entry 是一个正常的值 以上是 sync.Map 的数据结构，下面我们重点看看 Load、Store、Delete、Range 这四个方法，其它辅助方法可以参考这四个方法来理解。 Load加载方法，也就是提供一个键 key,查找对应的值 value,如果不存在，通过 ok 反映： 12345678910111213141516171819202122232425func (m *Map) Load(key interface&#123;&#125;) (value interface&#123;&#125;, ok bool) &#123; // 1.首先从m.read中得到只读readOnly,从它的map中查找，不需要加锁 read, _ := m.read.Load().(readOnly) e, ok := read.m[key] // 2. 如果没找到，并且m.dirty中有新数据，需要从m.dirty查找，这个时候需要加锁 if !ok &amp;&amp; read.amended &#123; m.mu.Lock() // 双检查，避免加锁的时候m.dirty提升为m.read,这个时候m.read可能被替换了。 read, _ = m.read.Load().(readOnly) e, ok = read.m[key] // 如果m.read中还是不存在，并且m.dirty中有新数据 if !ok &amp;&amp; read.amended &#123; // 从m.dirty查找 e, ok = m.dirty[key] // 不管m.dirty中存不存在，都将misses计数加一 // missLocked()中满足条件后就会提升m.dirty m.missLocked() &#125; m.mu.Unlock() &#125; if !ok &#123; return nil, false &#125; return e.load()&#125; 这里有两个值的关注的地方。一个是首先从 m.read 中加载，不存在的情况下，并且 m.dirty 中有新数据，加锁，然后从 m.dirty 中加载。 二是这里使用了双检查的处理，因为在下面的两个语句中，这两行语句并不是一个原子操作。 12if !ok &amp;&amp; read.amended &#123; m.mu.Lock() 虽然第一句执行的时候条件满足，但是在加锁之前，m.dirty 可能被提升为 m.read,所以加锁后还得再检查 m.read，后续的方法中都使用了这个方法。 双检查的技术 Java 程序员非常熟悉了，单例模式的实现之一就是利用双检查的技术。 可以看到，如果我们查询的键值正好存在于 m.read 中，无须加锁，直接返回，理论上性能优异。即使不存在于 m.read 中，经过 miss 几次之后，m.dirty 会被提升为 m.read，又会从 m.read 中查找。所以对于更新／增加较少，加载存在的 key 很多的 case,性能基本和无锁的 map 类似。 下面看看 m.dirty 是如何被提升的。missLocked方法中可能会将 m.dirty 提升。 123456789func (m *Map) missLocked() &#123; m.misses++ if m.misses &lt; len(m.dirty) &#123; return &#125; m.read.Store(readOnly&#123;m: m.dirty&#125;) m.dirty = nil m.misses = 0&#125; Store这个方法是更新或者新增一个 entry。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849func (m *Map) Store(key, value interface&#123;&#125;) &#123; // 如果m.read存在这个键，并且这个entry没有被标记删除，尝试直接存储。 // 因为m.dirty也指向这个entry,所以m.dirty也保持最新的entry。 read, _ := m.read.Load().(readOnly) if e, ok := read.m[key]; ok &amp;&amp; e.tryStore(&amp;value) &#123; return &#125; // 如果`m.read`不存在或者已经被标记删除 m.mu.Lock() read, _ = m.read.Load().(readOnly) if e, ok := read.m[key]; ok &#123; if e.unexpungeLocked() &#123; //标记成未被删除 m.dirty[key] = e //m.dirty中不存在这个键，所以加入m.dirty &#125; e.storeLocked(&amp;value) //更新 &#125; else if e, ok := m.dirty[key]; ok &#123; // m.dirty存在这个键，更新 e.storeLocked(&amp;value) &#125; else &#123; //新键值 if !read.amended &#123; //m.dirty中没有新的数据，往m.dirty中增加第一个新键 m.dirtyLocked() //从m.read中复制未删除的数据 m.read.Store(readOnly&#123;m: read.m, amended: true&#125;) &#125; m.dirty[key] = newEntry(value) //将这个entry加入到m.dirty中 &#125; m.mu.Unlock()&#125;func (m *Map) dirtyLocked() &#123; if m.dirty != nil &#123; return &#125; read, _ := m.read.Load().(readOnly) m.dirty = make(map[interface&#123;&#125;]*entry, len(read.m)) for k, e := range read.m &#123; if !e.tryExpungeLocked() &#123; m.dirty[k] = e &#125; &#125;&#125;func (e *entry) tryExpungeLocked() (isExpunged bool) &#123; p := atomic.LoadPointer(&amp;e.p) for p == nil &#123; // 将已经删除标记为nil的数据标记为expunged if atomic.CompareAndSwapPointer(&amp;e.p, nil, expunged) &#123; return true &#125; p = atomic.LoadPointer(&amp;e.p) &#125; return p == expunged&#125; 你可以看到，以上操作都是先从操作 m.read 开始的，不满足条件再加锁，然后操作 m.dirty。 Store 可能会在某种情况下(初始化或者 m.dirty 刚被提升后)从 m.read 中复制数据，如果这个时候 m.read 中数据量非常大，可能会影响性能。 Delete删除一个键值。 12345678910111213141516func (m *Map) Delete(key interface&#123;&#125;) &#123; read, _ := m.read.Load().(readOnly) e, ok := read.m[key] if !ok &amp;&amp; read.amended &#123; m.mu.Lock() read, _ = m.read.Load().(readOnly) e, ok = read.m[key] if !ok &amp;&amp; read.amended &#123; delete(m.dirty, key) &#125; m.mu.Unlock() &#125; if ok &#123; e.delete() &#125;&#125; 同样，删除操作还是从 m.read 中开始， 如果这个 entry 不存在于 m.read 中，并且 m.dirty 中有新数据，则加锁尝试从 m.dirty 中删除。 注意，还是要双检查的。 从 m.dirty 中直接删除即可，就当它没存在过，但是如果是从 m.read 中删除，并不会直接删除，而是打标记： 12345678910111213func (e *entry) delete() (hadValue bool) &#123; for &#123; p := atomic.LoadPointer(&amp;e.p) // 已标记为删除 if p == nil || p == expunged &#123; return false &#125; // 原子操作，e.p标记为nil if atomic.CompareAndSwapPointer(&amp;e.p, p, nil) &#123; return true &#125; &#125;&#125; Range因为for ... range map是内建的语言特性，所以没有办法使用for range遍历 sync.Map, 但是可以使用它的Range方法，通过回调的方式遍历。 1234567891011121314151617181920212223242526func (m *Map) Range(f func(key, value interface&#123;&#125;) bool) &#123; read, _ := m.read.Load().(readOnly) // 如果m.dirty中有新数据，则提升m.dirty,然后在遍历 if read.amended &#123; //提升m.dirty m.mu.Lock() read, _ = m.read.Load().(readOnly) //双检查 if read.amended &#123; read = readOnly&#123;m: m.dirty&#125; m.read.Store(read) m.dirty = nil m.misses = 0 &#125; m.mu.Unlock() &#125; // 遍历, for range是安全的 for k, e := range read.m &#123; v, ok := e.load() if !ok &#123; continue &#125; if !f(k, v) &#123; break &#125; &#125;&#125; Range 方法调用前可能会做一个 m.dirty 的提升，不过提升 m.dirty 不是一个耗时的操作。 sync.Map 的性能Go 1.9 源代码中提供了性能的测试： map_bench_test.go、map_reference_test.go 我也基于这些代码修改了一下，得到下面的测试数据，相比较以前的解决方案，性能多少回有些提升，如果你特别关注性能，可以考虑 sync.Map。 12345678910111213141516BenchmarkHitAll&#x2F;*sync.RWMutexMap-4 20000000 83.8 ns&#x2F;opBenchmarkHitAll&#x2F;*sync.Map-4 30000000 59.9 ns&#x2F;opBenchmarkHitAll_WithoutPrompting&#x2F;*sync.RWMutexMap-4 20000000 96.9 ns&#x2F;opBenchmarkHitAll_WithoutPrompting&#x2F;*sync.Map-4 20000000 64.1 ns&#x2F;opBenchmarkHitNone&#x2F;*sync.RWMutexMap-4 20000000 79.1 ns&#x2F;opBenchmarkHitNone&#x2F;*sync.Map-4 30000000 43.3 ns&#x2F;opBenchmarkHit_WithoutPrompting&#x2F;*sync.RWMutexMap-4 20000000 81.5 ns&#x2F;opBenchmarkHit_WithoutPrompting&#x2F;*sync.Map-4 30000000 44.0 ns&#x2F;opBenchmarkUpdate&#x2F;*sync.RWMutexMap-4 5000000 328 ns&#x2F;opBenchmarkUpdate&#x2F;*sync.Map-4 10000000 146 ns&#x2F;opBenchmarkUpdate_WithoutPrompting&#x2F;*sync.RWMutexMap-4 5000000 336 ns&#x2F;opBenchmarkUpdate_WithoutPrompting&#x2F;*sync.Map-4 5000000 324 ns&#x2F;opBenchmarkDelete&#x2F;*sync.RWMutexMap-4 10000000 155 ns&#x2F;opBenchmarkDelete&#x2F;*sync.Map-4 30000000 55.0 ns&#x2F;opBenchmarkDelete_WithoutPrompting&#x2F;*sync.RWMutexMap-4 10000000 173 ns&#x2F;opBenchmarkDelete_WithoutPrompting&#x2F;*sync.Map-4 10000000 其它sync.Map 没有 Len 方法，并且目前没有迹象要加上 (issue#20680),所以如果想得到当前 Map 中有效的 entries 的数量，需要使用 Range 方法遍历一次， 比较 X 疼。 LoadOrStore 方法如果提供的 key 存在，则返回已存在的值(Load)，否则保存提供的键值(Store)。","categories":[{"name":"Golang","slug":"Golang","permalink":"http://blog.crazylaw.cn/categories/Golang/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://blog.crazylaw.cn/tags/Golang/"}]},{"title":"【Redis】- 事务和Lua脚本","slug":"Redis/redis事务和lua","date":"2020-06-02T02:46:51.000Z","updated":"2021-03-20T16:25:01.800Z","comments":true,"path":"2020/06/02/Redis/redis事务和lua/","link":"","permalink":"http://blog.crazylaw.cn/2020/06/02/Redis/redis%E4%BA%8B%E5%8A%A1%E5%92%8Clua/","excerpt":"前言Redis 我们用的蛮多的了，但是一直也没有整理什么资料，刚好今天要处理一下，顺便一下相关的事务和 lua 脚本的内容，由于管道(pipeline)不在这次原子性问题当中，所以我们就不加进来比较说明了。","text":"前言Redis 我们用的蛮多的了，但是一直也没有整理什么资料，刚好今天要处理一下，顺便一下相关的事务和 lua 脚本的内容，由于管道(pipeline)不在这次原子性问题当中，所以我们就不加进来比较说明了。 说明Redis 的原子性问题，在一直高并发的场景下，是一个需要重视的问题，否则原子性不满足将会导致业务逻辑出现问题。 在这里，我们常用解决原子性问题的一般都是用 redis 自带的命令，例如 brpoplpush 之类的命令 但是我们可能需要更多的组合成一个原子性操作 因此，这里会有 2 种选择，分别如下： 事务 lua 脚本 事务redis 事务提供了一种“将多个命令打包， 然后一次性、按顺序地执行”的机制， 并且事务在执行的期间不会主动中断 我们可以通过 MULTI 命令开启一个事务，类似于 mysql 的 BEGIN TRANSACTION 语句 在该语句之后执行的命令都将被视为事务之内的操作 最后我们可以通过执行 EXEC/DISCARD 命令来提交/回滚该事务内的所有操作 这两个 Redis 命令可被视为等同于关系型数据库中的 COMMIT/ROLLBACK 语句 服务器在执行完事务中的所有命令之后， 才会继续处理其他客户端的其他命令 被执行的命令要么全部都被执行，要么一个也不执行，并且事务执行过程中不会被其他工作打断 一个 redis 事务从开始到执行会经历以下三个阶段： 开始事务 – MULTImulti 命令让客户端从非事务状态切换到事务状态 命令入队如果客户端处于非事务状态下，那么所有发送给服务端的命令都会立即被服务器执行，而如果客户端处于事务状态下，那么所有命令都还不会立即执行，而是被发送到一个事务队列中，返回 QUEUED，表示入队成功 事务队列是一个数组， 每个数组项是都包含三个属性 cmd – 要执行的命令 argv – 命令的参数 argc – 参数个数 例如，我们执行以下两个命令： 1234127.0.0.1:6379&gt; SET msg &quot;hello caiwenhui&quot;QUEUED127.0.0.1:6379&gt; GET msgQUEUED redis server 将创建以下事务队列： index cmd argv argc 0 SET [“msg”, “hello caiwenhui”] 2 1 GET [“msg”] 1 执行事务如果客户端正处于事务状态， 那么当 EXEC 命令执行时， 服务器根据客户端所保存的事务队列， 以先进先出（FIFO）的方式执行事务队列中的命令 然后将执行命令所得的结果以 FIFO 的顺序保存到一个回复队列中 例如，当我们执行上述两个命令后执行 EXEC 命令，将会创建如下回复队列： index 类型 内容 0 status code reply OK 1 bulk reply “hello moto” 123456789127.0.0.1:6379&gt; multiOK127.0.0.1:6379&gt; SET msg &quot;hello caiwenhi&quot;QUEUED127.0.0.1:6379&gt; GET msgQUEUED127.0.0.1:6379&gt; EXEC1) OK1) hello caiwenhui WATCHWatch 命令用于监视一个(或多个) key ，如果在事务执行之前这个(或这些) key 被其他命令所改动，那么事务将被打断 WATCH 命令只能在客户端进入事务状态之前执行， 在事务状态下发送 WATCH 命令会引发错误 redis 中保存了一个 watched_keys 字典，字典的键是这个数据库被监视的键，而字典的值则是一个链表，链表中保存了所有监视这个键的客户端 每当一个客户端执行 WATCH 命令，对应的 key 指向的链表中就会增加该客户端的节点 图意味着，key1 正在被 client1 和 client2 两个客户端监视着，key2 被 client3 监视着，key3 被 client4 监视着 一旦对数据库键空间进行的修改成功执行，multi.c 的 touchWatchedKey 函数都会被调用，他的工作就是遍历上述字典中该 key 所对应的整个链表的所有节点，打开每一个 WATCH 该 key 的 client 的 REDIS_DIRTY_CAS 选项 当客户端发送 EXEC 命令触发事务执行时，服务器会对客户端状态进行检查，如果客户端的 REDIS_DIRTY_CAS 选项已经被打开，那么说明被客户端监视的键至少有一个已经被修改，事务安全性已经被破坏，则服务端直接向客户端返回空回复，表示事务执行失败 当一个客户端结束它的事务时，无论事务是成功执行，还是失败， watched_keys 字典中和这个客户端相关的资料都会被清除 redis 事务的特性 如果在执行 exec 之前事务中断了，那么所有的命令都不会执行 如果某个命令语法错误，不仅会导致该命令入队失败，整个事务都将无法执行 如果执行了 exec 命令之后，那么所有的命令都会按序执行 当 redis 在执行命令时，如果出现了错误，那么 redis 不会终止其它命令的执行，这是与关系型数据库事务最大的区别，redis 事务不会因为某个命令执行失败而回滚 redis 事务的缺陷不满足原子性与关系型数据库的事务不同，redis 事务是不满足原子性的，一个事务执行过程中，其他事务或 client 是可以对相应的 key 进行修改的 想要避免这样的并发性问题就需要使用 WATCH 命令，但是通常来说，必须经过仔细考虑才能决定究竟需要对哪些 key 进行 WATCH 加锁 额外的 WATCH 会增加事务失败的可能，而缺少必要的 WATCH 又会让我们的程序产生竞争条件 后执行的命令无法依赖先执行命令的结果由于事务中的所有命令都是互相独立的，在遇到 exec 命令之前并没有真正的执行，所以我们无法在事务中的命令中使用前面命令的查询结果 我们唯一可以做的就是通过 watch 保证在我们进行修改时，如果其它事务刚好进行了修改，则我们的修改停止，然后应用层做相应的处理 事务中的每条命令都会与 redis 服务器进行网络交互redis 事务开启之后，每执行一个操作返回的都是 queued，这里就涉及到客户端与服务器端的多次交互 明明是需要一次批量执行的 n 条命令，还需要通过多次网络交互，显然非常浪费 这个就是为什么会有 pipeline 的原因，减少 RTT 的时间 redis 事务缺陷的解决 – LuaLua 是一个小巧的脚本语言，有标准 C 编写，几乎在所有操作系统和平台上都可以编译运行 一个完整的 Lua 解释器不过 200k，在目前所有脚本引擎中，Lua 的速度是最快的，这一切都决定了 Lua 是作为嵌入式脚本的最佳选择 redis 2.6 版本之后也内嵌了一个 Lua 解释器，可以用于一些简单的事务与逻辑运算 Redis 内嵌 Lua 的优势在服务端实现业务逻辑按照我们上面介绍的，redis 事务执行中，每一条指令之间是相互独立的，我们无法让后面的操作依赖前面命名的结果，这就让整个事务仅仅成为了一个命令集合，在命令之间我们完全无法做任何事 但是，Lua 作为一个脚本语言，可以拥有分支、循环等语法结构，可以进行业务逻辑的编写 原子性由于 Lua 脚本是提交到 Redis server 进行一次性执行的，整个执行过程中不会被其他任何工作打断，其它任何脚本或者命令都无法执行,也就不会引起竞争条件，从而本身就实现了事务的原子性 但是，这同样会引起一个问题，正如官方文档所说的，正是由于 script 执行的原子性，所以我们不要在 script 中执行过长开销的程序，否则会验证影响其它请求的执行 可复用所有 Lua 脚本都是可重用的，这样就减少了网络开销 EVAL script numkeys key[key …] arg [arg…] EVALSHA sha1 SCRIPT LOAD script SCRIPT EXISTS sha1 EVAL1EVAL script numkeys key [key ...] arg [arg ...] 参数 描述 script 一段 Lua 脚本或 Lua 脚本文件所在路径及文件名 numkeys Lua 脚本对应参数数量 key [key …] Lua 中通过全局变量 KEYS 数组存储的传入参数 arg [arg …] Lua 中通过全局变量 ARGV 数组存储的传入附加参数 12345EVAL &quot;return &#123;KEYS[1],KEYS[2],ARGV[1],ARGV[2]&#125;&quot; 2 key1 key2 first second1) &quot;key1&quot;2) &quot;key2&quot;3) &quot;first&quot;4) &quot;second&quot; SCRIPT LOAD 与 EVALSHA 命令对于不立即执行的 Lua 脚本，或需要重用的 Lua 脚本，可以通过 SCRIPT LOAD 提前载入 Lua 脚本，这个命令会立即返回对应的 SHA1 校验码 当需要执行函数时，通过 EVALSHA 调用 SCRIPT LOAD 返回的 SHA1 即可 12345678SCRIPT LOAD &quot;return &#123;KEYS[1],KEYS[2],ARGV[1],ARGV[2]&#125;&quot;&quot;232fd51614574cf0867b83d384a5e898cfd24e5a&quot;EVALSHA &quot;232fd51614574cf0867b83d384a5e898cfd24e5a&quot; 2 key1 key2 first second1) &quot;key1&quot;2) &quot;key2&quot;3) &quot;first&quot;4) &quot;second&quot; 通过 Lua 脚本执行 redis 命令在 Lua 脚本中，只要使用 redis.call 传入 redis 命令就可以直接执行 1eval &quot;return redis.call(&#39;set&#39;,KEYS[1],&#39;bar&#39;)&quot; 1 foo --等同于在服务端执行 set foo bar 使用 Lua 脚本实现访问频率限制12345678910111213141516171819202122---- KEYS[1] 要限制的ip-- ARGV[1] 限制的访问次数-- ARGV[2] 限制的时间--local key = \"rate.limit:\" .. KEYS[1]local limit = tonumber(ARGV[1])local expire_time = ARGV[2]local is_exists = redis.call(\"EXISTS\", key)if is_exists == 1 then if redis.call(\"INCR\", key) &gt; limit then return 0 else return 1 endelse redis.call(\"SET\", key, 1) redis.call(\"EXPIRE\", key, expire_time) return 1end","categories":[{"name":"Redis","slug":"Redis","permalink":"http://blog.crazylaw.cn/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://blog.crazylaw.cn/tags/Redis/"}]},{"title":"【Golang】- confluent-kafka-go","slug":"Golang/confluent-kafka-go","date":"2020-06-01T01:46:51.000Z","updated":"2021-03-20T16:25:01.799Z","comments":true,"path":"2020/06/01/Golang/confluent-kafka-go/","link":"","permalink":"http://blog.crazylaw.cn/2020/06/01/Golang/confluent-kafka-go/","excerpt":"前言由于我们部门的一些大数据服务是用到 kafka 的，这个时期正值我们对 golang 语言对一个转型阶段，对比了一下开源对 kafka 客户端，决定使用 confluent-kafka-go, 所以在这里记录一下confluent-kafka-go 的一些内容","text":"前言由于我们部门的一些大数据服务是用到 kafka 的，这个时期正值我们对 golang 语言对一个转型阶段，对比了一下开源对 kafka 客户端，决定使用 confluent-kafka-go, 所以在这里记录一下confluent-kafka-go 的一些内容 说明confluent-kafka-go 是一个 confluent 官方的 golang 语言库，其依赖于 librdkafka 实现，大多数机器，\blibrakafka 都已经预编译进去 golang 扩展了，不需要额外安装 librdkafka，如果不支持预编译的话，则需要额外安装。 提供一个 dockerfile 的 demo 1234567891011121314151617181920212223242526272829303132333435# debian10 : buster# debian9 : buster# debian8 : jessie# debian7 : wheezyFROM golang:1.14.3-busterENV GO111MODULE=onENV GOPROXY=https://goproxy.io,directENV GOPRIVATE=git.mingchao.com# 1. 换源# 2. 加入confluent的源，安装librakafkaRUN echo \\ deb http://mirrors.aliyun.com/debian/ buster main non-free contrib \\ deb-src http://mirrors.aliyun.com/debian/ buster main non-free contrib \\ deb http://mirrors.aliyun.com/debian-security buster/updates main \\ deb-src http://mirrors.aliyun.com/debian-security buster/updates main \\ deb http://mirrors.aliyun.com/debian/ buster-updates main non-free contrib \\ deb-src http://mirrors.aliyun.com/debian/ buster-updates main non-free contrib \\ deb http://mirrors.aliyun.com/debian/ buster-backports main non-free contrib \\ deb-src http://mirrors.aliyun.com/debian/ buster-backports main non-free contrib \\ &gt; /etc/apt/sources.list &amp;&amp; \\ apt-get update -y &amp;&amp; \\ wget -qO - https://packages.confluent.io/deb/5.5/archive.key | apt-key add - &gt; /dev/null &amp;&amp; \\ sed -i '$a deb [arch=amd64] https://packages.confluent.io/deb/5.5 stable main' /etc/apt/sources.list &amp;&amp; \\ apt-get install -y librdkafka-devARG GIT_USERNAME=gitARG GIT_PASSWORD=git-passwordARG GIT_CREDEN_FILE=/.git-credentialsRUN touch $&#123;GIT_CREDEN_FILE&#125; &amp;&amp; \\ chown 600 $&#123;GIT_CREDEN_FILE&#125; &amp;&amp; \\ git config --global credential.helper 'store --file '$&#123;GIT_CREDEN_FILE&#125; &amp;&amp; \\ echo https://$&#123;GIT_USERNAME&#125;:$&#123;GIT_PASSWORD&#125;@git.mingchao.com | tee $&#123;GIT_CREDEN_FILE&#125; 源码 Api 说明consumer.go 这是一个 consumer 相关的文件。 Subscribe(topic string, rebalanceCb RebalanceCb) error订阅一个 topic，这个 api 会覆盖之前设置过了的 topic 订阅 123func (c *Consumer) Subscribe(topic string, rebalanceCb RebalanceCb) error &#123; return c.SubscribeTopics([]string&#123;topic&#125;, rebalanceCb)&#125; SubscribeTopics(topics []string, rebalanceCb RebalanceCb) (err error) 订阅多个 topic 这个 api 会覆盖之前设置过了的 topic 订阅 1234567891011121314151617181920func (c *Consumer) SubscribeTopics(topics []string, rebalanceCb RebalanceCb) (err error) &#123; ctopics := C.rd_kafka_topic_partition_list_new(C.int(len(topics))) defer C.rd_kafka_topic_partition_list_destroy(ctopics) for _, topic := range topics &#123; ctopic := C.CString(topic) defer C.free(unsafe.Pointer(ctopic)) C.rd_kafka_topic_partition_list_add(ctopics, ctopic, C.RD_KAFKA_PARTITION_UA) &#125; e := C.rd_kafka_subscribe(c.handle.rk, ctopics) if e != C.RD_KAFKA_RESP_ERR_NO_ERROR &#123; return newError(e) &#125; c.rebalanceCb = rebalanceCb c.handle.currAppRebalanceEnable = c.rebalanceCb != nil || c.appRebalanceEnable return nil&#125; Unsubscribe() (err error) 取消当前对 topic 的订阅 1234func (c *Consumer) Unsubscribe() (err error) &#123; C.rd_kafka_unsubscribe(c.handle.rk) return nil&#125; Assign(partitions []TopicPartition) (err error) 分配一组要使用的 partition 这个 api 会覆盖之前分配过的 12345678910111213func (c *Consumer) Assign(partitions []TopicPartition) (err error) &#123; c.appReassigned = true cparts := newCPartsFromTopicPartitions(partitions) defer C.rd_kafka_topic_partition_list_destroy(cparts) e := C.rd_kafka_assign(c.handle.rk, cparts) if e != C.RD_KAFKA_RESP_ERR_NO_ERROR &#123; return newError(e) &#125; return nil&#125; Unassign() (err error) 取消当前分配的 partition 12345678910func (c *Consumer) Unassign() (err error) &#123; c.appReassigned = true e := C.rd_kafka_assign(c.handle.rk, nil) if e != C.RD_KAFKA_RESP_ERR_NO_ERROR &#123; return newError(e) &#125; return nil&#125; Commit() ([]TopicPartition, error) 提交当前已经分配的 partition 的 offset 值 基于 StoreOffsets(offsets []TopicPartition) (storedOffsets []TopicPartition, err error) 这是一个阻塞请求，如果需要异步操作，需要调用者自行用协程 返回成功提交 offset 的 topicPartition 123func (c *Consumer) Commit() ([]TopicPartition, error) &#123; return c.commit(nil)&#125; CommitMessage(m *Message) ([]TopicPartition, error) 这个 API 基于 message 结构体 这是一个阻塞请求，如果需要异步操作，需要调用者自行用协程 返回成功提交 offset 的 topicPartition 12345678func (c *Consumer) CommitMessage(m *Message) ([]TopicPartition, error) &#123; if m.TopicPartition.Error != nil &#123; return nil, newErrorFromString(ErrInvalidArg, \"Can't commit errored message\") &#125; offsets := []TopicPartition&#123;m.TopicPartition&#125; offsets[0].Offset++ return c.commit(offsets)&#125; CommitOffsets(offsets []TopicPartition) ([]TopicPartition, error) 根据 []TopicPartition 来提交 offset 这是一个阻塞请求，如果需要异步操作，需要调用者自行用协程 返回成功提交 offset 的 topicPartition 123func (c *Consumer) CommitOffsets(offsets []TopicPartition) ([]TopicPartition, error) &#123; return c.commit(offsets)&#125; StoreOffsets(offsets []TopicPartition) (storedOffsets []TopicPartition, err error) 根据 []TopicPartition 来记录将会被提交的 offset（如果允许自动提交的话，那么会受auto.commit.interval.ms的影响，一定周期性提交，如果是手动提交的话则依赖 Commit()Api） 返回成功存储的 offsets，如果至少有一个偏移量无法存储，则返回一个错误和偏移量列表。每个偏移量都可以通过它的来检查特定的错误 123456789101112131415func (c *Consumer) StoreOffsets(offsets []TopicPartition) (storedOffsets []TopicPartition, err error) &#123; coffsets := newCPartsFromTopicPartitions(offsets) defer C.rd_kafka_topic_partition_list_destroy(coffsets) cErr := C.rd_kafka_offsets_store(c.handle.rk, coffsets) // coffsets might be annotated with an error storedOffsets = newTopicPartitionsFromCparts(coffsets) if cErr != C.RD_KAFKA_RESP_ERR_NO_ERROR &#123; return storedOffsets, newError(cErr) &#125; return storedOffsets, nil&#125; Seek(partition TopicPartition, timeoutMs int) error 获取指定 partition 的 offset 如果timeoutMs不是 0，则调用将等待这么长时间以执行查找。如果超时到达，内部状态将未知，并且此函数返回 ErrTimedOut。 如果timeoutMs 为 0，它将发起查找，但立即返回，不报告任何错误(例如，异步)。 Seek()只能用于已经使用的分区(通过 Assign()或隐式使用通过自平衡订阅())。 要设置起始偏移量，最好使用 Assign()并为每个分区提供一个起始偏移量。 1234567891011func (c *Consumer) Seek(partition TopicPartition, timeoutMs int) error &#123; rkt := c.handle.getRkt(*partition.Topic) cErr := C.rd_kafka_seek(rkt, C.int32_t(partition.Partition), C.int64_t(partition.Offset), C.int(timeoutMs)) if cErr != C.RD_KAFKA_RESP_ERR_NO_ERROR &#123; return newError(cErr) &#125; return nil&#125; Poll(timeoutMs int) (event Event) 轮询消息或事件。 将阻塞最多 timeoutMs 的超时时间 以下回调可能会被触发 Subscribe()’s rebalanceCb 如果超时则返回 nil，否则返回一个事件 1234func (c *Consumer) Poll(timeoutMs int) (event Event) &#123; ev, _ := c.handle.eventPoll(nil, timeoutMs, 1, nil) return ev&#125; ReadMessage(timeout time.Duration) (*Message, error) 返回一条消息 这是一个方便的 API，它封装了 Poll()，只返回消息或错误。所有其他事件类型都被丢弃。 该调用最多会阻塞 timeout 等待新消息或错误。timeout可以设置为-1，表示无限期等待。 超时将会返回(nil, err) 当 err 是 kafka.(Error).Code == Kafka.ErrTimedOut 消息将会返回 (msg, nil), 当有错误当时候将会返回 (nil, err), 当指定 partition 错误的时候（topic，partition，offset），将会返回 (msg,err) 全部其他的事件类型，像PartitionEOF,AssingedPartitions等等将会被默认丢弃 123456789101112131415161718192021222324252627282930313233343536373839func (c *Consumer) ReadMessage(timeout time.Duration) (*Message, error) &#123; var absTimeout time.Time var timeoutMs int if timeout &gt; 0 &#123; absTimeout = time.Now().Add(timeout) timeoutMs = (int)(timeout.Seconds() * 1000.0) &#125; else &#123; timeoutMs = (int)(timeout) &#125; for &#123; ev := c.Poll(timeoutMs) switch e := ev.(type) &#123; case *Message: if e.TopicPartition.Error != nil &#123; return e, e.TopicPartition.Error &#125; return e, nil case Error: return nil, e default: // Ignore other event types &#125; if timeout &gt; 0 &#123; // Calculate remaining time timeoutMs = int(math.Max(0.0, absTimeout.Sub(time.Now()).Seconds()*1000.0)) &#125; if timeoutMs == 0 &amp;&amp; ev == nil &#123; return nil, newError(C.RD_KAFKA_RESP_ERR__TIMED_OUT) &#125; &#125;&#125; Close() (err error) 关闭一个 Consumer 对象 调用后，对象不再可用。 1234567891011121314151617181920212223func (c *Consumer) Close() (err error) &#123; // Wait for consumerReader() or pollLogEvents to terminate (by closing readerTermChan) close(c.readerTermChan) c.handle.waitGroup.Wait() if c.eventsChanEnable &#123; close(c.events) &#125; C.rd_kafka_queue_destroy(c.handle.rkq) c.handle.rkq = nil e := C.rd_kafka_consumer_close(c.handle.rk) if e != C.RD_KAFKA_RESP_ERR_NO_ERROR &#123; return newError(e) &#125; c.handle.cleanup() C.rd_kafka_destroy(c.handle.rk) return nil&#125; GetMetadata(topic string, allTopics bool, timeoutMs int) (Metadata, error) 用于查询集群中 broker 和 topic 的元数据 如果 topic 参数不为 nil，则返回和 topoic 相关数据，否则（如果allTopics参数为 false，那么将会返回当前使用 topic 的元数据，如果 allTopics参数为 true, 那么将返回 broker 中所有 topic 的元数据） GetMetadata 相当于 Java API 中的 listTopics、describeTopics 和 describeCluster。 123func (c *Consumer) GetMetadata(topic *string, allTopics bool, timeoutMs int) (*Metadata, error) &#123; return getMetadata(c, topic, allTopics, timeoutMs)&#125; QueryWatermarkOffsets(topic string, partition int32, timeoutMs int) (low, high int64, err error) 根据 topic 和 partition，查询当前 broker 中他们的低水位和高水位的 offset 123func (c *Consumer) QueryWatermarkOffsets(topic string, partition int32, timeoutMs int) (low, high int64, err error) &#123; return queryWatermarkOffsets(c, topic, partition, timeoutMs)&#125; GetWatermarkOffsets(topic string, partition int32) (low, high int64, err error) 根据 topic 和 partition 返回当前服务存储的低水位和高水位的 offset 每个 fetch 响应或通过调用 QueryWatermarkOffsets 填充高水位的 offset 如果设置了 statistics.interval.ms, 低水位将会有一个 statistics.interval.ms 的周期来更新 123func (c *Consumer) GetWatermarkOffsets(topic string, partition int32) (low, high int64, err error) &#123; return getWatermarkOffsets(c, topic, partition)&#125; OffsetsForTimes(times []TopicPartition, timeoutMs int) (offsets []TopicPartition, err error) 每个分区返回的偏移量是最早的偏移量，其时间戳大于或等于相应分区中的给定时间戳。如果提供的时间戳超过分区中最后一条消息的时间戳，则返回-1 值。 123func (c *Consumer) OffsetsForTimes(times []TopicPartition, timeoutMs int) (offsets []TopicPartition, err error) &#123; return offsetsForTimes(c, times, timeoutMs)&#125; Subscription() (topics []string, err error) 返回当前被订阅的 topic 12345678910111213141516171819func (c *Consumer) Subscription() (topics []string, err error) &#123; var cTopics *C.rd_kafka_topic_partition_list_t cErr := C.rd_kafka_subscription(c.handle.rk, &amp;cTopics) if cErr != C.RD_KAFKA_RESP_ERR_NO_ERROR &#123; return nil, newError(cErr) &#125; defer C.rd_kafka_topic_partition_list_destroy(cTopics) topicCnt := int(cTopics.cnt) topics = make([]string, topicCnt) for i := 0; i &lt; topicCnt; i++ &#123; crktpar := C._c_rdkafka_topic_partition_list_entry(cTopics, C.int(i)) topics[i] = C.GoString(crktpar.topic) &#125; return topics, nil&#125; Assignment() (partitions []TopicPartition, err error) 返回当前指派的 partition 12345678910111213func (c *Consumer) Assignment() (partitions []TopicPartition, err error) &#123; var cParts *C.rd_kafka_topic_partition_list_t cErr := C.rd_kafka_assignment(c.handle.rk, &amp;cParts) if cErr != C.RD_KAFKA_RESP_ERR_NO_ERROR &#123; return nil, newError(cErr) &#125; defer C.rd_kafka_topic_partition_list_destroy(cParts) partitions = newTopicPartitionsFromCparts(cParts) return partitions, nil&#125; Committed(partitions []TopicPartition, timeoutMs int) (offsets []TopicPartition, err error) 查询已经提交 commit 的 offset 12345678910func (c *Consumer) Committed(partitions []TopicPartition, timeoutMs int) (offsets []TopicPartition, err error) &#123; cparts := newCPartsFromTopicPartitions(partitions) defer C.rd_kafka_topic_partition_list_destroy(cparts) cerr := C.rd_kafka_committed(c.handle.rk, cparts, C.int(timeoutMs)) if cerr != C.RD_KAFKA_RESP_ERR_NO_ERROR &#123; return nil, newError(cerr) &#125; return newTopicPartitionsFromCparts(cparts), nil&#125; Position(partitions []TopicPartition) (offsets []TopicPartition, err error) 根据 partition 返回其 offset 典型的用法是调用 assign()来获取分区列表，然后将其传递给 Position()来获取每个分区的当前 offset 消费的位置是分区读取的下一个消息，例如（最后一条信息的+1） 12345678910func (c *Consumer) Position(partitions []TopicPartition) (offsets []TopicPartition, err error) &#123; cparts := newCPartsFromTopicPartitions(partitions) defer C.rd_kafka_topic_partition_list_destroy(cparts) cerr := C.rd_kafka_position(c.handle.rk, cparts) if cerr != C.RD_KAFKA_RESP_ERR_NO_ERROR &#123; return nil, newError(cerr) &#125; return newTopicPartitionsFromCparts(cparts), nil&#125; Pause(partitions []TopicPartition) (err error) 根据提供的 partition 暂停消费 如果设置了go.events.channel.enable，只会受到go.events.channel.size的影响，这个 API 将不会生效 123456789func (c *Consumer) Pause(partitions []TopicPartition) (err error) &#123; cparts := newCPartsFromTopicPartitions(partitions) defer C.rd_kafka_topic_partition_list_destroy(cparts) cerr := C.rd_kafka_pause_partitions(c.handle.rk, cparts) if cerr != C.RD_KAFKA_RESP_ERR_NO_ERROR &#123; return newError(cerr) &#125; return nil&#125; Resume(partitions []TopicPartition) (err error) 唤醒被暂停的 partition 123456789func (c *Consumer) Resume(partitions []TopicPartition) (err error) &#123; cparts := newCPartsFromTopicPartitions(partitions) defer C.rd_kafka_topic_partition_list_destroy(cparts) cerr := C.rd_kafka_resume_partitions(c.handle.rk, cparts) if cerr != C.RD_KAFKA_RESP_ERR_NO_ERROR &#123; return newError(cerr) &#125; return nil&#125; GetConsumerGroupMetadata() (*ConsumerGroupMetadata, error) 返回当前消费者组的元数据 这个返回的对象，应该传递给事务生产者的 SendOffsetsToTransaction() API 1234567891011121314func (c *Consumer) GetConsumerGroupMetadata() (*ConsumerGroupMetadata, error) &#123; cgmd := C.rd_kafka_consumer_group_metadata(c.handle.rk) if cgmd == nil &#123; return nil, NewError(ErrState, \"Consumer group metadata not available\", false) &#125; defer C.rd_kafka_consumer_group_metadata_destroy(cgmd) serialized, err := serializeConsumerGroupMetadata(cgmd) if err != nil &#123; return nil, err &#125; return &amp;ConsumerGroupMetadata&#123;serialized&#125;, nil&#125;","categories":[{"name":"Golang","slug":"Golang","permalink":"http://blog.crazylaw.cn/categories/Golang/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://blog.crazylaw.cn/tags/Golang/"},{"name":"Kafka","slug":"Kafka","permalink":"http://blog.crazylaw.cn/tags/Kafka/"}]},{"title":"【Docker】如何选择hub.docker中的tag标签","slug":"docker/docker-tags的选择","date":"2020-05-25T03:28:30.000Z","updated":"2021-03-20T16:25:01.805Z","comments":true,"path":"2020/05/25/docker/docker-tags的选择/","link":"","permalink":"http://blog.crazylaw.cn/2020/05/25/docker/docker-tags%E7%9A%84%E9%80%89%E6%8B%A9/","excerpt":"前言很多小伙伴或许还不清楚各个tag的区别，其实最大的一个问题在于，你需要了解各大操作系统的不同。","text":"前言很多小伙伴或许还不清楚各个tag的区别，其实最大的一个问题在于，你需要了解各大操作系统的不同。 操作系统 debian ubuntu centos alpine debianDebian 发行版本Debian 一直维护着至少三个发行版本：稳定版（stable），测试版（testing）和不稳定版（unstable）。 稳定版（stable）稳定版包含了 Debian 官方最近一次发行的软件包。 作为 Debian 的正式发行版本，它是我们优先推荐给用户您选用的版本。 当前 Debian 的稳定版版本号是 10，开发代号为 buster。最初版本为 10，于 2019年07月06日 发布，其更新 10.4 已于 2020年05月09日 发布。 测试版（testing）测试版包含了那些暂时未被收录进入稳定版的软件包，但它们已经进入了候选队列。使用这个版本的最大益处在于它拥有更多版本较新的软件。 想要了解 什么是测试版以及 如何成为稳定版的更多信息，请看 Debian FAQ。 当前的测试版版本代号是 bullseye。 不稳定版（unstable）不稳定版存放了 Debian 现行的开发工作。通常，只有开发者和那些喜欢过惊险刺激生活的人选用该版本。推荐使用不稳定版的用户订阅 debian-devel-announce 邮件列表，以接收关于重大变更的通知，比如有可能导致问题的升级。 不稳定版的版本代号永远都被称为 sid。 下一代 Debian 正式发行版的代号为 bullseye — 发布时间尚未确定 Debian 10（buster） — 当前的稳定版（stable） Debian 9（stretch） — 旧的稳定版（oldstable） Debian 8（jessie） — 更旧的稳定版（oldoldstable） Debian 7（wheezy） — 被淘汰的稳定版 Debian 6.0（squeeze） — 被淘汰的稳定版 Debian GNU/Linux 5.0（lenny） — 被淘汰的稳定版 Debian GNU/Linux 4.0（etch） — 被淘汰的稳定版 Debian GNU/Linux 3.1（sarge） — 被淘汰的稳定版 Debian GNU/Linux 3.0（woody） — 被淘汰的稳定版 Debian GNU/Linux 2.2（potato） — 被淘汰的稳定版 Debian GNU/Linux 2.1（slink） — 被淘汰的稳定版 Debian GNU/Linux 2.0（hamm） — 被淘汰的稳定版","categories":[{"name":"Docker","slug":"Docker","permalink":"http://blog.crazylaw.cn/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://blog.crazylaw.cn/tags/Docker/"}]},{"title":"【大数据】- Impala函数使用文档","slug":"大数据/impala函数总结文档","date":"2020-05-12T01:56:40.000Z","updated":"2021-03-20T16:25:01.815Z","comments":true,"path":"2020/05/12/大数据/impala函数总结文档/","link":"","permalink":"http://blog.crazylaw.cn/2020/05/12/%E5%A4%A7%E6%95%B0%E6%8D%AE/impala%E5%87%BD%E6%95%B0%E6%80%BB%E7%BB%93%E6%96%87%E6%A1%A3/","excerpt":"前言目前一些传统的公司比较早期使用大数据计算引擎的，都会选择使用imapla做为计算引擎，所以对impala对一些sql使用还是需要熟悉的。","text":"前言目前一些传统的公司比较早期使用大数据计算引擎的，都会选择使用imapla做为计算引擎，所以对impala对一些sql使用还是需要熟悉的。 条件函数故名思议，条件函数区域主要都是一些条件判定的函数 case when1234567用法 1:CASE a WHEN b THEN c [WHEN d THEN e]... [ELSE f] END用法 2:CASE WHEN a THEN b [WHEN c THEN d]... [ELSE e] END 用法1: 1234567select case xµÂ when 1 then 'one' when 2 then 'two'µ when 0 then 'zero' else 'out of range' end from t1; 用法2: 12345678select case when dayname(now()) in ('Saturday','Sunday') then 'result undefined on weekends' when x &gt; y then 'x greater than y' when x = y then 'x and y are equal' when x is null or y is null then 'one of the columns is null' else null end from t1;","categories":[{"name":"大数据","slug":"大数据","permalink":"http://blog.crazylaw.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"http://blog.crazylaw.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"Impala","slug":"Impala","permalink":"http://blog.crazylaw.cn/tags/Impala/"}]},{"title":"【消息队列】- Rabbitmq安装与配置","slug":"消息队列/rabbitmq","date":"2020-05-08T01:46:51.000Z","updated":"2021-03-20T16:25:01.819Z","comments":true,"path":"2020/05/08/消息队列/rabbitmq/","link":"","permalink":"http://blog.crazylaw.cn/2020/05/08/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/rabbitmq/","excerpt":"前言由于部门的大数据服务一直都是用kafka，但是一些常规的业务系统，我们并不希望依赖kafka，所以我们引入了rabbitmq。","text":"前言由于部门的大数据服务一直都是用kafka，但是一些常规的业务系统，我们并不希望依赖kafka，所以我们引入了rabbitmq。 鉴于我们要对配置做特殊处理，我们写了一个rabbitmq-env.conf来管理具体内容。 12345678910111213141516171819202122232425262728293031323334353637383940# 这是rabbitmq的配置文件,也支持shell语言# 该文件受RABBITMQ_CONF_ENV_FILE环境变量影响路径，默/etc/rabbitmq/rabbitmq-env.conf# 这个配置的环境变量最终都会生成自带有“RABBITMQ_”的前缀，所以请不要手动设置RABBITMQ_# 由于我们用的FQDN，所以要配置longname环境变量, 因此我们不固定节点名# https://www.rabbitmq.com/configure.html#environment-env-file-unix# 基础变量rabbitmq_uid=rabbitmqrabbitmq_gid=rabbitmqrabbitmq_config_file=\"/etc/rabbitmq/rabbitmq.config\"rabbitmq_mnesia_base=\"/data/database/rabbitmq\"rabbitmq_log_base=\"/data/logs/rabbitmq\"# 以下为执行命令mkdir -p $&#123;rabbitmq_mnesia_base&#125; &amp;&amp; chown $&#123;rabbitmq_uid&#125;:$&#123;rabbitmq_gid&#125; $&#123;rabbitmq_mnesia_base&#125;mkdir -p $&#123;rabbitmq_log_base&#125; &amp;&amp; chown $&#123;rabbitmq_uid&#125;:$&#123;rabbitmq_gid&#125; $&#123;rabbitmq_log_base&#125;# 以下为环境变量, 切勿手动设置\"RABBITMQ_\"前缀，否则不生效# https://www.rabbitmq.com/relocate.html#environment-variables# https://www.rabbitmq.com/configure.html#supported-environment-variables# 节点名称定义# https://www.rabbitmq.com/cli.html#node-names# NODENAME=rabbitmq@node1# 节点名字使用FQDN# https://www.rabbitmq.com/cli.html#node-namesUSE_LONGNAME=true# 配置文件路径CONFIG_FILE=$&#123;rabbitmq_config_file&#125;# 配置持久化数据库路径MNESIA_BASE=$&#123;rabbitmq_mnesia_base&#125;# rabbitmq日志存储路径LOG_BASE=$&#123;rabbitmq_log_base&#125; 由于我们的hostname一般是采用FQDN（全限定性域名），并且我们采用的是镜像集群的模式，我认为hostname对我们来说，并不需要固定。即使是发生了故障，只要集群中存在一台机器，那么持久化对数据也并不会丢失。 为什么我们会采用镜像模式？这个模式不是会让大量消息对时候导致内网流量暴涨吗？ 确实是会这样子，但是我们一般是后台系统使用居多，目前来说，并不会有大量消息的出现，如果真的是这样子的化，我们更推荐用kafka或者rocketmq。我们更关注的是数据的完整性可靠性。 注意我们的所有节点都需要开启rabbitmq-plugins enable rabbitmq_management_agent，这样子在管理后台中，我们才可以看到所有节点的状态。 我们的原则是： 12345678910如何选择远程或本地的RabbitMQ？场景1：A、B 服务在同台机器，一产一消，使用本地（php系统该场景目前的 laravel + redis 方案已能够满足）场景2：A、B 服务在多台机器，一产一消，使用远程场景3：A、B、C 服务在同台机器，一产多消，使用本地场景4：A、B、C 服务在多台机器，一产多消，使用远程如果确定vhost名称？1、仅限于单系统并且跨机器使用，则使用系统名称作为 vhost 名称2、多系统并且跨机器使用，则使用 common 作为 vhost 名称","categories":[{"name":"消息队列","slug":"消息队列","permalink":"http://blog.crazylaw.cn/categories/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"}],"tags":[{"name":"消息队列，Rabbitmq","slug":"消息队列，Rabbitmq","permalink":"http://blog.crazylaw.cn/tags/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%EF%BC%8CRabbitmq/"}]},{"title":"【rust】包和模块","slug":"Rust语言/rust-包和模块","date":"2020-03-13T02:43:43.000Z","updated":"2021-03-20T16:25:01.800Z","comments":true,"path":"2020/03/13/Rust语言/rust-包和模块/","link":"","permalink":"http://blog.crazylaw.cn/2020/03/13/Rust%E8%AF%AD%E8%A8%80/rust-%E5%8C%85%E5%92%8C%E6%A8%A1%E5%9D%97/","excerpt":"前言Rust 中，crate 是一个独立的可编译单元。具体说来，就是一个或一批文件（如果是一批文件，那么有一个文件是这个 crate 的入口）。它编译后，会对应着生成一个可执行文件或一个库。 Rust 提供了一个关键字 mod，它可以在一个文件中定义一个模块，或者引用另外一个文件中的模块。","text":"前言Rust 中，crate 是一个独立的可编译单元。具体说来，就是一个或一批文件（如果是一批文件，那么有一个文件是这个 crate 的入口）。它编译后，会对应着生成一个可执行文件或一个库。 Rust 提供了一个关键字 mod，它可以在一个文件中定义一个模块，或者引用另外一个文件中的模块。 包执行 cargo new -lib foo，会得到如下目录层级： 1234foo├── Cargo.toml└── src └── lib.rs 这里，lib.rs 就是一个 crate（入口），它编译后是一个库。一个工程下可以包含不止一个 crate，本工程只有一个。 执行 cargo new bar，会得到如下目录层级： 1234bar├── Cargo.toml└── src └── main.rs 这里，main.rs 就是一个 crate（入口），它编译后是一个可执行文件。 模块关于模块的一些要点： 每个 crate 中，默认实现了一个隐式的 根模块（root module）； 模块的命名风格也是 lower_snake_case，跟其它的 Rust 的标识符一样； 模块可以嵌套； 模块中可以写任何合法的 Rust 代码； 在文件中定义一个模块比如，在上述 lib.rs 中，我们写上如下代码： 1234567mod aaa &#123; const X: i32 = 10; fn print_aaa() &#123; println!(\"&#123;&#125;\", 42); &#125;&#125; 我们可以继续写如下代码： 12345678910111213mod aaa &#123; const X: i32 = 10; fn print_aaa() &#123; println!(\"&#123;&#125;\", 42); &#125; mod BBB &#123; fn print_bbb() &#123; println!(\"&#123;&#125;\", 37); &#125; &#125;&#125; 还可以继续写： 1234567891011121314151617181920mod aaa &#123; const X: i32 = 10; fn print_aaa() &#123; println!(\"&#123;&#125;\", 42); &#125; mod bbb &#123; fn print_bbb() &#123; println!(\"&#123;&#125;\", 37); &#125; &#125;&#125;mod ccc &#123; fn print_ccc() &#123; println!(\"&#123;&#125;\", 25); &#125;&#125; 模块的可见性我们前面写了一些模块，但实际上，我们写那些模块，目前是没有什么作用的。写模块的目的一是为了分隔逻辑块，二是为了提供适当的函数，或对象，供外部访问。而模块中的内容，默认是私有的，只有模块内部能访问。 为了让外部能使用模块中 item，需要使用 pub 关键字。外部引用的时候，使用 use 关键字。例如： 12345678910111213mod ccc &#123; pub fn print_ccc() &#123; println!(\"&#123;&#125;\", 25); &#125;&#125;fn main() &#123; use ccc::print_ccc; print_ccc(); // 或者 ccc::print_ccc();&#125; 规则很简单，一个 item（函数，绑定，Trait 等），前面加了 pub，那么就它变成对外可见（访问，调用）的了。 引用外部文件模块通常，我们会在单独的文件中写模块内容，然后使用 mod 关键字来加载那个文件作为我们的模块。 比如，我们在 src 下新建了文件 aaa.rs。现在目录结构是下面这样子： 12345foo├── Cargo.toml└── src └── aaa.rs └── main.rs 我们在 aaa.rs 中，写上： 123pub fn print_aaa() &#123; println!(\"&#123;&#125;\", 25);&#125; 在 main.rs 中，写上： 1234567mod aaa;use aaa::print_aaa;fn main () &#123; print_aaa();&#125; 编译后，生成一个可执行文件。 细心的朋友会发现，aaa.rs 中，没有使用 mod xxx {} 这样包裹起来，是因为 mod xxx; 相当于把 xxx.rs 文件用 mod xxx {} 包裹起来了。初学者往往会多加一层，请注意。 多文件模块的层级关系Rust 的模块支持层级结构，但这种层级结构本身与文件系统目录的层级结构是解耦的。 mod xxx; 这个 xxx 不能包含 :: 号。也即在这个表达形式中，是没法引用多层结构下的模块的。也即，你不可能直接使用 mod a::b::c::d; 的形式来引用 a/b/c/d.rs 这个模块。 那么，Rust 的多层模块遵循如下两条规则： 优先查找 xxx.rs 文件 main.rs、lib.rs、mod.rs 中的 mod xxx; 默认优先查找同级目录下的 xxx.rs 文件； 其他文件 yyy.rs 中的 mod xxx;默认优先查找同级目录的 yyy 目录下的 xxx.rs 文件； 如果 xxx.rs 不存在，则查找 xxx/mod.rs 文件，即 xxx 目录下的 mod.rs 文件。 上述两种情况，加载成模块后，效果是相同的。Rust 就凭这两条规则，通过迭代使用，结合 pub 关键字，实现了对深层目录下模块的加载； 下面举个例子，现在我们建了一个测试工程，目录结构如下： 123456789src├── a│ ├── b│ │ ├── c│ │ │ ├── d.rs│ │ │ └── mod.rs│ │ └── mod.rs│ └── mod.rs└── main.rs a/b/c/d.rs 文件内容： 123pub fn print_ddd() &#123; println!(\"i am ddd.\");&#125; a/b/c/mod.rs 文件内容： 1pub mod d; a/b/mod.rs 文件内容： 1pub mod c; a/mod.rs 文件内容： 1pub mod b; main.rs 文件内容： 1234567mod a;use a::b::c::d;fn main() &#123; d::print_ddd();&#125; 输出结果为：i am ddd. 仔细理解本例子，就明白 Rust 的层级结构模块的用法了。 至于为何 Rust 要这样设计，有几下几个原因： Rust 本身模块的设计是与操作系统文件系统目录解耦的，因为 Rust 本身可用于操作系统的开发； Rust 中的一个文件内，可包含多个模块，直接将 a::b::c::d 映射到 a/b/c/d.rs 会引起一些歧义； Rust 一切从安全性、显式化立场出发，要求引用路径中的每一个节点，都是一个有效的模块，比如上例，d 是一个有效的模块的话，那么，要求 c, b, a 分别都是有效的模块，可单独引用。 路径前面我们提到，一个 crate 是一个独立的可编译单元。它有一个入口文件，这个入口文件是这个 crate（里面可能包含若干个 module）的模块根路径。整个模块的引用，形成一个链，每个模块，都可以用一个精确的路径（比如：a::b::c::d）来表示； 与文件系统概念类似，模块路径也有相对路径和绝对路径的概念。为此，Rust 提供了 self 和 super 两个关键字。 self 在路径中，有两种意思： use self::xxx 表示，加载当前模块中的 xxx。此时 self 可省略； use xxx::{self, yyy}，表示，加载当前路径下模块 xxx 本身，以及模块 xxx 下的 yyy； super 表示，当前模块路径的上一级路径，可以理解成父模块。 1use super::xxx; 表示引用父模块中的 xxx。 另外，还有一种特殊的路径形式： 1::xxx::yyy 它表示，引用根路径下的 xxx::yyy，这个根路径，指的是当前 crate 的根路径。 路径中的 * 符号： 1use xxx::*; 表示导入 xxx 模块下的所有可见 item（加了 pub 标识的 item）。 Re-exporting我们可以结合使用 pub use 来实现 Re-exporting。Re-exporting 的字面意思就是 重新导出。它的意思是这样的，把深层的 item 导出到上层目录中，使调用的时候，更方便。接口设计中会大量用到这个技术。 还是举上面那个 a::b::c::d 的例子。我们在 main.rs 中，要调用 d，得使用 use a::b::c::d; 来调用。而如果我们修改 a/mod.rs 文件为： a/mod.rs 文件内容： 12pub mod b;pub use b::c::d; 那么，我们在 main.rs 中，就可以使用 use a::d; 来调用了。从这个例子来看没觉得方便多少。但是如果开发的一个库中有大量的内容，而且是在不同层次的模块中。那么，通过统一导出到一个地方，就能大大方便接口使用者。 加载外部 crate前面我们讲的，都是在当前 crate 中的技术。真正我们在开发时，会大量用到外部库。外部库是通过 1extern crate xxx; 这样来引入的。 注：要使上述引用生效，还必须在 Cargo.toml 的 dependecies 段，加上 xxx=”version num” 这种依赖说明，详情见 Cargo 项目管理 这一章。 引入后，就相当于引入了一个符号 xxx，后面可以直接以这个 xxx 为根引用这个 crate 中的 item： 123extern crate xxx;use xxx::yyy::zzz; 引入的时候，可以通过 as 关键字重命名。 123extern crate xxx as foo;use foo::yyy::zzz;","categories":[{"name":"rust","slug":"rust","permalink":"http://blog.crazylaw.cn/categories/rust/"}],"tags":[{"name":"rust","slug":"rust","permalink":"http://blog.crazylaw.cn/tags/rust/"}]},{"title":"【消息队列】- Kafka数据动态迁移","slug":"消息队列/kafka-migrants","date":"2020-02-27T09:25:40.000Z","updated":"2021-03-20T16:25:01.819Z","comments":true,"path":"2020/02/27/消息队列/kafka-migrants/","link":"","permalink":"http://blog.crazylaw.cn/2020/02/27/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka-migrants/","excerpt":"前言kafka作为高效的消息队列，其数据的维护也是十分重要的。有时候，我们可能存在对broker进行数据迁移，或者增加或者减少broker。对于我们已经创建了对topic。其配置不会随便进行更新。isr中依然存在着已经移除了对broker，这个时候，我们就需要更新它们的信息，告诉kafka数据分布的情况。并且在最少isr的问题到来之前，提前减少事故的事发。","text":"前言kafka作为高效的消息队列，其数据的维护也是十分重要的。有时候，我们可能存在对broker进行数据迁移，或者增加或者减少broker。对于我们已经创建了对topic。其配置不会随便进行更新。isr中依然存在着已经移除了对broker，这个时候，我们就需要更新它们的信息，告诉kafka数据分布的情况。并且在最少isr的问题到来之前，提前减少事故的事发。 1. 创建topic1234bash-4.4# kafka-topics.sh --create --topic test-topic \\--zookeeper mzookeeper \\--replication-factor 2 --partitions 3Created topic \"test-topic\". 2.查看test-topic12345bash-4.4# kafka-topics.sh --describe --zookeeper mzookeeper --topic test-topicTopic:test-topic PartitionCount:3 ReplicationFactor:2 Configs: Topic: test-topic Partition: 0 Leader: 1002 Replicas: 1002,1003 Isr: 1002,1003 Topic: test-topic Partition: 1 Leader: 1003 Replicas: 1003,1004 Isr: 1003,1004 Topic: test-topic Partition: 2 Leader: 1004 Replicas: 1004,1001 Isr: 1004,1001 3. 产生一批数据1234567891011bash-4.4# kafka-console-producer.sh --topic test-topic --broker-list mkafka1:9092111111222223333344444555555666661111222233334444 4. kafka-reassign-partitions重分区(假设需要减少节点broker 1004，本测试通过关闭对应kafka broker节点模拟)12345bash-4.4# kafka-topics.sh --describe --zookeeper mzookeeper --topic test-topicTopic:test-topic PartitionCount:3 ReplicationFactor:2 Configs: Topic: test-topic Partition: 0 Leader: 1002 Replicas: 1002,1003 Isr: 1002,1003 Topic: test-topic Partition: 1 Leader: 1003 Replicas: 1003,1004 Isr: 1003 Topic: test-topic Partition: 2 Leader: 1001 Replicas: 1004,1001 Isr: 1001 这里，我们看到partition=2的Leader由1004变为了1001，并且isr中的1004已经不见了。 5. 新建文件topic-to-move.json ，比加入如下内容123cat&gt;topic-to-move.json&lt;&lt;EOF&#123;\"topics\": [&#123;\"topic\":\"test-topic\"&#125;], \"version\": 1&#125;EOF 6. 使用–generate生成迁移计划，broker-list根据自己环境设置，我的环境由于broker 10004挂掉了，只剩下1001和1002和10031234567bash-4.4# kafka-reassign-partitions.sh --zookeeper mzookeeper --topics-to-move-json-file topic-to-move.json --broker-list \"1001,1002,1003\" --generateCurrent partition replica assignment&#123;\"version\":1,\"partitions\":[&#123;\"topic\":\"test-topic\",\"partition\":0,\"replicas\":[1002,1003]&#125;,&#123;\"topic\":\"test-topic\",\"partition\":2,\"replicas\":[1004,1001]&#125;,&#123;\"topic\":\"test-topic\",\"partition\":1,\"replicas\":[1003,1004]&#125;]&#125;Proposed partition reassignment configuration&#123;\"version\":1,\"partitions\":[&#123;\"topic\":\"test-topic\",\"partition\":0,\"replicas\":[1003,1001]&#125;,&#123;\"topic\":\"test-topic\",\"partition\":2,\"replicas\":[1002,1003]&#125;,&#123;\"topic\":\"test-topic\",\"partition\":1,\"replicas\":[1001,1002]&#125;]&#125; 我们把最下面分配好的建议的分区副本分布配置拿出来，写入一个新的文件中(生产上一般会保留当前分区副本分布，仅更改下线的分区，这样数据移动更少) 123cat &gt; kafka-reassign-execute.json &lt;&lt;EOF&#123;\"version\":1,\"partitions\":[&#123;\"topic\":\"test-topic\",\"partition\":0,\"replicas\":[1003,1001]&#125;,&#123;\"topic\":\"test-topic\",\"partition\":2,\"replicas\":[1002,1003]&#125;,&#123;\"topic\":\"test-topic\",\"partition\":1,\"replicas\":[1001,1002]&#125;]&#125;EOF 7. 使用–execute执行迁移计划 (有数据移动，broker 1004上的数据会移到broker 1001和1002和1003上，如果数据量大，执行的时间会比较久，耐心等待即可)123456789bash-4.4# kafka-reassign-partitions.sh --zookeeper mzookeeper \\--reassignment-json-file kafka-reassign-execute.json \\--executeCurrent partition replica assignment&#123;\"version\":1,\"partitions\":[&#123;\"topic\":\"test-topic\",\"partition\":0,\"replicas\":[1002,1003]&#125;,&#123;\"topic\":\"test-topic\",\"partition\":2,\"replicas\":[1004,1001]&#125;,&#123;\"topic\":\"test-topic\",\"partition\":1,\"replicas\":[1003,1004]&#125;]&#125;Save this to use as the --reassignment-json-file option during rollbackSuccessfully started reassignment of partitions &#123;\"version\":1,\"partitions\":[&#123;\"topic\":\"test-topic\",\"partition\":0,\"replicas\":[1003,1001]&#125;,&#123;\"topic\":\"test-topic\",\"partition\":2,\"replicas\":[1002,1003]&#125;,&#123;\"topic\":\"test-topic\",\"partition\":1,\"replicas\":[1001,1002]&#125;]&#125; 这里还是列出了原始的配置和最新的配置，并且有一句提示：如果需要回滚的话，请使用第一份配置，第二份配置已经成功开始重新分发。 8. 使用-verify查看迁移进度1234567bash-4.4# kafka-reassign-partitions.sh --zookeeper mzookeeper \\--reassignment-json-file kafka-reassign-execute.json \\--verifyStatus of partition reassignment:Reassignment of partition [test-topic,0] completed successfullyReassignment of partition [test-topic,2] completed successfullyReassignment of partition [test-topic,1] completed successfully 成功处理完毕了。 9. 查看详情12345bash-4.4# kafka-topics.sh --describe --zookeeper mzookeeper --topic test-topicTopic:test-topic PartitionCount:3 ReplicationFactor:2 Configs: Topic: test-topic Partition: 0 Leader: 1003 Replicas: 1003,1001 Isr: 1003,1001 Topic: test-topic Partition: 1 Leader: 1001 Replicas: 1001,1002 Isr: 1001,1002 Topic: test-topic Partition: 2 Leader: 1002 Replicas: 1002,1003 Isr: 1003,1002 可以看到1004的broker_id已经被彻底移除了。 9. 通过消费者验证，可知，并未丢失数据。注意需要加–from-beginning。1234567891011bash-4.4# kafka-console-consumer.sh --topic test-topic --from-beginning --zookeeper mzookeeper111111444441111444433333666663333222225555552222","categories":[{"name":"消息队列","slug":"消息队列","permalink":"http://blog.crazylaw.cn/categories/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"}],"tags":[{"name":"消息队列，Kafka","slug":"消息队列，Kafka","permalink":"http://blog.crazylaw.cn/tags/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%EF%BC%8CKafka/"}]},{"title":"【Linux】顺序写盘和随机写盘","slug":"Linux/顺序写盘和随机写盘","date":"2019-12-10T09:43:43.000Z","updated":"2021-03-20T16:25:01.800Z","comments":true,"path":"2019/12/10/Linux/顺序写盘和随机写盘/","link":"","permalink":"http://blog.crazylaw.cn/2019/12/10/Linux/%E9%A1%BA%E5%BA%8F%E5%86%99%E7%9B%98%E5%92%8C%E9%9A%8F%E6%9C%BA%E5%86%99%E7%9B%98/","excerpt":"linux 的文件描述符文件描述符（FD:file descriptors），也可以说是文件句柄，当某个程序打开文件时，内核返回相应的文件描述符，程序为了处理该文件必须引用此描述符。文件描述符是一个正整数，用以标明每一个被进程所打开的文件和 socket。最前面的三个文件描述符（0，1，2）分别与标准输入（stdin），标准输出（stdout）和标准错误（stderr）对应，后面打开的文件依此类推对应 3、4…… 。 linux 系统对每个用户、进程、或整个系统的可打开文件描述符数量都有一个限制，一般默认为 1024。当我们在系统或应用的日志中碰到“too many open files”错误记录时，这个其实不是说打开的文件过多，而是打开的文件描述符数量已达到了限制，这时就需要增加文件描述符的数量限制了。","text":"linux 的文件描述符文件描述符（FD:file descriptors），也可以说是文件句柄，当某个程序打开文件时，内核返回相应的文件描述符，程序为了处理该文件必须引用此描述符。文件描述符是一个正整数，用以标明每一个被进程所打开的文件和 socket。最前面的三个文件描述符（0，1，2）分别与标准输入（stdin），标准输出（stdout）和标准错误（stderr）对应，后面打开的文件依此类推对应 3、4…… 。 linux 系统对每个用户、进程、或整个系统的可打开文件描述符数量都有一个限制，一般默认为 1024。当我们在系统或应用的日志中碰到“too many open files”错误记录时，这个其实不是说打开的文件过多，而是打开的文件描述符数量已达到了限制，这时就需要增加文件描述符的数量限制了。 获取系统打开的文件描述符数量12[root@localhost ~]# cat /proc/sys/fs/file-nr1216 0 197787 第一列 1216 ：为已分配的 FD 数量 第二列 0 ：为已分配但尚未使用的 FD 数量 第三列 197787 ：为系统可用的最大 FD 数量 已用 FD 数量＝为已分配的 FD 数量 - 为已分配但尚未使用的 FD 数量。注意，这些数值是系统层面的。 获取进程打开的文件描述符数量1234567[root@localhost ~]# pidof vim3253[root@localhost ~]# ll /proc/3253/fdlrwx------. 1 test test 64 6月 8 18:11 0 -&gt; /dev/pts/0lrwx------. 1 test test 64 6月 8 18:11 1 -&gt; /dev/pts/0lrwx------. 1 test test 64 6月 8 18:11 2 -&gt; /dev/pts/0lrwx------. 1 test test 64 6月 8 18:11 4 -&gt; /home/test/.bash_history.swp 可以看到 vim 进程用了 4 个 FD 更改文件描述符限制当碰到“too many open files”错误时，就需要增加文件描述符的限制数量，系统的默认文件描述符都比较大，一般来说，只需增加用户或进程的就可以了 用户或进程12345[root@localhost ~]# ulimit -n1024[root@localhost ~]# ulimit -n 10240[root@localhost ~]# ulimit -n10240 注意，使用 ulimit 命令更改后只是在当前会话生效，当退出当前会话重新登录后又会回到默认值 1024，要永久更改可以修改文件 /etc/security/limit.conf，如 1[root@localhost ~]#vi /etc/security/limits.conf 加入 “abc hard nofile 10240” abc：用户名，即允许 test 使用 ulimit 命令更改 FD 限制，最大值不超过 10240，更改后 abc 用户的每一个进程（以 abc 用户运行的进程）可打开的 FD 数量为 10240 个 hard：限制类型，有 soft/hard 两种，达到 soft 限制会在系统的日志（一般为/var/log/messages）里面记录一条告警日志，但不影响使用。hard，达到这个限制，有日志且会影响使用。 nofile：限制的内容，nofile - max number of open files 1024 ：值 更改后，退出终端重新登录，用 ulimit 看看有没有生效，如果没生效，可以在 abc 用户的.bash_profile 文件加上 ulimit -n 10240 ,以使用户 abc 每次登录时都会将 FD 最大值更改为 10240，如： 12345[root@localhost ~]#echo \"ulimit -n 10240\" &gt;&gt; /home/abc/ .bash_profile10240[root@localhost ~]# su - abc[abc@localhost ~]\\$ ulimit -n10240 limit.conf 文件里面本身已有很详细的使用方法，这个下次可以说说。 系统级别将整个操作系统可以使用的 FD 数量更改为 51200 123[root@localhost ~]# echo \"51200\" &gt; /proc/sys/fs/file-max[root@localhost ~]# cat /proc/sys/fs/file-nr1184 0 51200 像这样更改在系统重启后会恢复到默认值，要永久更改可以在 sysctl.conf 文件加上 fs.file-max = 51200 如： 1[root@localhost ~]# echo \"fs.file-max = 51200\" &gt;&gt; /etc/sysctl.conf 获取打开的文件数量linux 的一切皆为文件，那么如何知道系统/应用打开了哪些或是多少个文件呢？很简单，用 lsof 命令就行了，lsof，list open files 的简写，可列出程序或系统正在使用的文件。 获取整个系统打开的文件数量12[root@localhost ~]# lsof |wc -l1864 获取某个用户打开的文件数量12[root@localhost ~]# lsof -u test |wc -l15 获取某个程序打开的文件数量1234[root@localhost ~]# pidof vim3253[root@localhost ~]# lsof -p 3253 |wc -l31 上面所示只是用 lsof 来显示已打开的文件数量，lsof 的功能远不止这些，有兴趣可以 man lsof 看一下 指标 测试值 测试时长 12 小时 15 分 数据量 108000000 平均消费速率（TPS） 6500 条/秒 总结 在这种高峰数据量的情况下，mthinkingdata 会堵塞一部分流量，除非优化利用 Redis 的机制，使服务支持多个 Redis 实例存储","categories":[{"name":"Linux","slug":"Linux","permalink":"http://blog.crazylaw.cn/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.crazylaw.cn/tags/Linux/"}]},{"title":"【Linux】如何获取打开文件和文件描述符数量","slug":"Linux/如何获取打开文件和文件描述符数量","date":"2019-12-10T09:43:43.000Z","updated":"2021-03-20T16:25:01.800Z","comments":true,"path":"2019/12/10/Linux/如何获取打开文件和文件描述符数量/","link":"","permalink":"http://blog.crazylaw.cn/2019/12/10/Linux/%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96%E6%89%93%E5%BC%80%E6%96%87%E4%BB%B6%E5%92%8C%E6%96%87%E4%BB%B6%E6%8F%8F%E8%BF%B0%E7%AC%A6%E6%95%B0%E9%87%8F/","excerpt":"linux 的文件描述符文件描述符（FD:file descriptors），也可以说是文件句柄，当某个程序打开文件时，内核返回相应的文件描述符，程序为了处理该文件必须引用此描述符。文件描述符是一个正整数，用以标明每一个被进程所打开的文件和 socket。最前面的三个文件描述符（0，1，2）分别与标准输入（stdin），标准输出（stdout）和标准错误（stderr）对应，后面打开的文件依此类推对应 3、4…… 。 linux 系统对每个用户、进程、或整个系统的可打开文件描述符数量都有一个限制，一般默认为 1024。当我们在系统或应用的日志中碰到“too many open files”错误记录时，这个其实不是说打开的文件过多，而是打开的文件描述符数量已达到了限制，这时就需要增加文件描述符的数量限制了。","text":"linux 的文件描述符文件描述符（FD:file descriptors），也可以说是文件句柄，当某个程序打开文件时，内核返回相应的文件描述符，程序为了处理该文件必须引用此描述符。文件描述符是一个正整数，用以标明每一个被进程所打开的文件和 socket。最前面的三个文件描述符（0，1，2）分别与标准输入（stdin），标准输出（stdout）和标准错误（stderr）对应，后面打开的文件依此类推对应 3、4…… 。 linux 系统对每个用户、进程、或整个系统的可打开文件描述符数量都有一个限制，一般默认为 1024。当我们在系统或应用的日志中碰到“too many open files”错误记录时，这个其实不是说打开的文件过多，而是打开的文件描述符数量已达到了限制，这时就需要增加文件描述符的数量限制了。 获取系统打开的文件描述符数量12[root@localhost ~]# cat /proc/sys/fs/file-nr1216 0 197787 第一列 1216 ：为已分配的 FD 数量 第二列 0 ：为已分配但尚未使用的 FD 数量 第三列 197787 ：为系统可用的最大 FD 数量 已用 FD 数量＝为已分配的 FD 数量 - 为已分配但尚未使用的 FD 数量。注意，这些数值是系统层面的。 获取进程打开的文件描述符数量1234567[root@localhost ~]# pidof vim3253[root@localhost ~]# ll /proc/3253/fdlrwx------. 1 test test 64 6月 8 18:11 0 -&gt; /dev/pts/0lrwx------. 1 test test 64 6月 8 18:11 1 -&gt; /dev/pts/0lrwx------. 1 test test 64 6月 8 18:11 2 -&gt; /dev/pts/0lrwx------. 1 test test 64 6月 8 18:11 4 -&gt; /home/test/.bash_history.swp 可以看到 vim 进程用了 4 个 FD 更改文件描述符限制当碰到“too many open files”错误时，就需要增加文件描述符的限制数量，系统的默认文件描述符都比较大，一般来说，只需增加用户或进程的就可以了 用户或进程12345[root@localhost ~]# ulimit -n1024[root@localhost ~]# ulimit -n 10240[root@localhost ~]# ulimit -n10240 注意，使用 ulimit 命令更改后只是在当前会话生效，当退出当前会话重新登录后又会回到默认值 1024，要永久更改可以修改文件 /etc/security/limit.conf，如 1[root@localhost ~]#vi /etc/security/limits.conf 加入 “abc hard nofile 10240” abc：用户名，即允许 test 使用 ulimit 命令更改 FD 限制，最大值不超过 10240，更改后 abc 用户的每一个进程（以 abc 用户运行的进程）可打开的 FD 数量为 10240 个 hard：限制类型，有 soft/hard 两种，达到 soft 限制会在系统的日志（一般为/var/log/messages）里面记录一条告警日志，但不影响使用。hard，达到这个限制，有日志且会影响使用。 nofile：限制的内容，nofile - max number of open files 1024 ：值 更改后，退出终端重新登录，用 ulimit 看看有没有生效，如果没生效，可以在 abc 用户的.bash_profile 文件加上 ulimit -n 10240 ,以使用户 abc 每次登录时都会将 FD 最大值更改为 10240，如： 12345[root@localhost ~]#echo \"ulimit -n 10240\" &gt;&gt; /home/abc/ .bash_profile10240[root@localhost ~]# su - abc[abc@localhost ~]\\$ ulimit -n10240 limit.conf 文件里面本身已有很详细的使用方法，这个下次可以说说。 系统级别将整个操作系统可以使用的 FD 数量更改为 51200 123[root@localhost ~]# echo \"51200\" &gt; /proc/sys/fs/file-max[root@localhost ~]# cat /proc/sys/fs/file-nr1184 0 51200 像这样更改在系统重启后会恢复到默认值，要永久更改可以在 sysctl.conf 文件加上 fs.file-max = 51200 如： 1[root@localhost ~]# echo \"fs.file-max = 51200\" &gt;&gt; /etc/sysctl.conf 获取打开的文件数量linux 的一切皆为文件，那么如何知道系统/应用打开了哪些或是多少个文件呢？很简单，用 lsof 命令就行了，lsof，list open files 的简写，可列出程序或系统正在使用的文件。 获取整个系统打开的文件数量12[root@localhost ~]# lsof |wc -l1864 获取某个用户打开的文件数量12[root@localhost ~]# lsof -u test |wc -l15 获取某个程序打开的文件数量1234[root@localhost ~]# pidof vim3253[root@localhost ~]# lsof -p 3253 |wc -l31 上面所示只是用 lsof 来显示已打开的文件数量，lsof 的功能远不止这些，有兴趣可以 man lsof 看一下 指标 测试值 测试时长 12 小时 15 分 数据量 108000000 平均消费速率（TPS） 6500 条/秒 总结 在这种高峰数据量的情况下，mthinkingdata 会堵塞一部分流量，除非优化利用 Redis 的机制，使服务支持多个 Redis 实例存储","categories":[{"name":"Linux","slug":"Linux","permalink":"http://blog.crazylaw.cn/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.crazylaw.cn/tags/Linux/"}]},{"title":"【数据库开发知识】B+树","slug":"数据库开发知识/B+树","date":"2019-12-05T09:43:43.000Z","updated":"2021-03-20T16:25:01.816Z","comments":true,"path":"2019/12/05/数据库开发知识/B+树/","link":"","permalink":"http://blog.crazylaw.cn/2019/12/05/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%BC%80%E5%8F%91%E7%9F%A5%E8%AF%86/B+%E6%A0%91/","excerpt":"前沿B/B+树在数据存储上一直都是最高效的数据结构。B 树的概念比较清晰，但是 B+树的概念却有不同的解释。","text":"前沿B/B+树在数据存储上一直都是最高效的数据结构。B 树的概念比较清晰，但是 B+树的概念却有不同的解释。","categories":[{"name":"数据结构","slug":"数据结构","permalink":"http://blog.crazylaw.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"数据库","slug":"数据结构/数据库","permalink":"http://blog.crazylaw.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"数据库开发知识","slug":"数据库开发知识","permalink":"http://blog.crazylaw.cn/tags/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%BC%80%E5%8F%91%E7%9F%A5%E8%AF%86/"}]},{"title":"【数据库开发知识】基于B+树实现一个高并发数据库","slug":"数据库开发知识/基于B+树实现一个高并发数据库","date":"2019-12-05T09:43:43.000Z","updated":"2021-03-20T16:25:01.817Z","comments":true,"path":"2019/12/05/数据库开发知识/基于B+树实现一个高并发数据库/","link":"","permalink":"http://blog.crazylaw.cn/2019/12/05/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%BC%80%E5%8F%91%E7%9F%A5%E8%AF%86/%E5%9F%BA%E4%BA%8EB+%E6%A0%91%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E9%AB%98%E5%B9%B6%E5%8F%91%E6%95%B0%E6%8D%AE%E5%BA%93/","excerpt":"开发笔记12345678910111213141516171819202122#define M (3) // \b\bB+树的阶数#define LIMIT_M_2 (M &gt;&gt; 1) // M的中点#define True (1)#define False (0)#define INDEX_NAME (\"index\") // 索引文件#define SUCCESS (1)#define ERROR (-1)#define OPEN_MODE (O_RDWR | O_CREAT | O_TRUNC) // 打开文件的模式：可读可写打开 | 若此文件不存在则创建它 | 如果文件已存在，并且以只写或可读可写方式打开，则将其长度截断（Truncate）为0字节#define FILE_MODE (S_IRUSR | S_IWUSR | S_IRGRP | S_IROTH) // 文件的权限：660typedef int KeyType; // 关键字的数据类型typedef off_t Record; // 索引所对应的值typedef struct BPTNode&#123; int isLeaf; // 是否是叶子节点 int keynum; // 关键字个数 KeyType key[M + 1]; // 存放的关键字的连续内存 struct BPTNode *ptr[M + 1];// 关键字子树的连续内存 struct BPTNode *next; Record value[M + 1]; // 关键字对应的value，当节点为叶子节点的时候才有效&#125;BPTNode, *BPTree; 我们看到 BPTNode 是我们的节点结构体，这个结构体占用的内存大小我们计算一下： 记住内存对齐的原则，并且指针占用 8 个字节，off_t 一般是 64 位，占用 8 个字节 14(int)+4(int)+4(KeyType)*4(M+1)+4(*pts)*4(M+1)+4(*next)+8(Record)*4(M+1) &#x3D; 4+4+16+32+8+32 &#x3D; 96bytes","text":"开发笔记12345678910111213141516171819202122#define M (3) // \b\bB+树的阶数#define LIMIT_M_2 (M &gt;&gt; 1) // M的中点#define True (1)#define False (0)#define INDEX_NAME (\"index\") // 索引文件#define SUCCESS (1)#define ERROR (-1)#define OPEN_MODE (O_RDWR | O_CREAT | O_TRUNC) // 打开文件的模式：可读可写打开 | 若此文件不存在则创建它 | 如果文件已存在，并且以只写或可读可写方式打开，则将其长度截断（Truncate）为0字节#define FILE_MODE (S_IRUSR | S_IWUSR | S_IRGRP | S_IROTH) // 文件的权限：660typedef int KeyType; // 关键字的数据类型typedef off_t Record; // 索引所对应的值typedef struct BPTNode&#123; int isLeaf; // 是否是叶子节点 int keynum; // 关键字个数 KeyType key[M + 1]; // 存放的关键字的连续内存 struct BPTNode *ptr[M + 1];// 关键字子树的连续内存 struct BPTNode *next; Record value[M + 1]; // 关键字对应的value，当节点为叶子节点的时候才有效&#125;BPTNode, *BPTree; 我们看到 BPTNode 是我们的节点结构体，这个结构体占用的内存大小我们计算一下： 记住内存对齐的原则，并且指针占用 8 个字节，off_t 一般是 64 位，占用 8 个字节 14(int)+4(int)+4(KeyType)*4(M+1)+4(*pts)*4(M+1)+4(*next)+8(Record)*4(M+1) &#x3D; 4+4+16+32+8+32 &#x3D; 96bytes \b\b## 构建 B+树 123456789101112131415161718192021222324252627282930313233343536373839404142// 从索引文件中读取数据，创建b+树extern BPTree CreatBPTree(BPTree T)&#123; int fd; int i = 0, ret = -1; unsigned long file_len = -1; struct stat statbuff; KeyType temp = 0; KeyType *q; Record t_record = 0; Record *p; q = &amp;temp; p = &amp;t_record; file_len = get_file_size(INDEX_NAME); if ((fd = open(INDEX_NAME, O_RDONLY)) == -1) &#123; printf(\"Open index file failled!\\n\"); return NULL; &#125; while (i &lt; file_len) &#123; if ((ret = pread(fd, q, 4, i)) == -1) &#123; close(fd); return NULL; &#125; if ((ret = pread(fd, p, 4, i + 4)) == -1) &#123; close(fd); return NULL; &#125; T = Insert(T, *q, *p); i = i + 8; &#125; close(fd); return T;&#125; 以只读的方式打开索引未见，用于构建 B+树。","categories":[{"name":"数据结构","slug":"数据结构","permalink":"http://blog.crazylaw.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"数据库","slug":"数据结构/数据库","permalink":"http://blog.crazylaw.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"数据库开发知识","slug":"数据库开发知识","permalink":"http://blog.crazylaw.cn/tags/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%BC%80%E5%8F%91%E7%9F%A5%E8%AF%86/"}]},{"title":"【消息队列】- kafka-swoole","slug":"消息队列/kafka-swoole","date":"2019-11-04T02:37:40.000Z","updated":"2021-03-20T16:25:01.819Z","comments":true,"path":"2019/11/04/消息队列/kafka-swoole/","link":"","permalink":"http://blog.crazylaw.cn/2019/11/04/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka-swoole/","excerpt":"前言目前 kafka 客户端在 php 中比较出名的仅有 2 个，这 2 个项目都有各自的利弊。在这里我选择几个来列一下 weiboad/kafka-php 协议非结构化封装，自定义性较强，不好维护 目前的 API 在消费者中由于单例设计的原因，不允许在消费者中生产消息 不支持多种压缩协议 单进程“协程式”逻辑，数据未实现分离，容易堵塞消费 arnaud-lb/php-rdkafka 协议非结构化封装，自定义性较强，不好维护 不支持多种压缩协议 利用多线程，但是 PHP 对多线程对支持并不友好，相对于用 swoole 而言，劣势较为明显，容易出 bug，且维护性不高 基于以上几点，我们在基于swoole-4.3以上重新开发了kafka-swoole项目，优点如下： 多进程多核处理 支持 kafka 多个 sink 方式，实现拉取 kafka 数据和业务逻辑分离，从而不堵塞数据拉取 首个支持多种压缩方式（normal(不压缩)/gzip/snappy）的 php 客户端，从而在大吞吐对情况下减少带宽占用，提高传输速率 协议封装采用 OOP 的方式，结构化了协议的封装，利于维护和封装，对协议利用反射来统一封包和解包 利用协程来实现单个进程中对异步逻辑处理 提供了 runtime 的 rpc 命令，实时获取 kafka 成员进程对内部数据，实时查看消费情况 提供了 kafka 命令，方便获取 kafka 服务相关信息和 kafka 相关操作 在成员进程挂掉的情况下，记录错误信息，自动拉起。 对于常驻进程，不排除业务逻辑写出了内存泄漏的情况，所以所有的子进程都带有内存临界值重启机制来释放内存 主进程退出的情况下，发起了离开消费者组的请求，使得 kafka 能快速响应消费者组最新状态，从而更好平滑重新加入消费者","text":"前言目前 kafka 客户端在 php 中比较出名的仅有 2 个，这 2 个项目都有各自的利弊。在这里我选择几个来列一下 weiboad/kafka-php 协议非结构化封装，自定义性较强，不好维护 目前的 API 在消费者中由于单例设计的原因，不允许在消费者中生产消息 不支持多种压缩协议 单进程“协程式”逻辑，数据未实现分离，容易堵塞消费 arnaud-lb/php-rdkafka 协议非结构化封装，自定义性较强，不好维护 不支持多种压缩协议 利用多线程，但是 PHP 对多线程对支持并不友好，相对于用 swoole 而言，劣势较为明显，容易出 bug，且维护性不高 基于以上几点，我们在基于swoole-4.3以上重新开发了kafka-swoole项目，优点如下： 多进程多核处理 支持 kafka 多个 sink 方式，实现拉取 kafka 数据和业务逻辑分离，从而不堵塞数据拉取 首个支持多种压缩方式（normal(不压缩)/gzip/snappy）的 php 客户端，从而在大吞吐对情况下减少带宽占用，提高传输速率 协议封装采用 OOP 的方式，结构化了协议的封装，利于维护和封装，对协议利用反射来统一封包和解包 利用协程来实现单个进程中对异步逻辑处理 提供了 runtime 的 rpc 命令，实时获取 kafka 成员进程对内部数据，实时查看消费情况 提供了 kafka 命令，方便获取 kafka 服务相关信息和 kafka 相关操作 在成员进程挂掉的情况下，记录错误信息，自动拉起。 对于常驻进程，不排除业务逻辑写出了内存泄漏的情况，所以所有的子进程都带有内存临界值重启机制来释放内存 主进程退出的情况下，发起了离开消费者组的请求，使得 kafka 能快速响应消费者组最新状态，从而更好平滑重新加入消费者 服务架构 kafka-swoole 的大体架构图如上，可以笼统的概括为 2 个部分组成，分别的 kafka-client 和consumer/sinker。 kafka-client主要是负责从 kafka 服务中拉取数据，consumer/sinker主要负责消费数据。 consumer和sinker 的区别在于，数据是否经过了中间存储介质，如果不经过中间存储介质的话，那么就是consumer，就是我们接下来会在配置说到的KAFKA_MESSAGE_STORAGE=Directly，这也将会是我们是否启用额外的 sinker 进程的关键所在，如果它被选择之后，client 和消费者将会在同一个进程中处理，所以如果你的业务逻辑比较耗时的话，势必会影响数据拉取速度。如果采用中间存储介质的话，那么目前支持 2 种存储介质，分别是Redis,File。目前来说更加推荐采用Redis的方式，虽然借助了第三方服务，但是它在其他各个方面都比 File 好，就举一个例子来说，如果用 File 作为存储介质的话，File 的话还需要自己做磁盘备份来保证数据不会丢失。 runtime 的 rpc 命令的实现细节本项目中，所谓的 rpc 的命名，主要是由主进程想 kafka-client 进程发起 rpc 请求的行为。用于获取 kakfa-client 实时状况而存在的一种方式，所以，这里我们需要实现进程间的通信，这里的选型，我们选择选择了AF_UNIX的方式来进行通信，而没有使用端口服务的方式，是因为，我们的 rpc 请求不对外提供服务，并且是针对本项目，并且AF_UNIX也让我们通信更加高效。 由于我们这里需要实现三方通信，所有发起请求的命令独占一个UNIX_FILE，我们主进程占一个UNIX_FILE，每个 kafka-client 同样占一个UNIX_FILE，实现三方通信。 rpc 协议协议本身十分简单，协议头有 4 个字节的无符号整形封包协议主体的长度，协议主体是由role,rpc,method一起组成的 json_encode 后的字符串。 1234567$cmd = [ 'role' =&gt; $external, 'rpc' =&gt; $rpc, 'method' =&gt; $method, ];$data = json_encode($cmd);$package = pack('N', strlen($data)) . $data; 处理流程在 core 项目 src/RPC中，是我们的所有的 RPC 服务处理类，类有 2 种类型的方法，分别是getXxx和onXxx，它们的区别在于 getXxx：各个 kafka-client 进程处理该 RPC 请求处理的逻辑 onXxx：负责主进程节奏各个 kafka-client 进程处理后的结果，进行汇总处理 sinker 相关的内容当我们使用kafka-swoole项目的时候，要如何使用呢，其实业务方并不需要过多的关注 kafka-client 的细节，因为它的作用仅仅是拉取数据到存储介质，我们写业务的时候，其实主要都是在sinker中写业务。 我们在 app/Controller中有一个SinkerController，里面的hanlder(array $messages)就是接收从存储介质中读取回来的数据，但是这里需要注意的是，从存储介质中拉取出来的时候是否被消费这个问题，这里不管是Redis还是File，都没有自带的一个ack机制让我们保证数据正确被消费了。但是，我们迂回得实现了这个方式，利用的就是 redis 的可靠性队列。 通过上图所示的方式，我们的 ack 机制通过业务方来确认，所以，该方法的返回结果为一个数组，数组下标对应每条消息的状态，当状态为true的时候代表被顺利消费了，消息会在 processing的队列中被移除，如果被标记为了false，那么消息将不会被移除，对于失败的消息，目前我们并没有做任何的处理，因为对于部分消息，可能是重试的情况，部分消息，可能是真的错误的情况。因此这部分消息如果重新消费的话，需要业务方确认，并且通过命令来把数据拉回到pending队列中来实现重新消费的情况。如果是错误的话，就需要通过命令来清空队列。 kafka 相关的命令的提供12345678910111213141516171819202122php bin/kafka-clientConsole ToolUsage: command [options] [arguments]Options: -h, --help Display this help message -q, --quiet Do not output any message -V, --version Display this application version --ansi Force ANSI output --no-ansi Disable ANSI output -n, --no-interaction Do not ask any interactive question -v|vv|vvv, --verbose Increase the verbosity of messages: 1 for normal output, 2 for more verbose output and 3 for debugAvailable commands: help Displays help for a command kafka.describeGroups See consumer group details kafka.produce Send a message list Lists commands rpc Built-in runtime RPC command start Start Kafka-Swoole 带有 kafka 前缀的命令，都是直接请求 kafka 相关的 API 的命令。 kafka.describeGroups用于查询某个消费者组订阅的 topic 的详情 1234567891011121314151617181920php bin/kafka-client kafka.describeGroups -hDescription: See consumer group detailsUsage: kafka.describeGroups [options]Options: -t, --topic=TOPIC Which topic is subscribed by the consumer group? -g, --group=GROUP Which consumer group? -h, --help Display this help message -q, --quiet Do not output any message -V, --version Display this application version --ansi Force ANSI output --no-ansi Disable ANSI output -n, --no-interaction Do not ask any interactive question -v|vv|vvv, --verbose Increase the verbosity of messages: 1 for normal output, 2 for more verbose output and 3 for debugHelp: See consumer group details... 例子如下图： 12345678910111213141516171819202122php bin/kafka-client kafka.describeGroups -t mulog_clean_24 -g kafka-swooleDescribeGruops-BaseInfo======================= -------------- ------------ -------------- -------------- groupId groupState protocolType protocolData -------------- ------------ -------------- -------------- kafka-swoole Stable consumer Range -------------- ------------ -------------- -------------- --------------------------------------------------- -------------- -------------- ---------------- ----------- memberId clientId clientHost topcic paritions --------------------------------------------------- -------------- -------------- ---------------- ----------- kafka-swoole-44857c49-b019-439b-90dd-d71112b2c01e kafka-swoole /192.167.8.2 mulog_clean_24 0,1 --------------------------------------------------- -------------- -------------- ---------------- ----------- --------------------------------------------------- -------------- -------------- ---------------- ----------- memberId clientId clientHost topcic paritions --------------------------------------------------- -------------- -------------- ---------------- ----------- kafka-swoole-5714cd77-a0dd-4d29-aa20-718f9d713908 kafka-swoole /192.167.8.2 mulog_clean_24 2,3 --------------------------------------------------- -------------- -------------- ---------------- ----------- kafka.produce用于生产一条消息到某个 topic 中，支持多种压缩方式，支持多种生产消息策略 生产者 partition 策略： -p/--partition= 采用指定 partition 的策略 -k/--key= 采用通过 key 的哈希到 partition 策略 以上两个都不选，则采用 随机发送 partition 策略 生产者压缩方式： -c/--compress= 采用的压缩方式：0=不压缩/1=采用 gzip 压缩/2=使用 snappy 压缩/3=使用 lz4 压缩 不填写，则不进行压缩 12345678910111213141516171819202122232425php bin/kafka-client kafka.produce --helpDescription: Send a messageUsage: kafka.produce [options] [--] &lt;message&gt;Arguments: message The message you wish to send.Options: -t, --topic[=TOPIC] Which is the topic you want to send? -p, --partition[=PARTITION] Which is the topic you want to send to partition? -k, --key[=KEY] Which is the topic you want to send to partition by key? -c, --compress[=COMPRESS] Which one do you want to compress: 0: normal 1:gzip 2:snappy 3:lz4 -h, --help Display this help message -q, --quiet Do not output any message -V, --version Display this application version --ansi Force ANSI output --no-ansi Disable ANSI output -n, --no-interaction Do not ask any interactive question -v|vv|vvv, --verbose Increase the verbosity of messages: 1 for normal output, 2 for more verbose output and 3 for debugHelp: This command will help you send separate messages to a topic... 例子如下图： 123456789101112131415161718192021222324252627282930php bin/kafka-client kafka.produce -t test_topic_A -- 'This is my test'array(3) &#123; [\"responses\"]=&gt; array(1) &#123; [0]=&gt; array(2) &#123; [\"topic\"]=&gt; string(12) \"test_topic_A\" [\"partition_responses\"]=&gt; array(1) &#123; [0]=&gt; array(3) &#123; [\"partition\"]=&gt; int(0) [\"errorCode\"]=&gt; int(0) [\"baseOffset\"]=&gt; int(0) &#125; &#125; &#125; &#125; [\"responseHeader\"]=&gt; array(1) &#123; [\"correlationId\"]=&gt; int(0) &#125; [\"size\"]=&gt; int(40)&#125; 目前输出方式并没有优化，后续进行输出样式优化 rpc提供实时获取 kafka-client 成员的内部情况 123456789101112131415161718192021222324252627php bin/kafka-client rpc -hDescription: Built-in runtime RPC commandUsage: rpc &lt;type&gt;Arguments: type which you want to execute command?Options: -h, --help Display this help message -q, --quiet Do not output any message -V, --version Display this application version --ansi Force ANSI output --no-ansi Disable ANSI output -n, --no-interaction Do not ask any interactive question -v|vv|vvv, --verbose Increase the verbosity of messages: 1 for normal output, 2 for more verbose output and 3 for debugHelp: The following are the built-in RPC command options： kafka_lag offset_checker block_size member_leader metadata_brokers metadata_topics 例子如下图： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849php bin/kafka-client rpc kafka_lag1000php bin/kafka-client rpc offset_checker -------------- ----------- ---------------- ------------------ ----------------- topic partition current-offset kafka-max-offset remaining-count -------------- ----------- ---------------- ------------------ ----------------- kafka-swoole 2 50223 50223 0 kafka-swoole 3 70353 70353 0 kafka-swoole 0 52395 52395 0 kafka-swoole 1 50407 50407 0 -------------- ----------- ---------------- ------------------ -----------------php bin/kafka-client rpc block_size254php bin/kafka-client rpc member_leader --------------------------------------------------- consumer-group-leaderId --------------------------------------------------- kafka-swoole-da43c9a0-b12d-46df-9941-ee80456ec9a2 --------------------------------------------------- --------------------------------------------------- consumer-group-membersId --------------------------------------------------- kafka-swoole-6080eb8e-3bfb-4be0-a923-037bb99a2666 kafka-swoole-da43c9a0-b12d-46df-9941-ee80456ec9a2 --------------------------------------------------- php bin/kafka-client rpc metadata_brokers --------- --------- ------ node-id host port --------- --------- ------ 1003 mkafka3 9092 1004 mkafka4 9092 1001 mkafka1 9092 1002 mkafka2 9092 --------- --------- ------ php bin/kafka-client rpc metadata_topics -------------- ----------- ----------- --------------- ----------- topic partition leader-id replica-nodes isr-nodes -------------- ----------- ----------- --------------- ----------- kafka-swoole 2 1001 1001,1004 1001,1004 kafka-swoole 1 1004 1004,1003 1004,1003 kafka-swoole 3 1002 1002,1001 1002,1001 kafka-swoole 0 1003 1003,1002 1002,1003 -------------- ----------- ----------- --------------- ----------- kafka 客户端启动到拉取数据流程以下列出所有的必备请求，到Fetch请求位置，都是同步请求，OffsetCommit和HeartBeat都是分开的独立请求 Metadata 获取元数据信息 FindCoordinator 让 kafka 分配消费者组协调器入口 JoinGroup 告诉 kafka 当前 kafka-client 需要加入消费者组 SyncGroup 同步消费者组消息（如果发起请求的是 leader 的话，则需要带上所有消费者组成员所需要订阅 topic 和 partition 的信息，所以由 leader 分配） ListsOffsets 获取当前 topic 在 kafka/zookeeper 中存储的 offset 的最大值和最小值 OffsetFetch 获取当前消费者组在 kafka/zookeeper 中存储的 offset 的值 Fetch 拉取数据 OffsetCommit 提交当前消费者组处理到的 offset HeartBeat \b 心跳请求 Rebalance 流程发送加入组请求（Rebalance 流程）消费者首次加入 group 也可以认为是 Rebalance 的一种，其中包含了两类请求：JoinGroup 和 SyncGroup 请求。我们先看一下两次请求的流程： 当组内成员加入 group 时，它会向协调者发送一个 JoinGroup 请求。请求中会将自己要订阅的 Topic 上报，这样协调者就可以收集到所有成员的订阅信息。收集完订阅信息之后，通常情况下，第一个发送 JoinGroup 请求的成员将会自动称为 Leader。所有后棉其他成员加入的时候，就发生了 Rebalance 的情况了。 新成员入组 组成员主动离组 组成员崩溃离组 Rebalance 时组内成员需要提交 offset Consumer 分区分配策略RangeAssignorRangeAssignor 是按照 Topic 的维度进行分配的，也就是说按照 Topic 对应的每个分区平均的按照范围区段分配给 Consumer 实例。这种分配方案是按照 Topic 的维度去分发分区的，此时可能会造成先分配分区的 Consumer 实例的任务过重。 从上图的最终分配结果看来，因为是按照 Topic A 和 Topic B 的维度进行分配的。对于 Topic A 而言，有 2 个消费者，此时算出来 C0 得到 2 个分区，C1 得到 1 个分区；对于 Topic B 的维度也是一样，也就是先分配的 Consumer 会得到的更多，从而造成倾斜。需要注意一点的是，RangeAssignor 是按照范围截断分配的，不是按顺序分发的。 RoundRobinAssignorRoundRobinAssignor 中文可以翻译为轮询，也就是顺序一个一个的分发。其中代码里面的大概逻辑如下：拿到组内所有 Consumer 订阅的 TopicPartition，按照顺序挨个分发给 Consumer，此时如果和当前 Consumer 没有订阅关系，则寻找下一个 Consumer。从上面逻辑可以看出，如果组内每个消费者的订阅关系是同样的，这样 TopicPartition 的分配是均匀的。 当组内每个消费者订阅的 Topic 是不同的，这样就可能会造成分区订阅的倾斜。 StickyAssignorStickyAssignor 是 Kafka Java 客户端提供的 3 中分配策略中最复杂的一种，从字面上可以看出是具有“粘性”的分区策略。Kafka 从 0.11 版本开始引入，其主要实现了两个功能： 1、主题分区的分配要尽可能的均匀。 2、当 Rebalance 发生时，尽可能保持上一次的分配方案。 当然，当上面两个条件发生冲突是，第一个提交件要优先于第二个提交，这样可以使分配更加均匀。下面我们看一下官方提供的 2 个例子，来看一下 RoundRoubin 和 Sticky 两者的区别。 从上面我们可以看出，初始状态各个 Consumer 订阅是相同的时候，并且主题的分区数也是平均的时候，两种分配方案的结果是相同的。但是当 Rebalance 发生时，可能就会不太相同了，加入上面的 C1 发生了离组操作，此时分别会有下面的 Rebalance 结果： 从上面 Rebalance 后的结果可以看出，虽然两者最后分配都是均匀的，但是 RoundRoubin 完全是重新分配了一遍，而 Sticky 则是在原先的基础上达到了均匀的状态。 下面我们再看一个 Consumer 订阅主题不均匀的例子。 从上面的订阅关系可以看出，Consumer 的订阅主题个数不均匀，并且各个主题的分区数也是不相同的。此时两种分配方案的结果有了较大的差异，但是相对来说 Sticky 方式的分配相对来说是最合理的。下面我们看一下 C1 发生离组时，Rebalance 之后的分配结果。 从上面结果可以看出，RoundRoubin 的方案在 Rebalance 之后造成了严重的分配倾斜。因此在生产上如果想要减少 Rebalance 的开销，可以选用 Sticky 的分区分配策略。 协议封装细节规则官方协议说明链接：Apache-kafka-protocol非官方协议说明链接：非官方协议说明 项目中协议的封装在我们的 core 项目中的src/Protocol里面。整个树形图如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120rpc/Protocol├── AbstractRequest.php├── AbstractRequestOrResponse.php├── AbstractResponse.php├── CommonRequest.php├── CommonResponse.php├── Request│ ├── Common│ │ └── RequestHeader.php│ ├── CreateTopics│ │ ├── AssignmentsCreateTopics.php│ │ ├── ConfigsCreateTopics.php│ │ └── TopicsCreateTopics.php│ ├── CreateTopicsRequest.php│ ├── DescribeGroupsRequest.php│ ├── Fetch│ │ ├── PartitionsFetch.php│ │ └── TopicsFetch.php│ ├── FetchRequest.php│ ├── FindCoordinatorRequest.php│ ├── HeartbeatRequest.php│ ├── JoinGroup│ │ ├── ProtocolMetadataJoinGroup.php│ │ ├── ProtocolNameJoinGroup.php│ │ ├── ProtocolsJoinGroup.php│ │ └── TopicJoinGroup.php│ ├── JoinGroupRequest.php│ ├── LeaveGroupRequest.php│ ├── ListOffsets│ │ ├── PartitionsListsOffsets.php│ │ └── TopicsListsOffsets.php│ ├── ListOffsetsRequest.php│ ├── Metadata│ │ └── TopicMetadata.php│ ├── MetadataRequest.php│ ├── OffsetCommit│ │ ├── PartitionsOffsetCommit.php│ │ └── TopicsOffsetCommit.php│ ├── OffsetCommitRequest.php│ ├── OffsetFetch│ │ ├── PartitionsOffsetFetch.php│ │ └── TopicsOffsetFetch.php│ ├── OffsetFetchRequest.php│ ├── Produce│ │ ├── DataProduce.php│ │ ├── MessageProduce.php│ │ ├── MessageSetProduce.php│ │ └── TopicDataProduce.php│ ├── ProduceRequest.php│ ├── SyncGroup│ │ ├── AssignmentsSyncGroup.php│ │ ├── GroupAssignmentsSyncGroup.php│ │ ├── MemberAssignmentsSyncGroup.php│ │ └── PartitionAssignmentsSyncGroup.php│ └── SyncGroupRequest.php├── Response│ ├── Common│ │ └── ResponseHeader.php│ ├── CreateTopics│ │ └── TopicsCreateTopics.php│ ├── CreateTopicsResponse.php│ ├── DescribeGroups│ │ ├── GroupsDescribeGroups.php│ │ ├── MembersAssignmentDescribeGroups.php│ │ ├── MembersDescribeGroups.php│ │ ├── MembersMetadataDescribeGroups.php│ │ └── PartitionsAssignmentDescribeGroups.php│ ├── DescribeGroupsResponse.php│ ├── Fetch│ │ ├── MessageFetch.php│ │ ├── MessageSetFetch.php│ │ ├── PartitionHeaderFetch.php│ │ ├── PartitionResponsesFetch.php│ │ └── ResponsesFetch.php│ ├── FetchResponse.php│ ├── FindCoordinatorResponse.php│ ├── HeartbeatResponse.php│ ├── JoinGroup│ │ ├── MembersJoinGroup.php│ │ ├── ProtocolMetadataJoinGroup.php│ │ └── TopicJoinGroup.php│ ├── JoinGroupResponse.php│ ├── LeaveGroupResponse.php│ ├── ListOffsets│ │ ├── PartitionsResponsesListOffsets.php│ │ └── ResponsesListOffsets.php│ ├── ListOffsetsResponse.php│ ├── Metadata│ │ ├── BrokerMetadata.php│ │ ├── PartitionMetadata.php│ │ └── TopicMetadata.php│ ├── MetadataResponse.php│ ├── OffsetCommit│ │ ├── PartitionsOffsetCommit.php│ │ └── TopicOffsetCommit.php│ ├── OffsetCommitResponse.php│ ├── OffsetFetch│ │ ├── PartitionsResponsesOffsetFetch.php│ │ └── ResponsesOffsetFetch.php│ ├── OffsetFetchResponse.php│ ├── Produce│ │ ├── PartitionResponsesProduce.php│ │ └── ResponsesProduce.php│ ├── ProduceResponse.php│ ├── SyncGroup│ │ ├── MemberAssignmentsSyncGroup.php│ │ └── PartitionAssignmentsSyncGroup.php│ └── SyncGroupResponse.php├── TraitStructure│ ├── ToArrayTrait.php│ └── ValueTrait.php└── Type ├── AbstractType.php ├── Arrays32.php ├── Bytes32.php ├── Int16.php ├── Int32.php ├── Int64.php ├── Int8.php └── String16.php 我们先说明一下以此目录为根目录到情况下，一级文件（非目录）的作用： AbstractRequestOrResponse.php (这个是无论是 request 还是 response 都必须有都字段：字节长度) AbstractRequest.php (继承 AbstractRequestOrResponse，包含了请求协议头，封包核心逻辑) AbstractResponse.php （继承 AbstractRequestOrResponse，包含了响应协议头，解包核心逻辑） CommonRequest.php （继承 AbstractRequest，用于辅助封装协议中特殊的字段处理方式） CommonResponse.php（继承 AbstractResponse，用于辅助解开协议中特殊的字段处理方式） Type在这个目录中，全部都是数据类型，我们整个协议的所有字段的定义都来自于此目录的数据类型，目前用到的数据类型有 Arrays32 (数组类型，占 32bits，PHP 中的标记为：N) Bytes32 (字节类型，占 32bits，PHP 中的标记为：N) String16 (字符串类型，占 16bits，PHP 中的标记为：n) Int8 (整型类型，占 8bits，PHP 中的标记为：C) Int16 (整型类型，占 16bits，PHP 中的标记为：n) Int32 (整型类型，占 32bits，PHP 中的标记为：N) Int64 (整型类型，占 64bits，PHP 中的标记为：N2) Request在这个目录中，全部都是请求协议体的内容，以此为根目录下的一级文件（非目录），都是对应的每一个 API 协议，它们全部都继承AbstractRequest类，并且属性必须为private修饰符，更重要的是，不要忘记了写上关键信息，就是这个属性的数据类型的注解(@var Xxx $ooo)，这将会我们封包关键所在，这个数据类型，都来自于Type目录。而根目录下的二级目录则是对应的协议结构中的子结构，规则和之前描述的大体一致，需要注意的是，这个时候子结构并没有继承了任何父类。 其中每个类中可能存在onXxx的方法，这个时候AbstractRequest并不会正常的解析这个类中的字段，而是去执行每个类中特定的回调方法，进行定制封包。 Response在这个目录中，全部都是请求协议体的内容，以此为根目录下的一级文件（非目录），都是对应的每一个 API 协议，它们全部都继承AbstractResponse类，并且属性必须为private修饰符，更重要的是，不要忘记了写上关键信息，就是这个属性的数据类型的注解(@var Xxx $ooo)，这将会我们封包关键所在，这个数据类型，都来自于Type目录。而根目录下的二级目录则是对应的协议结构中的子结构，规则和之前描述的大体一致，需要注意的是，这个时候子结构并没有继承了任何父类。 其中每个类中可能存在onXxx的方法，这个时候AbstractRespose并不会正常的解析这个类中的字段，先执行回调方法，进行定制解包。通过返回 true 或者返回 false 来判断是否需要继续正常解析这个字段，如果返回 true 则是跳过这个字段不再解析，说明了这个字段在onXxx中可以解析完毕了。否则将继续解析这个字段。 TraitStructure在这个目录中，都是一些复用体（trait），目前来说用上的只有ToArrayTrait，这个结构体主要的职责是把对象返回成数组结构。 压缩方式详解经过我对上文中提到对 2 个最多 star 的仓库的考察，发现这 2 个仓库要么是压缩方式不支持，要么就是协议方式封装错误。 所以，在这里可以很对压缩方式做一些说明。 正常情况下，我们的数据是可以不压缩的，但是，当我们的生产者在决定用压缩数据的方式来传输数据的时候，就代表你这的消费端就必须支持对应的解压方式。 在压缩的情况下，我们的数据量可以被压缩存储在 kafka 中，不但可以节约服务器的磁盘空间，而且还减少了带宽的占用，提高了传输速率。所以在生产者和消费者的机器 CPU 有条件的情况下，最好还是对数据进行压缩传输。 由于我们目前只对 VERSION=0 的协议进行封装，所以用此来说明。 1234567891011121314MessageSet (Version: 0) &#x3D;&gt; [offset message_size message] offset &#x3D;&gt; INT64 message_size &#x3D;&gt; INT32 message &#x3D;&gt; crc magic_byte attributes key value crc &#x3D;&gt; INT32 magic_byte &#x3D;&gt; INT8 attributes &#x3D;&gt; INT8 bit 0~2: 0: no compression 1: gzip 2: snappy bit 3~7: unused key &#x3D;&gt; BYTES value &#x3D;&gt; BYTES 我们看到这个协议说明中，message 结构体中包含了以下几个关键内容： crc magic_byte attributes 第[0-2]bit 代表着压缩方式 第[3-7]bit 留空 key value 所以我们在解协议的时候，从 attributes 的第[0-2]个 bit 中可以知道 value 的数据是否需要进行压缩或者解压。 在代码层面中就是： 压缩： 123456789101112131415161718192021222324252627282930313233343536373839/** * @param $protocol * * @throws \\Kafka\\Exception\\ProtocolTypeException * @throws \\ReflectionException */public function onMessageSetSize(&amp;$protocol)&#123; if (($this-&gt;getMessage()-&gt;getAttributes()-&gt;getValue() &amp; 0x07) !== CompressionCodecEnum::NORMAL) &#123; $wrapperMessage = clone $this-&gt;getMessage(); $this-&gt;getMessage()-&gt;setAttributes(Int8::value(CompressionCodecEnum::NORMAL)) -&gt;setValue($this-&gt;getMessage() -&gt;getValue()); $commentRequest = new CommonRequest(); $data = $commentRequest-&gt;packProtocol(MessageProduce::class, $this-&gt;getMessage()); $data = pack(Int32::getWrapperProtocol(), strlen($data)) . $data; $left = 0xffffffff00000000; $right = 0x00000000ffffffff; $l = (-1 &amp; $left) &gt;&gt; 32; $r = -1 &amp; $right; $data = pack(Int64::getWrapperProtocol(), $l, $r) . $data; if (($wrapperMessage-&gt;getAttributes()-&gt;getValue() &amp; 0x07) === CompressionCodecEnum::SNAPPY) &#123; $compressValue = snappy_compress($data); &#125; elseif (($wrapperMessage-&gt;getAttributes()-&gt;getValue() &amp; 0x07) === CompressionCodecEnum::GZIP) &#123; $compressValue = gzencode($data); &#125; else &#123; throw new RuntimeException('not support lz4'); &#125; $wrapperMessage-&gt;setKey(Bytes32::value(''))-&gt;setValue(Bytes32::value($compressValue)); $this-&gt;setMessage($wrapperMessage); &#125; $commentRequest = new CommonRequest(); $data = $commentRequest-&gt;packProtocol(MessageProduce::class, $this-&gt;getMessage()); $this-&gt;setMessageSetSize(Int32::value(strlen($data))); $protocol .= pack(Int32::getWrapperProtocol(), $this-&gt;getMessageSetSize()-&gt;getValue());&#125; 解压： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384/** * @param $protocol * * @return bool * @throws \\Kafka\\Exception\\ProtocolTypeException * @throws \\ReflectionException */public function onRecordSet(&amp;$protocol)&#123; $recordSet = []; while (is_string($protocol) &amp;&amp; strlen($protocol) &gt; 0) &#123; $commonResponse = new CommonResponse(); $instance = new MessageSetFetch(); $commonResponse-&gt;unpackProtocol(MessageSetFetch::class, $instance, $protocol); // Insufficient reading sub-section, the message is put on the next read if ($instance-&gt;getMessage()-&gt;getCrc()-&gt;getValue() === null) &#123; continue; &#125; // Internal decompression if ($instance-&gt;getMessage()-&gt;getAttributes()-&gt;getValue() !== CompressionCodecEnum::NORMAL) &#123; $buffer = $instance-&gt;getMessage()-&gt;getValue()-&gt;getValue(); $commonResponse-&gt;unpackProtocol(MessageSetFetch::class, $instance, $buffer); &#125; $recordSet[] = $instance; &#125; $this-&gt;setRecordSet($recordSet); return true;&#125;/** * @param $protocol * * @return bool */public function onValue(&amp;$protocol)&#123; if (($this-&gt;getAttributes()-&gt;getValue() &amp; 0x07) === CompressionCodecEnum::SNAPPY) &#123; /* snappy-java adds its own header (SnappyCodec) which is not compatible with the official Snappy implementation. 8: magic, 4: version, 4: compatible followed by any number of chunks: 4: length ...: snappy-compressed data. */ $protocol = substr($protocol, 20); $ret = []; SnappyDecompression: if (!is_string($protocol) || (is_string($protocol) &amp;&amp; strlen($protocol) &lt;= 0)) &#123; $ret = implode('', $ret); &#125; else &#123; $buffer = substr($protocol, 0, ProtocolTypeEnum::B32); $protocol = substr($protocol, ProtocolTypeEnum::B32); $len = unpack(ProtocolTypeEnum::getTextByCode(ProtocolTypeEnum::B32), $buffer); $len = is_array($len) ? array_shift($len) : $len; $data = substr($protocol, 0, $len); $protocol = substr($protocol, $len); $ret[] = snappy_uncompress($data); goto SnappyDecompression; &#125; $this-&gt;setValue(Bytes32::value($ret)); return true; &#125; else if (($this-&gt;getAttributes()-&gt;getValue() &amp; 0x07) === CompressionCodecEnum::GZIP) &#123; $buffer = substr($protocol, 0, ProtocolTypeEnum::B32); $protocol = substr($protocol, ProtocolTypeEnum::B32); $len = unpack(ProtocolTypeEnum::getTextByCode(ProtocolTypeEnum::B32), $buffer); $len = is_array($len) ? array_shift($len) : $len; $data = substr($protocol, 0, $len); $protocol = substr($protocol, $len); $this-&gt;setValue(Bytes32::value(gzdecode($data))); return true; &#125; // Normal return false;&#125; 在压缩的时候，我们需要注意的是，在对数据进行压缩处理的时候，attributes属性必须设置为 0，这是和解压逻辑相对应的。只要在压缩完毕之后，二次封装的时候，attributes 在第二次赋值的时候才会设置成真正的值。 所以这里，我们通过$this-&gt;getAttributes()-&gt;getValue() &amp; 0x07的方式，来判断当前数据是否需要解压。需要注意的是，当采用 snappy/zip 的压缩方式的时候,数据压缩了，细节被屏蔽了起来，消息更加的安全了，我们在第一次对messageSet解压后得出来后来，value 其实仍然并没有被解压，这个时候，我们需要判断attributes的值，如果经过第一次解压，attributes不为 0 的情况下，那么它需要再进行一次解压，这一次，数据将重新覆盖整个messageSet结构体，并且arttibutes将会被设置为 0，以此来告诉客户端，协议已经正确解析完毕。 还有一点需要注意的是 kafka 并不仅仅只是单单用了 snappy 压缩方式，它还加入了它自己的协议头（这个真的是个巨坑），所以你需要忽略前面的 20 个字节。后续以被 4 个字节的解析出来的字符长度来递归解压。相对于 snappy 的压缩方式，gzip 的压缩方式就是中规中矩了。 如何利用 docker 环境尝试加入开发项目中，已经把开发用的docker-composer.yaml已经写好了，只需要采用docker-composer up即可。其他的都是常规操作。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263version: &quot;3&quot;services: mzookeeper: image: wurstmeister&#x2F;zookeeper container_name: kafka-swoole-zookeeper mkafka1: image: wurstmeister&#x2F;kafka:2.11-0.9.0.1 container_name: kafka-swoole-kafka1 environment: KAFKA_ADVERTISED_LISTENERS: PLAINTEXT:&#x2F;&#x2F;:9092 KAFKA_LISTENERS: PLAINTEXT:&#x2F;&#x2F;:9092 KAFKA_ZOOKEEPER_CONNECT: mzookeeper:2181 KAFKA_NUM_PARTITIONS: 4 depends_on: - mzookeeper mkafka2: image: wurstmeister&#x2F;kafka:2.11-0.9.0.1 container_name: kafka-swoole-kafka2 environment: KAFKA_ADVERTISED_LISTENERS: PLAINTEXT:&#x2F;&#x2F;:9092 KAFKA_LISTENERS: PLAINTEXT:&#x2F;&#x2F;:9092 KAFKA_ZOOKEEPER_CONNECT: mzookeeper:2181 KAFKA_NUM_PARTITIONS: 4 depends_on: - mzookeeper mkafka3: image: wurstmeister&#x2F;kafka:2.11-0.9.0.1 container_name: kafka-swoole-kafka3 environment: KAFKA_ADVERTISED_LISTENERS: PLAINTEXT:&#x2F;&#x2F;:9092 KAFKA_LISTENERS: PLAINTEXT:&#x2F;&#x2F;:9092 KAFKA_ZOOKEEPER_CONNECT: mzookeeper:2181 KAFKA_NUM_PARTITIONS: 4 depends_on: - mzookeeper mkafka4: image: wurstmeister&#x2F;kafka:2.11-0.9.0.1 container_name: kafka-swoole-kafka3 environment: KAFKA_ADVERTISED_LISTENERS: PLAINTEXT:&#x2F;&#x2F;:9092 KAFKA_LISTENERS: PLAINTEXT:&#x2F;&#x2F;:9092 KAFKA_ZOOKEEPER_CONNECT: mzookeeper:2181 KAFKA_NUM_PARTITIONS: 4 depends_on: - mzookeeper kafka-swoole: build: .&#x2F; container_name: kafka-swoole-php volumes: - .&#x2F;:&#x2F;data&#x2F;www depends_on: - mzookeeper - mkafka1 - mkafka2 - mkafka3 - mkafka4","categories":[{"name":"消息队列","slug":"消息队列","permalink":"http://blog.crazylaw.cn/categories/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"}],"tags":[{"name":"Swoole","slug":"Swoole","permalink":"http://blog.crazylaw.cn/tags/Swoole/"},{"name":"消息队列，Kafka","slug":"消息队列，Kafka","permalink":"http://blog.crazylaw.cn/tags/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%EF%BC%8CKafka/"}]},{"title":"【消息队列】- Kafka相关","slug":"消息队列/kafka","date":"2019-10-01T07:25:40.000Z","updated":"2021-03-20T16:25:01.819Z","comments":true,"path":"2019/10/01/消息队列/kafka/","link":"","permalink":"http://blog.crazylaw.cn/2019/10/01/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/","excerpt":"前言目前 kafka 客户端在 php 中比较出名的仅有 2 个，但是其各有利弊，在这里就不展开详细说明了。因此，我们实现了一个叫kafka-swoole的项目，项目由swoole4.x协程+多进程实现，实现在串行化协程的基础上实现并行操作。 由于很多小伙伴对 kafka 并不是特别了解，所以在这里记录一下一些基础的 kafka 自带的命令使用方式","text":"前言目前 kafka 客户端在 php 中比较出名的仅有 2 个，但是其各有利弊，在这里就不展开详细说明了。因此，我们实现了一个叫kafka-swoole的项目，项目由swoole4.x协程+多进程实现，实现在串行化协程的基础上实现并行操作。 由于很多小伙伴对 kafka 并不是特别了解，所以在这里记录一下一些基础的 kafka 自带的命令使用方式 我的 kafka 集群是 4 个 broker，是用 docker 连接在一起的 kafka 集群。分别设置了 hostname mkafka1:9092 mkafka2:9092 mkafka3:9092 mkafka4:9092 mzookeeper:2181 kafka-topics.sh这个脚本主要是用于 kafka 创建 topics 而使用。 创建 topic Option Description –alter 修改 topic 的分区数，副本指派以及 topic 的现有配置 –config &lt;name=value&gt; 创建 topic 的时候会用到默认配置，置顶具体的配置参数，具体查看 kafka 的配置项 –create 创建 topic –delete 删除 topic –delete-config 创建 topic 的时候移除某一个配置项 –describe 某一个 topic 的详情描述 –list 所有可用的 topic 列表 –partitions 指定 topic 数目 –replica-assignment &lt;broker_id_for_part1_replica1,…&gt; 创建或更改的主题的手动分区到代理的分配列表。 –replication-factor topic 的副本因子 –topic 需要创建的 topic 名字 –unavailable-partitions –describe 主题的时候，则只显示其 leader 不可用的分区 –zookeeper zookeeper 节点 1234kafka-topics.sh --create --zookeeper mzookeeper:2181 --replication-factor 2 --partitions 4 --topic kafka-swoole## 标准输出Created topic &quot;kafka-swoole&quot;. 这样子执行命令，我们就可以创建一个副本因子为 2，并且分区数为 4 ，名字叫 kafka-swoole 的 topic 查看所有的 topic 列表12345678kafka-topics.sh --list --zookeeper mzookeeper:2181## 标准输出__consumer_offsetscaiwenhuicaiwenhui2kafka-swooletest 查看 某个 topic 的详情12345678kafka-topics.sh --zookeeper mzookeeper:2181 --topic kafka-swoole --describe## 标准输出Topic:kafka-swoole PartitionCount:4 ReplicationFactor:2 Configs: Topic: kafka-swoole Partition: 0 Leader: 1003 Replicas: 1003,1002 Isr: 1003,1002 Topic: kafka-swoole Partition: 1 Leader: 1004 Replicas: 1004,1003 Isr: 1004,1003 Topic: kafka-swoole Partition: 2 Leader: 1001 Replicas: 1001,1004 Isr: 1001,1004 Topic: kafka-swoole Partition: 3 Leader: 1002 Replicas: 1002,1001 Isr: 1002,1001 向某个 topic 写入消息1kafka-console-producer.sh --broker-list mkafka1:9092 --topic kafka-swoole 从某个 topic 读取消息从最开始的 offset 读取1kafka-console-consumer.sh --bootstrap-server mkafka1:9092 --topic kafka-swoole --from-beginning 从最新的 offset 中读1kafka-console-consumer.sh --bootstrap-server mkafka1:9092 --topic kafka-swoole 查看某个 topic 的某个时刻的 offset最早之前的消息的 offset1234567kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list 192.168.11.148:9092 -time -2 --topic kafka-swoole# 标准输出kafka-swoole:2:0kafka-swoole:1:0kafka-swoole:3:0kafka-swoole:0:0 最近的消息 offset1234567kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list 192.168.11.148:9092 -time -1 --topic kafka-swoole# 标准输出kafka-swoole:2:0kafka-swoole:1:0kafka-swoole:3:0kafka-swoole:0:6 某一个时刻的 offset1kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list 192.168.11.148:9092 -time 1569943885 --topic kafka-swoole","categories":[{"name":"消息队列","slug":"消息队列","permalink":"http://blog.crazylaw.cn/categories/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"}],"tags":[{"name":"消息队列，Kafka","slug":"消息队列，Kafka","permalink":"http://blog.crazylaw.cn/tags/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%EF%BC%8CKafka/"}]},{"title":"【大数据】- Glow 源码剖析","slug":"大数据/Glow源码剖析","date":"2019-08-11T16:07:40.000Z","updated":"2021-03-20T16:25:01.814Z","comments":true,"path":"2019/08/12/大数据/Glow源码剖析/","link":"","permalink":"http://blog.crazylaw.cn/2019/08/12/%E5%A4%A7%E6%95%B0%E6%8D%AE/Glow%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90/","excerpt":"前言犹豫公司的流式计算，并没有用类似于 Hadoop 的 mapreduce 机制或者 storm 或者 flink，是我们自研基于 erlang 的单节点服务，其优点就是：部署和迁移都十分简单，并且犹豫 erlang 的天然的良好的利用了多核 CPU 的优势，可以实现效率较高的大数据流式计算。但是由于其单机性，导致对单台机器的要求过于苛刻，并且不能进行扩展机器提高计算能力是其致命的缺点，所以目前我规划利用 golang，写一个支持分布式并行计算的服务，在此之前，了解了各大流式计算的基本思想，并且结合 golang 语言的特性，找到了一个叫glow的服务，想要写好一个分布式流式计算的服务，我们先来看看 glow 有什么好的借鉴的思想和思路。","text":"前言犹豫公司的流式计算，并没有用类似于 Hadoop 的 mapreduce 机制或者 storm 或者 flink，是我们自研基于 erlang 的单节点服务，其优点就是：部署和迁移都十分简单，并且犹豫 erlang 的天然的良好的利用了多核 CPU 的优势，可以实现效率较高的大数据流式计算。但是由于其单机性，导致对单台机器的要求过于苛刻，并且不能进行扩展机器提高计算能力是其致命的缺点，所以目前我规划利用 golang，写一个支持分布式并行计算的服务，在此之前，了解了各大流式计算的基本思想，并且结合 golang 语言的特性，找到了一个叫glow的服务，想要写好一个分布式流式计算的服务，我们先来看看 glow 有什么好的借鉴的思想和思路。 源码分析我们要记得这 5 个内容，这是构成整个 flow 的核心名词 上下文 Context 步进 Step 任务 Task 数据集 Dataset 数据分片 DatasetShard 上下文（Context）上下文有 4 个属性，其中 2 个问数组 Id int Steps []flow.Step Datasets []flow.Dataset ChannelBufferSize int 步进（Step） &amp; 任务（Task）步进（Step）Step 有 6 个属性 Id int Name string Inputs []flow.Dataset （每一步的来源结果集） Output flow.Dataset （每一个需要输出的结果集） Function function (每一步操作接口提供的用户自定义业务逻辑) Tasks []flow.Task （任务数基于上一步中的 Output 中 Task 中 Outputs 的数据分区数量） 任务（Task）Task 有 5 个属性 Id int Inputs []flow.DatasetShard Outputs []flow.DatasetShard （输出结果集的分区，各个分区处于平等关系） Step flow.Step （所属哪一步的任务） InputChans []reflect.Value 一个任务 Inputs 等于一个上一步中的 Output 中 Task 中的 Outputs 的数量 数据集（Dataset） &amp; 数据粉分片（DatasetShard）数据集（Dataset）Dataset 有 10 个属性 Id int （Step 输出的数据结集） context flow.FlowContext Type reflect.Type | *reflect.rtype Shards []flow.DatasetShard （对应 Step 中 Tasks 中 Outputs 的数据分区） Step flow.Step （属于哪一个结果集） ReadingSteps []flow.Step （对应下一步的 Step） ExternalInputChans []reflect.Value ExternalOutputChans []reflect.Value IsKeyPartitioned bool isKeyLocalSorted bool 数据集分片（DatasetShard）DatasetShard 有 9 个属性 Id int Parent flow.Dataset （所属的结果集） WriteChan reflect.Value ReadingTasks []flow.Task （Step 上有几个 Tasks 就有几个） Counter int ReadyTime time.Time CloseTime time.Time lock sync.RWMutex readingChans []reflect.Value word_count和其他流式计算一样，提供了一个单词统计的例子。 1234567891011121314151617181920flow.New().TextFile( \"/etc/passwd\", 2,).Filter(func(line string) bool &#123; //println(\"filter:\", line) return !strings.HasPrefix(line, \"#\")&#125;).Map(func(line string, ch chan string) &#123; for _, token := range strings.Split(line, \":\") &#123; ch &lt;- token &#125;&#125;).Map(func(key string) int &#123; println(\"map:\", key) return 1&#125;).Reduce(func(x int, y int) int &#123; println(\"x:\", x) println(\"y:\", y) println(\"reduce:\", x+y) return x + y&#125;).Map(func(x int) &#123; println(\"count:\", x)&#125;).Run() 我们看一下这个执行流程。 flow.New() 生成 flow.FlowContext TextFile(“/etc/passwd”, 2) 打开/etc/passwd 文件，并且数据分片数量为：2 这是第一个 Step Filter(func) 将返回true的数据筛选出来 这是第二个 Step Map(func(line string, ch chan string)) 需要运执行 map 运算，第一个参数为上一个 Step 的结果值，第二个参数说明需要通过一个可读可写的 chan 来写入传输数据，相当于二次拆分数据 这是第三个 Step Map(func(key string)) 从上一步的 Step 中的 chan 中读取出来的数据，每次来一个 key，都返回一个整型：1 Reduce(func(x int, y int)）进行Reduce的操作，将数据合并汇总，x 代表上一次 step 的总数，y 代表最近一次得到的值。但是这里比较特殊，在前面所有 step 都处理完毕之后，如果你是进行了数据分片的话，会把数据分片再合并一次。 Map(func(x int)) 由于我们进行了 Reduce 了，所以在 Reduce 之后的 map 只会进行一次运行，这个时候 x 就代表我们 Reduce 的 API 的最终结果 Run() 运行流式计算 按照看源码的套路 运行 demo，理解 demo 找到 demo 的运行入口 根据入口来查看运行方式 再回过头来看 demo 的细节 运行逻辑123func (d *Dataset) Run() &#123; d.context.Run()&#125; 123456789func (fc *FlowContext) Run() &#123; if taskRunner != nil &amp;&amp; taskRunner.IsTaskMode() &#123; taskRunner.Run(fc) &#125; else if contextRunner != nil &amp;&amp; contextRunner.IsDriverMode() &#123; contextRunner.Run(fc) &#125; else &#123; fc.runFlowContextInStandAloneMode() &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950func (fc *FlowContext) runFlowContextInStandAloneMode() &#123; // 设置一个用于等于协程全部运行完毕的计数`wg` var wg sync.WaitGroup // 生成一个`map[int]bool`类型的类似于 HashTable 一样的 K/V 映射结构的 Map 类型 isDatasetStarted := make(map[int]bool) // 设置接收到中断信号的处理回调函数 OnInterrupt(fc.OnInterrupt, nil) // 启动所有的任务边界 // start all task edges for _, step := range fc.Steps &#123; // 循环所有的Steps，每一个API就代表一个Step for _, input := range step.Inputs &#123; // 循环每个step中的Inputs if _, ok := isDatasetStarted[input.Id]; !ok &#123; // 如果Dataset已经启动，那么就跳过，否则进行启动逻辑 wg.Add(1) // 每一次需要运行input的时候，协程计数器+1 go func(input *Dataset) &#123; // 每一个input都启动一个协程运行主逻辑 defer wg.Done() // 每个协程结束的时候，协程计数器-1 input.RunDatasetInStandAloneMode() // 在协程环境下运行每个input启动自身逻辑 &#125;(input) isDatasetStarted[input.Id] = true // 创建协程完毕之后设置这个input已经处理过了 &#125; &#125; wg.Add(1) // 每一次需要运行step的时候，协程计数器+1 go func(step *Step) &#123; // 创建协程运行主逻辑 defer wg.Done() // 每个协程结束的时候，协程计数器-1 step.RunStep() // 在协程环境下运行step的自身逻辑 &#125;(step) if step.Output != nil &#123; // 如果step的output不等于nil的话，进行逻辑 if _, ok := isDatasetStarted[step.Output.Id]; !ok &#123;// 如果没有运行过的话，就运行否则就跳出 wg.Add(1) // 每一次需要运行Output的时候，协程计数器+1 go func(step *Step) &#123; // 创建协程运行主逻辑 defer wg.Done() // 每个协程结束的时候，协程计数器-1 step.Output.RunDatasetInStandAloneMode() // 在协程环境下运行Output的自身逻辑 &#125;(step) isDatasetStarted[step.Output.Id] = true // 创建协程完毕之后设置这个Output已经处理过了 &#125; &#125; &#125; wg.Wait() // 当所有协程都执行完毕之后再退出主协程&#125;/* 总结： 1. step.Inputs 和 step.Output 都是Dataset 2. step.Inputs中所有input运行完逻辑之后，再运行step的逻辑，最后再运行step的Outpu逻辑，再接着下一个step 3. 每个逻辑的运行都是在创建协程之后运行*/ 看到这里的逻辑比较核心的有func (fc *FlowContext) runFlowContextInStandAloneMode()，func (s *Step) RunStep()我们就看到了最终的入口了，解释我写在代码中。 接下来，我们看一下 input/Output 运行的主逻辑 12345678910111213141516171819202122232425262728293031323334353637383940414243func (d *Dataset) RunDatasetInStandAloneMode() &#123; // 设置一个用于等于协程全部运行完毕的计数`wg` var wg sync.WaitGroup if len(d.ExternalInputChans) &gt; 0 &#123; // 如果数据集存在外部chan的话 d.connectExternalInputChansToRead(&amp;wg) // 连接外部chan进行处理，并且如果用到协程的话，需要同步更新协程计数器 for _, shard := range d.Shards &#123; // 循环数据集的分片 shard.SetupReadingChans() // 数据集分片运行主要逻辑 &#125; &#125; else &#123; // 如果不存在外部chan的话 for _, shard := range d.Shards &#123; // 循环数据集的分片 wg.Add(1) // 每一次需要运行数据分片的时候，协程计数器+1 go func(shard *DatasetShard) &#123; // 创建协程运行主逻辑 defer wg.Done() // 每个协程结束的时候，协程计数器-1 // println(\"setup shard reading chans\", shard.Name()) shard.SetupReadingChans() // 数据分片部署读取的数据的chan // start to run var t reflect.Value // 定义个reflect.Value的类型 for ok := true; ok; &#123; // 死循环 if t, ok = shard.WriteChan.Recv(); ok &#123; // \b\b\b\b\b\b数据分片写chan阻塞接收数据，如果有数据来的话就执行下面的逻辑 shard.SendForRead(t) // 数据分片发送数据到Readchan，参数为reflect.Value类型 // hookup output channels d.sendToExternalOutputChans(t) // 发送数据到外部OutputChan &#125; &#125; // println(\"close shard reading\", shard.Name()) shard.CloseRead() // 数据分片关闭 &#125;(shard) &#125; &#125; wg.Wait() // 当所有协程都执行完毕之后再运行下面的逻辑 d.closeExternalOutputChans() // 关闭外部Outputchan return/* 总结： 1. 每个数据分片都需要关联Task中的input的chan 2. 当数据分片的WriteChan可读取的时候，把数据传递给readingChans 3. 发送完毕之后，还会记录关闭时间*/&#125; 12345678910111213141516171819202122232425262728293031func (shard *DatasetShard) SetupReadingChans() &#123; // get unique list of tasks since ReadingTasks can have duplicates // especially when one dataset is used twice in a task, e.g. selfJoin() // 获取唯一的任务列表，因为ReadingTasks可能有重复的任务 // 特别是当一个数据集在一个任务中使用两次时，例如selfJoin() // 定义变量uniqTasks var uniqTasks []*Task // 生成一个`map[*Task]bool`类型的类似于 HashTable 一样的 K/V 映射结构的 Map 类型 seenTasks := make(map[*Task]bool) for _, task := range shard.ReadingTasks &#123; // 循环数据分片中的ReadingTasks，读取任务 if ok := seenTasks[task]; ok &#123; // 如果任务已经处理过了，就处理下一个任务 continue &#125; seenTasks[task] = true // 开始处理任务，设置为true uniqTasks = append(uniqTasks, task) // 加入uniqTasks的list &#125; shard.lock.Lock() // 数据分片上加上`RWMutex`，并且进行写锁 defer shard.lock.Unlock() // 函数结束的时候，进行写解锁 for _, task := range uniqTasks &#123; // 再写锁读情况下进行：循环唯一的Tasks for i, s := range task.Inputs &#123; // 循环每个Task的Inputs if s == shard &#123; // 找到对应的索引i shard.readingChans = append(shard.readingChans, task.InputChans[i]) // 把输入任务上的inputchan加入到分区需要读取的chan去 &#125; &#125; &#125; shard.ReadyTime = time.Now() // 数据分片准备就绪的时间 // fmt.Printf(\"shard %s has reading tasks:%d channel:%d\\n\", shard.Name(), len(shard.ReadingTasks), len(shard.readingChans))&#125; 12345678910func (s *DatasetShard) SendForRead(t reflect.Value) &#123; s.lock.RLock() // 数据分片进行读锁 defer s.lock.RUnlock() // 数结束的时候，进行读解锁 s.Counter++ // 发送次数+1 for _, c := range s.readingChans &#123; // 往读chan发送数据 // println(s.Name(), \"send chan\", i, \"entry:\", s.counter) c &lt;- t &#125;&#125; 123456789func (s *DatasetShard) CloseRead() &#123; s.lock.RLock() // 数据分片进行读锁 defer s.lock.RUnlock() // 数结束的时候，进行读解锁 for _, c := range s.readingChans &#123; close(c) // 关闭所有读chan &#125; s.CloseTime = time.Now() // 记录关闭时间&#125; 接下来，我们看一下 RunStep 的主逻辑： 1234567891011121314func (s *Step) RunStep() &#123; // 设置一个用于等于协程全部运行完毕的计数`wg` var wg sync.WaitGroup for i, t := range s.Tasks &#123; // 循环所有Step的任务 wg.Add(1) // 每一次需要运行数据分片的时候，协程计数器+1 go func(i int, t *Task) &#123; // 创建协程运行主逻辑 defer wg.Done() // 每个协程结束的时候，协程计数器-1 t.RunTask() // 运行任务的主逻辑 &#125;(i, t) &#125; wg.Wait() // 当所有协程都执行完毕之后才算完毕 return&#125; 1234567891011121314// source -&gt;w:ds:r -&gt; task -&gt; w:ds:r// source close next ds' w chan// ds close its own r chan// task closes its own channel to next ds' w:dsfunc (t *Task) RunTask() &#123; // println(\"start\", t.Name()) t.Step.Function(t) // 运行每一个Step自定义的处理逻辑函数 for _, out := range t.Outputs &#123; // println(t.Name(), \"close WriteChan of\", out.Name()) out.WriteChan.Close() // 关闭每个output的chan，那个每个Step.Function中的协程将会结束 &#125; // println(\"stop\", t.Name()) &#125; 回到我们的程序： ================= 123456789101112131415161718192021func (fc *FlowContext) TextFile(fname string, shard int) (ret *Dataset) &#123; fn := func(out chan string) &#123; file, err := os.Open(fname) if err != nil &#123; // FIXME collect errors log.Panicf(\"Can not open file %s: %v\", fname, err) return &#125; defer file.Close() scanner := bufio.NewScanner(file) for scanner.Scan() &#123; out &lt;- scanner.Text() &#125; if err := scanner.Err(); err != nil &#123; log.Printf(\"Scan file %s: %v\", fname, err) &#125; &#125; return fc.Source(fn, shard)&#125; 这里，我们看到func (fc *FlowContext) TextFile(fname string, shard int) (ret *Dataset)， 12345678910111213141516171819202122232425262728293031323334353637383940// Source returns a new Dataset which evenly distributes the data items produced by f// among multiple shards. f must be a function defined in the form func(chan &lt;some_type&gt;).func (fc *FlowContext) Source(f interface&#123;&#125;, shard int) (ret *Dataset) &#123; ret = fc.newNextDataset(shard, guessFunctionOutputType(f)) step := fc.AddOneToAllStep(nil, ret) step.Name = \"Source\" step.Function = func(task *Task) &#123; ctype := reflect.ChanOf(reflect.BothDir, ret.Type) outChan := reflect.MakeChan(ctype, 0) fn := reflect.ValueOf(f) var wg sync.WaitGroup wg.Add(1) go func() &#123; defer wg.Done() defer outChan.Close() fn.Call([]reflect.Value&#123;outChan&#125;) &#125;() wg.Add(1) go func() &#123; defer wg.Done() var t reflect.Value i := 0 for ok := true; ok; &#123; if t, ok = outChan.Recv(); ok &#123; task.Outputs[i].WriteChan.Send(t) i++ if i == shard &#123; i = 0 &#125; &#125; &#125; &#125;() wg.Wait() &#125; return&#125;","categories":[{"name":"大数据","slug":"大数据","permalink":"http://blog.crazylaw.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"Golang","slug":"大数据/Golang","permalink":"http://blog.crazylaw.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Golang/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://blog.crazylaw.cn/tags/Golang/"},{"name":"大数据，流式计算","slug":"大数据，流式计算","permalink":"http://blog.crazylaw.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%EF%BC%8C%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"}]},{"title":"【网络协议】TCP - tcpdump用法","slug":"网络协议/TCP-tcpdump","date":"2019-08-07T07:57:13.000Z","updated":"2021-03-20T16:25:01.822Z","comments":true,"path":"2019/08/07/网络协议/TCP-tcpdump/","link":"","permalink":"http://blog.crazylaw.cn/2019/08/07/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE/TCP-tcpdump/","excerpt":"前言由于我们在服务器上编程或者调试的时候，都不可能是各种抓包客户端，所以我们需要学会运用 tcpdump 来对网络的数据进行抓包。用简单的话来定义 tcpdump，就是：dump the traffic on a network，根据使用者的定义对网络上的数据包进行截获的包分析工具。 tcpdump 可以将网络中传送的数据包的“头”完全截获下来提供分析。它支持针对网络层、协议、主机、网络或端口的过滤，并提供 and、or、not 等逻辑语句来帮助你去掉无用的信息。","text":"前言由于我们在服务器上编程或者调试的时候，都不可能是各种抓包客户端，所以我们需要学会运用 tcpdump 来对网络的数据进行抓包。用简单的话来定义 tcpdump，就是：dump the traffic on a network，根据使用者的定义对网络上的数据包进行截获的包分析工具。 tcpdump 可以将网络中传送的数据包的“头”完全截获下来提供分析。它支持针对网络层、协议、主机、网络或端口的过滤，并提供 and、or、not 等逻辑语句来帮助你去掉无用的信息。 命令说明123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130-A 以ASCII码方式显示每一个数据包(不会显示数据包中链路层头部信息). 在抓取包含网页数据的数据包时, 可方便查看数据(nt: 即Handy for capturing web pages).-c count tcpdump将在接受到count个数据包后退出.-C file-size (nt: 此选项用于配合-w file 选项使用) 该选项使得tcpdump 在把原始数据包直接保存到文件中之前, 检查此文件大小是否超过file-size. 如果超过了, 将关闭此文件,另创一个文件继续用于原始数据包的记录. 新创建的文件名与-w 选项指定的文件名一致, 但文件名后多了一个数字.该数字会从1开始随着新创建文件的增多而增加. file-size的单位是百万字节(nt: 这里指1,000,000个字节,并非1,048,576个字节, 后者是以1024字节为1k, 1024k字节为1M计算所得, 即1M&#x3D;1024 ＊ 1024 ＝ 1,048,576)-d 以容易阅读的形式,在标准输出上打印出编排过的包匹配码, 随后tcpdump停止.(nt | rt: human readable, 容易阅读的,通常是指以ascii码来打印一些信息. compiled, 编排过的. packet-matching code, 包匹配码,含义未知, 需补充)-dd 以C语言的形式打印出包匹配码.-ddd 以十进制数的形式打印出包匹配码(会在包匹配码之前有一个附加的&#39;count&#39;前缀).-D 打印系统中所有tcpdump可以在其上进行抓包的网络接口. 每一个接口会打印出数字编号, 相应的接口名字, 以及可能的一个网络接口描述. 其中网络接口名字和数字编号可以用在tcpdump 的-i flag 选项(nt: 把名字或数字代替flag), 来指定要在其上抓包的网络接口. 此选项在不支持接口列表命令的系统上很有用(nt: 比如, Windows 系统, 或缺乏 ifconfig -a 的UNIX系统); 接口的数字编号在windows 2000 或其后的系统中很有用, 因为这些系统上的接口名字比较复杂, 而不易使用. 如果tcpdump编译时所依赖的libpcap库太老,-D 选项不会被支持, 因为其中缺乏 pcap_findalldevs()函数.-e 每行的打印输出中将包括数据包的数据链路层头部信息-E spi@ipaddr algo:secret,... 可通过spi@ipaddr algo:secret 来解密IPsec ESP包(nt | rt:IPsec Encapsulating Security Payload,IPsec 封装安全负载, IPsec可理解为, 一整套对ip数据包的加密协议, ESP 为整个IP 数据包或其中上层协议部分被加密后的数据,前者的工作模式称为隧道模式; 后者的工作模式称为传输模式 . 工作原理, 另需补充). 需要注意的是, 在终端启动tcpdump 时, 可以为IPv4 ESP packets 设置密钥(secret）. 可用于加密的算法包括des-cbc, 3des-cbc, blowfish-cbc, rc3-cbc, cast128-cbc, 或者没有(none).默认的是des-cbc(nt: des, Data Encryption Standard, 数据加密标准, 加密算法未知, 另需补充).secret 为用于ESP 的密钥, 使用ASCII 字符串方式表达. 如果以 0x 开头, 该密钥将以16进制方式读入. 该选项中ESP 的定义遵循RFC2406, 而不是 RFC1827. 并且, 此选项只是用来调试的, 不推荐以真实密钥(secret)来使用该选项, 因为这样不安全: 在命令行中输入的secret 可以被其他人通过ps 等命令查看到. 除了以上的语法格式(nt: 指spi@ipaddr algo:secret), 还可以在后面添加一个语法输入文件名字供tcpdump 使用(nt：即把spi@ipaddr algo:secret,... 中...换成一个语法文件名). 此文件在接受到第一个ESP 包时会打开此文件, 所以最好此时把赋予tcpdump 的一些特权取消(nt: 可理解为, 这样防范之后, 当该文件为恶意编写时,不至于造成过大损害).-f 显示外部的IPv4 地址时(nt: foreign IPv4 addresses, 可理解为, 非本机ip地址), 采用数字方式而不是名字.(此选项是用来对付Sun公司的NIS服务器的缺陷(nt: NIS, 网络信息服务, tcpdump 显示外部地址的名字时会用到她提供的名称服务): 此NIS服务器在查询非本地地址名字时,常常会陷入无尽的查询循环). 由于对外部(foreign)IPv4地址的测试需要用到本地网络接口(nt: tcpdump 抓包时用到的接口)及其IPv4 地址和网络掩码. 如果此地址或网络掩码不可用, 或者此接口根本就没有设置相应网络地址和网络掩码(nt: linux 下的 &#39;any&#39; 网络接口就不需要设置地址和掩码, 不过此&#39;any&#39;接口可以收到系统中所有接口的数据包), 该选项不能正常工作.-F file 使用file 文件作为过滤条件表达式的输入, 此时命令行上的输入将被忽略.-i interface 指定tcpdump 需要监听的接口. 如果没有指定, tcpdump 会从系统接口列表中搜寻编号最小的已配置好的接口(不包括 loopback 接口).一但找到第一个符合条件的接口, 搜寻马上结束. 在采用2.2版本或之后版本内核的Linux 操作系统上, &#39;any&#39; 这个虚拟网络接口可被用来接收所有网络接口上的数据包(nt: 这会包括目的是该网络接口的, 也包括目的不是该网络接口的). 需要注意的是如果真实网络接口不能工作在&#39;混杂&#39;模式(promiscuous)下,则无法在&#39;any&#39;这个虚拟的网络接口上抓取其数据包. 如果 -D 标志被指定, tcpdump会打印系统中的接口编号，而该编号就可用于此处的interface 参数.-l 对标准输出进行行缓冲(nt: 使标准输出设备遇到一个换行符就马上把这行的内容打印出来).在需要同时观察抓包打印以及保存抓包记录的时候很有用. 比如, 可通过以下命令组合来达到此目的: &#96;&#96;tcpdump -l | tee dat&#39;&#39; 或者 &#96;&#96;tcpdump -l &gt; dat &amp; tail -f dat&#39;&#39;.(nt: 前者使用tee来把tcpdump 的输出同时放到文件dat和标准输出中, 而后者通过重定向操作&#39;&gt;&#39;, 把tcpdump的输出放到dat 文件中, 同时通过tail把dat文件中的内容放到标准输出中)-L 列出指定网络接口所支持的数据链路层的类型后退出.(nt: 指定接口通过-i 来指定)-m module 通过module 指定的file 装载SMI MIB 模块(nt: SMI，Structure of Management Information, 管理信息结构MIB, Management Information Base, 管理信息库. 可理解为, 这两者用于SNMP(Simple Network Management Protoco)协议数据包的抓取. 具体SNMP 的工作原理未知, 另需补充). 此选项可多次使用, 从而为tcpdump 装载不同的MIB 模块.-M secret 如果TCP 数据包(TCP segments)有TCP-MD5选项(在RFC 2385有相关描述), 则为其摘要的验证指定一个公共的密钥secret.-n 不对地址(比如, 主机地址, 端口号)进行数字表示到名字表示的转换.-N 不打印出host 的域名部分. 比如, 如果设置了此选现, tcpdump 将会打印&#39;nic&#39; 而不是 &#39;nic.ddn.mil&#39;.-O 不启用进行包匹配时所用的优化代码. 当怀疑某些bug是由优化代码引起的, 此选项将很有用.-p 一般情况下, 把网络接口设置为非&#39;混杂&#39;模式. 但必须注意 , 在特殊情况下此网络接口还是会以&#39;混杂&#39;模式来工作； 从而, &#39;-p&#39; 的设与不设, 不能当做以下选现的代名词:&#39;ether host &#123;local-hw-add&#125;&#39; 或 &#39;ether broadcast&#39;(nt: 前者表示只匹配以太网地址为host 的包, 后者表示匹配以太网地址为广播地址的数据包).-q 快速(也许用&#39;安静&#39;更好?)打印输出. 即打印很少的协议相关信息, 从而输出行都比较简短.-R 设定tcpdump 对 ESP&#x2F;AH 数据包的解析按照 RFC1825而不是RFC1829(nt: AH, 认证头, ESP， 安全负载封装, 这两者会用在IP包的安全传输机制中). 如果此选项被设置, tcpdump 将不会打印出&#39;禁止中继&#39;域(nt: relay prevention field). 另外,由于ESP&#x2F;AH规范中没有规定ESP&#x2F;AH数据包必须拥有协议版本号域,所以tcpdump不能从收到的ESP&#x2F;AH数据包中推导出协议版本号.-r file 从文件file 中读取包数据. 如果file 字段为 &#39;-&#39; 符号, 则tcpdump 会从标准输入中读取包数据.-S 打印TCP 数据包的顺序号时, 使用绝对的顺序号, 而不是相对的顺序号.(nt: 相对顺序号可理解为, 相对第一个TCP 包顺序号的差距,比如, 接受方收到第一个数据包的绝对顺序号为232323, 对于后来接收到的第2个,第3个数据包, tcpdump会打印其序列号为1, 2分别表示与第一个数据包的差距为1 和 2. 而如果此时-S 选项被设置, 对于后来接收到的第2个, 第3个数据包会打印出其绝对顺序号:232324, 232325).-s snaplen 设置tcpdump的数据包抓取长度为snaplen, 如果不设置默认将会是68字节(而支持网络接口分接头(nt: NIT, 上文已有描述,可搜索&#39;网络接口分接头&#39;关键字找到那里)的SunOS系列操作系统中默认的也是最小值是96).68字节对于IP, ICMP(nt: Internet Control Message Protocol,因特网控制报文协议), TCP 以及 UDP 协议的报文已足够, 但对于名称服务(nt: 可理解为dns, nis等服务), NFS服务相关的数据包会产生包截短. 如果产生包截短这种情况, tcpdump的相应打印输出行中会出现&#39;&#39;[|proto]&#39;&#39;的标志（proto 实际会显示为被截短的数据包的相关协议层次). 需要注意的是, 采用长的抓取长度(nt: snaplen比较大), 会增加包的处理时间, 并且会减少tcpdump 可缓存的数据包的数量， 从而会导致数据包的丢失. 所以, 在能抓取我们想要的包的前提下, 抓取长度越小越好.把snaplen 设置为0 意味着让tcpdump自动选择合适的长度来抓取数据包.-T type 强制tcpdump按type指定的协议所描述的包结构来分析收到的数据包. 目前已知的type 可取的协议为: aodv (Ad-hoc On-demand Distance Vector protocol, 按需距离向量路由协议, 在Ad hoc(点对点模式)网络中使用), cnfp (Cisco NetFlow protocol), rpc(Remote Procedure Call), rtp (Real-Time Applications protocol), rtcp (Real-Time Applications con-trol protocol), snmp (Simple Network Management Protocol), tftp (Trivial File Transfer Protocol, 碎文件协议), vat (Visual Audio Tool, 可用于在internet 上进行电 视电话会议的应用层协议), 以及wb (distributed White Board, 可用于网络会议的应用层协议).-t 在每行输出中不打印时间戳-tt 不对每行输出的时间进行格式处理(nt: 这种格式一眼可能看不出其含义, 如时间戳打印成1261798315)-ttt tcpdump 输出时, 每两行打印之间会延迟一个段时间(以毫秒为单位)-tttt 在每行打印的时间戳之前添加日期的打印-u 打印出未加密的NFS 句柄(nt: handle可理解为NFS 中使用的文件句柄, 这将包括文件夹和文件夹中的文件)-U 使得当tcpdump在使用-w 选项时, 其文件写入与包的保存同步.(nt: 即, 当每个数据包被保存时, 它将及时被写入文件中,而不是等文件的输出缓冲已满时才真正写入此文件) -U 标志在老版本的libcap库(nt: tcpdump 所依赖的报文捕获库)上不起作用, 因为其中缺乏pcap_cump_flush()函数.-v 当分析和打印的时候, 产生详细的输出. 比如, 包的生存时间, 标识, 总长度以及IP包的一些选项. 这也会打开一些附加的包完整性检测, 比如对IP或ICMP包头部的校验和.-vv 产生比-v更详细的输出. 比如, NFS回应包中的附加域将会被打印, SMB数据包也会被完全解码.-vvv 产生比-vv更详细的输出. 比如, telent 时所使用的SB, SE 选项将会被打印, 如果telnet同时使用的是图形界面, 其相应的图形选项将会以16进制的方式打印出来(nt: telnet 的SB,SE选项含义未知, 另需补充).-w 把包数据直接写入文件而不进行分析和打印输出. 这些包数据可在随后通过-r 选项来重新读入并进行分析和打印.-W filecount 此选项与-C 选项配合使用, 这将限制可打开的文件数目, 并且当文件数据超过这里设置的限制时, 依次循环替代之前的文件, 这相当于一个拥有filecount 个文件的文件缓冲池. 同时, 该选项会使得每个文件名的开头会出现足够多并用来占位的0, 这可以方便这些文件被正确的排序.-x 当分析和打印时, tcpdump 会打印每个包的头部数据, 同时会以16进制打印出每个包的数据(但不包括连接层的头部).总共打印的数据大小不会超过整个数据包的大小与snaplen 中的最小值. 必须要注意的是, 如果高层协议数据没有snaplen 这么长,并且数据链路层(比如, Ethernet层)有填充数据, 则这些填充数据也会被打印.(nt: so for link layers that pad, 未能衔接理解和翻译, 需补充 )-xx tcpdump 会打印每个包的头部数据, 同时会以16进制打印出每个包的数据, 其中包括数据链路层的头部.-X 当分析和打印时, tcpdump 会打印每个包的头部数据, 同时会以16进制和ASCII码形式打印出每个包的数据(但不包括连接层的头部).这对于分析一些新协议的数据包很方便.-XX 当分析和打印时, tcpdump 会打印每个包的头部数据, 同时会以16进制和ASCII码形式打印出每个包的数据, 其中包括数据链路层的头部.这对于分析一些新协议的数据包很方便.-y datalinktype 设置tcpdump 只捕获数据链路层协议类型是datalinktype的数据包-Z user 使tcpdump 放弃自己的超级权限(如果以root用户启动tcpdump, tcpdump将会有超级用户权限), 并把当前tcpdump的用户ID设置为user, 组ID设置为user首要所属组的ID(nt: tcpdump 此处可理解为tcpdump 运行之后对应的进程) 此选项也可在编译的时候被设置为默认打开.(nt: 此时user 的取值未知, 需补充) 实用命令实例默认启动1tcpdump 普通情况下，直接启动 tcpdump 将监视第一个网络接口(网卡)上所有流过的数据包。 监视指定网络接口的数据包1tcpdump -i eth1 如果不指定网卡，默认 tcpdump 只会监视第一个网络接口，一般是 eth0，下面的例子都没有指定网络接口。 监视指定主机的数据包1tcpdump host sundown 过滤出所有进入或离开域名为：ccinn 的数据包. 1tcpdump host 210.27.48.10 也可以指定 ip,例如截获所有 210.27.48.10 的主机收到的和发出的所有的数据包 1tcpdump host ccinn and \\( baa or bee \\) 过滤出域名为 ccinn 与 baa 或者与 bee 之间通信的数据包 1tcpdump host 210.27.48.1 and \\ (210.27.48.2 or 210.27.48.3 \\) 同域名的原理，截获主机 IP 210.27.48.1 和主机 IP 210.27.48.2 或 210.27.48.3 的通信 1tcpdump ip host ace and not helios 打印 ace 与任何其他主机之间通信的 IP 数据包, 但不包括与 helios 之间的数据包. 1tcpdump ip host 210.27.48.1 and ! 210.27.48.2 如果想要获取主机 210.27.48.1 除了和主机 210.27.48.2 之外所有主机通信的 ip 包. 1tcpdump -i eth0 src host ccinn.com 截获主机 ccinn.com 发送的所有数据 1tcpdump -i eth0 dst host ccinn.com 截获主机 ccinn.com 接收的所有数据 监视指定主机和端口的数据包1tcpdump tcp port 23 and host 210.27.48.1 如果想要获取主机 210.27.48.1 并且端口为 23 的接收或发出的 tcp 包 1tcpdump udp port 123 对本机的 udp 123 端口进行过滤 监视指定协议的数据包这个是灵活度比较高的用法，需要我们对底层协议有一定了解才比较好处理。这里会用 TCP 协议作为例子。如果对 TCP 协议不了解的话，这个高级用法可能派不上用场。 下面详细介绍proto [ expr : size] Proto即protocol的缩写，它表示这里要指定的是某种协议名称，如ip,tcp,udp等。总之可以指定的协议有十多种，如链路层协议 ether,fddi,tr,wlan,ppp,slip,link, 网络层协议ip,ip6,arp,rarp,icmp传输层协议tcp,udp等。 expr用来指定数据报字节单位的偏移量，该偏移量相对于指定的协议层，默认的起始位置是0；而size表示从偏移量的位置开始提取多少个字节，可以设置为 1、2、4,默认为1字节。如果只设置了expr，而没有设置size，则默认提取1个字节。比如ip[2:2]，就表示提取出第3、4个字节；而ip[0]则表示提取ip协议头的 第一个字节。在我们提取了特定内容之后，我们就需要设置我们的过滤条件了，我们可用的“比较操作符”包括：&gt;，&lt;，&gt;=，&lt;=，=，!=，总共有6个。 举个例子：抓取带有特殊标志的的 TCP 包(如 SYN-ACK 标志, URG-ACK 标志等). 在 TCP 的头部中, 有 8 比特(bit)用作控制位区域, 其取值为:CWR | ECE | URG | ACK | PSH | RST | SYN | FIN 现假设我们想要监控建立一个 TCP 连接整个过程中所产生的数据包. 可回忆如下:TCP 使用 3 次握手协议来建立一个新的连接; 其与此三次握手连接顺序对应，并带有相应 TCP 控制标志的数据包如下: 连接发起方发送 SYN 标志的数据包 接收方用带有 SYN 和 ACK 标志的数据包进行回应 发起方收到接收方回应后再发送带有 ACK 标志的数据包进行回应 1234567891011121314151617181920212223240 15 31-----------------------------------------------------------------| source port | destination port |-----------------------------------------------------------------| sequence number |-----------------------------------------------------------------| acknowledgment number |-----------------------------------------------------------------| HL | rsvd |C|E|U|A|P|R|S|F| window size |-----------------------------------------------------------------| TCP checksum | urgent pointer |-----------------------------------------------------------------| 0 7 | 15 | 23 | 31 || ---------------- | ---------------------- | --------------- | ---------------- || HL \\| rsvd | C\\|E\\|U\\|A\\|P\\|R\\|S\\|F | window size || ---------------- | --------------- | --------------- | ---------------- || | 13th octet | | ||C|E|U|A|P|R|S|F||---------------||0 0 0 0 0 0 1 0||---------------||7 6 5 4 3 2 1 0| 我们看回这张图，TCP 头部一般是有 20 个字节的数据，在标志位，也就是第三行开始，\b 第三行开始的话就已经经过了 12 个字节了，那么向左移动 8bit，就是刚好我们的标志位数据，也就是刚好从第 13 个字节的数据开始，就是我们的标志位数据，标志位一共占用 8 个标志位，SYN 标志位从低往高数，就是第二位，ACK 标志位就是第五位。 所以我们的表达式为： 1( ( value of octet 13 ) AND ( 2 ) ) &#x3D;&#x3D; ( 2 ) 也就是： 1tcpdump -iany &#39;tcp[13] &amp; 2 &#x3D;&#x3D; 2&#39; 这个命令就是获取TCP协议中数据包中代表SYN消息的数据。 这样子，我们捕捉到的就是只有如下消息， 12345root@8152f1016ea8:&#x2F;data# tcpdump -iany &#39;tcp port 6379 and tcp[13] &amp; 2 &#x3D;&#x3D; 2&#39;tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on any, link-type LINUX_SLL (Linux cooked), capture size 262144 bytes10:06:37.491460 IP localhost.34716 &gt; localhost.6379: Flags [S], seq 998071933, win 43690, options [mss 65495,sackOK,TS val 11418013 ecr 0,nop,wscale 7], length 010:06:37.491501 IP localhost.6379 &gt; localhost.34716: Flags [S.], seq 2636200817, ack 998071934, win 43690, options [mss 65495,sackOK,TS val 11418013 ecr 11418013,nop,wscale 7], length 0 全部都是带有[S]标志的。 使用 tcpdump 抓取 HTTP 包1tcpdump -XvvennSs 0 -i eth0 tcp[20:2]&#x3D;0x4745 or tcp[20:2]&#x3D;0x4854 0x4745 为”GET”前两个字母”GE”,0x4854 为”HTTP”前两个字母”HT”。 总结tcpdump 对截获的数据并没有进行彻底解码，数据包内的大部分内容是使用十六进制的形式直接打印输出的。显然这不利于分析网络故障，通常的解决办法是先使用带-w 参数的 tcpdump 截获数据并保存到文件中保存成 cap 文件，然后再使用其他程序(如 Wireshark)进行解码分析。当然也应该定义过滤规则，以避免捕获的数据包填满整个硬盘。 如果需要最完整对说明对话，可以参考官网说明tcpdump.","categories":[{"name":"网络协议","slug":"网络协议","permalink":"http://blog.crazylaw.cn/categories/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE/"}],"tags":[{"name":"TCP","slug":"TCP","permalink":"http://blog.crazylaw.cn/tags/TCP/"}]},{"title":"【网络协议】TCP - 重传机制","slug":"网络协议/TCP-重传机制","date":"2019-08-07T03:33:40.000Z","updated":"2021-03-20T16:25:01.822Z","comments":true,"path":"2019/08/07/网络协议/TCP-重传机制/","link":"","permalink":"http://blog.crazylaw.cn/2019/08/07/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE/TCP-%E9%87%8D%E4%BC%A0%E6%9C%BA%E5%88%B6/","excerpt":"前言TCP 要保证所有的数据包都可以到达，所以，必需要有重传机制。 其中涉及 ACK 包比较关键，因为在窗口的这篇文章中，已经说过了，ACK 机制起始就是类似于一种反馈机制。","text":"前言TCP 要保证所有的数据包都可以到达，所以，必需要有重传机制。 其中涉及 ACK 包比较关键，因为在窗口的这篇文章中，已经说过了，ACK 机制起始就是类似于一种反馈机制。 TCP 重传机制注意，接收端给发送端的 Ack 确认只会确认最后一个连续的包，比如，发送端发了 1,2,3,4,5 一共五份数据，接收端收到了 1，2，于是回 ack 3，然后收到了 4（注意此时 3 没收到），此时的 TCP 会怎么办？我们要知道，因为正如前面所说的，SeqNum 和 Ack 是以字节数为单位，所以 ack 的时候，不能跳着确认，只能确认最大的连续收到的包，不然，发送端就以为之前的都收到了。 超时重传机制一种是不回 ack，死等 3，当发送方发现收不到 3 的 ack 超时后，会重传 3。一旦接收方收到 3 后，会 ack 回 4——意味着 3 和 4 都收到了。 但是，这种方式会有比较严重的问题，那就是因为要死等 3，所以会导致 4 和 5 即便已经收到了，而发送方也完全不知道发生了什么事，因为没有收到 Ack，所以，发送方可能会悲观地认为也丢了，所以有可能也会导致 4 和 5 的重传。 对此有两种选择： 一种是仅重传 timeout 的包。也就是第 3 份数据。 另一种是重传 timeout 后所有的数据，也就是第 3，4，5 这三份数据。 这两种方式有好也有不好。第一种会节省带宽，但是慢，第二种会快一点，但是会浪费带宽，也可能会有无用功。但总体来说都不好。因为都在等 timeout，timeout 可能会很长（在下篇会说 TCP 是怎么动态地计算出 timeout 的） 快速重传机制于是，TCP 引入了一种叫 Fast Retransmit 的算法，不以时间驱动，而以数据驱动重传。也就是说，如果，包没有连续到达，就 ack 最后那个可能被丢了的包，如果发送方连续收到 3 次相同的 ack，就重传。Fast Retransmit 的好处是不用等 timeout 了再重传。 比如：如果发送方发出了 1，2，3，4，5 份数据，第一份先到送了，于是就 ack 回 2，结果 2 因为某些原因没收到，3 到达了，于是还是 ack 回 2，后面的 4 和 5 都到了，但是还是 ack 回 2，因为 2 还是没有收到，于是发送端收到了三个 ack=2 的确认，知道了 2 还没有到，于是就马上重转 2。然后，接收端收到了 2，此时因为 3，4，5 都收到了，于是 ack 回 6。示意图如下： Fast Retransmit 只解决了一个问题，就是 timeout 的问题，它依然面临一个艰难的选择，就是重转之前的一个还是重装所有的问题。对于上面的示例来说，是重传#2 呢还是重传#2，#3，#4，#5 呢？因为发送端并不清楚这连续的 3 个 ack(2)是谁传回来的？也许发送端发了 20 份数据，是#6，#10，#20 传来的呢。这样，发送端很有可能要重传从 2 到 20 的这堆数据（这就是某些 TCP 的实际的实现）。可见，这是一把双刃剑。 SACK 方法另外一种更好的方式叫：Selective Acknowledgment (SACK)（参看 RFC 2018），这种方式需要在 TCP 头里加一个 SACK 的东西，ACK 还是 Fast Retransmit 的 ACK，SACK 则是汇报收到的数据碎版。参看下图： 这样，在发送端就可以根据回传的 SACK 来知道哪些数据到了，哪些没有到。于是就优化了 Fast Retransmit 的算法。当然，这个协议需要两边都支持。在 Linux 下，可以通过 tcp_sack 参数打开这个功能（Linux 2.4 后默认打开）。s这里还需要注意一个问题——接收方 Reneging，所谓 Reneging 的意思就是接收方有权把已经报给发送端 SACK 里的数据给丢了。这样干是不被鼓励的，因为这个事会把问题复杂化了，但是，接收方这么做可能会有些极端情况，比如要把内存给别的更重要的东西。所以，发送方也不能完全依赖 SACK，还是要依赖 ACK，并维护 Time-Out，如果后续的 ACK 没有增长，那么还是要把 SACK 的东西重传，另外，接收端这边永远不能把 SACK 的包标记为 Ack。 注意：SACK 会消费发送方的资源，试想，如果一个攻击者给数据发送方发一堆 SACK 的选项，这会导致发送方开始要重传甚至遍历已经发出的数据，这会消耗很多发送端的资源。详细的东西请参看《TCP SACK 的性能权衡》 Duplicate SACKDuplicate SACK 又称 D-SACK，其主要使用了 SACK 来告诉发送方有哪些数据被重复接收了。RFC-2833 里有详细描述和示例。下面举几个例子（来源于 RFC-2833） D-SACK 使用了 SACK 的第一个段来做标志， 如果 SACK 的第一个段的范围被 ACK 所覆盖，那么就是 D-SACK 如果 SACK 的第一个段的范围被 SACK 的第二个段覆盖，那么就是 D-SACK 示例一：ACK 丢包下面的示例中，丢了两个 ACK，所以，发送端重传了第一个数据包（3000-3499），于是接收端发现重复收到，于是回了一个 SACK=3000-3500，因为 ACK 都到了 4000 意味着收到了 4000 之前的所有数据，所以这个 SACK 就是 D-SACK——旨在告诉发送端我收到了重复的数据，而且我们的发送端还知道，数据包没有丢，丢的是 ACK 包。 123456789Transmitted Received ACK SentSegment Segment (Including SACK Blocks)3000-3499 3000-3499 3500 (ACK dropped)3500-3999 3500-3999 4000 (ACK dropped)3000-3499 3000-3499 4000, SACK&#x3D;3000-3500 示例二，网络延误下面的示例中，网络包（1000-1499）被网络给延误了，导致发送方没有收到 ACK，而后面到达的三个包触发了“Fast Retransmit 算法”，所以重传，但重传时，被延误的包又到了，所以，回了一个 SACK=1000-1500，因为 ACK 已到了 3000，所以，这个 SACK 是 D-SACK——标识收到了重复的包。 这个案例下，发送端知道之前因为“Fast Retransmit 算法”触发的重传不是因为发出去的包丢了，也不是因为回应的 ACK 包丢了，而是因为网络延时了。 1234567891011121314151617Transmitted Received ACK SentSegment Segment (Including SACK Blocks)500-999 500-999 10001000-1499 (delayed)1500-1999 1500-1999 1000, SACK&#x3D;1500-20002000-2499 2000-2499 1000, SACK&#x3D;1500-25002500-2999 2500-2999 1000, SACK&#x3D;1500-30001000-1499 1000-1499 3000 1000-1499 3000, SACK&#x3D;1000-1500 可见，引入了 D-SACK，有这么几个好处：s1）可以让发送方知道，是发出去的包丢了，还是回来的 ACK 包丢了。 2）是不是自己的 timeout 太小了，导致重传。 3）网络上出现了先发的包后到的情况（又称 reordering） 4）网络上是不是把我的数据包给复制了。 知道这些东西可以很好得帮助 TCP 了解网络情况，从而可以更好的做网络上的流控。 Linux 下的 tcp_dsack 参数用于开启这个功能（Linux 2.4 后默认打开）","categories":[{"name":"网络协议","slug":"网络协议","permalink":"http://blog.crazylaw.cn/categories/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE/"}],"tags":[{"name":"TCP","slug":"TCP","permalink":"http://blog.crazylaw.cn/tags/TCP/"}]},{"title":"【网络协议】TCP 窗口","slug":"网络协议/TCP-窗口","date":"2019-08-06T09:43:43.000Z","updated":"2021-03-20T16:25:01.822Z","comments":true,"path":"2019/08/06/网络协议/TCP-窗口/","link":"","permalink":"http://blog.crazylaw.cn/2019/08/06/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE/TCP-%E7%AA%97%E5%8F%A3/","excerpt":"前言由于最近写了一篇关于 TCP 协议的文章，所以一些更加细节的内容，更偏向 TCP 独立协议的，拿出来独立记录。 为了获得最优的连接速率，使用 TCP 窗口来控制流速率（flow control），滑动窗口就是一种主要的机制。这个窗口允许源端在给定连接传送数据分段而不用等待目标端返回 ACK，一句话描述：窗口的大小决定在不需要对端响应（acknowledgement）情况下传送数据的数量。​ 官方定义：“The amount of octets that can be transmitted without receiving an acknowledgement from the other side”。","text":"前言由于最近写了一篇关于 TCP 协议的文章，所以一些更加细节的内容，更偏向 TCP 独立协议的，拿出来独立记录。 为了获得最优的连接速率，使用 TCP 窗口来控制流速率（flow control），滑动窗口就是一种主要的机制。这个窗口允许源端在给定连接传送数据分段而不用等待目标端返回 ACK，一句话描述：窗口的大小决定在不需要对端响应（acknowledgement）情况下传送数据的数量。​ 官方定义：“The amount of octets that can be transmitted without receiving an acknowledgement from the other side”。 TCP 窗口机制比如我让发送的每一个包都有一个 id，接收端必须对每一个包进行确认，这样设备 A 一次多发送几个片段，而不必等候 ACK，同时接收端也要告知它能够收多少，这样发送端发起来也有个限制，当然还需要保证顺序性，不要乱序，对于乱序的状况，我们可以允许等待一定情况下的乱序，比如说先缓存提前到的数据，然后去等待需要的数据，如果一定时间没来就 DROP 掉，来保证顺序性！ 在 TCP/IP 协议栈中，滑动窗口的引入可以解决此问题，先来看从概念上数据分为哪些类 TCP header 中有一个 Window Size 字段，它其实是指接收端的窗口，即接收窗口，用来告知发送端自己所能接收的数据量，从而达到一部分流控的目的。其实 TCP 在整个发送过程中，也在度量当前的网络状态，目的是为了维持一个健康稳定的发送过程，比如拥塞控制。因此，数据是在某些机制的控制下进行传输的，就是窗口机制。发送端的发送窗口是基于接收端的接收窗口来计算的，也就是我们常说的 TCP 是有连接的发送，数据传输需要对端确认，发送的数据分为如下四类来看，图 1 和图 2 介绍的同一个东西 已经发送并且对端确认（Sent/ACKed），发送窗外，缓冲区外 已经发送但未收到确认数据（Sent/UnACKed），发送窗内，缓冲区内 允许发送但尚未防的数据 ​（Unsent/Inside），发送窗内，缓冲区内 未发送暂不允许（Unsent/Outside），发送窗外，缓冲区内 ​ Sent and Acknowledged：这些数据表示已经发送成功并已经被确认的数据，比如图中的前 31 个 bytes，这些数据其实的位置是在窗口之外了，因为窗口内顺序最低的被确认之后，要移除窗口，实际上是窗口进行合拢，同时打开接收新的带发送的数据 Send But Not Yet Acknowledged：这部分数据称为发送但没有被确认，数据被发送出去，没有收到接收端的 ACK，认为并没有完成发送，这个属于窗口内的数据。 Not Sent，Recipient Ready to Receive：这部分是尽快发送的数据，这部分数据已经被加载到缓存中，也就是窗口中了，等待发送，其实这个窗口是完全有接收方告知的，接收方告知还是能够接受这些包，所以发送方需要尽快的发送这些包 Not Sent，Recipient Not Ready to Receive： 这些数据属于未发送，同时接收端也不允许发送的，因为这些数据已经超出了接收端所接收的范围 对于接收端也是有一个接收窗口的，类似发送端，接收端的数据有 3 个分类，因为接收端并不需要等待 ACK 所以它没有类似的接收并确认了的分类，情况如下 Received and ACK Not Send to Process：这部分数据属于接收了数据但是还没有被上层的应用程序接收，也是被缓存在窗口内 Received Not ACK: 已经接收并，但是还没有回复 ACK，这些包可能输属于 Delay ACK 的范畴了 Not Received：有空位，还没有被接收的数据。 TCP 窗口就是这样逐渐滑动，发送新的数据，滑动的依据就是发送数据已经收到 ACK，确认对端收到，才能继续窗口滑动发送新的数据。可以看到窗口大小对于吞吐量有着重要影响，同时 ACK 响应与系统延时又密切相关。需要说明的是：如果发送端的窗口过大会引起接收端关闭窗口，处理不过来反之，如果窗口设置较小，结果就是不能充分利用带宽，所以仔细调节窗口对于适应不同延迟和带宽要求的系统很重要。 发送窗口和可用窗口对于发送方来讲，窗口内的包括两部分，就是发送窗口（已经发送了，但是没有收到 ACK），可用窗口，接收端允许发送但是没有发送的那部分称为可用窗口。 Send Window ： 20 个 bytes 这部分值是有接收方在三次握手的时候进行通告的，同时在接收过程中也不断的通告可以发送的窗口大小，来进行适应 Window Already Sent: 已经发送的数据，但是并没有收到 ACK。 滑动窗口原理TCP 并不是每一个报文段都会回复 ACK 的，可能会对两个报文段发送一个 ACK，也可能会对多个报文段发送 1 个 ACK【累计 ACK】，比如说发送方有 1/2/3 3 个报文段，先发送了 2,3 两个报文段，但是接收方期望收到 1 报文段，这个时候 2,3 报文段就只能放在缓存中等待报文 1 的空洞被填上，如果报文 1，一直不来，报文 2/3 也将被丢弃，如果报文 1 来了，那么会发送一个 ACK 对这 3 个报文进行一次确认。 举一个例子来说明一下滑动窗口的原理： 假设 32~45 这些数据，是上层 Application 发送给 TCP 的，TCP 将其分成四个 Segment 来发往 internet seg1 3234 seg2 3536 seg3 3741 seg4 4245 这四个片段，依次发送出去，此时假设接收端之接收到了 seg1 seg2 seg4 此时接收端的行为是回复一个 ACK 包说明已经接收到了 32~36 的数据，并将 seg4 进行缓存（保证顺序，产生一个保存 seg3 的 hole） 发送端收到 ACK 之后，就会将 32~36 的数据包从发送并没有确认切到发送已经确认，提出窗口，这个时候窗口向右移动 假设接收端通告的 Window Size 仍然不变，此时窗口右移，产生一些新的空位，这些是接收端允许发送的范畴 对于丢失的 seg3，如果超过一定时间，TCP 就会重新传送（重传机制），重传成功会 seg3 seg4 一块被确认，不成功，seg4 也将被丢弃 就是不断重复着上述的过程，随着窗口不断滑动，将真个数据流发送到接收端，实际上接收端的 Window Size 通告也是会变化的，接收端根据这个值来确定何时及发送多少数据，从对数据流进行流控。原理图如下图所示： 滑动窗口动态调整主要是根据接收端的接收情况，动态去调整 Window Size，然后来控制发送端的数据流量。 客户端不断快速发送数据，服务器接收相对较慢，看下实验的结果： 包 175，发送 ACK 携带 WIN = 384，告知客户端，现在只能接收 384 个字节 包 176，客户端果真只发送了 384 个字节，Wireshark 也比较智能，也宣告 TCP Window Full 包 177，服务器回复一个 ACK，并通告窗口为 0，说明接收方已经收到所有数据，并保存到缓冲区，但是这个时候应用程序并没有接收这些数据，导致缓冲区没有更多的空间，故通告窗口为 0, 这也就是所谓的零窗口，零窗口期间，发送方停止发送数据 客户端察觉到窗口为 0，则不再发送数据给接收方 包 178，接收方发送一个窗口通告，告知发送方已经有接收数据的能力了，可以发送数据包了 包 179，收到窗口通告之后，就发送缓冲区内的数据了. TCP 窗口大小最早 TCP 协议涉及用来大范围网络传输时候，其实是没有超过 56Kb/s 的 ​ 连接速度的。因此，TCP 包头中只保留了 16bit 用来标识窗口大小，允许的最大缓存大小不超过 64KB。为了打破这一限制，RFC1323 规定了 TCP 窗口尺寸选择，是在 TCP 连接开始的时候三步握手的时候协商的（SYN, SYN-ACK,ACK），会协商一个 Window size scaling factor，之后交互数据中的是 Window size value，所以最终的窗口大小是二者的乘积. Window size value: 64 or 0000 0000 0100 0000 (16 bits) ​- Window size scaling factor: 256 or 2 ^ 8 (as advertised by the 1st packet) The actual window size is 16,384 (64 * 256) 这里的窗口大小就意味着，直到发送 16384 个字节，才会停止等待对方的 ACK.随着双方回话继续，窗口的大小可以修改 window size value 参数完成变窄或变宽，但是注意：Window size scaling factor 乘积因子必须保持不变。在 RFC1323 中规定的偏移（shift count）是 14，也就是说最大的窗口可以达到 Gbit，很大。 TCP 窗口的参数设置TCP 窗口起着控制流量的作用，实际使用时这是一个双端协调的过程，还涉及到 TCP 的慢启动​（Rapid Increase/Multiplicative Decrease），拥塞避免，拥塞窗口和拥塞控制。可以记住，发送速率是由 min（拥塞窗口[cwnd]，接收窗口[rwnd]），接收窗口在下文有讲。 TCP 窗口优化设置 ​TCP​ 窗口既然那么重要，那要怎么设置，一个简单的原则是 2 倍的 BDP.这里的 BDP 的意思是 bandwidth-delay product，也就是带宽和时延的乘积，带宽对于网络取最差连接的带宽。 1buffer size &#x3D; 2 * bandwidth * delay​ 还有一种简单的方式，使用 ping 来计算网络的环回时延（RTT），然后表达为： 1buffer size &#x3D; bandwidth * RTT​ 为什么是 2 倍？因为可以这么想，如果滑动窗口是 bandwidth\\*delay，当发送一次数据最后一个字节刚到时，对端要回 ACK 才能继续发送，就需要等待一次单向时延的时间(可以统称为RTT/2)，所以当是 2 倍时，刚好就能在等 ACK 的时间继续发送数据，等收到 ACK 时数据刚好发送完成，这样就提高了效率。 举个例子：带宽是 20Mbps,通过 ping 我们计算单向时延是 20ms，那么可以计算：20000000bps*8*0.02 = 52,428bytes​，因此我们最优窗口用 104,856 bytes = 2 x 52,428，所以说当发送者发送 104,856 bytes 数据后才需要等待一个 ACK 响应，当发送了一半的时候，对端已经收到并且返回 ACK（理想情况），等到 ACK 回来，又把剩下的一半发送出去了，所以发送端就无需等待 ACK 返回。 注意我们这里的 bps(bit peer second)，所以转成 bytes 的时候需要注意 8 倍的转换 发现了么？这里的窗口已经明显大于 64KB 了，所以机制改善了。 TCP 拥塞控制现在我们看看到底如何控制流量。TCP 在传输数据时和 windows size 关系密切，本身窗口用来控制流量，在传输数据时，发送方数据超过接收方就会丢包，流量控制，流量控制要求数据传输双方在每次交互时声明各自的接收窗口「rwnd」大小，用来表示自己最大能保存多少数据，这主要是针对接收方而言的，通俗点儿说就是让发送方知道接收方能吃几碗饭，如果窗口衰减到零，也就是发送方不能再发了，那么就说明吃饱了，必须消化消化，如果硬撑胀漏了，那就是丢包了。 TCP 的拥塞控制主要依赖于 拥塞窗口(congestion window, cwnd) 和 慢启动阈值(slow start threshold, ssthresh)。cwnd 是发送端根据网络的拥塞程度所预设的一个窗口大小，而 ssthresh 则是慢启动窗口的阈值，cwnd 超过此阈值则转变控制策略。 TCP 拥塞控制的主要算法有 慢启动(Slow Start)、拥塞避免(Congestion Avoidance)、快速重传(Fast Retransmit)、快速恢复(Fast Recovery)等。 慢启动(Slow Start)虽然流量控制可以避免发送方过载接收方，但是却无法避免过载网络，这是因为接收窗口「rwnd」只反映了服务器个体的情况，却无法反映网络整体的情况。s为了避免网络过载，慢启动引入了拥塞窗口「cwnd」s 的概念，用来表示发送方在得到接收方确认前，最大允许传输的未经确认的数据。「cwnd」同「rwnd」相比不同的是：它只是发送方的一个内部参数，无需通知给接收方，其初始值往往比较小，然后随着数据包被接收方确认，窗口成倍扩大，有点类似于拳击比赛，开始时不了解敌情，往往是次拳试探，慢慢心里有底了，开始逐渐加大重拳进攻的力度。 在慢启动的过程中，随着「cwnd」的增加，可能会出现网络过载，其外在表现就是丢包，一旦出现此类问题，「cwnd」的大小会迅速衰减，以便网络能够缓过来。 说明：网络中实际传输的未经确认的数据大小取决于「rwnd」和「cwnd」中的小值。 拥塞避免 ​从慢启动的介绍中，我们能看到，发送方通过对「cwnd」大小的控制，能够避免网络过载，在此过程中，丢包与其说是一个网络问题，倒不如说是一种反馈机制，通过它我们可以感知到发生了网络拥塞，进而调整数据传输策略，实际上，这里还有一个慢启动阈值「ssthresh」的概念，如果「cwnd」小于「ssthresh」，那么表示在慢启动阶段；如果「cwnd」大于「ssthresh」，那么表示在拥塞避免阶段，此时「cwnd」不再像慢启动阶段那样呈指数级整整，而是趋向于线性增长，以期避免网络拥塞，此阶段有多种算法实现，通常保持缺省即可。 如何调整「rwnd」到一个合理值很多时候 TCP 的传输速率异常偏低，很有可能是接收窗口「rwnd」过小导致，尤其对于时延较大的网络，实际上接收窗口「rwnd」的合理值取决于 BDP 的大小，也就是带宽和延迟的乘积。假设带宽是 100Mbps，延迟是 100ms，那么计算过程如下： 1BDP &#x3D; 100Mbps * 100ms &#x3D; (100 &#x2F; 8) * (100 &#x2F; 1000) &#x3D; 1.25MB​ 此问题下如果想最大限度提升吞度量，接收窗口「rwnd」的大小不应小于 1.25MB。 如何调整「cwnd」到一个合理值一般来说「cwnd」的初始值取决于 MSS 的大小，计算方法如下： 1min(4 * MSS, max(2 * MSS, 4380)) 具体来说，新建 TCP 连接时，cwnd 需初始化为一个或几个最大发送报文段大小(send maximum segment size, SMSS 或者有一些也叫 MSS)。具体规则（IW 为初始窗口大小）： 1234567IW &#x3D; 1*(SMSS) (if SMSS &lt;&#x3D; 2190 bytes)IW &#x3D; 2*(SMSS) and not more than 2 segments (if SMSS &gt; 2190 bytes)IW &#x3D; 3*(SMSS) and not more than 3 segments (if 2190 ≥ SMSS &gt; 1095 bytes)IW &#x3D; 4*(SMSS) and not more than 4 segments (otherwise) 以太网标准的 MSS 大小通常是 1460，所以「cwnd」的初始值是 3MSS。当我们浏览视频或者下载软件的时候，「cwnd」初始值的影响并不明显，这是因为传输的数据量比较大，时间比较长，相比之下，即便慢启动阶段「cwnd」初始值比较小，也会在相对很短的时间内加速到满窗口，基本上可以忽略不计。 不过当我们浏览网页的时候，情况就不一样了，这是因为传输的数据量比较小，时间比较短，相比之下，如果慢启动阶段「cwnd」初始值比较小，那么很可能还没来得及加速到满窗口，通讯就结束了。这就好比博尔特参加百米比赛，如果起跑慢的话，即便他的加速很快，也可能拿不到好成绩，因为还没等他完全跑起来，终点线已经到了。 如果 TCP 连接一建立就向服务器大量发包，很容易导致拥塞。因此，新建立的连接不能一开始就大量发送数据包，而是应该根据网络状况，逐步地增加每次发送数据包的量，这就是慢启动。慢启动通常在新建立 TCP 连接或由于 RTO（重传超时） 而丢包时执行。 当 cwnd 值超过 ssthresh 值时，慢启动过程结束，进入拥塞避免阶段。在拥塞避免阶段，cwnd 将不再呈指数增长，而是呈线性增长。 收到一个 ACK 时，cwnd = cwnd + 1/cwnd 当每过一个 RTT 时，cwnd = cwnd + 1 这样放缓了拥塞窗口的增长速率，避免增长过快导致网络拥塞，慢慢的增加调整到网络的最佳值。 拥塞状态 等待 RTO 超时，重传数据包，此时 TCP 反应强烈： 将 ssthresh 降低为此时 cwnd 的一半 将 cwnd 重新设为初始值(IW) 重新进入慢启动阶段 原则：加法增大、乘法减小。 连续收到 3 个 duplicate ACK 时，重传数据包，无须等待 RTO。 快速重传TCP 在收到一个乱序的报文段时，会立即发送一个重复的 ACK，并且此 ACK 不可被延迟。 如果连续收到 3 个或 3 个以上重复的 ACK，TCP 会判定此报文段丢失，需要重新传递，而无需等待 RTO。这就叫做快速重传。 快速恢复快速恢复是指快速重传后直接进入拥塞避免阶段而非慢启动阶段。总结一下快速恢复的步骤（以 SMSS 为单位）： 当收到 3 个重复的 ACK 时，将 ssthresh 设置为 cwnd 的一半(ssthresh = cwnd/2)，然后将 cwnd 的值设为 ssthresh 加 3(cwnd = ssthresh + 3)，然后快速重传丢失的报文段 每次收到重复的 ACK 时，cwnd 增加 1(cwnd += 1)，并发送 1 个 packet(如果允许的话) 当收到新的 ACK 时，将 cwnd 设置为第一步中 ssthresh 的值(cwnd = ssthresh)，代表恢复过程结束快速恢复后将进入拥塞避免阶段。 还有其他的充传相关的内容会放在其他文章中。 总结从传输数据来讲，TCP/UDP 以及其他协议都可以完成数据的传输，从一端传输到另外一端，TCP 比较出众的一点就是提供一个可靠的，流控的数据传输，所以实现起来要比其他协议复杂的多，先来看下这两个修饰词的意义： Reliability ，提供 TCP 的可靠性，TCP 的传输要保证数据能够准确到达目的地，如果不能，需要能检测出来并且重新发送数据。 Data Flow Control，提供 TCP 的流控特性，管理发送数据的速率，不要超过设备的承载能力","categories":[{"name":"网络协议","slug":"网络协议","permalink":"http://blog.crazylaw.cn/categories/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE/"}],"tags":[{"name":"TCP","slug":"TCP","permalink":"http://blog.crazylaw.cn/tags/TCP/"}]},{"title":"【网络协议】TCP/IP协议","slug":"网络协议/TCP-IP","date":"2019-08-06T02:43:43.000Z","updated":"2021-03-20T16:25:01.822Z","comments":true,"path":"2019/08/06/网络协议/TCP-IP/","link":"","permalink":"http://blog.crazylaw.cn/2019/08/06/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE/TCP-IP/","excerpt":"前言由于最近看了 Redis5 的源码，所以对网络编程又进一步加深了了解，在这里，由于没有很系统的整理过 TCP/IP 协议相关的内容，所以写下这篇文章，以此来记录我最近学习到的内容，内容会分为以下几个方面。 TCP/IP 协议，三次握手，四次挥手，协议细节 协议相关的细节，优化调整内核参数 模拟异常连接，优化调整内核参数 利用 C 语言，基于 epoll 写一个支持高并发的 TCP 聊天服务器","text":"前言由于最近看了 Redis5 的源码，所以对网络编程又进一步加深了了解，在这里，由于没有很系统的整理过 TCP/IP 协议相关的内容，所以写下这篇文章，以此来记录我最近学习到的内容，内容会分为以下几个方面。 TCP/IP 协议，三次握手，四次挥手，协议细节 协议相关的细节，优化调整内核参数 模拟异常连接，优化调整内核参数 利用 C 语言，基于 epoll 写一个支持高并发的 TCP 聊天服务器 TCP/IP 协议在这里，先推荐一本书TCP/IP 详解卷 1：协议 TCP 头部协议，大家一定要牢记这一幅图，才能更好的理解协议，记得，协议图的看法：从第一行开始到第二行，是连在一起读到，切勿将协议分段读取，否则会一脸懵逼，这样子就可以看懂整个协议和字节数了。 tcp 标志位CWR &amp;&amp; ECNCWR(Congestion Window Reduced) &amp; ECN（ECN-Echo, Explicit Congestion Notification） CWR 阻塞窗口已减少，意思是告诉对端我已经按照你的要求，进行阻塞窗口减少了，并启动阻塞算法来控制我的发包速度； ECN 显式阻塞窗口通知，意思通知发送方，我接收的报文出现了阻塞，请控制发包速度。也就是说，CWR 和 ECN 必须配合使用，CWR 是收到 ECN 的应答。此外，在 tcp 三次握手时，这两个标志表明 tcp 端是否支持 ECN。如果建立连接一方支持，则在发送的 SYN 包，将 ECN 标志置为 1，如果服务端也支持，则在 ACK 包只设置 ECN。 缘由：tcp 建立连接后，报文经过经过路由或网关等网络设备后，在路由器或网关等网络设备出现阻塞时，路由器或网关等设备设置 IP 层的某个标志表明出现阻塞，这样接收可以明确知道报文出现了阻塞。然而，需要知道阻塞进行阻塞控制的是报文发送方而非接收方。所以接收方会在 ACK 报文中设置 ECN 标志，同时发送方在 ACK 中设置 CWR 标志，表明已经收到 ECN，并进行了减少阻塞窗口操作和启用阻塞算法。 URGURG(Urgent) 这就是传说中的带外数据。因为 tcp 是没有消息边界的，假如有一种情况，你已经发送了一些数据，但是此时，你要发送一些数据优先处理，就可以设置这些标志。同时如果设置了这个标志，紧急指针(报文头中, Urgent Pointer(16Bit)部分)也会设置为相应的偏移。当接受方收到 URG 数据时，不缓存在接收窗口，直接往上传给上层。具体的使用带外数据大体的方法，就是，调用 send 和 recv 是要加上 MSG_OOB 参数。同时接收方要处理 SIGURG 信号。使用 MSG_OOB 是需要注意： 紧急指针只能标示一个字节数据，所以如果发送带外数据多于一个字节，其他数据将当成是正常的数据。 接收端需要调用 fcntl(sockfd,F_SETOWN, getpid());，对 socket 描述符号进行宿主设置，否则无法捕获 SIGURG 信号。 如果设置选项 SO_OOBINLINE，那么将不能使用 MSG_OOB 参数接收的报文(调用报错)，紧急指针的字符将被正常读出来，如果需要判断是否紧急数据，则需要提前判断：ioctl (fd,SIOCATMARK,&amp;flag);if (flag) {read(sockfd,&amp;ch,1);。 不过，据说这个带外数据在实际上，用得很少。 PSHPSH（Push） tcp 报文的流动，先是发送方塞进发送方的缓存再发送；同样接收方是先塞到接收方的缓存再投递到应用。PSH 标志的意思是，无论接收或发送方，都不用缓存报文，直接接收投递给上层应用或直接发送。PSH 标志可以提供报文发送的实时性。如果设置了 SO_NODELAY 选项(也就是关闭 Nagle 算法)，可以强制设置这个标志。 SYN &amp;&amp; ACK &amp;&amp; FIN &amp;&amp; RST SYN(Synchronize), ACK(Acknowledgement), FIN（Finish）和 RST(Reset) 这几个标记比较容易理解。 SYN, Synchronize sequence numbers。 ACK, Acknowledgement Number 有效，应答标记。 FIN,发送端结束发送。 RST 连接不可达。 tcp 选项(不完全)tcp 除了 20 字节基本数据外，后面还包括了最多 40 个字节的 tcp 的选项。tcp 选项一般存储为 kind/type(1byte) length(1byte) value 的格式式，不同的选项具体格式有所不同。这里简单罗列一些常见的 tcp 选项并做简单介绍。 MSS(Maximum Segment Size)tcp 报文最大传输长读，tcp 在三次握手建立阶段，在 SYN 报文交互该值，注意的是，这个数值并非协商出来的，而是由网络设备属性得出。MSS 一个常见的值是 1460(MTU1500 - IP 头部 - TCP 头部)。 详细请查看《TCP - 窗口》这一篇文章。 SACK(Selective Acknowledgements)选择 ACK，用于处理 segment 不连续的情况，这样可以减少报文重传。比如： A 向 B 发送 4 个 segment，B 收到了 1,2,4 个 segment，网络丢失了 3 这个 segment。B 收到 1,2segment 后，回应 ACK 3，表示 1,2 这两个 ACK 已经收到，同时在选项字段里面，包括 4 这个段，表示 4 这个 segment 也收到了。于是 A 就重传 3 这个 segment，不必重传 4 这个 segment。B 收到 3 这个 segment 后，直接 ACK 5，表明 3,4 都收到了。 详细请查看《TCP - 重传机制》这一篇文章。 WS(Window Scale)在 tcp 头部，Window Size(16Bit)表面接收窗口大小，但是对于现代网络而言，这个值太小了。所以 tcp 通过选项来增加这个窗口的值。WS 值的范围 0 ～ 14，表示 Window Size(16Bit)数值先向左移动的位数。这样实际上窗口的大小可达 31 位。在程序网络设计时，有个 SO_RECVBUF，表示设置接收缓冲的大小，然而需要注意的是，这个值和接收窗口的大小不完全相等，但是这个数值和接收窗口存在一定的关系，在内核配置的范围内，大小比较接近。 TS(Timestamps)Timestamps 在 tcp 选项中包括两个 32 位的 timestamp: TSval(Timestamp value)和 TSecr(Timestamp Echo Reply)。如果设置了 TS 这个选项，发送方发送时，将当前时间填入 TSval，接收方回应时，将发送方的 TSval 填入 TSecr 即可(注意发送或接收都有设置 TSval 和 TSecr )。TS 选项的存在有两个重要作用： 一是可以更加精确计算 RTT(Round-Trip-Time)，只需要在回应报文里面用当前时间减去 TSecr 即可； 二是 PAWS(Protection Against Wrapped Sequence number, 防止 sequence 回绕) 什么意思呢？比如说，发送大量的数据：0-10G，假设 segment 比较大为 1G 而且 sequence 比较小为 5G，接收端接收 1,3,4,5 数据段正常接收，收到的发送时间分别 1,3,4,5，第 2 segment 丢失了，由于 SACK，导致 2 被重传，在接收 6 时，sequence 由于回绕变成了 1，这时收到的发送时间为 6，然后又收到迷途的 2，seq 为 2，发送时间为 2，这个时间比 6 小，是不合法的，tcp 直接丢弃这个迷途的报文。 UTO(User Timeout)UTO 指的是发送 SYN，收到 ACK 的超时时间，如果在 UTO 内没有收到，则认为对端已挂。 在网络程序设计的时候，为了探测对端是否存活，经常涉及心跳报文，通过 tcp 的 keepalive 和 UTO 机制也可以实现，两者的区别是，前者可以通过心跳报文实时知道对端是否存活，二后者只有等待下次调用发送或接收函数才可以断定： 1) SO_KEEPALIVE 相关选项 设置 SO_KEEPALIVE 选项，打开 keepalive 机制。 设置 TCP_KEEPIDLE 选项，空闲时间间隔启动 keepalive 机制，默认为 2 小时。 设置 TCP_KEEPINTVL 选项，keepalive 机制启动后，每隔多长时间发送一个 keepalive 报文。默认为 75 秒。 设置 TCP_KEEPCNT 选项，设置发送多少个 keepalive 数据包都没有正常响应，则断定对端已经崩溃。默认为 9。 由于 tcp 有超时重传机制，如果对于 ACK 丢失的情况，keepalive 机制将有可能失效。 三次握手在这里我们老生常谈的一个东西，就是 TCP/IP 协议，基本上我们常用的链接，大多数都是基于 TCP 协议来的，那么这个协议中包含了 2 个比较重要的概念，分别上 三次握手 和 四次挥手，相信大家当了一定程度，都对这 2 个概念有所耳闻，那么这 2 个概念的流程是怎么样的呢。我们用图表示出来。 tcpdump 抓包查看 TCP 协议这里，我们用 redis 来测试。 123456root@8152f1016ea8:&#x2F;data# tcpdump -iany tcp port 6379 -Stcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on any, link-type LINUX_SLL (Linux cooked), capture size 262144 bytes06:18:05.136476 IP localhost.34698 &gt; localhost.6379: Flags [S], seq 493310144, win 43690, options [mss 65495,sackOK,TS val 10045219 ecr 0,nop,wscale 7], length 006:18:05.136570 IP localhost.6379 &gt; localhost.34698: Flags [S.], seq 3870646421, ack 493310145, win 43690, options [mss 65495,sackOK,TS val 10045219 ecr 10045219,nop,wscale 7], length 006:18:05.136602 IP localhost.34698 &gt; localhost.6379: Flags [.], ack 3870646422, win 342, options [nop,nop,TS val 10045219 ecr 10045219], length 0 看上述情况，我们用tcpdump 监听了所有网卡，并且是TCP协议，并且 src 或者 dst 的端口为 6379 的协议，加上-S是为了现实绝对值，否则tcpdump将显示相对值，对于小白来说，绝对值比相对值更好看。对于老手来说，相对值可以排除无效的内容，简化内容，更利于查看分析。 第一次握手： 106:18:05.136476 IP localhost.34698 &gt; localhost.6379: Flags [S], seq 493310144, win 43690, options [mss 65495,sackOK,TS val 10045219 ecr 0,nop,wscale 7], length 0 这条协议，我们看到是我们本地生成打开了一个 fd，并且监听了 34698 端口，并且这个协议是发送SYN协议的一条协议，代表这个协议中，SYN 标识位被设置为了 1，序列包被设置为了：493310144，并且滑动窗口大小为：43690，代表客户端目前最大可以接收 43690 字节的数据，options 代表 tcp 可选配置，在这里，设置了 MSS 最大发送的数据包：65495 sackOK：启用了 SACK 算法， TS 时间戳 val ：发送端时间 10045219 ecr ：接收端时间 0 nop: 占位符，没任何选项 wscale ：窗口因子 7 length: 数据包长度为 0 这时候客户端连接会变成：SYN-SENT 状态 第二次握手： 106:18:05.136570 IP localhost.6379 &gt; localhost.34698: Flags [S.], seq 3870646421, ack 493310145, win 43690, options [mss 65495,sackOK,TS val 10045219 ecr 10045219,nop,wscale 7], length 0 这条协议，我们看到 redis 服务器返回给了客户端一个协议，这个协议包含了SYN和ACK，代表这个协议的SYN和ACK标志位都被设置成了 1，所以这个协议包的seq和ack是有效的，滑动窗口大小为 43690,代表服务端可以读取 43690 字节的数据，options 代表 tcp 的可选配置。 MSS 最大发送的数据包：65495 sackOK：启用了 SACK 算法， TS 时间戳 val ：发送端时间 10045219 ecr ：接收端时间 10045219 nop: 占位符，没任何选项 wscale ：窗口因子 7 length: 数据包长度为 0 这时候服务端连接会变成：SYN-RECEIVED 状态 第三次握手： 106:18:05.136602 IP localhost.34698 &gt; localhost.6379: Flags [.], ack 3870646422, win 342, options [nop,nop,TS val 10045219 ecr 10045219], length 0 这条协议，我们看到，包含了ACK包，所以只有ACK的标志位被设置位了 1，代表只有ack这个值是有效的，客户端告诉服务端，当前窗口大小为 342，所以最大只能发送 342 字节的数据， op: 占位符，没任何选项 op: 占位符，没任何选项 TS 时间戳 val ：发送端时间 10045219 ecr ：接收端时间 10045219 length: 数据包长度为 0 到这里为止，我们的三次握手就完成了，2 者的变的状态都会变成ESTABLISHED，完成了我们的三次握手，接下来就可以发送数据包。 四次挥手为什么我们这里是四次挥手呢，这个也是一个老生常谈的话题了，因为我们的 TCP 协议 是全双工的，不能单边的关闭其中一边。 tcpdump 抓包查看 TCP 协议同样的，我们用 redis 的客户端和服务端为例子。 1234567891007:04:52.517203 IP localhost.6379 &gt; localhost.34698: Flags [.], ack 493310162, win 342, options [nop,nop,TS val 10326272 ecr 10317312], length 007:05:07.843078 IP localhost.34698 &gt; localhost.6379: Flags [.], ack 3870657890, win 1427, options [nop,nop,TS val 10327808 ecr 10326272], length 007:05:07.843136 IP localhost.6379 &gt; localhost.34698: Flags [.], ack 493310162, win 342, options [nop,nop,TS val 10327808 ecr 10317312], length 007:05:23.202386 IP localhost.34698 &gt; localhost.6379: Flags [.], ack 3870657890, win 1427, options [nop,nop,TS val 10329344 ecr 10327808], length 007:05:23.202425 IP localhost.6379 &gt; localhost.34698: Flags [.], ack 493310162, win 342, options [nop,nop,TS val 10329344 ecr 10317312], length 007:05:38.529675 IP localhost.34698 &gt; localhost.6379: Flags [.], ack 3870657890, win 1427, options [nop,nop,TS val 10330880 ecr 10329344], length 007:05:38.529706 IP localhost.6379 &gt; localhost.34698: Flags [.], ack 493310162, win 342, options [nop,nop,TS val 10330880 ecr 10317312], length 007:10:26.796243 IP localhost.34698 &gt; localhost.6379: Flags [F.], seq 493310162, ack 3870657890, win 1427, options [nop,nop,TS val 10359737 ecr 10358528], length 007:10:26.796488 IP localhost.6379 &gt; localhost.34698: Flags [F.], seq 3870657890, ack 493310163, win 342, options [nop,nop,TS val 10359737 ecr 10359737], length 007:10:26.796519 IP localhost.34698 &gt; localhost.6379: Flags [.], ack 3870657891, win 1427, options [nop,nop,TS val 10359737 ecr 10359737], length 0 在这里，我们看到了很多条消息，全部都是带有 ACK 包的，倒数第三条之前的，都是常规的 ACK 包，也可以理解为心跳包。 这里，我们让客户端接收到退出信号，用：quit 命令退出客户端。 客户端：第一次挥手： 107:10:26.796243 IP localhost.34698 &gt; localhost.6379: Flags [F.], seq 493310162, ack 3870657890, win 1427, options [nop,nop,TS val 10359737 ecr 10358528], length 0 客户端状态变成：FIN_WAIT1 这一条消息中，我们看到里面有FIN和ACK的标志位被设置为了 1，所以代表数据包中的：seq和 ack是有效的，seq=493310162，ack=3870657890，需要注意的是，这条消息中的 ACK 包并不是为了 4 次挥手而存在的，他只是单纯的告诉服务端，我下一条消息的字节需要从序号为 3870657890 开始读起，所以这里为了四次挥手而存在的协议只有 seq。 服务端：第二和第三次挥手： 107:10:26.796488 IP localhost.6379 &gt; localhost.34698: Flags [F.], seq 3870657890, ack 493310163, win 342, options [nop,nop,TS val 10359737 ecr 10359737], length 0 这一条消息中，我们看到里面同样有FIN和ACK的标志位被设置为了 1，所以代表数据包中的seq和ack是有效的，这个时候，我们的 seq=3870657890(ack)，ack=(上一条消息的 seq+1)，这个时候这个 ack 包是为了四次挥手而存在的。所以这 2 个不同标志位：分别代表了第二次和第三次挥手，其中 ACK 代表第二次挥手，FIN 代表第三次挥手 这个时候，客户端会根据 ACK 包变成：FIN-WAIT-2，接着又因为接收到服务端的 FIN 包，所以会变成 TIME_WAIT 状态，服务端发送了 ACK 包变成：CLOSE-WAIT 状态，等待客户端 ACK 最后一次握手消息。 客户端：第四次挥手： 107:10:26.796519 IP localhost.34698 &gt; localhost.6379: Flags [.], ack 3870657891, win 1427, options [nop,nop,TS val 10359737 ecr 10359737], length 0 这里客户端发送了一条 ack 包，ack=(上一条消息的 seq+1)，所以服务端会在这里断开链接，变成完全关闭状态（CLOSED）。 疑问为什么建立连接协议是三次握手，而关闭连接却是四次握手呢？这是因为服务端的 LISTEN 状态下的 SOCKET 当收到 SYN 报文的建连请求后，它可以把 ACK 和 SYN（ACK 起应答作用，而 SYN 起同步作用）放在一个报文里来发送。但关闭连接时，当收到对方的 FIN 报文通知时，它仅仅表示对方没有数据发送给你了；但未必你所有的数据都全部发送给对方了，所以你可能未必会马上会关闭 SOCKET,也即你可能还需要发送一些数据给对方之后，再发送 FIN 报文给对方来表示你同意现在可以关闭连接了，所以它这里的 ACK 报文和 FIN 报文多数情况下都是分开发送的。 为什么 TIME_WAIT 状态还需要等 2MSL 后才能返回到 CLOSED 状态这是因为虽然双方都同意关闭连接了，而且握手的 4 个报文也都协调和发送完毕，按理可以直接回到 CLOSED 状态（就好比从 SYN_SEND 状态到 ESTABLISH 状态那样）： 一方面是可靠的实现 TCP 全双工连接的终止，也就是当最后的 ACK 丢失后，被动关闭端会重发 FIN，因此主动关闭端需要维持状态信息，以允许它重新发送最终的 ACK。 另一方面，但是因为我们必须要假想网络是不可靠的，你无法保证你最后发送的 ACK 报文会一定被对方收到，因此对方处于 LAST_ACK 状态下的 SOCKET 可能会因为超时未收到 ACK 报文，而重发 FIN 报文，所以这个 TIME_WAIT 状态的作用就是用来重发可能丢失的 ACK 报文。 TCP 在 2MSL 等待期间，定义这个连接(4 元组)不能再使用，任何迟到的报文都会丢弃。设想如果没有 2MSL 的限制，恰好新到的连接正好满足原先的 4 元组，这时候连接就可能接收到网络上的延迟报文就可能干扰最新建立的连接。 TCP 整个生命周期的流程图： TCP 整个生命周期的状态图： 对于状态图，可能大家对这个图第一眼是蒙蔽的，在这里，我们叙述一下流程。 我们的Client或者 Server都是从 CLOSE状态启动的，Server 启动之后，就是到了LISTEN状态，Client 启动之后，就是连接，也就是三次握手的过程，第一步就是发送 SYN 数据包，Clinet 变成了 SYN SENT状态（这是三次握手的第一步）， 影响 TCP 连接的内核参数影响 TCP 连接的内核参数可以通过一下命令来查看 123456789101112131415161718192021222324252627282930313233343536sysctl -a | grep tcpnet.ipv4.tcp_base_mss &#x3D; 1024net.ipv4.tcp_ecn &#x3D; 2net.ipv4.tcp_ecn_fallback &#x3D; 1net.ipv4.tcp_fin_timeout &#x3D; 60net.ipv4.tcp_fwmark_accept &#x3D; 0net.ipv4.tcp_keepalive_intvl &#x3D; 75net.ipv4.tcp_keepalive_probes &#x3D; 9net.ipv4.tcp_keepalive_time &#x3D; 7200net.ipv4.tcp_l3mdev_accept &#x3D; 0net.ipv4.tcp_mtu_probing &#x3D; 0net.ipv4.tcp_notsent_lowat &#x3D; 4294967295net.ipv4.tcp_orphan_retries &#x3D; 0net.ipv4.tcp_probe_interval &#x3D; 600net.ipv4.tcp_probe_threshold &#x3D; 8net.ipv4.tcp_reordering &#x3D; 3net.ipv4.tcp_retries1 &#x3D; 3net.ipv4.tcp_retries2 &#x3D; 15net.ipv4.tcp_syn_retries &#x3D; 6net.ipv4.tcp_synack_retries &#x3D; 5net.ipv4.tcp_syncookies &#x3D; 1net.ipv4.vs.secure_tcp &#x3D; 0net.ipv4.vs.sloppy_tcp &#x3D; 0net.netfilter.nf_conntrack_tcp_be_liberal &#x3D; 0net.netfilter.nf_conntrack_tcp_loose &#x3D; 1net.netfilter.nf_conntrack_tcp_max_retrans &#x3D; 3net.netfilter.nf_conntrack_tcp_timeout_close &#x3D; 10net.netfilter.nf_conntrack_tcp_timeout_close_wait &#x3D; 60net.netfilter.nf_conntrack_tcp_timeout_established &#x3D; 432000net.netfilter.nf_conntrack_tcp_timeout_fin_wait &#x3D; 120net.netfilter.nf_conntrack_tcp_timeout_last_ack &#x3D; 30net.netfilter.nf_conntrack_tcp_timeout_max_retrans &#x3D; 300net.netfilter.nf_conntrack_tcp_timeout_syn_recv &#x3D; 60net.netfilter.nf_conntrack_tcp_timeout_syn_sent &#x3D; 120net.netfilter.nf_conntrack_tcp_timeout_time_wait &#x3D; 120net.netfilter.nf_conntrack_tcp_timeout_unacknowledged &#x3D; 300 如果又一些参数不在上面的话，就代表这个参数并未被开启，可以手动开启。可以通过命令直接设置，sysctl *，也可以操作 vi /etc/sysctl.conf。 写入sysctl.conf的参数并不会立刻生效，需要手动执行sysctl -p 发现系统存在大量 TIME_WAIT 状态的连接1234net.ipv4.tcp_syncookies &#x3D; 1net.ipv4.tcp_tw_reuse &#x3D; 1net.ipv4.tcp_tw_recycle &#x3D; 1net.ipv4.tcp_fin_timeout &#x3D; 30 net.ipv4.tcp_syncookies = 1 表示开启 SYN Cookies。当出现 SYN 等待队列溢出时，启用 cookies 来处理，可防范少量 SYN 攻击，默认为 0，表示关闭； net.ipv4.tcp_tw_reuse = 1 表示开启重用。允许将 TIME-WAIT sockets 重新用于新的 TCP 连接，默认为 0，表示关闭； net.ipv4.tcp_tw_recycle = 1 表示开启 TCP 连接中 TIME-WAIT sockets 的快速回收，默认为 0，表示关闭。 net.ipv4.tcp_fin_timeout 修改系統默认的 TIMEOUT 时间 backlog TCP 建立连接是要进行三次握手，但是否完成三次握手后，服务器就处理（accept）呢？ backlog 其实是连接队列，在 Linux 内核 2.2 之前，backlog 大小包括半连接状态和全连接状态两种队列大小。 半连接状态为：服务器处于 Listen 状态时收到客户端 SYN 报文时放入半连接队列中，即 SYN queue（服务器端口状态为：SYN_RCVD）。 全连接状态为：TCP 的连接状态从服务器（SYN+ACK）响应客户端后，到客户端的 ACK 报文到达服务器之前，则一直保留在半连接状态中；当服务器接收到客户端的 ACK 报文后，该条目将从半连接队列搬到全连接队列尾部，即 accept queue （服务器端口状态为：ESTABLISHED）。 在 Linux 内核 2.2 之后，分离为两个 backlog 来分别限制半连接（SYN_RCVD 状态）队列大小和全连接（ESTABLISHED 状态）队列大小。 当应用程序调用 listen 系统调用让一个 socket 进入 LISTEN 状态时，需要指定一个参数：backlog。这个参数经常被描述为，新连接队列的长度限制。 由于 TCP 建立连接需要进行 3 次握手，一个新连接在到达 ESTABLISHED 状态可以被 accept 系统调用返回给应用程序前，必须经过一个中间状态 SYN RECEIVED。这意味着，TCP/IP 协议栈在实现 backlog 队列时，有两种不同的选择： 仅使用一个队列，队列规模由 listen 系统调用 backlog 参数指定。当协议栈收到一个 SYN 包时，响应 SYN/ACK 包并且将连接加进该队列。当服务端相应的 ACK 响应包收到后，连接变为 ESTABLISHED 状态，可以向应用程序返回。这意味着队列里的连接可以有两种不同的状态：SEND RECEIVED 和 ESTABLISHED。只有后一种连接才能被 accept 系统调用返回给应用程序。 使用两个队列——SYN 队列(待完成连接队列)和 accept 队列(已完成连接队列)。状态为 SYN RECEIVED 的连接进入 SYN 队列，后续当状态变更为 ESTABLISHED 时移到 accept 队列(即收到 3 次握手中最后一个 ACK 包)。顾名思义，accept 系统调用就只是简单地从 accept 队列消费新连接。在这种情况下，listen 系统调用 backlog 参数决定 accept 队列的最大规模。 历史上，起源于 BSD 的 TCP 实现使用第一种方法。这个方案意味着，但 backlog 限制达到，系统将停止对 SYN 包响应 SYN/ACK 包。通常，协议栈只是丢弃 SYN 包(而不是回一个 RST 包)以便客户端可以重试(而不是异常退出)。TCP/IP 详解 卷 3 第 14.5 节中有提到这一点。书中作者提到，BSD 实现虽然使用了两个独立的队列，但是行为跟使用一个队列并没什么区别。在 Linux 上，情况有所不同，情况 listen 系统调用 man 文档页： 1234backlog参数的行为在Linux2.2之后有所改变。现在，它指定了等待accept系统调用的已建立连接队列的长度，而不是待完成连接请求数。待完成连接队列长度由&#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;tcp_max_syn_backlog指定；在syncookies启用的情况下，逻辑上没有最大值限制，这个设置便被忽略。 也就是说，当前版本的 Linux 实现了第二种方案，使用两个队列——一个 SYN 队列，长度系统级别可设置以及一个 accept 队列长度由应用程序指定。 总结一下： Linux 内核协议栈在收到 3 次握手最后一个 ACK 包，确认一个新连接已完成，而 accept 队列已满的情况下，会忽略这个包。一开始您可能会对此感到奇怪——别忘了 SYN RECEIVED 状态下有一个计时器实现：如果 ACK 包没有收到(或者是我们讨论的忽略)，服务端会利用计时器, 协议栈会重发 SYN/ACK 包(重试次数由/proc/sys/net/ipv4/tcp_synack_retries 决定)。 例子：一个客户正尝试连接一个已经达到其最大 backlog 的 socket 1234567891011121314151617181920212223240.000 127.0.0.1 -&gt; 127.0.0.1 TCP 74 53302 &gt; 9999 [SYN] Seq&#x3D;0 Len&#x3D;00.000 127.0.0.1 -&gt; 127.0.0.1 TCP 74 9999 &gt; 53302 [SYN, ACK] Seq&#x3D;0 Ack&#x3D;1 Len&#x3D;00.000 127.0.0.1 -&gt; 127.0.0.1 TCP 66 53302 &gt; 9999 [ACK] Seq&#x3D;1 Ack&#x3D;1 Len&#x3D;00.000 127.0.0.1 -&gt; 127.0.0.1 TCP 71 53302 &gt; 9999 [PSH, ACK] Seq&#x3D;1 Ack&#x3D;1 Len&#x3D;50.207 127.0.0.1 -&gt; 127.0.0.1 TCP 71 [TCP Retransmission] 53302 &gt; 9999 [PSH, ACK] Seq&#x3D;1 Ack&#x3D;1 Len&#x3D;50.623 127.0.0.1 -&gt; 127.0.0.1 TCP 71 [TCP Retransmission] 53302 &gt; 9999 [PSH, ACK] Seq&#x3D;1 Ack&#x3D;1 Len&#x3D;51.199 127.0.0.1 -&gt; 127.0.0.1 TCP 74 9999 &gt; 53302 [SYN, ACK] Seq&#x3D;0 Ack&#x3D;1 Len&#x3D;01.199 127.0.0.1 -&gt; 127.0.0.1 TCP 66 [TCP Dup ACK 6#1] 53302 &gt; 9999 [ACK] Seq&#x3D;6 Ack&#x3D;1 Len&#x3D;01.455 127.0.0.1 -&gt; 127.0.0.1 TCP 71 [TCP Retransmission] 53302 &gt; 9999 [PSH, ACK] Seq&#x3D;1 Ack&#x3D;1 Len&#x3D;53.123 127.0.0.1 -&gt; 127.0.0.1 TCP 71 [TCP Retransmission] 53302 &gt; 9999 [PSH, ACK] Seq&#x3D;1 Ack&#x3D;1 Len&#x3D;53.399 127.0.0.1 -&gt; 127.0.0.1 TCP 74 9999 &gt; 53302 [SYN, ACK] Seq&#x3D;0 Ack&#x3D;1 Len&#x3D;03.399 127.0.0.1 -&gt; 127.0.0.1 TCP 66 [TCP Dup ACK 10#1] 53302 &gt; 9999 [ACK] Seq&#x3D;6 Ack&#x3D;1 Len&#x3D;06.459 127.0.0.1 -&gt; 127.0.0.1 TCP 71 [TCP Retransmission] 53302 &gt; 9999 [PSH, ACK] Seq&#x3D;1 Ack&#x3D;1 Len&#x3D;57.599 127.0.0.1 -&gt; 127.0.0.1 TCP 74 9999 &gt; 53302 [SYN, ACK] Seq&#x3D;0 Ack&#x3D;1 Len&#x3D;07.599 127.0.0.1 -&gt; 127.0.0.1 TCP 66 [TCP Dup ACK 13#1] 53302 &gt; 9999 [ACK] Seq&#x3D;6 Ack&#x3D;1 Len&#x3D;013.131 127.0.0.1 -&gt; 127.0.0.1 TCP 71 [TCP Retransmission] 53302 &gt; 9999 [PSH, ACK] Seq&#x3D;1 Ack&#x3D;1 Len&#x3D;515.599 127.0.0.1 -&gt; 127.0.0.1 TCP 74 9999 &gt; 53302 [SYN, ACK] Seq&#x3D;0 Ack&#x3D;1 Len&#x3D;015.599 127.0.0.1 -&gt; 127.0.0.1 TCP 66 [TCP Dup ACK 16#1] 53302 &gt; 9999 [ACK] Seq&#x3D;6 Ack&#x3D;1 Len&#x3D;026.491 127.0.0.1 -&gt; 127.0.0.1 TCP 71 [TCP Retransmission] 53302 &gt; 9999 [PSH, ACK] Seq&#x3D;1 Ack&#x3D;1 Len&#x3D;531.599 127.0.0.1 -&gt; 127.0.0.1 TCP 74 9999 &gt; 53302 [SYN, ACK] Seq&#x3D;0 Ack&#x3D;1 Len&#x3D;031.599 127.0.0.1 -&gt; 127.0.0.1 TCP 66 [TCP Dup ACK 19#1] 53302 &gt; 9999 [ACK] Seq&#x3D;6 Ack&#x3D;1 Len&#x3D;053.179 127.0.0.1 -&gt; 127.0.0.1 TCP 71 [TCP Retransmission] 53302 &gt; 9999 [PSH, ACK] Seq&#x3D;1 Ack&#x3D;1 Len&#x3D;5106.491 127.0.0.1 -&gt; 127.0.0.1 TCP 71 [TCP Retransmission] 53302 &gt; 9999 [PSH, ACK] Seq&#x3D;1 Ack&#x3D;1 Len&#x3D;5106.491 127.0.0.1 -&gt; 127.0.0.1 TCP 54 9999 &gt; 53302 [RST] Seq&#x3D;1 Len&#x3D;0 由于客户端的 TCP 实现在收到多个 SYN/ACK 包时，认为 ACK 包已经丢失了并且重传它。如果在 SYN/ACK 重试次数达到限制前，服务端应用从 accept 队列接收连接，使得 backlog 减少，那么协议栈会处理这些重传的 ACK 包，将连接状态从 SYN RECEIVED 变更到 ESTABLISHED 并且将其加入 accept 队列。否则，正如以上包跟踪所示，客户读会收到一个 RST 包宣告连接失败。 在客户端看来，第一次收到 SYN/ACK 包之后，连接就会进入 ESTABLISHED 状态。如果这时客户端首先开始发送数据，那么数据也会被重传。好在 TCP 有慢启动机制，在服务端还没进入 ESTABLISHED 之前，客户端能发送的数据非常有限。 相反，如果客户端一开始就在等待服务端，而服务端 backlog 没能减少，那么最后的结果是连接在客户端看来是 ESTABLISHED 状态，但在服务端看来是 CLOSED 状态。这也就是所谓的半开连接。 SYN queue 队列长度由其中一个参数指定，默认为 2048 1&#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;tcp_max_syn_backlog Accept queue 队列长度由 /proc/sys/net/core/somaxconn 和使用 listen 函数时传入的参数，二者取最小值。默认为 128。在 Linux 内核 2.4.25 之前，是写死在代码常量 SOMAXCONN ，在 Linux 内核 2.4.25 之后，在配置文件 /proc/sys/net/core/somaxconn 中直接修改，或者在 /etc/sysctl.conf 中配置 net.core.somaxconn = 128 。 1&#x2F;proc&#x2F;sys&#x2F;net&#x2F;core&#x2F;somaxconn 1234567[root@localhost ~]# ss -lState Recv-Q Send-Q Local Address:Port Peer Address:PortLISTEN 0 128 *:http *:*LISTEN 0 128 :::ssh :::*LISTEN 0 128 *:ssh *:*LISTEN 0 100 ::1:smtp :::*LISTEN 0 100 127.0.0.1:smtp *:* 在 LISTEN 状态，其中 Send-Q 即为 Accept queue 的最大值，Recv-Q 则表示 Accept queue 中等待被服务器 accept()。 另外客户端 connect()返回不代表 TCP 连接建立成功，有可能此时 accept queue 已满，系统会直接丢弃后续 ACK 请求；客户端误以为连接已建立，开始调用等待至超时；服务器则等待 ACK 超时，会重传 SYN+ACK 给客户端，重传次数受限 net.ipv4.tcp_synack_retries ，默认为 5，表示重发 5 次，每次等待 30~40 秒，即半连接默认时间大约为 180 秒，该参数可以在 tcp 被洪水攻击是临时启用这个参数。 keepalive其实 keepalive 的原理就是 TCP 内嵌的一个心跳包。 以服务器端为例，如果当前 server 端检测到超过一定时间（默认是 7,200,000 milliseconds，也就是 2 个小时）没有数据传输，那么会向 client 端发送一个 keep-alive packet（该 keep-alive packet 就是 ACK 和当前 TCP 序列号减一的组合），此时 client 端应该为以下三种情况之一： client 端仍然存在，网络连接状况良好。此时 client 端会返回一个 ACK。server 端接收到 ACK 后重置计时器（复位存活定时器），在 2 小时后再发送探测。如果 2 小时内连接上有数据传输，那么在该时间基础上向后推延 2 个小时。 客户端异常关闭，或是网络断开。在这两种情况下，client 端都不会响应。服务器没有收到对其发出探测的响应，并且在一定时间（系统默认为 1000 ms）后重复发送 keep-alive packet，并且重复发送一定次数（2000 XP 2003 系统默认为 5 次, Vista 后的系统默认为 10 次）。 客户端曾经崩溃，但已经重启。这种情况下，服务器将会收到对其存活探测的响应，但该响应是一个复位，从而引起服务器对连接的终止。 对于应用程序来说，2 小时的空闲时间太长。因此，我们需要手工开启 Keepalive 功能并设置合理的 Keepalive 参数。 全局设置可更改/etc/sysctl.conf,加上: 123net.ipv4.tcp_keepalive_intvl &#x3D; 20net.ipv4.tcp_keepalive_probes &#x3D; 3net.ipv4.tcp_keepalive_time &#x3D; 60 在程序中设置如下: 12345678910111213141516#include &lt;sys/socket.h&gt;#include &lt;netinet/in.h&gt;#include &lt;arpa/inet.h&gt;#include &lt;sys/types.h&gt;#include &lt;netinet/tcp.h&gt;// 注意我这里故意用心跳包和探测包来区分，心跳包是正常心跳包，探测包是心跳包发送失败，或者接收返回的ack失败之后，发送的异常心跳包int keepAlive = 1; // 开启keepalive属性int keepIdle = 60; // 每次心跳包发送的时间间隔int keepInterval = 5; // 每个探测包的时间间隔为5 秒int keepCount = 3; // 探测尝试的次数.如果第1次探测包就收到响应了,则后2次的不再发，变回正常的心跳包setsockopt(rs, SOL_SOCKET, SO_KEEPALIVE, (void *)&amp;keepAlive, sizeof(keepAlive));setsockopt(rs, SOL_TCP, TCP_KEEPIDLE, (void*)&amp;keepIdle, sizeof(keepIdle));setsockopt(rs, SOL_TCP, TCP_KEEPINTVL, (void *)&amp;keepInterval, sizeof(keepInterval));setsockopt(rs, SOL_TCP, TCP_KEEPCNT, (void *)&amp;keepCount, sizeof(keepCount)); 在程序中表现为,当 tcp 检测到对端 socket 不再可用时(不能发出探测包,或探测包没有收到 ACK 的响应包),select 会返回 socket 可读,并且在 recv 时返回-1,同时置上 errno 为 ETIMEDOUT. 模拟异常连接基于 Epoll 的聊天服务器","categories":[{"name":"网络协议","slug":"网络协议","permalink":"http://blog.crazylaw.cn/categories/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE/"}],"tags":[{"name":"TCP","slug":"TCP","permalink":"http://blog.crazylaw.cn/tags/TCP/"}]},{"title":"【源码剖析】Redis主流程","slug":"源码剖析/redis5源码剖析","date":"2019-07-10T02:43:43.000Z","updated":"2021-03-20T16:25:01.820Z","comments":true,"path":"2019/07/10/源码剖析/redis5源码剖析/","link":"","permalink":"http://blog.crazylaw.cn/2019/07/10/%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90/redis5%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90/","excerpt":"前言从 main 函数开始，沿着代码执行路径，实际上我们可以一直追下去。但为了让本文不至于太过冗长，我们还是限定一下范围。本文的目标就定为：引领读者从 main 函数开始，一步步追踪下去，最终到达任一 Redis 命令的执行入口。 后续，再剖析各个命令的内部实现 本文基于 redis5.0 分支 为了表述清楚，本文按照如下思路进行： 先概括地介绍整个代码初始化流程（从 main 函数开始）和事件循环的结构； 再概括地介绍对于 Redis 命令请求的处理流程； 重点介绍事件机制； 对于前面介绍的各个代码处理流程，给出详细的代码调用关系，方便随时查阅； 根据这样几部分的划分，如果你只想粗读大致的处理流程，那么只需要阅读前两个部分就可以了。而后两部分则会深入到某些值得关注的细节。","text":"前言从 main 函数开始，沿着代码执行路径，实际上我们可以一直追下去。但为了让本文不至于太过冗长，我们还是限定一下范围。本文的目标就定为：引领读者从 main 函数开始，一步步追踪下去，最终到达任一 Redis 命令的执行入口。 后续，再剖析各个命令的内部实现 本文基于 redis5.0 分支 为了表述清楚，本文按照如下思路进行： 先概括地介绍整个代码初始化流程（从 main 函数开始）和事件循环的结构； 再概括地介绍对于 Redis 命令请求的处理流程； 重点介绍事件机制； 对于前面介绍的各个代码处理流程，给出详细的代码调用关系，方便随时查阅； 根据这样几部分的划分，如果你只想粗读大致的处理流程，那么只需要阅读前两个部分就可以了。而后两部分则会深入到某些值得关注的细节。 初始化流程和事件循环概述Redis 源码的 main 函数在源文件 server.c 中。main 函数开始执行后的逻辑可以分为两个阶段： 各种初始化（包括事件循环的初始化） 执行事件循环 这两个执行阶段可以用下面的流程图来表达（点击看大图）： 首先，我们看一下初始化阶段中的各个步骤： 配置加载和初始化。这一步表示 Redis 服务器基本数据结构和各种参数的初始化。在 Redis 源码中，Redis 服务器是用一个叫做 redisServer 的 struct 来表达的，里面定义了 Redis 服务器赖以运行的各种参数，比如监听的端口号和文件描述符、当前连接的各个 client 端、Redis 命令表(command table)配置、持久化相关的各种参数，等等，以及后面马上会讨论的事件循环结构。Redis 服务器在运行时就是由一个 redisServer 类型的全局变量来表示的（变量名就叫 server），这一步的初始化主要就是对于这个全局变量进行初始化。在整个初始化过程中，有一个需要特别关注的函数：populateCommandTable。它初始化了 Redis 命令表，通过它可以由任意一个 Redis 命令的名字查找该命令的配置信息（比如该命令接收的命令参数个数、执行函数入口等）。在本文的第二部分，我们将会一起来看一看如何从接收一个 Redis 命令的请求开始，一步步执行到来查阅这个命令表，从而找到该命令的执行入口。另外，这一步中还有一个值得一提的地方：在对全局的 redisServer 结构进行了初始化之后，还需要从配置文件（redis.conf）中加载配置。这个过程可能覆盖掉之前初始化过的 redisServer 结构中的某些参数。换句话说，就是先经过一轮初始化，保证 Redis 的各个内部数据结构以及参数都有缺省值，然后再从配置文件中加载自定义的配置。 创建事件循环。在 Redis 中，事件循环是用一个叫 aeEventLoop 的 struct 来表示的。「创建事件循环」这一步主要就是创建一个 aeEventLoop 结构，并存储到 server 全局变量（即前面提到的 redisServer 类型的结构）中。另外，事件循环的执行依赖系统底层的 I/O 多路复用机制(I/O multiplexing)，比如 Linux 系统上的 epoll 机制[1]。因此，这一步也包含对于底层 I/O 多路复用机制的初始化（调用系统 API）。 开始 socket 监听。服务器程序需要监听才能收到请求。根据配置，这一步可能会打开两种监听：对于 TCP 连接的监听和对于 Unix domain socket[2]的监听。「Unix domain socket」是一种高效的进程间通信(IPC[3])机制，在 POSIX 规范[4]中也有明确的定义[5]，用于在同一台主机上的两个不同进程之间进行通信，比使用 TCP 协议性能更高（因为省去了协议栈的开销）。当使用 Redis 客户端连接同一台机器上的 Redis 服务器时，可以选择使用「Unix domain socket」进行连接。但不管是哪一种监听，程序都会获得文件描述符，并存储到 server 全局变量中。对于 TCP 的监听来说，由于监听的 IP 地址和端口可以绑定多个，因此获得的用于监听 TCP 连接的文件描述符也可以包含多个。后面，程序就可以拿这一步获得的文件描述符去注册 I/O 事件回调了。 注册 timer 事件回调。Redis 作为一个单线程(single-threaded)的程序，它如果想调度一些异步执行的任务，比如周期性地执行过期 key 的回收动作，除了依赖事件循环机制，没有其它的办法。这一步就是向前面刚刚创建好的事件循环中注册一个 timer 事件，并配置成可以周期性地执行一个回调函数：serverCron。由于 Redis 只有一个主线程，因此这个函数周期性的执行也是在这个线程内，它由事件循环来驱动（即在合适的时机调用），但不影响同一个线程上其它逻辑的执行（相当于按时间分片了）。serverCron 函数到底做了什么呢？实际上，它除了周期性地执行过期 key 的回收动作，还执行了很多其它任务，比如主从重连、Cluster 节点间的重连、BGSAVE 和 AOF rewrite 的触发执行，等等。这个不是本文的重点，这里就不展开描述了。 注册 I/O 事件回调。Redis 服务端最主要的工作就是监听 I/O 事件，从中分析出来自客户端的命令请求，执行命令，然后返回响应结果。对于 I/O 事件的监听，自然也是依赖事件循环。前面提到过，Redis 可以打开两种监听：对于 TCP 连接的监听和对于 Unix domain socket 的监听。因此，这里就包含对于这两种 I/O 事件的回调的注册，两个回调函数分别是 acceptTcpHandler 和 acceptUnixHandler。对于来自 Redis 客户端的请求的处理，就会走到这两个函数中去。我们在下一部分就会讨论到这个处理过程。另外，其实 Redis 在这里还会注册一个 I/O 事件，用于通过管道(pipe[6])机制与 module 进行双向通信。这个也不是本文的重点，我们暂时忽略它。 初始化后台线程。Redis 会创建一些额外的线程，在后台运行，专门用于处理一些耗时的并且可以被延迟执行的任务（一般是一些清理工作）。在 Redis 里面这些后台线程被称为 bio(Background I/O service)。它们负责的任务包括：可以延迟执行的文件关闭操作(比如 unlink 命令的执行)，AOF 的持久化写库操作(即 fsync 调用，但注意只有可以被延迟执行的 fsync 操作才在后台线程执行)，还有一些大 key 的清除操作(比如 flushdb async 命令的执行)。可见 bio 这个名字有点名不副实，它做的事情不一定跟 I/O 有关。对于这些后台线程，我们可能还会产生一个疑问：前面的初始化过程，已经注册了一个 timer 事件回调，即 serverCron 函数，按说后台线程执行的这些任务似乎也可以放在 serverCron 中去执行。因为 serverCron 函数也是可以用来执行后台任务的。实际上这样做是不行的。前面我们已经提到过，serverCron 由事件循环来驱动，执行还是在 Redis 主线程上，相当于和主线程上执行的其它操作（主要是对于命令请求的执行）按时间进行分片了。这样的话，serverCron 里面就不能执行过于耗时的操作，否则它就会影响 Redis 执行命令的响应时间。因此，对于耗时的、并且可以被延迟执行的任务，就只能放到单独的线程中去执行了。 启动事件循环。前面创建好了事件循环的结构，但还没有真正进入循环的逻辑。过了这一步，事件循环就运行起来，驱动前面注册的 timer 事件回调和 I/O 事件回调不断执行。 注意：Redis 服务器的初始化其实还要完成很多很多事，比如加载数据到内存，Cluster 集群的初始化，module 的初始化，等等。但为了简化，上面讨论的初始化流程，只列出了我们当前关注的步骤。本文关注的是由事件驱动的整个运行机制以及跟命令执行直接相关的部分，因此我们暂时忽略掉其它不太相关的步骤。 现在，我们继续去讨论上面流程图中的第二个阶段：事件循环。 我们先想一下为什么这里需要一个循环。 一个程序启动后，如果没有循环，那么它从第一条指令一直执行到最后一条指令，然后就只能退出了。而 Redis 作为一个服务端程序，是要等着客户端不停地发来请求然后做相应的处理，不能自己执行完就退出了。因此，Redis 启动后必定要进入一个无限循环。显然，程序在每一次的循环执行中，如果有事件（包括客户端请求的 I/O 事件）发生，就会去处理这些事件。但如果没有事件发生呢？程序显然也不应该空转，而是应该等待，把整个循环阻塞住。这里的等待，就是上面流程图里的「等待事件发生」这个步骤。那么，当整个循环被阻塞住之后，什么时候再恢复执行呢？自然是等待的事件发生的时候，程序被重新唤醒，循环继续下去。这里需要的等待和唤醒操作，怎么实现呢？它们都需要依赖系统的能力才能做到（我们在文章第三部分会详细介绍）。 实际上，这种事件循环机制，对于开发过手机客户端的同学来说，是非常常见且基础的机制。比如跑在 iOS/Android 上面的 App，这些程序都有一个消息循环，负责等待各种 UI 事件（点击、滑动等）的发生，然后进行处理。同理，对应到服务端，这个循环的原理可以认为差不多，只是等待和处理的事件变成是 I/O 事件了。另外，除了 I/O 事件，整个系统在运行过程中肯定还需要根据时间来调度执行一些任务，比如延迟 100 毫秒再执行某个操作，或者周期性地每隔 1 秒执行某个任务，这就需要等待和处理另外一种事件——timer 事件。 timer 事件和 I/O 事件是两种截然不同的事件，如何由事件循环来统一调度呢？假设事件循环在空闲的时候去等待 I/O 事件的发生，那么有可能一个 timer 事件先发生了，这时事件循环就没有被及时唤醒（仍在等待 I/O 事件）；反之，如果事件循环在等待 timer 事件，而一个 I/O 事件先发生了，那么同样没能够被及时唤醒。因此，我们必须有一种机制能够同时等待这两种事件的发生。而恰好，一些系统的 API 可以做到这一点（比如我们前面提到的 epoll 机制）。 前面流程图的第二阶段已经比较清楚地表达出了事件循环的执行流程。在这里我们对于其中一些步骤需要关注的地方做一些补充说明： 查找最近的 timer 事件。如前所述，事件循环需要等待 timer 和 I/O 两种事件。对于 I/O 事件，只需要明确要等待的是哪些文件描述符就可以了；而对于 timer 事件，还需要经过一番比较，明确在当前这一轮循环中需要等待多长时间。由于系统运行过程中可能注册多个 timer 事件回调，比如先要求在 100 毫秒后执行一个回调，同时又要求在 200 毫秒后执行另一个回调，这就要求事件循环在它的每一轮执行之前，首先要找出最近需要执行的那次 timer 事件。这样事件循环在接下来的等待中就知道该等待多长时间（在这个例子中，我们需要等待 100 毫秒）。 等待事件发生。这一步我们需要能够同时等待 timer 和 I/O 两种事件的发生。要做到这一点，我们依赖系统底层的 I/O 多路复用机制(I/O multiplexing)。这种机制一般是这样设计的：它允许我们针对多个文件描述符来等待对应的 I/O 事件发生，并同时可以指定一个最长的阻塞超时时间。如果在这段阻塞时间内，有 I/O 事件发生，那么程序会被唤醒继续执行；如果一直没有 I/O 事件发生，而是指定的时间先超时了，那么程序也会被唤醒。对于 timer 事件的等待，就是依靠这里的超时机制。当然，这里的超时时间也可以指定成无限长，这就相当于只等待 I/O 事件。我们再看一下上一步查找最近的 timer 事件，查找完之后可能有三种结果，因此这一步等待也可能出现三种对应的情况： 第一种情况，查找到了一个最近的 timer 事件，它要求在未来某一个时刻触发。那么，这一步只需要把这个未来时刻转换成阻塞超时时间即可。 第二种情况，查找到了一个最近的 timer 事件，但它要求的时刻已经过去了。那么，这时候它应该立刻被触发，而不应该再有任何等待。当然，在实现的时候还是调用了事件等待的 API，只是把超时事件设置成 0 就可以达到这个效果。 第三种情况，没有查找到任何注册的 timer 事件。那么，这时候应该把超时时间设置成无限长。接下来只有 I/O 事件发生才能唤醒。 判断有 I/O 事件发生还是超时。这里是程序从上一步（可能的）阻塞状态中恢复后执行的判断逻辑。如果是 I/O 事件发生了，那么先执行 I/O 事件回调，然后根据需要把到期的 timer 事件的回调也执行掉（如果有的话）；如果是超时先发生了，那么表示只有 timer 事件需要触发（没有 I/O 事件发生），那么就直接把到期的 timer 事件的回调执行掉。 执行 I/O 事件回调。我们前面提到的对于 TCP 连接的监听和对于 Unix domain socket 的监听，这两种 I/O 事件的回调函数 acceptTcpHandler 和 acceptUnixHandler，就是在这一步被调用的。 执行 timer 事件回调。我们前面提到的周期性的回调函数 serverCron，就是在这一步被调用的。一般情况下，一个 timer 事件被处理后，它就会被从队列中删除，不会再次执行了。但 serverCron 却是被周期性调用的，这是怎么回事呢？这是因为 Redis 对于 timer 事件回调的处理设计了一个小机制：timer 事件的回调函数可以返回一个需要下次执行的毫秒数。如果返回值是正常的正值，那么 Redis 就不会把这个 timer 事件从事件循环的队列中删除，这样它后面还有机会再次执行。例如，按照默认的设置，serverCron 返回值是 100，因此它每隔 100 毫秒会执行一次（当然这个执行频率可以在 redis.conf 中通过 hz 变量来调整）。 至此，Redis 整个事件循环的轮廓我们就清楚了。Redis 主要的处理流程，包括接收请求、执行命令，以及周期性地执行后台任务（serverCron），都是由这个事件循环驱动的。当请求到来时，I/O 事件被触发，事件循环被唤醒，根据请求执行命令并返回响应结果；同时，后台异步任务（如回收过期的 key）被拆分成若干小段，由 timer 事件所触发，夹杂在 I/O 事件处理的间隙来周期性地运行。这种执行方式允许仅仅使用一个线程来处理大量的请求，并能提供快速的响应时间。当然，这种实现方式之所以能够高效运转，除了事件循环的结构之外，还得益于系统提供的异步的 I/O 多路复用机制(I/O multiplexing)。事件循环使得 CPU 资源被分时复用了，不同代码块之间并没有「真正的」并发执行，但 I/O 多路复用机制使得 CPU 和 I/O 的执行是真正并发的。而且，使用单线程还有额外的好处：避免了代码的并发执行，在访问各种数据结构的时候都无需考虑线程安全问题，从而大大降低了实现的复杂度。 Redis 命令请求的处理流程概述我们在前面讨论「注册 I/O 事件回调」的时候提到过，Redis 对于来自客户端的请求的处理，都会走到 acceptTcpHandler 或 acceptUnixHandler 这两个回调函数中去。实际上，这样描述还过于粗略。 Redis 客户端向服务器发送命令，其实可以细分为两个过程： 连接建立。客户端发起连接请求（通过 TCP 或 Unix domain socket），服务器接受连接。 命令发送、执行和响应。连接一旦建立好，客户端就可以在这个新建立的连接上发送命令数据，服务器收到后执行这个命令，并把执行结果返回给客户端。而且，在新建立的连接上，这整个的「命令发送、执行和响应」的过程就可以反复执行。上述第一个过程，「连接建立」，对应到服务端的代码，就是会走到 acceptTcpHandler 或 acceptUnixHandler 这两个回调函数中去。换句话说，Redis 服务器每收到一个新的连接请求，就会由事件循环触发一个 I/O 事件，从而执行到 acceptTcpHandler 或 acceptUnixHandler 回调函数的代码。 接下来，从 socket 编程的角度，服务器应该调用 accept 系统 API[7]来接受连接请求，并为新的连接创建出一个 socket。这个新的 socket 也就对应着一个新的文件描述符。为了在新的连接上能接收到客户端发来的命令，接下来必须在事件循环中为这个新的文件描述符注册一个 I/O 事件回调。这个过程的流程图如下： 从上面流程图可以看出，新的连接注册了一个 I/O 事件回调，即 readQueryFromClient。也就是说，对应前面讲的第二个过程，「命令发送、执行和响应」，当服务器收到命令数据的时候，也会由事件循环触发一个 I/O 事件，执行到 readQueryFromClient 回调。这个函数的实现就是在处理命令的「执行和响应」了。因此，下面我们看一下这个函数的执行流程图： 上述流程图有几个需要注意的点： 从 socket 中读入数据，是按照流的方式。也就是说，站在应用层的角度，从底层网络层读入的数据，是由一个个字节组成的字节流。而我们需要从这些字节流中解析出完整的 Redis 命令，才能知道接下来如何处理。但由于网络传输的特点，我们并不能控制一次读入多少个字节。实际上，即使服务器只是收到一个 Redis 命令的部分数据（哪怕只有一个字节），也有可能触发一次 I/O 事件回调。这时我们是调用 read 系统 API[8]来读入数据的。虽然调用 read 时我们可以指定期望读取的字节数，但它并不会保证一定能返回期望长度的数据。比如我们想读 100 个字节，但可能只能读到 80 个字节，剩下的 20 个字节可能还在网络传输中没有到达。这种情况给接收 Redis 命令的过程造成了很大的麻烦：首先，可能我们读到的数据还不够一个完整的命令，这时我们应该继续等待更多的数据到达。其次，我们可能一次性收到了大量的数据，里面包含不止一个命令，这时我们必须把里面包含的所有命令都解析出来，而且要正确解析到最后一个完整命令的边界。如果最后一个完整命令后面还有多余的数据，那么这些数据应该留在下次有更多数据到达时再处理。这个复杂的过程一般称为「粘包」。 「粘包」处理的第一个表现，就是当尝试解析出一个完整的命令时，如果解析失败了，那么上面的流程就直接退出了。接下来，如果有更多数据到达，事件循环会再次触发 I/O 事件回调，重新进入上面的流程继续处理。 「粘包」处理的第二个表现，是上面流程图中的大循环。只要暂存输入数据的 query buffer 中还有数据可以处理，那么就不停地去尝试解析完整命令，直到把里面所有的完整命令都处理完，才退出循环。 查命令表那一步，就是查找本文前面提到的由 populateCommandTable 初始化的命令表，这个命令表存储在 server.c 的全局变量 redisCommandTable 当中。命令表中存有各个 Redis 命令的执行入口。 对于命令的执行结果，在上面的流程图中只是最后存到了一个输出 buffer 中，并没有真正输出给客户端。输出给客户端的过程不在这个流程当中，而是由另外一个同样是由事件循环驱动的过程来完成。这个过程涉及很多细节，我们在这里先略过，留在后面第四部分再来讨论。 事件机制介绍在本文第一部分，我们提到过，我们必须有一种机制能够同时等待 I/O 和 timer 这两种事件的发生。这一机制就是系统底层的 I/O 多路复用机制(I/O multiplexing)。但是，在不同的系统上，存在多种不同的 I/O 多路复用机制。因此，为了方便上层程序实现，Redis 实现了一个简单的事件驱动程序库，即 ae.c 的代码，它屏蔽了系统底层在事件处理上的差异，并实现了我们前面一直在讨论的事件循环。 在 Redis 的事件库的实现中，目前它底层支持 4 种 I/O 多路复用机制： select 系统调用[9]。这应该是最早出现的一种 I/O 多路复用机制了，于 1983 年在 4.2BSD Unix 中被首次使用[10]。它是 POSIX 规范的一部分。另外，跟 select 类似的还有一个 poll 系统调用[11]，它是 1986 年在 SVR3 Unix 系统中首次使用的[10]，也遵循 POSIX 规范。只要是遵循 POSIX 规范的操作系统，它就能支持 select 和 poll 机制，因此在目前我们常见的系统中这两种 I/O 事件机制一般都是支持的。 epoll 机制[1]。epoll 是比 select 更新的一种 I/O 多路复用机制，最早出现在 Linux 内核的 2.5.44 版本中[12]。它被设计出来是为了代替旧的 select 和 poll，提供一种更高效的 I/O 机制。注意，epoll 是 Linux 系统所特有的，它不属于 POSIX 规范。 kqueue 机制[13]。kqueue 最早是 2000 年在 FreeBSD 4.1 上被设计出来的，后来也支持 NetBSD、OpenBSD、DragonflyBSD 和 macOS 系统[14]。它和 Linux 系统上的 epoll 是类似的。 event ports。这是在 illumos 系统[15]上特有的一种 I/O 事件机制。既然在不同系统上有不同的事件机制，那么 Redis 在不同系统上编译时采用的是哪个机制呢？由于在上面四种机制中，后三种是更现代，也是比 select 和 poll 更高效的方案，因此 Redis 优先选择使用后三种机制。 通过上面对各种 I/O 机制所适用的操作系统的总结，我们很容易看出，如果你在 macOS 上编译 Redis，那么它底层会选用 kqueue；而如果在 Linux 上编译则会选择 epoll，这也是 Redis 在实际运行中比较常见的情况。 需要注意的是，这里所依赖的 I/O 事件机制，与如何实现高并发的网络服务关系密切。很多技术同学应该都听说过 C10K 问题[16]。随着硬件和网络的发展，单机支撑 10000 个连接，甚至单机支撑百万个连接，都成为可能[17]。高性能网络编程与这些底层机制息息相关。这里推荐几篇 blog，有兴趣的话可以去仔细阅读（访问链接请参见文末参考文献）： The C10K problem[16]； Epoll is fundamentally broken[18]； The Implementation of epoll[19]； 现在我们回过头来再看一下底层的这些 I/O 事件机制是如何支持了 Redis 的事件循环的（下面的描述是对本文前面第一部分中事件循环流程的细化）： 首先，向事件循环中注册 I/O 事件回调的时候，需要指定哪个回调函数注册到哪个事件上（事件用文件描述符来表示）。事件和回调函数的对应关系，由 Redis 上层封装的事件驱动程序库来维护。具体参见函数 aeCreateFileEvent 的代码。 类似地，向事件循环中注册 timer 事件回调的时候，需要指定多长时间之后执行哪个回调函数。这里需要记录哪个回调函数预期在哪个时刻被调用，这也是由 Redis 上层封装的事件驱动程序库来维护的。具体参见函数 aeCreateTimeEvent 的代码。 底层的各种事件机制都会提供一个等待事件的操作，比如 epoll 提供的 epoll_wait API。这个等待操作一般可以指定预期等待的事件列表（事件用文件描述符来表示），并同时可以指定一个超时时间（即最大等待多长时间）。在事件循环中需要等待事件发生的时候，就调用这个等待操作，传入之前注册过的所有 I/O 事件，并把最近的 timer 事件所对应的时刻转换成这里需要的超时时间。具体参见函数 aeProcessEvents 的代码。 从上一步的等待操作中唤醒，有两种情况：如果是 I/O 事件发生了，那么就根据触发的事件查到 I/O 回调函数，进行调用；如果是超时了，那么检查所有注册过的 timer 事件，对于预期调用时刻超过当前时间的回调函数都进行调用。 最后，关于事件机制，还有一些信息值得关注：业界已经有一些比较成熟的开源的事件库了，典型的比如 libevent[20]和 libev[21]。一般来说，这些开源库屏蔽了非常复杂的底层系统细节，并对不同的系统版本实现做了兼容，是非常有价值的。那为什么 Redis 的作者还是自己实现了一套呢？在 Google Group 的一个帖子上，Redis 的作者给出了一些原因。帖子地址如下： https://groups.google.com/group/redis-db/browse_thread/thread/b52814e9ef15b8d0/ 原因大致总结起来就是： 不想引入太大的外部依赖。比如 libevent 太大了，比 Redis 的代码库还大。 方便做一些定制化的开发。 第三方库有时候会出现一些意想不到的 bug。 代码调用关系对于本文前面分析的各个代码处理流程，包括初始化、事件循环、接收命令请求、执行命令、返回响应结果等等，为了方便大家查阅，下面用一个树型图展示了部分关键函数的调用关系（图比较大，点击可以看大图）。再次提醒：下面的调用关系图基于 Redis 源码的 5.0 分支，未来很可能随着 Redis 代码库的迭代而有所变化。 上图中添加了部分注释，应该可以很清楚地和本文前面介绍过的一些流程对应上。另外，图中一些可能需要注意的细节，如下列出： 初始化过程增加了 aeSetBeforeSleepProc 和 aeSetAfterSleepProc，注册了两个回调函数，这在本文前面没有提到过。一个用于在事件循环每轮开始时调用，另一个会在每轮事件循环的阻塞等待后（即 aeApiPoll 返回后）调用。图中下面第 5 个调用流程的入口 beforeSleep，就是由这里的 aeSetBeforeSleepProc 来注册到事件循环中的。 前文提到的 serverCron 周期性地执行，就是指的在processTimeEvents 这个调用分支中调用的 timeProc 这个函数。 在数据接收处理的流程 readQueryFromClient 中，通过 lookupCommand 来查询 Redis 命令表，这个命令表也就是前面初始化时由 populateCommandTable 初始化的 redisCommandTable 全局结构。查找命令入口后，调用 server.c 的 call 函数来执行命令。图中 call 函数的下一层，就是调用各个命令的入口函数（图中只列出了几个例子）。以 get 命令的入口函数 getCommand 为例，它执行完的执行结果，最终会调用 addReply 存入到输出 buffer 中，即 client 结构的 buf 或 reply 字段中（根据执行结果的大小不同）。需要注意的是，就像前面「Redis 命令请求的处理流程」最后讨论的一样，这里只是把执行结果存到了一个输出 buffer 中，并没有真正输出给客户端。真正把响应结果发送给客户端的执行逻辑，在后面的 beforeSleep 和 sendReplyToClient 流程中。 最后将命令执行结果发送给客户端的过程，由 beforeSleep 来触发。它检查输出 buffe 中有没有需要发送给客户端的执行结果数据，如果有的话，会调用 writeToClient 尝试进行发送。如果一次性没有把数据发送完毕，那么还需要再向事件循环中注册一个写 I/O 事件回调 sendReplyToClient，在恰当的时机再次调用 writeToClient 来尝试发送。如果还是有剩余数据没有发送完毕，那么后面会由 beforeSleep 回调来再次触发这个流程。 外加一些我个人总计的 xmind 的思维导图： 参考文献： Redis 源码从哪里读起 epoll − I/O event notification facility Unix domain socket Inter-process communication POSIX.1-2017 Definitions for UNIX domain sockets Create descriptor pair for interprocess communication BSD System Calls Manual ACCEPT(2) BSD System Calls Manual READ(2) BSD System Calls Manual SELECT(2) poll vs select vs event-based BSD System Calls Manual POLL(2) Epoll from Wikipedia BSD System Calls Manual KQUEUE(2) Kqueue from Wikipedia illumos from Wikipedia The C10K problem C10k problem from Wikipedia Epoll is fundamentally broken The Implementation of epoll libevent libev","categories":[{"name":"Redis","slug":"Redis","permalink":"http://blog.crazylaw.cn/categories/Redis/"}],"tags":[{"name":"Redis，源码剖析","slug":"Redis，源码剖析","permalink":"http://blog.crazylaw.cn/tags/Redis%EF%BC%8C%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90/"}]},{"title":"【数据结构】二叉树","slug":"数据结构/二叉树","date":"2019-07-09T07:20:00.000Z","updated":"2021-03-20T16:25:01.818Z","comments":true,"path":"2019/07/09/数据结构/二叉树/","link":"","permalink":"http://blog.crazylaw.cn/2019/07/09/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E4%BA%8C%E5%8F%89%E6%A0%91/","excerpt":"","text":"数据结构中有很多树的结构，其中包括二叉树、二叉搜索树、2-3 树、红黑树等等。本文中对数据结构中常见的几种树的概念和用途进行了汇总，不求严格精准，但求简单易懂。 二叉树是数据结构中一种重要的数据结构，也是树表家族最为基础的结构。 二叉树的定义：二叉树的每个结点至多只有二棵子树(不存在度大于 2 的结点)，二叉树的子树有左右之分，次序不能颠倒。二叉树的第 i 层至多有 2^(i-1) 个结点；深度为 k 的二叉树至多有 (2^k)-1 个结点；对任何一棵二叉树 T，如果其终端结点数为 n0，度为 2 的结点数为 n2，则 n0=n2+1。","categories":[{"name":"数据结构","slug":"数据结构","permalink":"http://blog.crazylaw.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"数据结构","slug":"数据结构","permalink":"http://blog.crazylaw.cn/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"【C语言】- Makefile","slug":"C语言/Makefile","date":"2019-07-08T02:11:09.000Z","updated":"2021-03-20T16:25:01.798Z","comments":true,"path":"2019/07/08/C语言/Makefile/","link":"","permalink":"http://blog.crazylaw.cn/2019/07/08/C%E8%AF%AD%E8%A8%80/Makefile/","excerpt":"前言什么是 makefile？或许很多 Winodws 的程序员都不知道这个东西，因为那些 Windows 的 IDE 都为你做了这个工作，但我觉得要作一个好的和 professional 的程序员，makefile 还是要懂。这就好像现在有这么多的 HTML 的编辑器，但如果你想成为一个专业人士，你还是要了解 HTML 的标识的含义。特别在 Unix 下的软件编译，你就不能不自己写 makefile 了，会不会写 makefile，从一个侧面说明了一个人是否具备完成大型工程的能力。因为，makefile 关系到了整个工程的编译规则。一个工程中的源文件不计数，其按类型、功能、模块分别放在若干个目录中，makefile 定义了一系列的规则来指定，哪些文件需要先编译，哪些文件需要后编译，哪些文件需要重新编译，甚至于进行更复杂的功能操作，因为 makefile 就像一个 Shell 脚本一样，其中也可以执行操作系统的命令。makefile 带来的好处就是——“自动化编译”，一旦写好，只需要一个 make 命令，整个工程完全自动编译，极大的提高了软件开发的效率。make 是一个命令工具，是一个解释 makefile 中指令的命令工具，一般来说，大多数的 IDE 都有这个命令，比如：Delphi 的 make，Visual C++的 nmake，Linux 下 GNU 的 make。可见，makefile 都成为了一种在工程方面的编译方法。 其实在我眼中，感觉 makefile 文件其实就是等于一个 shell 文件，用于处理“自动化”的内容，只不过它是由 C 语言程序本身解析的。","text":"前言什么是 makefile？或许很多 Winodws 的程序员都不知道这个东西，因为那些 Windows 的 IDE 都为你做了这个工作，但我觉得要作一个好的和 professional 的程序员，makefile 还是要懂。这就好像现在有这么多的 HTML 的编辑器，但如果你想成为一个专业人士，你还是要了解 HTML 的标识的含义。特别在 Unix 下的软件编译，你就不能不自己写 makefile 了，会不会写 makefile，从一个侧面说明了一个人是否具备完成大型工程的能力。因为，makefile 关系到了整个工程的编译规则。一个工程中的源文件不计数，其按类型、功能、模块分别放在若干个目录中，makefile 定义了一系列的规则来指定，哪些文件需要先编译，哪些文件需要后编译，哪些文件需要重新编译，甚至于进行更复杂的功能操作，因为 makefile 就像一个 Shell 脚本一样，其中也可以执行操作系统的命令。makefile 带来的好处就是——“自动化编译”，一旦写好，只需要一个 make 命令，整个工程完全自动编译，极大的提高了软件开发的效率。make 是一个命令工具，是一个解释 makefile 中指令的命令工具，一般来说，大多数的 IDE 都有这个命令，比如：Delphi 的 make，Visual C++的 nmake，Linux 下 GNU 的 make。可见，makefile 都成为了一种在工程方面的编译方法。 其实在我眼中，感觉 makefile 文件其实就是等于一个 shell 文件，用于处理“自动化”的内容，只不过它是由 C 语言程序本身解析的。 Makefile 书写命令显示命令通常，make 会把其要执行的命令行在命令执行前输出到屏幕上。当我们用“@”字符在命令行前，那么，这个命令将不被 make 显示出来，最具代表性的例子是，我们用这个功能来像屏幕显示一些信息。如： 1@echo 正在编译 XXX 模块...... 当 make 执行时，会输出“正在编译 XXX 模块……”字串，但不会输出命令，如果没有“@”，那么，make 将输出： 123echo 正在编译 XXX 模块......正在编译 XXX 模块...... 如果 make 执行时，带入 make 参数“-n”或“--just-print”，那么其只是显示命令，但不会执行命令，这个功能很有利于我们调试我们的 Makefile，看看我们书写的命令是执行起来是什么样子的或是什么顺序的。 而 make 参数“-s”或“--slient”则是全面禁止命令的显示。 类似于 bash -x 一样 命令执行当依赖目标新于目标时，也就是当规则的目标需要被更新时，make 会一条一条的执行其后的命令。需要注意的是，如果你要让上一条命令的结果应用在下一条命令时，你应该使用分号分隔这两条命令。比如你的第一条命令是 cd 命令，你希望第二条命令得在 cd 之后的基础上运行，那么你就不能把这两条命令写在两行上，而应该把这两条命令写在一行上，用分号分隔。 错误例子： 123exec: cd /home/ccinn pwd 正确例子: 12exec: cd /home/ccinn; pwd 命令出错每当命令运行完后，make 会检测每个命令的返回码，如果命令返回成功，那么 make 会执行下一条命令，当规则中所有的命令成功返回后，这个规则就算是成功完成了。如果一个规则中的某个命令出错了（命令退出码非零），那么 make 就会终止执行当前规则，这将有可能终止所有规则的执行。 有些时候，命令的出错并不表示就是错误的。例如 mkdir 命令，我们一定需要建立一个目录，如果目录不存在，那么 mkdir 就成功执行，万事大吉，如果目录存在，那么就出错了。我们之所以使用 mkdir 的意思就是一定要有这样的一个目录，于是我们就不希望 mkdir 出错而终止规则的运行。 为了做到这一点，忽略命令的出错，我们可以在 Makefile 的命令行前加一个减号“-”（在 Tab 键之后），标记为不管命令出不出错都认为是成功的。如： 12build: -mdkir /home/ccinn 还有一个全局的办法是，给 make 加上“-i”或是“--ignore-errors”参数，那么，Makefile 中所有命令都会忽略错误。而如果一个规则是以“.IGNORE”作为目标的，那么这个规则中的所有命令将会忽略错误。这些是不同级别的防止命令出错的方法，你可以根据你的不同喜欢设置。 还有一个要提一下的 make 的参数的是“-k”或是“--keep-going”，这个参数的意思是，如果某规则中的命令出错了，那么就终目该规则的执行，但继续执行其它规则。 嵌套执行 make在一些大的工程中，我们会把我们不同模块或是不同功能的源文件放在不同的目录中，我们可以在每个目录中都书写一个该目录的 Makefile，这有利于让我们的 Makefile 变得更加地简洁，而不至于把所有的东西全部写在一个 Makefile 中，这样会很难维护我们的 Makefile，这个技术对于我们模块编译和分段编译有着非常大的好处。 例如，我们有一个子目录叫 subdir，这个目录下有个 Makefile 文件，来指明了这个目录下文件的编译规则。那么我们总控的 Makefile 可以这样书写： 12subsystem: cd subdir &amp;&amp; $(MAKE) 其等价于： 12subsystem: $(MAKE) -C subdir 定义$(MAKE)宏变量的意思是，也许我们的 make 需要一些参数，所以定义成一个变量比较利于维护。这两个例子的意思都是先进入“subdir”目录，然后执行 make 命令。 我们把这个 Makefile 叫做“总控 Makefile”，总控 Makefile 的变量可以传递到下级的 Makefile 中（如果你显示的声明），但是不会覆盖下层的 Makefile 中所定义的变量，除非指定了“-e”参数。 如果你要传递变量到下级 Makefile 中，那么你可以使用这样的声明： 1export&lt;variable ...&gt; 如果你不想让某些变量传递到下级 Makefile 中，那么你可以这样声明： 1unexport&lt;variable ...&gt; 示例一：1export variable = value 其等价于： 123variable = valueexport variable 其等价于： 1export variable := value 其等价于： 123variable := valueexport variable 示例二：1export variable += value 其等价于： 123variable += valueexport variable 如果你要传递所有的变量，那么，只要一个export就行了。后面什么也不用跟，表示传递所有的变量。 需要注意的是，有两个变量，一个是 SHELL，一个是MAKEFLAGS，这两个变量不管你是否 export，其总是要传递到下层 Makefile 中，特别是 MAKEFILES 变量，其中包含了 make 的参数信息，如果我们执行“总控 Makefile”时有 make 参数或是在上层 Makefile 中定义了这个变量，那么 MAKEFILES 变量将会是这些参数，并会传递到下层 Makefile 中，这是一个系统级的环境变量。 但是 make 命令中的有几个参数并不往下传递，它们是“-C”,“-f”,“-h”“-o”和“-W”（有关 Makefile 参数的细节将在后面说明），如果你不想往下层传递参数，那么，你可以这样来： 12subsystem: cd subdir &amp;&amp; $(MAKE) MAKEFLAGS= 如果你定义了环境变量 MAKEFLAGS，那么你得确信其中的选项是大家都会用到的，如果其中有“-t”,“-n”,和“-q”参数，那么将会有让你意想不到的结果，或许会让你异常地恐慌。 还有一个在“嵌套执行”中比较有用的参数，“-w”或是“--print-directory”会在 make 的过程中输出一些信息，让你看到目前的工作目录。比如，如果我们的下级 make 目录是“/home/ccinn/gnu/make”，如果我们使用“make -w”来执行，那么当进入该目录时，我们会看到： 1make: Entering directory `/home/ccinn/gnu/make'. 而在完成下层 make 后离开目录时，我们会看到： 1make: Leaving directory `/home/ccinn/gnu/make' 当你使用“-C”参数来指定 make 下层 Makefile 时，“-w”会被自动打开的。 如果参数中有“-s”（“–slient”）或是“–no-print-directory”，那么，“-w”总是失效的。","categories":[{"name":"C语言","slug":"C语言","permalink":"http://blog.crazylaw.cn/categories/C%E8%AF%AD%E8%A8%80/"}],"tags":[{"name":"C语言","slug":"C语言","permalink":"http://blog.crazylaw.cn/tags/C%E8%AF%AD%E8%A8%80/"}]},{"title":"【LeeCode】- 最长公公前缀","slug":"算法/leetcode/\b最长公共前缀","date":"2019-07-06T02:11:09.000Z","updated":"2021-03-20T16:25:01.821Z","comments":true,"path":"2019/07/06/算法/leetcode/\b最长公共前缀/","link":"","permalink":"http://blog.crazylaw.cn/2019/07/06/%E7%AE%97%E6%B3%95/leetcode/%08%E6%9C%80%E9%95%BF%E5%85%AC%E5%85%B1%E5%89%8D%E7%BC%80/","excerpt":"","text":"题目编写一个函数来查找字符串数组中的最长公共前缀。 如果不存在公共前缀，返回空字符串 “”。 示例 1: 12输入: [&quot;flower&quot;,&quot;flow&quot;,&quot;flight&quot;]输出: &quot;fl&quot; 示例 2: 1234输入: [&quot;dog&quot;,&quot;racecar&quot;,&quot;car&quot;]输出: &quot;&quot;解释: 输入不存在公共前缀。说明: 所有输入只包含小写字母 a-z 。 解法解法一：水平扫描法首先，我们将描述一种查询一组字符串的最长公共前缀 LCP(S1...Sn)，我们得到结论是： LCP(S1...Sn)=LCP(LCP(LCP(S1,S2),S3),...Sn) 从公式可以看出，两两比较的字符串的公共字符串就是我们的运算过程。 1234567891011121314151617181920212223242526272829303132333435363738394041424344#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;stdlib.h&gt;char *longestCommonPrefix(char **strs, int strSize)&#123; int i, j; if (strSize == 0) &#123; return \"\"; &#125; char *str = (char *)malloc(sizeof(char) * 1000); strcpy(str, strs[0]); for (i = 1; i &lt; strSize; i++) &#123; j = 0; while (str[j] &amp;&amp; strs[i][j] &amp;&amp; str[j] == strs[i][j]) &#123; j++; &#125; str[j] = '\\0'; &#125; return str;&#125;int main()&#123; char *strs[] = &#123;\"lees\", \"leetcode\", \"leet\", \"leets\"&#125;; int size = sizeof(strs) / sizeof(char *); char *lcp; lcp = longestCommonPrefix(strs, size); printf(\"%s\\n\", lcp); return 0;&#125; 具体请查看我的 github 仓库项目 leetcode-practice","categories":[{"name":"LeeCode","slug":"LeeCode","permalink":"http://blog.crazylaw.cn/categories/LeeCode/"},{"name":"算法","slug":"LeeCode/算法","permalink":"http://blog.crazylaw.cn/categories/LeeCode/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://blog.crazylaw.cn/tags/%E7%AE%97%E6%B3%95/"},{"name":"LeeCode","slug":"LeeCode","permalink":"http://blog.crazylaw.cn/tags/LeeCode/"}]},{"title":"【API网关】- Orange","slug":"API网关/orange","date":"2019-07-05T10:15:00.000Z","updated":"2021-03-20T16:25:01.797Z","comments":true,"path":"2019/07/05/API网关/orange/","link":"","permalink":"http://blog.crazylaw.cn/2019/07/05/API%E7%BD%91%E5%85%B3/orange/","excerpt":"前言 版本：0.7 API 网关/网关，充当的职责无非以下几点： 鉴权 限流 分流 负载均衡 隐含的功能就是动态 upstream 大多与的 API 网关都是用利用 lua 实现的 Openresty 来实现的，因为它可以很好的和 nginx 结合在一起，可以同时运行 lua 脚本和 C 库，并且由于 lua 在协程上实现得十分早，对于 IO 密集型的处理十分的高效。 在给予 Openresty 实现的 API 网关中，有一些是比较出名的，例如 kong/orange kong 算是行业认为最强大成熟的一个组件的，但是过于复杂了。 orange 利用插件的形式来处理请求，并且间接直接，易于二次开发和扩展","text":"前言 版本：0.7 API 网关/网关，充当的职责无非以下几点： 鉴权 限流 分流 负载均衡 隐含的功能就是动态 upstream 大多与的 API 网关都是用利用 lua 实现的 Openresty 来实现的，因为它可以很好的和 nginx 结合在一起，可以同时运行 lua 脚本和 C 库，并且由于 lua 在协程上实现得十分早，对于 IO 密集型的处理十分的高效。 在给予 Openresty 实现的 API 网关中，有一些是比较出名的，例如 kong/orange kong 算是行业认为最强大成熟的一个组件的，但是过于复杂了。 orange 利用插件的形式来处理请求，并且间接直接，易于二次开发和扩展 Orange由于 3 个部分构成： main-server: 一个 location 的表示格式 api-server: orange 的 api 服务 dashboard: 面板 依赖的存储服务： mysql 前端技栈： html css jquery Orange 脑图 插件BASE_AUTH基于 Basic Authorization 的插件 DIVIDE分流插件 DYNAMIC_UPSTREAM动态 upstream 插件","categories":[{"name":"API网关","slug":"API网关","permalink":"http://blog.crazylaw.cn/categories/API%E7%BD%91%E5%85%B3/"}],"tags":[{"name":"API网关","slug":"API网关","permalink":"http://blog.crazylaw.cn/tags/API%E7%BD%91%E5%85%B3/"}]},{"title":"【服务注册和发现】- Etcd","slug":"服务注册和发现/etcd","date":"2019-07-05T03:15:00.000Z","updated":"2021-03-20T16:25:01.819Z","comments":true,"path":"2019/07/05/服务注册和发现/etcd/","link":"","permalink":"http://blog.crazylaw.cn/2019/07/05/%E6%9C%8D%E5%8A%A1%E6%B3%A8%E5%86%8C%E5%92%8C%E5%8F%91%E7%8E%B0/etcd/","excerpt":"Etcd is a distributed, consistent key-value store for shared configuration and service discovery 是一个分布式的，一致的 key-value 存储，主要用途是共享配置和服务发现 服务注册与服务发现是在分布式服务架构中常常会涉及到的东西，业界常用的服务注册与服务发现工具有 ZooKeeper、etcd、Consul 和 Eureka。","text":"Etcd is a distributed, consistent key-value store for shared configuration and service discovery 是一个分布式的，一致的 key-value 存储，主要用途是共享配置和服务发现 服务注册与服务发现是在分布式服务架构中常常会涉及到的东西，业界常用的服务注册与服务发现工具有 ZooKeeper、etcd、Consul 和 Eureka。 \bEtcd 架构 从 etcd 的架构图中我们可以看到，etcd 主要分为四个部分。 HTTP Server： 用于处理用户发送的 API 请求以及其它 etcd 节点的同步与心跳信息请求。 Store：用于处理 etcd 支持的各类功能的事务，包括数据索引、节点状态变更、监控与反馈、事件处理与执行等等，是 etcd 对用户提供的大多数 API 功能的具体实现。 Raft：Raft 强一致性算法的具体实现，是 etcd 的核心。 WAL：Write Ahead Log（预写式日志），是 etcd 的数据存储方式。除了在内存中存有所有数据的状态以及节点的索引以外，etcd 就通过 WAL 进行持久化存储。WAL 中，所有的数据提交前都会事先记录日志。Snapshot 是为了防止数据过多而进行的状态快照；Entry 表示存储的具体日志内容。 其实像 RocketMq 也是支持这种预写式消息，主要是为了用于二次确认 通常，一个用户的请求发送过来，会经由 HTTP Server 转发给 Store 进行具体的事务处理，如果涉及到节点的修改，则交给 Raft 模块进行状态的变更、日志的记录，然后再同步给别的 etcd 节点以确认数据提交，最后进行数据的提交，再次同步。 Etcd 概念词汇表 Raft：etcd 所采用的保证分布式系统强一致性的算法。 Node：一个 Raft 状态机实例。 Member： 一个 etcd 实例。它管理着一个 Node，并且可以为客户端请求提供服务。 Cluster：由多个 Member 构成可以协同工作的 etcd 集群。 Peer：对同一个 etcd 集群中另外一个 Member 的称呼。 Client： 向 etcd 集群发送 HTTP 请求的客户端。 WAL：预写式日志，etcd 用于持久化存储的日志格式。 snapshot：etcd 防止 WAL 文件过多而设置的快照，存储 etcd 数据状态。 Proxy：etcd 的一种模式，为 etcd 集群提供反向代理服务。 Leader：Raft 算法中通过竞选而产生的处理所有数据提交的节点。 Follower：竞选失败的节点作为 Raft 中的从属节点，为算法提供强一致性保证。 Candidate：当 Follower 超过一定时间接收不到 Leader 的心跳时转变为 Candidate 开始竞选。 Term：某个节点成为 Leader 到下一次竞选时间，称为一个 Term。 Index：数据项编号。Raft 中通过 Term 和 Index 来定位数据。 ETCD 数据存储etcd 的存储分为内存存储和持久化（硬盘）存储两部分，内存中的存储除了顺序化的记录下所有用户对节点数据变更的记录外，还会对用户数据进行索引、建堆等方便查询的操作。而持久化则使用预写式日志（WAL：Write Ahead Log）进行记录存储。 在 WAL 的体系中，所有的数据在提交之前都会进行日志记录。在 etcd 的持久化存储目录中，有两个子目录。一个是 WAL，存储着所有事务的变化记录；另一个则是 snapshot，用于存储某一个时刻 etcd 所有目录的数据。通过 WAL 和 snapshot 相结合的方式，etcd 可以有效的进行数据存储和节点故障恢复等操作。 既然有了 WAL 实时存储了所有的变更，为什么还需要 snapshot 呢？随着使用量的增加，WAL 存储的数据会暴增，为了防止磁盘很快就爆满，etcd 默认每 10000 条记录做一次 snapshot，经过 snapshot 以后的 WAL 文件就可以删除。而通过 API 可以查询的历史 etcd 操作默认为 1000 条。 首次启动时，etcd 会把启动的配置信息存储到 data-dir 参数指定的数据目录中。配置信息包括本地节点的 ID、集群 ID 和初始时集群信息。用户需要避免 etcd 从一个过期的数据目录中重新启动，因为使用过期的数据目录启动的节点会与集群中的其他节点产生不一致（如：之前已经记录并同意 Leader 节点存储某个信息，重启后又向 Leader 节点申请这个信息）。所以，为了最大化集群的安全性，一旦有任何数据损坏或丢失的可能性，你就应该把这个节点从集群中移除，然后加入一个不带数据目录的新节点。 预写式日志（WAL）WAL（Write Ahead Log）最大的作用是记录了整个数据变化的全部历程。在 etcd 中，所有数据的修改在提交前，都要先写入到 WAL 中。使用 WAL 进行数据的存储使得 etcd 拥有两个重要功能。 故障快速恢复： 当你的数据遭到破坏时，就可以通过执行所有 WAL 中记录的修改操作，快速从最原始的数据恢复到数据损坏前的状态。 数据回滚（undo）/重做（redo）：因为所有的修改操作都被记录在 WAL 中，需要回滚或重做，只需要方向或正向执行日志中的操作即可。 WAL 与 snapshot 在 etcd 中的命名规则在 etcd 的数据目录中，WAL 文件以$seq-$index.wal 的格式存储。最初始的 WAL 文件是 0000000000000000-0000000000000000.wal，表示是所有 WAL 文件中的第 0 个，初始的 Raft 状态编号为 0。运行一段时间后可能需要进行日志切分，把新的条目放到一个新的 WAL 文件中。 假设，当集群运行到 Raft 状态为 20 时，需要进行 WAL 文件的切分时，下一份 WAL 文件就会变为 0000000000000001-0000000000000021.wal。如果在 10 次操作后又进行了一次日志切分，那么后一次的 WAL 文件名会变为 0000000000000002-0000000000000031.wal。可以看到-符号前面的数字是每次切分后自增 1，而-符号后面的数字则是根据实际存储的 Raft 起始状态来定。 snapshot 的存储命名则比较容易理解，以$term-$index.wal 格式进行命名存储。term 和 index 就表示存储 snapshot 时数据所在的 raft 节点状态，当前的任期编号以及数据项位置信息。 关键部分源码解析从代码逻辑中可以看到，WAL 有两种模式，读模式（read）和数据添加（append）模式，两种模式不能同时成立。一个新创建的 WAL 文件处于 append 模式，并且不会进入到 read 模式。一个本来存在的 WAL 文件被打开的时候必然是 read 模式，并且只有在所有记录都被读完的时候，才能进入 append 模式，进入 append 模式后也不会再进入 read 模式。这样做有助于保证数据的完整与准确。 集群在进入到 etcdserver/server.go 的 NewServer 函数准备启动一个 etcd 节点时，会检测是否存在以前的遗留 WAL 数据。 检测的第一步是查看 snapshot 文件夹下是否有符合规范的文件，若检测到 snapshot 格式是 v0.4 的，则调用函数升级到 v0.5。从 snapshot 中获得集群的配置信息，包括 token、其他节点的信息等等，然后载入 WAL 目录的内容，从小到大进行排序。根据 snapshot 中得到的 term 和 index，找到 WAL 紧接着 snapshot 下一条的记录，然后向后更新，直到所有 WAL 包的 entry 都已经遍历完毕，Entry 记录到 ents 变量中存储在内存里。此时 WAL 就进入 append 模式，为数据项添加进行准备。 当 WAL 文件中数据项内容过大达到设定值（默认为 10000）时，会进行 WAL 的切分，同时进行 snapshot 操作。这个过程可以在 etcdserver/server.go 的 snapshot 函数中看到。所以，实际上数据目录中有用的 snapshot 和 WAL 文件各只有一个，默认情况下 etcd 会各保留 5 个历史文件。 使用场景集合服务发现和注册（Service Discovery）服务发现要解决的也是分布式系统中最常见的问题之一，即在同一个分布式集群中的进程或服务，要如何才能找到对方并建立连接。本质上来说，服务发现就是想要了解集群中是否有进程在监听 udp 或 tcp 端口，并且通过名字就可以查找和连接。要解决服务发现的问题，需要有下面三大支柱，缺一不可。 一个强一致性、高可用的服务存储目录。基于 Raft 算法的 etcd 天生就是这样一个强一致性高可用的服务存储目录。 一种注册服务和监控服务健康状态的机制。用户可以在 etcd 中注册服务，并且对注册的服务设置 key TTL，定时保持服务的心跳以达到监控健康状态的效果。 一种查找和连接服务的机制。通过在 etcd 指定的主题下注册的服务也能在对应的主题下查找到。为了确保连接，我们可以在每个服务机器上都部署一个 Proxy 模式的 etcd，这样就可以确保能访问 etcd 集群的服务都能互相连接。 消息发布与订阅在分布式系统中，最适用的一种组件间通信方式就是消息发布与订阅。即构建一个配置共享中心，数据提供者在这个配置中心发布消息，而消息使用者则订阅他们关心的主题，一旦主题有消息发布，就会实时通知订阅者。通过这种方式可以做到分布式系统配置的集中式管理与动态更新。 应用中用到的一些配置信息放到 etcd 上进行集中管理。这类场景的使用方式通常是这样：应用在启动的时候主动从 etcd 获取一次配置信息，同时，在 etcd 节点上注册一个 Watcher 并等待，以后每次配置有更新的时候，etcd 都会实时通知订阅者，以此达到获取最新配置信息的目的。 分布式搜索服务中，索引的元信息和服务器集群机器的节点状态存放在 etcd 中，供各个客户端订阅使用。使用 etcd 的 key TTL 功能可以确保机器状态是实时更新的。 分布式日志收集系统。这个系统的核心工作是收集分布在不同机器的日志。收集器通常是按照应用（或主题）来分配收集任务单元，因此可以在 etcd 上创建一个以应用（主题）命名的目录 P，并将这个应用（主题相关）的所有机器 ip，以子目录的形式存储到目录 P 上，然后设置一个 etcd 递归的 Watcher，递归式的监控应用（主题）目录下所有信息的变动。这样就实现了机器 IP（消息）变动的时候，能够实时通知到收集器调整任务分配。 系统中信息需要动态自动获取与人工干预修改信息请求内容的情况。通常是暴露出接口，例如 JMX 接口，来获取一些运行时的信息。引入 etcd 之后，就不用自己实现一套方案了，只要将这些信息存放到指定的 etcd 目录中即可，etcd 的这些目录就可以通过 HTTP 的接口在外部访问。 负载均衡在场景一中也提到了负载均衡，本文所指的负载均衡均为软负载均衡。分布式系统中，为了保证服务的高可用以及数据的一致性，通常都会把数据和服务部署多份，以此达到对等服务，即使其中的某一个服务失效了，也不影响使用。由此带来的坏处是数据写入性能下降，而好处则是数据访问时的负载均衡。因为每个对等服务节点上都存有完整的数据，所以用户的访问流量就可以分流到不同的机器上。 etcd 本身分布式架构存储的信息访问支持负载均衡。etcd 集群化以后，每个 etcd 的核心节点都可以处理用户的请求。所以，把数据量小但是访问频繁的消息数据直接存储到 etcd 中也是个不错的选择，如业务系统中常用的二级代码表（在表中存储代码，在 etcd 中存储代码所代表的具体含义，业务系统调用查表的过程，就需要查找表中代码的含义）。 利用 etcd 维护一个负载均衡节点表。etcd 可以监控一个集群中多个节点的状态，当有一个请求发过来后，可以轮询式的把请求转发给存活着的多个状态。类似 Kafka，通过 ZooKeeper 来维护生产者和消费者的负载均衡。同样也可以用 etcd 来做 ZooKeeper 的工作 分布式通知与协调这里说到的分布式通知与协调，与消息发布和订阅有些相似。都用到了 etcd 中的 Watcher 机制，通过注册与异步通知机制，实现分布式环境下不同系统之间的通知与协调，从而对数据变更做到实时处理。实现方式通常是这样：不同系统都在 etcd 上对同一个目录进行注册，同时设置 Watcher 观测该目录的变化（如果对子目录的变化也有需要，可以设置递归模式），当某个系统更新了 etcd 的目录，那么设置了 Watcher 的系统就会收到通知，并作出相应处理。 通过 etcd 进行低耦合的心跳检测。检测系统和被检测系统通过 etcd 上某个目录关联而非直接关联起来，这样可以大大减少系统的耦合性。 通过 etcd 完成系统调度。某系统有控制台和推送系统两部分组成，控制台的职责是控制推送系统进行相应的推送工作。管理人员在控制台作的一些操作，实际上是修改了 etcd 上某些目录节点的状态，而 etcd 就把这些变化通知给注册了 Watcher 的推送系统客户端，推送系统再作出相应的推送任务。 通过 etcd 完成工作汇报。大部分类似的任务分发系统，子任务启动后，到 etcd 来注册一个临时工作目录，并且定时将自己的进度进行汇报（将进度写入到这个临时目录），这样任务管理者就能够实时知道任务进度。 分布式锁因为 etcd 使用 Raft 算法保持了数据的强一致性，某次操作存储到集群中的值必然是全局一致的，所以很容易实现分布式锁。锁服务有两种使用方式，一是保持独占，二是控制时序。 保持独占即所有获取锁的用户最终只有一个可以得到。etcd 为此提供了一套实现分布式锁原子操作 CAS（CompareAndSwap）的 API。通过设置 prevExist 值，可以保证在多个节点同时去创建某个目录时，只有一个成功。而创建成功的用户就可以认为是获得了锁。 控制时序，即所有想要获得锁的用户都会被安排执行，但是获得锁的顺序也是全局唯一的，同时决定了执行顺序。etcd 为此也提供了一套 API（自动创建有序键），对一个目录建值时指定为 POST 动作，这样 etcd 会自动在目录下生成一个当前最大的值为键，存储这个新的值（客户端编号）。同时还可以使用 API 按顺序列出所有当前目录下的键值。此时这些键的值就是客户端的时序，而这些键中存储的值可以是代表客户端的编号。 分布式队列分布式队列的常规用法与场景五中所描述的分布式锁的控制时序用法类似，即创建一个先进先出的队列，保证顺序。 另一种比较有意思的实现是在保证队列达到某个条件时再统一按顺序执行。这种方法的实现可以在/queue 这个目录中另外建立一个/queue/condition 节点。 condition 可以表示队列大小。比如一个大的任务需要很多小任务就绪的情况下才能执行，每次有一个小任务就绪，就给这个 condition 数字加 1，直到达到大任务规定的数字，再开始执行队列里的一系列小任务，最终执行大任务。 condition 可以表示某个任务在不在队列。这个任务可以是所有排序任务的首个执行程序，也可以是拓扑结构中没有依赖的点。通常，必须执行这些任务后才能执行队列中的其他任务。 condition 还可以表示其它的一类开始执行任务的通知。可以由控制程序指定，当 condition 出现变化时，开始执行队列任务。 集群监控与 Leader 竞选通过 etcd 来进行监控实现起来非常简单并且实时性强。 前面几个场景已经提到 Watcher 机制，当某个节点消失或有变动时，Watcher 会第一时间发现并告知用户。 节点可以设置 TTL key，比如每隔 30s 发送一次心跳使代表该机器存活的节点继续存在，否则节点消失。这样就可以第一时间检测到各节点的健康状态，以完成集群的监控要求。 另外，使用分布式锁，可以完成 Leader 竞选。这种场景通常是一些长时间 CPU 计算或者使用 IO 操作的机器，只需要竞选出的 Leader 计算或处理一次，就可以把结果复制给其他的 Follower。从而避免重复劳动，节省计算资源。 这个的经典场景是搜索系统中建立全量索引。如果每个机器都进行一遍索引的建立，不但耗时而且建立索引的一致性不能保证。通过在 etcd 的 CAS 机制同时创建一个节点，创建成功的机器作为 Leader，进行索引计算，然后把计算结果分发到其它节点。 实战操作服务发现和注册","categories":[{"name":"服务注册和发现","slug":"服务注册和发现","permalink":"http://blog.crazylaw.cn/categories/%E6%9C%8D%E5%8A%A1%E6%B3%A8%E5%86%8C%E5%92%8C%E5%8F%91%E7%8E%B0/"}],"tags":[{"name":"服务注册和发现","slug":"服务注册和发现","permalink":"http://blog.crazylaw.cn/tags/%E6%9C%8D%E5%8A%A1%E6%B3%A8%E5%86%8C%E5%92%8C%E5%8F%91%E7%8E%B0/"}]},{"title":"【服务注册和发现】- Consul","slug":"服务注册和发现/consul","date":"2019-07-04T03:15:00.000Z","updated":"2021-03-20T16:25:01.818Z","comments":true,"path":"2019/07/04/服务注册和发现/consul/","link":"","permalink":"http://blog.crazylaw.cn/2019/07/04/%E6%9C%8D%E5%8A%A1%E6%B3%A8%E5%86%8C%E5%92%8C%E5%8F%91%E7%8E%B0/consul/","excerpt":"服务注册与服务发现是在分布式服务架构中常常会涉及到的东西，业界常用的服务注册与服务发现工具有 ZooKeeper、etcd、Consul 和 Eureka。Consul 的主要功能有服务发现、健康检查、KV 存储、安全服务沟通和多数据中心。Consul 与其他几个工具的区别可以在这里查看 Consul vs. Other Software。","text":"服务注册与服务发现是在分布式服务架构中常常会涉及到的东西，业界常用的服务注册与服务发现工具有 ZooKeeper、etcd、Consul 和 Eureka。Consul 的主要功能有服务发现、健康检查、KV 存储、安全服务沟通和多数据中心。Consul 与其他几个工具的区别可以在这里查看 Consul vs. Other Software。 为什么需要有服务注册与服务发现？假设在分布式系统中有两个服务 Service-A （下文以“S-A”代称）和 Service-B（下文以“S-B”代称），当 S-A 想调用 S-B 时，我们首先想到的时直接在 S-A 中请求 S-B 所在服务器的 IP 地址和监听的端口，这在服务规模很小的情况下是没有任何问题的，但是在服务规模很大每个服务不止部署一个实例的情况下是存在一些问题的，比如 S-B 部署了三个实例 S-B-1、S-B-2 和 S-B-3，这时候 S-A 想调用 S-B 该请求哪一个服务实例的 IP 呢？还是将 3 个服务实例的 IP 都写在 S-A 的代码里，每次调用 S-B 时选择其中一个 IP？这样做显得很不灵活，这时我们想到了 Nginx 刚好就能很好的解决这个问题，引入 Nginx 后现在的架构变成了如下图这样： 我们还需要实现一个动态 upstream，就是当我们的 S-B 的服务换了机器或者更换机器 IP 之后，依然能够热重启 nginx。 在这个架构中： 首先 S-B 的实例启动后将自身的服务信息（主要是服务所在的 IP 地址和端口号）注册到注册工具中。不同注册工具服务的注册方式各不相同，后文会讲 Consul 的具体注册方式。 服务将服务信息注册到注册工具后，注册工具就可以对服务做健康检查，以此来确定哪些服务实例可用哪些不可用。 S-A 启动后就可以通过服务注册和服务发现工具获取到所有健康的 S-B 实例的 IP 和端口，并将这些信息放入自己的内存中，S-A 就可用通过这些信息来调用 S-B。 S-A 可以通过监听（Watch）注册工具来更新存入内存中的 S-B 的服务信息。比如 S-B-1 挂了，健康检查机制就会将其标为不可用，这样的信息变动就被 S-A 监听到了，S-A 就更新自己内存中 S-B-1 的服务信息。 所以务注册与服务发现工具除了服务本身的服务注册和发现功能外至少还需要有健康检查和状态变更通知的功能。 ConsulConsul 作为一种分布式服务工具，为了避免单点故障常常以集群的方式进行部署，在 Consul 集群的节点中分为 Server 和 Client 两种节点（所有的节点也被称为 Agent），Server 节点保存数据，Client 节点负责健康检查及转发数据请求到 Server；Server 节点有一个 Leader 节点和多个 Follower 节点，Leader 节点会将数据同步到 Follower 节点，在 Leader 节点挂掉的时候会启动选举机制产生一个新的 Leader。 Client 节点很轻量且无状态，它以 RPC 的方式向 Server 节点做读写请求的转发，此外也可以直接向 Server 节点发送读写请求。下面是 Consul 的架构图： Consul 的安装和具体使用及其他详细内容可浏览官方文档。 Consul 默认端口 Use Default Ports 用途说明 DNS: The DNS server (TCP and UDP) 8600 用于解析 DNS 查询。 HTTP: The HTTP API (TCP Only) 8500 客户端使用它来与 HTTP API 通信。 HTTPS: The HTTPs API disabled (8501)* （可选）默认情况下处于关闭状态，但端口 8501 是各种工具默认使用的约定。 gRPC: The gRPC API disabled (8502)* （可选）。目前 gRPC 仅用于将 xDS API 公开给 Envoy 代理。默认情况下它处于关闭状态，但端口 8502 是各种工具用作默认值的约定。在-dev 模式下默认为 8502 。 LAN Serf: The Serf LAN port (TCP and UDP) 8301 用于处理 LAN 中的八卦。所有代理商都要求。 Wan Serf: The Serf WAN port TCP and UDP) 8302 服务器使用它来通过 WAN 闲聊到其他服务器。从 Consul 0.8 开始，WAN 加入泛洪功能要求 Serf WAN 端口（TCP / UDP）在 WAN 和 LAN 接口上进行监听。 server: Server RPC address (TCP Only) 8300 服务器使用它来处理来自其他代理的传入请求。 Sidecar Proxy Min: Inclusive min port number to use for automatically assigned sidecar service registrations. 21000 无 Sidecar Proxy Max: Inclusive max port number to use for automatically assigned sidecar service registrations.. 21255 无 默认端口可以通过 agent configuration 来修改 运行 Consul 服务下面是我用 Docker 的方式搭建了一个有 2 个 Server 节点和 1 个 Client 节点的 Consul 集群。 12# 这是Consul服务主节点docker run -d --name=c1 -p 8500:8500 -e CONSUL_BIND_INTERFACE=eth0 consul agent --server=true --bootstrap-expect=2 --client=0.0.0.0 -ui 12345678910111213141516docker exec -it c1 ifconfigeth0 Link encap:Ethernet HWaddr 02:42:AC:11:00:03 inet addr:172.17.0.3 Bcast:172.17.255.255 Mask:255.255.0.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:24 errors:0 dropped:0 overruns:0 frame:0 TX packets:16 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:5835 (5.6 KiB) TX bytes:1619 (1.5 KiB)lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B) 这里，我们看到我们的 server 的主节点的 IP 为:172.17.0.3 让 follwer 节点加入 leader 节点 1docker run -d --name=c2 -e CONSUL_BIND_INTERFACE=eth0 consul agent --server=true --client=0.0.0.0 --join 172.17.0.3 启动 client 节点 12#下面是启动 Client 节点docker run -d --name=c3 -e CONSUL_BIND_INTERFACE=eth0 consul agent --server=false --client=0.0.0.0 --join 172.17.0.3 12345docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES262fcb21e07a consul \"docker-entrypoint.s…\" 28 seconds ago Up 27 seconds 8300-8302/tcp, 8500/tcp, 8301-8302/udp, 8600/tcp, 8600/udp c3ffd0c626653f consul \"docker-entrypoint.s…\" 41 minutes ago Up 41 minutes 8300-8302/tcp, 8500/tcp, 8301-8302/udp, 8600/tcp, 8600/udp c2d6bb5fee079d consul \"docker-entrypoint.s…\" 43 minutes ago Up 43 minutes 8300-8302/tcp, 8301-8302/udp, 8600/tcp, 8600/udp, 0.0.0.0:8500-&gt;8500/tcp c1 操作 Consul 有 Commands 和 HTTP API 两种方式，进入任意一个容器执行 consul members 都可以有如下的输出，说明 Consul 集群就已经搭建成功了。 12345docker exec -it c1 consul membersNode Address Status Type Build Protocol DC Segmentd6bb5fee079d 172.17.0.3:8301 alive server 1.5.2 2 dc1 &lt;all&gt;ffd0c626653f 172.17.0.4:8301 alive server 1.5.2 2 dc1 &lt;all&gt;262fcb21e07a 172.17.0.5:8301 alive client 1.5.2 2 dc1 &lt;default&gt; 服务注册服务注册对方式有 2 种方式 通过 client 的配置来注册 通过 Http-Api 来注册服务 由于我们的需求是需要用 Http api 来注册服务，所以接下来的例子都是以http-api-register-service为例子 请求 Method：PUT 请求 Path：/agent/server/register 请求 Content-type: application/json Blocking Queries：No Consistency Modes：None Agent Caching：None ACL Required：service:write （Acl 需要拥有写入的权限） 参数参数不仅仅只支持 StudlyCaps（首字母大写的驼峰写法camel_case）,还支持 蛇形大小写，snake_case。 Name (string: &lt;required&gt;): 指定服务的逻辑名称。许多服务实例可以共享相同的逻辑服务名称。 ID (string: “”) : 指定此服务的唯一 ID。每个代理必须是唯一的。Name 如果未提供，则默认为参数。 Tags (array&lt;string&gt;: nil) : 指定要分配给服务的标记列表。这些标记可用于以后的过滤，并通过 API 公开。 Address (string: “”) : 指定服务的地址。如果未提供，则在 DNS 查询期间将代理的地址用作服务的地址。 Meta (map&lt;string|string&gt;: nil) : 指定链接到服务实例的任意 KV 元数据。 Port (int: 0) : 指定服务的端口。 Kind (string: “”) : 默认为空，代表典型的 consul 服务，这个值可填写的：connect-proxy，代表另一个服务的支持 Connect 的代理服务 Connect (Connect: nil)- 指定 Connect 的 配置。有关支持的字段，请参阅下面的连接结构 Check (Check: nil) - 指定检查。有关接受的字段的详细信息，请参阅检查文档。如果您没有为支票提供名称或 ID，则会生成它们。要提供自定义 ID 和/或名称，请设置 CheckID 和/或 Name 字段。 Checks (array&lt;Check&amp;gt: nil) - 指定检查列表。有关接受的字段的详细信息，请参阅 检查文档。如果您没有为支票提供名称或 ID，则会生成它们。要提供自定义 ID 和/或名称，请设置 CheckID 和/或 Name 字段。自动生成 Name 并 CheckID 依赖于数组中检查的位置，因此即使行为是确定性的，建议所有检查要么让 consul CheckID 通过将字段留空/省略来设置，或者提供唯一值。 EnableTagOverride (bool: false) - 指定禁用此服务标签的反熵功能。如果 EnableTagOverride 设置为，true 则外部代理可以在目录中更新此服务 并修改标记。此代理的后续本地同步操作将忽略更新的标记。例如，如果外部代理修改了此服务的标记和端口，并且 EnableTagOverride 设置为 true 在下一个同步周期之后，则服务的端口将恢复为原始值，但标记将保持更新的值。作为一个反例，如果一个外部代理修改了这个服务的标签和端口，并 EnableTagOverride 设置为 false 在下一个同步周期后服务的端口和 标签将恢复为原始值，并且所有修改都将丢失。 Weights (Weights: nil) - 指定服务的权重。有关权重的更多信息，请参阅 服务文档。如果未提供此字段，则权限将默认为 {“Passing”: 1, “Warning”: 1}。请务必注意，这仅适用于本地注册的服务。如果您有多个节点都注册相同的服务，则其 EnableTagOverride 配置和所有其他服务配置项彼此独立。更新在一个节点上注册的服务的标记与在另一个节点上注册的相同服务（按名称）无关。如果 EnableTagOverride 未指定，则默认值为 false。有关详细信息，请参阅反熵同步。 服务注册样本载体(Sample Payload)1234567891011121314151617181920212223## cat payload1.json&#123; \"ID\": \"redis1\", \"Name\": \"redis\", \"Tags\": [ \"primary\", \"v1\" ], \"Address\": \"127.0.0.1\", \"Port\": 8000, \"Meta\": &#123; \"redis_version\": \"4.0\" &#125;, \"EnableTagOverride\": false, \"Check\": &#123; \"HTTP\": \"http://127.0.0.1:5000/health\", \"Interval\": \"10s\" &#125;, \"Weights\": &#123; \"Passing\": 1, \"Warning\": 1 &#125;&#125; 12345678910111213141516171819## cat payload2.json&#123; \"ID\": \"redis2\", \"Name\": \"redis\", \"Tags\": [ \"primary\", \"v1\" ], \"Address\": \"127.0.0.1\", \"Port\": 8000, \"Meta\": &#123; \"redis_version\": \"4.0\" &#125;, \"EnableTagOverride\": false, \"Weights\": &#123; \"Passing\": 1, \"Warning\": 1 &#125;&#125; 12345678910111213141516171819## cat payload3.json&#123; \"ID\": \"redis3\", \"Name\": \"redis\", \"Tags\": [ \"primary\", \"v1\" ], \"Address\": \"127.0.0.1\", \"Port\": 8001, \"Meta\": &#123; \"redis_version\": \"4.0\" &#125;, \"EnableTagOverride\": false, \"Weights\": &#123; \"Passing\": 1, \"Warning\": 1 &#125;&#125; 样本请求(Sample Request)1234curl \\ --request PUT \\ --data @payload1.json \\ http://127.0.0.1:8500/v1/agent/service/register 1234curl \\ --request PUT \\ --data @payload2.json \\ http://127.0.0.1:8500/v1/agent/service/register 1234curl \\ --request PUT \\ --data @payload3.json \\ http://127.0.0.1:8500/v1/agent/service/register 在 consul-ui 上可以看到是否注册成功，也可以通过 http-api 查看是否注册成功 从上图看出，有check机制的那个有一个节点目前是心跳检测失败的。 心跳机制(check)Http 的 check 机制只要返回的状态为2xx，就算为成功，如果是4xx,就算为危险，其他状态码算为严重. 服务发现服务注册之后，我们就需要从 consul 中拿到这些服务信息 Health 相关的 API Health-service 的 API 1curl http://127.0.0.1:8500/v1/health/service/redis?passing=true passing （这个参数可以帮我们只显示可用的服务节点） filter （服务需要定制过滤的可以传入这个参数，详情看文档） 这个时候，我们可以拿到所有可用的服务列表之后，需要找到地方(内存)把这个&quot;列表&quot;存储起来，在我们的 Client 或者 API 网关进程内部实现一个内部LB的机制，这样子就不需要每次请求都去拿一次这个可用服务的信息。 内部的LB机制一般就是轮训或者随机。 服务更新 LB 信息(Watch)由于我们的服务不一定一直都处于可用的状态，所以我们要跟随者服务注册中心来更新我们的内部的LB内容，所以我们需要服务注册中心告诉我们我要访问的那些服务，哪一些服务变成不可用了，我要移除LB。 所以，这里，我们需要利用 consul 的watch 机制。 Consul 有两种 Watch 的方式 script （触发 client 端本地的脚本） http (触发远端的 url，类似钩子的行为) 这里，我们选择 http 的方式。 Watch 的类型 key - Watch a specific KV pair keyprefix - Watch a prefix in the KV store services - Watch the list of available services nodes - Watch the list of nodes service- Watch the instances of a service checks - Watch the value of health checks event - Watch for custom user events \b\b 这里，我们需要 watch 的是service，因为我们需要动态的去更新我们自身的LB。 123456789101112&#123; &quot;type&quot;: &quot;service&quot;, &quot;service&quot;: &quot;redis&quot;, &quot;handler_type&quot;: &quot;http&quot;, &quot;http_handler_config&quot;: &#123; &quot;path&quot;:&quot;https:&#x2F;&#x2F;localhost:8000&#x2F;callBackWatch&quot;, &quot;method&quot;: &quot;POST&quot;, &quot;header&quot;: &#123;&quot;x-foo&quot;:[&quot;bar&quot;, &quot;baz&quot;]&#125;, &quot;timeout&quot;: &quot;10s&quot;, &quot;tls_skip_verify&quot;: false &#125;&#125; 这里，我们提供了一个callBackWatch的服务用于接手 consul 的 watch 到 service 变化的 payload，用于更新我们自己的LB 内容。 常规部署架构图 首先需要有一个正常的 Consul 集群，有 Server，有 Leader。这里在服务器 Server1、Server2、Server3 上分别部署了 Consul Server，假设他们选举了 Server2 上的 Consul Server 节点为 Leader。这些服务器上最好只部署 Consul 程序，以尽量维护 Consul Server 的稳定。 然后在服务器 Server4 和 Server5 上通过 Consul Client 分别注册 Service A、B、C，这里每个 Service 分别部署在了两个服务器上，这样可以避免 Service 的单点问题。服务注册到 Consul 可以通过 HTTP API（8500 端口）的方式，也可以通过 Consul 配置文件的方式。Consul Client 可以认为是无状态的，它将注册信息通过 RPC 转发到 Consul Server，服务信息保存在 Server 的各个节点中，并且通过 Raft 实现了强一致性。 最后在服务器 Server6 中 Program D 需要访问 Service B，这时候 Program D 首先访问本机 Consul Client 提供的 HTTP API，本机 Client 会将请求转发到 Consul Server，Consul Server 查询到 Service B 当前的信息返回，最终 Program D 拿到了 Service B 的所有部署的 IP 和端口，然后就可以选择 Service B 的其中一个部署并向其发起请求了。如果服务发现采用的是 DNS 方式，则 Program D 中直接使用 Service B 的服务发现域名，域名解析请求首先到达本机 DNS 代理，然后转发到本机 Consul Client，本机 Client 会将请求转发到 Consul Server，Consul Server 查询到 Service B 当前的信息返回，最终 Program D 拿到了 Service B 的某个部署的 IP 和端口。 部署架构官方推荐的是那个主机都安装一个 clinet，这个是十分不科学的做法，如果你实在不想在每个主机部署 Consul Client，还有一个多路注册的方案可供选择。 如图所示，在专门的服务器上部署 Consul Client，然后每个服务都注册到多个 Client，这里为了避免服务单点问题还是每个服务部署多份，需要服务发现时，程序向一个提供负载均衡的程序发起请求，该程序将请求转发到某个 Consul Client。这种方案需要注意将 Consul 的 8500 端口绑定到私网 IP 上，默认只有 127.0.0.1。 这个架构的优势： Consul 节点服务器与应用服务器隔离，互相干扰少； 不用每台主机都部署 Consul，方便 Consul 的集中管理； 某个 Consul Client 挂掉的情况下，注册到其上的服务仍有机会被访问到； 但也需要注意其缺点： 引入更多技术栈：负载均衡的实现，不仅要考虑 Consul Client 的负载均衡，还要考虑负载均衡本身的单点问题。 Client 的节点数量：单个 Client 如果注册的服务太多，负载较重，需要有个算法（比如 hash 一致）合理分配每个 Client 上的服务数量，以及确定 Client 的总体数量。 服务发现要过滤掉重复的注册，因为注册到了多个节点会认为是多个部署（DNS 接口不会有这个问题）。 这个方案其实还可以优化，服务发现使用的负载均衡可以直接代理 Server 节点，因为相关请求还是会转发到 Server 节点，不如直接就发到 Server。 Consul 的健康检查Consul 做服务发现是专业的，健康检查是其中一项必不可少的功能，其提供 Script/TCP/HTTP+Interval，以及 TTL 等多种方式。服务的健康检查由服务注册到的 Agent 来处理，这个 Agent 既可以是 Client 也可以是 Server。 很多同学都使用 ZooKeeper 或者 etcd 做服务发现，使用 Consul 时发现节点挂掉后服务的状态变为不可用了，所以有同学问服务为什么不在各个节点之间同步？这个根本原因是服务发现的实现原理不同。 Consul 与 ZooKeeper、etcd 的区别后边这两个工具是通过键值存储来实现服务的注册与发现。 ZooKeeper 利用临时节点的机制，业务服务启动时创建临时节点，节点在服务就在，节点不存在服务就不存在。 etcd 利用 TTL 机制，业务服务启动时创建键值对，定时更新 ttl，ttl 过期则服务不可用。 ZooKeeper 和 etcd 的键值存储都是强一致性的，也就是说键值对会自动同步到多个节点，只要在某个节点上存在就可以认为对应的业务服务是可用的。 Consul 的数据同步也是强一致性的，服务的注册信息会在 Server 节点之间同步，相比 ZK、etcd，服务的信息还是持久化保存的，即使服务部署不可用了，仍旧可以查询到这个服务部署。但是业务服务的可用状态是由注册到的 Agent 来维护的，Agent 如果不能正常工作了，则无法确定服务的真实状态，并且 Consul 是相当稳定了，Agent 挂掉的情况下大概率服务器的状态也可能是不好的，此时屏蔽掉此节点上的服务是合理的。Consul 也确实是这样设计的，DNS 接口会自动屏蔽挂掉节点上的服务，HTTP API 也认为挂掉节点上的服务不是 passing 的。 鉴于 Consul 健康检查的这种机制，同时避免单点故障，所有的业务服务应该部署多份，并注册到不同的 Consul 节点。部署多份可能会给你的设计带来一些挑战，因为调用方同时访问多个服务实例可能会由于会话不共享导致状态不一致，这个有许多成熟的解决方案，可以去查询，这里不做说明。 健康检查能不能支持故障转移？上边提到健康检查是由服务注册到的 Agent 来处理的，那么如果这个 Agent 挂掉了，会不会有别的 Agent 来接管健康检查呢？答案是否定的。 从问题产生的原因来看，在应用于生产环境之前，肯定需要对各种场景进行测试，没有问题才会上线，所以显而易见的问题可以屏蔽掉；如果是新版本 Consul 的 BUG 导致的，此时需要降级；如果这个 BUG 是偶发的，那么只需要将 Consul 重新拉起来就可以了，这样比较简单；如果是硬件、网络或者操作系统故障，那么节点上服务的可用性也很难保障，不需要别的 Agent 接管健康检查。 从实现上看，选择哪个节点是个问题，这需要实时或准实时同步各个节点的负载状态，而且由于业务服务运行状态多变，即使当时选择出了负载比较轻松的节点，无法保证某个时段任务又变得繁重，可能造成新的更大范围的崩溃。如果原来的节点还要启动起来，那么接管的健康检查是否还要撤销，如果要，需要记录服务们最初注册的节点，然后有一个监听机制来触发，如果不要，通过服务发现就会获取到很多冗余的信息，并且随着时间推移，这种数据会越来越多，系统变的无序。 从实际应用看，节点上的服务可能既要被发现，又要发现别的服务，如果节点挂掉了，仅提供被发现的功能实际上服务还是不可用的。当然发现别的服务也可以不使用本机节点，可以通过访问一个 Nginx 实现的若干 Consul 节点的负载均衡来实现，这无疑又引入了新的技术栈。 如果不是上边提到的问题，或者你可以通过一些方式解决这些问题，健康检查接管的实现也必然是比较复杂的，因为分布式系统的状态同步是比较复杂的。同时不要忘了服务部署了多份，挂掉一个不应该影响系统的快速恢复，所以没必要去做这个接管。","categories":[{"name":"服务注册和发现","slug":"服务注册和发现","permalink":"http://blog.crazylaw.cn/categories/%E6%9C%8D%E5%8A%A1%E6%B3%A8%E5%86%8C%E5%92%8C%E5%8F%91%E7%8E%B0/"}],"tags":[{"name":"服务注册和发现","slug":"服务注册和发现","permalink":"http://blog.crazylaw.cn/tags/%E6%9C%8D%E5%8A%A1%E6%B3%A8%E5%86%8C%E5%92%8C%E5%8F%91%E7%8E%B0/"}]},{"title":"【Nginx】map-详解","slug":"nginx/【Nginx】map-详解","date":"2019-06-13T03:13:30.000Z","updated":"2021-03-20T16:25:01.812Z","comments":true,"path":"2019/06/13/nginx/【Nginx】map-详解/","link":"","permalink":"http://blog.crazylaw.cn/2019/06/13/nginx/%E3%80%90Nginx%E3%80%91map-%E8%AF%A6%E8%A7%A3/","excerpt":"前言map 指令是由 ngx_http_map_module 模块提供的，默认情况下安装 nginx 都会安装该模块。 map 的主要作用是创建自定义变量，通过使用 nginx 的内置变量，去匹配某些特定规则，如果匹配成功则设置某个值给自定义变量。 而这个自定义变量又可以作于他用。","text":"前言map 指令是由 ngx_http_map_module 模块提供的，默认情况下安装 nginx 都会安装该模块。 map 的主要作用是创建自定义变量，通过使用 nginx 的内置变量，去匹配某些特定规则，如果匹配成功则设置某个值给自定义变量。 而这个自定义变量又可以作于他用。","categories":[{"name":"nginx","slug":"nginx","permalink":"http://blog.crazylaw.cn/categories/nginx/"}],"tags":[{"name":"nginx","slug":"nginx","permalink":"http://blog.crazylaw.cn/tags/nginx/"}]},{"title":"【kubernetes】dashboard","slug":"k8s/k8s-dashboard","date":"2019-06-12T09:00:30.000Z","updated":"2021-03-20T16:25:01.806Z","comments":true,"path":"2019/06/12/k8s/k8s-dashboard/","link":"","permalink":"http://blog.crazylaw.cn/2019/06/12/k8s/k8s-dashboard/","excerpt":"前言dashboard dashboard 是一款基于容器的监控 k8s 集群情况的一个仪表盘项目 本文中使用的版本为：v1.10.1","text":"前言dashboard dashboard 是一款基于容器的监控 k8s 集群情况的一个仪表盘项目 本文中使用的版本为：v1.10.1 安装从国内找到版本对应的镜像，拉取到本地并且重命名镜像 1docker pull gcrxio/kubernetes-dashboard-amd64:v1.10.1 &amp;&amp; docker tag gcrxio/kubernetes-dashboard-amd64:v1.10.1 k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1 &amp;&amp; docker rmi gcrxio/kubernetes-dashboard-amd64:v1.10.1 然后下载对应版本的 yaml，保存在 \b 本地，不以补丁的形式更新，我们用配置式的方式来创建 pods 1wget https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml 接下来，我们要至少修改 1 个细节 service 中添加 {“spec”:{“type”:”NodePort”}}/{“spec”:{“ports”:{“nodePort”:30001}}} 如果不指定 nodeport 的话，会随机分配一个 30000 以上的端口 1234567891011121314151617# ------------------- Dashboard Service ------------------- #kind: ServiceapiVersion: v1metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-systemspec: type: NodePort ports: - port: 443 targetPort: 8443 nodePort: 30001 selector: k8s-app: kubernetes-dashboard 完整的模板如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165# Copyright 2017 The Kubernetes Authors.## Licensed under the Apache License, Version 2.0 (the \"License\");# you may not use this file except in compliance with the License.# You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an \"AS IS\" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.# ------------------- Dashboard Secret ------------------- #apiVersion: v1kind: Secretmetadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-certs namespace: kube-systemtype: Opaque---# ------------------- Dashboard Service Account ------------------- #apiVersion: v1kind: ServiceAccountmetadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-system---# ------------------- Dashboard Role &amp; Role Binding ------------------- #kind: RoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: kubernetes-dashboard-minimal namespace: kube-systemrules: # Allow Dashboard to create 'kubernetes-dashboard-key-holder' secret. - apiGroups: [''] resources: ['secrets'] verbs: ['create'] # Allow Dashboard to create 'kubernetes-dashboard-settings' config map. - apiGroups: [''] resources: ['configmaps'] verbs: ['create'] # Allow Dashboard to get, update and delete Dashboard exclusive secrets. - apiGroups: [''] resources: ['secrets'] resourceNames: ['kubernetes-dashboard-key-holder', 'kubernetes-dashboard-certs'] verbs: ['get', 'update', 'delete'] # Allow Dashboard to get and update 'kubernetes-dashboard-settings' config map. - apiGroups: [''] resources: ['configmaps'] resourceNames: ['kubernetes-dashboard-settings'] verbs: ['get', 'update'] # Allow Dashboard to get metrics from heapster. - apiGroups: [''] resources: ['services'] resourceNames: ['heapster'] verbs: ['proxy'] - apiGroups: [''] resources: ['services/proxy'] resourceNames: ['heapster', 'http:heapster:', 'https:heapster:'] verbs: ['get']---apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata: name: kubernetes-dashboard-minimal namespace: kube-systemroleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: kubernetes-dashboard-minimalsubjects: - kind: ServiceAccount name: kubernetes-dashboard namespace: kube-system---# ------------------- Dashboard Deployment ------------------- #kind: DeploymentapiVersion: apps/v1metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-systemspec: replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: kubernetes-dashboard template: metadata: labels: k8s-app: kubernetes-dashboard spec: containers: - name: kubernetes-dashboard image: k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1 ports: - containerPort: 8443 protocol: TCP args: - --auto-generate-certificates # Uncomment the following line to manually specify Kubernetes API server Host # If not specified, Dashboard will attempt to auto discover the API server and connect # to it. Uncomment only if the default does not work. # - --apiserver-host=http://my-address:port volumeMounts: - name: kubernetes-dashboard-certs mountPath: /certs # Create on-disk volume to store exec logs - mountPath: /tmp name: tmp-volume livenessProbe: httpGet: scheme: HTTPS path: / port: 8443 initialDelaySeconds: 30 timeoutSeconds: 30 volumes: - name: kubernetes-dashboard-certs secret: secretName: kubernetes-dashboard-certs - name: tmp-volume emptyDir: &#123;&#125; serviceAccountName: kubernetes-dashboard # Comment the following tolerations if Dashboard must not be deployed on master tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule---# ------------------- Dashboard Service ------------------- #kind: ServiceapiVersion: v1metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-systemspec: type: NodePort ports: - port: 443 targetPort: 8443 nodePort: 30001 selector: k8s-app: kubernetes-dashboard 1kubectl apply -f kubernetes-dashboard.yaml --record 等一会就可以了 123[root@master ~]# kubectl get pods -n kube-systemNAME READY STATUS RESTARTS AGEkubernetes-dashboard-5f7b999d65-s2lgq 1&#x2F;1 Running 0 17h 然后我们看一下对应等网络端口监听 12[root@master ~]# netstat -anp | grep 30001tcp6 0 0 :::30001 :::* LISTEN 27609/kube-proxy 123[root@master ~]# kubectl get svc -n kube-systemNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes-dashboard NodePort 10.104.204.139 &lt;none&gt; 443:30001/TCP 17h 发现也生效了。\b 注意：我们访问的时候是通过&lt;NodeIP：NodePort&gt;的方式来访问，而且必须是https的形式 在浏览器输入如下信息 1https://192.168.8.171:30001 就会进入到登录页面. 这里有一些人会说谷歌浏览器登录不了，目前没遇到这个问题，暂时不太了解这个情况。 这里，可以通过 kube-config 文件来登录或者通过 token 来登录 登录认证有两种方式 Token Kubeconfig Token1kubectl -n kube-system describe $(kubectl -n kube-system get secret -n kube-system -o name | grep namespace) | grep token 用这个命令，我们可以看到 token 的信息，手动复制出来使用 1234[root@master ~]# kubectl -n kube-system describe $(kubectl -n kube-system get secret -n kube-system -o name | grep namespace) | grep tokenName: namespace-controller-token-ghvr9Type: kubernetes.io/service-account-tokentoken: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJuYW1lc3BhY2UtY29udHJvbGxlci10b2tlbi1naHZyOSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJuYW1lc3BhY2UtY29udHJvbGxlciIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjczZTg5OGJjLTgxMzEtMTFlOS1hNTdhLTAwNTA1NjlkMGYzMCIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTpuYW1lc3BhY2UtY29udHJvbGxlciJ9.S4nPVa2-JAmRX_yVtoA9sR-JAnmLF5XdqQwrPmtQiohswyHWaAtEJyCF5wc8SIQmXbqn3XpeGNMLcyGd5xfAR4pIG0ljk5z6zoDc18M0Icnx5If0hxs4fWrhSNYYfzK7YnYOe_cjlydhbgTQ0vJoAXMURnX5dLFuNcp8QNaesNNnapZS2GPrjRYLNHw_WyCyU4i_tIwfiHW4ktrGD8SuX5g564gDZ9Xdee4CVHFveWNOrgWkV63n0fy8eNu9S6QpTsRb32M3MaZwlsp-2SFzcJbmAUj3ZiJ8KuBmZtBF249007SxiGwWzP3YQ5sXU38orSf8n-Spv5r-7ZHIzt0brw 1eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJuYW1lc3BhY2UtY29udHJvbGxlci10b2tlbi1naHZyOSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJuYW1lc3BhY2UtY29udHJvbGxlciIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjczZTg5OGJjLTgxMzEtMTFlOS1hNTdhLTAwNTA1NjlkMGYzMCIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTpuYW1lc3BhY2UtY29udHJvbGxlciJ9.S4nPVa2-JAmRX_yVtoA9sR-JAnmLF5XdqQwrPmtQiohswyHWaAtEJyCF5wc8SIQmXbqn3XpeGNMLcyGd5xfAR4pIG0ljk5z6zoDc18M0Icnx5If0hxs4fWrhSNYYfzK7YnYOe_cjlydhbgTQ0vJoAXMURnX5dLFuNcp8QNaesNNnapZS2GPrjRYLNHw_WyCyU4i_tIwfiHW4ktrGD8SuX5g564gDZ9Xdee4CVHFveWNOrgWkV63n0fy8eNu9S6QpTsRb32M3MaZwlsp-2SFzcJbmAUj3ZiJ8KuBmZtBF249007SxiGwWzP3YQ5sXU38orSf8n-Spv5r-7ZHIzt0brw 在登录页面，输入这个 token，即可进入到仪表盘内部 Kubeconfig只需要在 kubeadm 生成的 admin.conf 文件末尾加上刚刚获取的 token 就可以了 12345- name: kubernetes-admin user: client-certificate-data: xxxxxxxx client-key-data: xxxxxx token: \"在这里加上token\" 内部系统结果图这样子算是完成了基本的部署和处理","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://blog.crazylaw.cn/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://blog.crazylaw.cn/tags/kubernetes/"}]},{"title":"【大数据】- Hadoop 基本概念","slug":"大数据/Hadoop基本概念","date":"2019-06-12T01:56:40.000Z","updated":"2021-03-20T16:25:01.815Z","comments":true,"path":"2019/06/12/大数据/Hadoop基本概念/","link":"","permalink":"http://blog.crazylaw.cn/2019/06/12/%E5%A4%A7%E6%95%B0%E6%8D%AE/Hadoop%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/","excerpt":"前言Hadoop 是一个由 Apache 基金会所开发的分布式系统基础架构，它可以使用户在不了解分布式底层细节的情況下开发分布式程序，充分利用集群的威力进行高速运算和存储。 从其定义就可以发现，它解決了两大问题：大数据存储、大数据分析。也就是 Hadoop 的两大核心：HDFS 和 MapReduce。 HDFS(Hadoop Distributed File System)是可扩展、容错、高性能的分布式文件系统，异步复制，一次写入多次读取，主要负责存储。 MapReduce 为分布式计算框架，包含 map(映射)和 reduce(归约)过程，负责在 HDFS 上进行计算。 但是由于目前我没有用到 MapReduce，并且 MapReduce 能效过于地下并且很占系统资料，所以一般数据分析都会用其他的来代替。 所以这里介绍的 Hadoop，会重点讲解 HDFS 和其工作原理。","text":"前言Hadoop 是一个由 Apache 基金会所开发的分布式系统基础架构，它可以使用户在不了解分布式底层细节的情況下开发分布式程序，充分利用集群的威力进行高速运算和存储。 从其定义就可以发现，它解決了两大问题：大数据存储、大数据分析。也就是 Hadoop 的两大核心：HDFS 和 MapReduce。 HDFS(Hadoop Distributed File System)是可扩展、容错、高性能的分布式文件系统，异步复制，一次写入多次读取，主要负责存储。 MapReduce 为分布式计算框架，包含 map(映射)和 reduce(归约)过程，负责在 HDFS 上进行计算。 但是由于目前我没有用到 MapReduce，并且 MapReduce 能效过于地下并且很占系统资料，所以一般数据分析都会用其他的来代替。 所以这里介绍的 Hadoop，会重点讲解 HDFS 和其工作原理。 Hadoop 存储 - HDFSHadoop 的存储系统是 HDFS(Hadoop Distributed File System)分布式文件系统，对外部客户端而言，HDFS 就像一个传统的分级文件系统，可以进行创建、删除、移动或重命名文件或文件夹等操作，与 Linux 文件系统类似。 但是，Hadoop HDFS 的架构是基于一组特定的节点构建的，名称节点（NameNode，仅一个)，它在 HDFS 内部提供元数据服务；第二名称节点(Secondary NameNode)，名称节点的帮助节点，主要是为了整合元数据操作(注意不是名称节点的备份)；数据节点(DataNode)，它为 HDFS 提供存储块。由于仅有一个 NameNode，因此这是 HDFS 的一个缺点(单点失败，在 Hadoop2.x 后有较大改善)。 NameNode它是一个通常在 HDFS 架构中单独机器上运行的组件，负责管理文件系统名称空间和控制外部客户机的访问。NameNode 决定是否将文件映射到 DataNode 上的复制块上。对于最常见的 3 个复制块，第一个复制块存储在同一机架的不同节点上，最后一个复制块存储在不同机架的某个节点上。 Secondary NameNode第二名称节点的作用在于为 HDFS 中的名称节点提供一个 Checkpoint，它只是名称节点的一个助手节点，这也是它在社区内被认为是 Checkpoint Node 的原因。 DatabNode数据节点也是一个通常在 HDFS 架构中的单独机器上运行的组件。Hadoop 集群包含一个 NameNode 和大量 DataNode。数据节点通常以机架的形式组织，机架通过一个交换机将所有系统连接起来。 数据节点响应来自 HDFS 客户机的读写请求。它们还响应来自 NameNode 的创建、删除和复制块的命令。名称节点依赖来自每个数据节点的定期心跳（heartbeat）消息。每条消息都包含一个块报告，名称节点可以根据这个报告验证块映射和其他文件系统元数据。如果数据节点不能发送心跳消息，名称节点将采取修复措施，重新复制在该节点上丢失的块。","categories":[{"name":"大数据","slug":"大数据","permalink":"http://blog.crazylaw.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"大数据，Hadoop","slug":"大数据，Hadoop","permalink":"http://blog.crazylaw.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%EF%BC%8CHadoop/"}]},{"title":"【kubernetes】nginx-ingress","slug":"k8s/k8s-nginx-ingress","date":"2019-06-04T03:28:30.000Z","updated":"2021-03-20T16:25:01.807Z","comments":true,"path":"2019/06/04/k8s/k8s-nginx-ingress/","link":"","permalink":"http://blog.crazylaw.cn/2019/06/04/k8s/k8s-nginx-ingress/","excerpt":"前言Ingress 是一个负载均衡的东西，其主要用来解决使用 NodePort 暴露 Service 的端口时 Node IP 会漂移的问题。同时，若大量使用 NodePort 暴露主机端口，管理会非常混乱。 好的解决方案就是让外界通过域名去访问 Service，而无需关心其 Node IP 及 Port。那为什么不直接使用 Nginx？这是因为在 K8S 集群中，如果每加入一个服务，我们都在 Nginx 中添加一个配置，其实是一个重复性的体力活，只要是重复性的体力活，我们都应该通过技术将它干掉。 Ingress 就可以解决上面的问题，其包含两个组件 Ingress Controller 和 Ingress： Ingress （将 Nginx 的配置抽象成一个 Ingress 对象，每添加一个新的服务只需写一个新的 Ingress 的 yaml 文件即可） Ingress Controller （将新加入的 Ingress 转化成 Nginx 的配置文件并使之生效）","text":"前言Ingress 是一个负载均衡的东西，其主要用来解决使用 NodePort 暴露 Service 的端口时 Node IP 会漂移的问题。同时，若大量使用 NodePort 暴露主机端口，管理会非常混乱。 好的解决方案就是让外界通过域名去访问 Service，而无需关心其 Node IP 及 Port。那为什么不直接使用 Nginx？这是因为在 K8S 集群中，如果每加入一个服务，我们都在 Nginx 中添加一个配置，其实是一个重复性的体力活，只要是重复性的体力活，我们都应该通过技术将它干掉。 Ingress 就可以解决上面的问题，其包含两个组件 Ingress Controller 和 Ingress： Ingress （将 Nginx 的配置抽象成一个 Ingress 对象，每添加一个新的服务只需写一个新的 Ingress 的 yaml 文件即可） Ingress Controller （将新加入的 Ingress 转化成 Nginx 的配置文件并使之生效） 集群内部访问 ClusterIp 集群外访问： NodePort Loadbalancer （云服务商） Ingress Ingress-nginxIngress-nginx 在我写这篇文章的时候，ingress-nginx 到最新版本为：0.24.1，但是秉承着稳定的想法，本次我们使用0.20.0作为我们的版本 由于我们服务器并没有翻墙，所以找了国内的几个镜像下载，重新打 tag 1docker pull registry.cn-qingdao.aliyuncs.com/kubernetes_xingej/defaultbackend-amd64:1.5 &amp;&amp; docker pull registry.cn-qingdao.aliyuncs.com/kubernetes_xingej/nginx-ingress-controller:0.20.0 &amp;&amp; docker tag registry.cn-qingdao.aliyuncs.com/kubernetes_xingej/defaultbackend-amd64:1.5 k8s.gcr.io/defaultbackend-amd64:1.5 &amp;&amp; docker tag registry.cn-qingdao.aliyuncs.com/kubernetes_xingej/nginx-ingress-controller:0.20.0 quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.20.0 接下来，我们去到 ingress-nginx 找到 对应 0.20.0的版本，下载 k8s 配置文件。 Ingress-nginx(v0.20.0) 我们只需要关注 mandatory.yaml 即可 1wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.20.0/deploy/mandatory.yaml 我们需要修改重点需要修改 hostNetwork: true (用户使得容器的网络 namespace 和宿主机的 namespace，通过暴露宿 node 节点的 80 端口来作为 ingress 入口节点端口) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324apiVersion: v1kind: Namespacemetadata: name: ingress-nginx---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: default-http-backend labels: app.kubernetes.io/name: default-http-backend app.kubernetes.io/part-of: ingress-nginx namespace: ingress-nginxspec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: default-http-backend app.kubernetes.io/part-of: ingress-nginx template: metadata: labels: app.kubernetes.io/name: default-http-backend app.kubernetes.io/part-of: ingress-nginx spec: terminationGracePeriodSeconds: 60 containers: - name: default-http-backend # Any image is permissible as long as: # 1. It serves a 404 page at / # 2. It serves 200 on a /healthz endpoint image: k8s.gcr.io/defaultbackend-amd64:1.5 livenessProbe: httpGet: path: /healthz port: 8080 scheme: HTTP initialDelaySeconds: 30 timeoutSeconds: 5 ports: - containerPort: 8080 resources: limits: cpu: 10m memory: 20Mi requests: cpu: 10m memory: 20Mi---apiVersion: v1kind: Servicemetadata: name: default-http-backend namespace: ingress-nginx labels: app.kubernetes.io/name: default-http-backend app.kubernetes.io/part-of: ingress-nginxspec: ports: - port: 80 targetPort: 8080 selector: app.kubernetes.io/name: default-http-backend app.kubernetes.io/part-of: ingress-nginx---kind: ConfigMapapiVersion: v1metadata: name: nginx-configuration namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx---kind: ConfigMapapiVersion: v1metadata: name: tcp-services namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx---kind: ConfigMapapiVersion: v1metadata: name: udp-services namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx---apiVersion: v1kind: ServiceAccountmetadata: name: nginx-ingress-serviceaccount namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRolemetadata: name: nginx-ingress-clusterrole labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginxrules: - apiGroups: - '' resources: - configmaps - endpoints - nodes - pods - secrets verbs: - list - watch - apiGroups: - '' resources: - nodes verbs: - get - apiGroups: - '' resources: - services verbs: - get - list - watch - apiGroups: - 'extensions' resources: - ingresses verbs: - get - list - watch - apiGroups: - '' resources: - events verbs: - create - patch - apiGroups: - 'extensions' resources: - ingresses/status verbs: - update---apiVersion: rbac.authorization.k8s.io/v1beta1kind: Rolemetadata: name: nginx-ingress-role namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginxrules: - apiGroups: - '' resources: - configmaps - pods - secrets - namespaces verbs: - get - apiGroups: - '' resources: - configmaps resourceNames: # Defaults to \"&lt;election-id&gt;-&lt;ingress-class&gt;\" # Here: \"&lt;ingress-controller-leader&gt;-&lt;nginx&gt;\" # This has to be adapted if you change either parameter # when launching the nginx-ingress-controller. - 'ingress-controller-leader-nginx' verbs: - get - update - apiGroups: - '' resources: - configmaps verbs: - create - apiGroups: - '' resources: - endpoints verbs: - get---apiVersion: rbac.authorization.k8s.io/v1beta1kind: RoleBindingmetadata: name: nginx-ingress-role-nisa-binding namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginxroleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: nginx-ingress-rolesubjects: - kind: ServiceAccount name: nginx-ingress-serviceaccount namespace: ingress-nginx---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: nginx-ingress-clusterrole-nisa-binding labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginxroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: nginx-ingress-clusterrolesubjects: - kind: ServiceAccount name: nginx-ingress-serviceaccount namespace: ingress-nginx---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nginx-ingress-controller namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginxspec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx template: metadata: labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx annotations: prometheus.io/port: '10254' prometheus.io/scrape: 'true' spec: serviceAccountName: nginx-ingress-serviceaccount hostNetwork: true containers: - name: nginx-ingress-controller imagePullPolicy: IfNotPresent image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.20.0 args: - /nginx-ingress-controller - --default-backend-service=$(POD_NAMESPACE)/default-http-backend - --configmap=$(POD_NAMESPACE)/nginx-configuration - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services - --udp-services-configmap=$(POD_NAMESPACE)/udp-services - --publish-service=$(POD_NAMESPACE)/ingress-nginx - --annotations-prefix=nginx.ingress.kubernetes.io securityContext: capabilities: drop: - ALL add: - NET_BIND_SERVICE # www-data -&gt; 33 runAsUser: 33 env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace ports: - name: http containerPort: 80 - name: https containerPort: 443 livenessProbe: failureThreshold: 3 httpGet: path: /healthz port: 10254 scheme: HTTP initialDelaySeconds: 10 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 readinessProbe: failureThreshold: 3 httpGet: path: /healthz port: 10254 scheme: HTTP periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1--- 修改完毕之后，我们就可以执行这个 yaml 1kubectl apply -f mandatory.yaml 1234[root@master 20]# kubectl get pods -n ingress-nginxNAME READY STATUS RESTARTS AGEdefault-http-backend-5c9bb94849-fhlt8 1/1 Running 0 39hnginx-ingress-controller-84d5b54fdf-2hxbh 1/1 Running 0 39h 这里我们看到了有 2 个 pod 了，其中一个是默认的 http 请求端口，这个 pod 里面的服务的作用是，当没有一个 rule 匹配到 ingress 的时候，就会被分发到这个 pod 上，然后返回 404 到相关信息 接下来，我们需要配置我们自己的后端服务了，以 nginx服务 为例子: nginx.yaml 12345678910111213141516171819202122232425262728293031323334apiVersion: apps/v1kind: Deploymentmetadata: name: my-nginx labels: app: my-nginxspec: replicas: 1 selector: matchLabels: app: my-nginx template: metadata: labels: app: my-nginx spec: containers: - name: my-nginx image: nginx:1.7.9 ports: - containerPort: 80---apiVersion: v1kind: Servicemetadata: name: my-nginxspec: ports: - port: 80 targetPort: 80 protocol: TCP selector: app: my-nginx 这个部署文件中，里面设置了 2 种 kind，分别是 Deployment，主要是用于设置和容器相关的信息，另外一个是 Service，主要是用于设置服务暴露端口和集群内部服务转发相关的信息。 Deployment：这里我们可以看到我的配置是 nginx:1.7.9 为基础镜像，并且容器暴露的端口为 80 端口Service：这里我们可以看到我的配置是 spec.ports[].port = 80，spec.ports[].targetPort = 80(这个不设置的话也可以，默认和port一致)。 这样子，我们的一个基础的 nginx 服务就设置完毕了。 接下里，我们需要设置ingress 配置，文件名为：nginx-ingress.yaml 123456789101112131415apiVersion: extensions/v1beta1kind: Ingressmetadata: name: ingress-mynginx annotations: kubernetes.io/ingress.class: 'nginx'spec: rules: - host: k8s-nginx.mingchao.com http: paths: - path: backend: serviceName: my-nginx servicePort: 80 这里，我们可以看到，我绑定了 k8s-nginx.mingchao.com 这个host(serviceName)(域名) 指向了后端服务名字叫 my-nginx，并且端口为 80 的服务。其实这里就是指向了我们刚才设置的 nginx服务 我们启动一下这个配置 1kubectl apply -f nginx-ingress.yaml 我们可以看到结果： 123[root@master nginx]# kubectl get ingressNAME HOSTS ADDRESS PORTS AGEingress-mynginx k8s-nginx.mingchao.com 80 16h 发现这个 Ingress 已经生效了。由于我们是在本地测试，并没有用公网的域名，所以公网的 DNS 是找到我们的域名，所以我们需要做本地 Host，打开 /etc/hosts 文件，进行添加 host 之后，我们就可以用浏览器打开了。 但是这里为了偷懒，我直接用 curl 指定 host 的方法访问试试，结果如下 记得，访问的是 node 节点 12345678910111213141516171819202122232425262728293031323334353637383940414243444546[root@master nginx]# curl -v http://192.168.8.174 -H 'host: k8s-nginx.mingchao.com'* About to connect() to 192.168.8.174 port 80 (#0)* Trying 192.168.8.174...* Connected to 192.168.8.174 (192.168.8.174) port 80 (#0)&gt; GET / HTTP/1.1&gt; User-Agent: curl/7.29.0&gt; Accept: */*&gt; host: k8s-nginx.mingchao.com&gt;&lt; HTTP/1.1 200 OK&lt; Server: nginx/1.15.5&lt; Date: Fri, 07 Jun 2019 01:51:22 GMT&lt; Content-Type: text/html&lt; Content-Length: 612&lt; Connection: keep-alive&lt; Vary: Accept-Encoding&lt; Last-Modified: Tue, 23 Dec 2014 16:25:09 GMT&lt; ETag: \"54999765-264\"&lt; Accept-Ranges: bytes&lt;&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt; body &#123; width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; &#125;&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;&lt;p&gt;If you see this page, the nginx web server is successfully installed andworking. Further configuration is required.&lt;/p&gt;&lt;p&gt;For online documentation and support please refer to&lt;a href=\"http://nginx.org/\"&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;Commercial support is available at&lt;a href=\"http://nginx.com/\"&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;* Connection #0 to host 192.168.8.174 left intact 这个时候，其实我们就可以发现，已经访问成功了，就这样子，通过 ingress-nginx 这个组件修改 hostNetwork = true，我们可以轻松实现，通过域名访问 80 端口从而转发到我们后端的任意一种后端服务 这样子，我们的 ingress-nginx 基本就算是完成了。","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://blog.crazylaw.cn/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://blog.crazylaw.cn/tags/kubernetes/"}]},{"title":"【Docker】搭建docker私有仓库","slug":"docker/搭建docker私有仓库","date":"2019-05-31T03:28:30.000Z","updated":"2021-03-20T16:25:01.805Z","comments":true,"path":"2019/05/31/docker/搭建docker私有仓库/","link":"","permalink":"http://blog.crazylaw.cn/2019/05/31/docker/%E6%90%AD%E5%BB%BAdocker%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93/","excerpt":"前言为了配合 k8s，需要搭建一个私有仓库，防止镜像泄漏出去。","text":"前言为了配合 k8s，需要搭建一个私有仓库，防止镜像泄漏出去。 机器准备最小化分布式集群安装，一台master，一台node。后续可以调整为多master，解决单点问题，node的话也可以增加以加入集群。 hostname IP 配置 操作系统 matser 192.168.8.171 4核8G CentOS Linux release 7.2.1511 (Core)，Linux version 3.10.0-327.el7.x86_64，gcc version 4.8.3 20140911 (Red Hat 4.8.3-9) (GCC) node1 192.168.8.174 4核8G CentOS Linux release 7.2.1511 (Core) ，Linux version 3.10.0-327.el7.x86_64，gcc version 4.8.3 20140911 (Red Hat 4.8.3-9) (GCC) \b## 安装 registry这个是 docker 官方提供的仓库镜像，现在已经进入了2.x的时代，对应的仓库在 github 上并不是docker-registry，docker-registry 已经归档了，并且不进行维护了，所以目前hub.docker 上的 registry 对应在 github 上的仓库名字叫做 docker/distribution 安装 registry 1docker run -d -p 5000:5000 -v `pwd`/data:/tmp/registry-dev --restart=always --name registry registry:2 12CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES15d7f8c43400 registry:2 \"/entrypoint.sh /e...\" 12 minutes ago Up 50 seconds 0.0.0.0:5000-&gt;5000/tcp registry 这个时候私有仓库镜像成功建立了，我们测试一下。 拉取镜像 1docker pull busybox:latest 重新打tag 1docker tag busybox:latest 192.168.8.171:5000/tonybai/busybox:latest 推送到指定仓库 , 结果发现推送 失败 123[root@master registry]# docker push 192.168.8.171:5000/tonybai/busyboxThe push refers to a repository [192.168.8.171:5000/tonybai/busybox]Get https://192.168.8.171:5000/v1/_ping: http: server gave HTTP response to HTTPS client 那是因为我们的docker，并没有设置不安全私有仓库白名单 所以这个时候我们需要修改/etc/docker/daemon.json 123456&#123; \"registry-mirrors\": [\"https://registry.docker-cn.com\"], \"insecure-registries\": [ \"192.168.8.171:5000\" ]&#125; 然后我们重启docker即可，容器会随之启动 1systemctl restart docker 启动成功后，然后再推送一次. 12[root@master registry]# docker rmi 10.10.105.71:5000/tonybai/busyboxUntagged: 10.10.105.71:5000/tonybai/busybox:latest 成功了，不过他告诉我我没有指定tags所以默认的他加上了latest标签。 secure registry 自签名证书 证书服务提供商 这里，我们主要是用自签名证书，所以需要在每个拉取我们私有仓库的镜像的时候，都需要拥有我们证书才可以拉取 1234567891011121314151617181920[root@master ~]# mkdir ~&#x2F;openssl&#x2F;certs&#x2F; &amp;&amp; cd ~&#x2F;openssl&#x2F;certs&#x2F; &amp;&amp; openssl req -newkey rsa:2048 -nodes -sha256 -keyout certs&#x2F;domain.key -x509 -days 365 -out certs&#x2F;domain.crtGenerating a 2048 bit RSA private key.....+++...................................................................................................................................................+++writing new private key to &#39;certs&#x2F;domain.key&#39;-----You are about to be asked to enter information that will be incorporatedinto your certificate request.What you are about to enter is what is called a Distinguished Name or a DN.There are quite a few fields but you can leave some blankFor some fields there will be a default value,If you enter &#39;.&#39;, the field will be left blank.-----Country Name (2 letter code) [XX]:CNState or Province Name (full name) []:GuangdongLocality Name (eg, city) [Default City]:GuangzhouOrganization Name (eg, company) [Default Company Ltd]:xxxOrganizational Unit Name (eg, section) []:xxCommon Name (eg, your name or your server&#39;s hostname) []:mcdockerhub.comEmail Address []:471113744@qq.com 由于我们的 Common Name 设置了 mcdockerhub.com，所以我们需要修改一下hosts（除非你有公网解析到这个IP） 123vim /etc/hosts/192.168.8.171 master mcdockerhub.com 移除之前我们创建的 registy，然后我们重新创建一个 1docker stop registy &amp;&amp; docker rm registy 1docker run -d -p 5000:5000 --restart=always --name registry -v /root/openssl/data:/tmp/registry-dev -v /root/openssl/certs:/certs -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt -e REGISTRY_HTTP_TLS_KEY=/certs/domain.key docker.io/registry:2 接着我们重新打一个对应registy的tag 1docker tag docker.io/busybox mcdockerhub.com:5000/tonybai/busybox 推送镜像到域名仓库 123[root@master ~]# docker push mcdockerhub.com:5000/tonybai/busyboxThe push refers to a repository [mcdockerhub.com:5000/tonybai/busybox]Get https://mcdockerhub.com:5000/v1/_ping: x509: certificate signed by unknown authority 我们发现报错了。这是因为 没有把证书 对应起来。 把证书放在docker请求的证书目录 1mkdir -p &#x2F;etc&#x2F;docker&#x2F;certs.d&#x2F;mcdockerhub.com:5000 &amp;&amp; cp ~&#x2F;openssl&#x2F;certs&#x2F;domain.crt &#x2F;etc&#x2F;docker&#x2F;certs.d&#x2F;mcdockerhub.com:5000&#x2F;ca.crt 再推送一次 1234[root@master ~]# docker push mcdockerhub.com:5000/tonybai/busyboxThe push refers to a repository [mcdockerhub.com:5000/tonybai/busybox]d1156b98822d: Pushedlatest: digest: sha256:4fe8827f51a5e11bb83afa8227cbccb402df840d32c6b633b7ad079bc8144100 size: 527 接着就是成功了。如果其他机器需要推送或者拉取的话，记得要把证书放在对应的机器的对应目录下，否则被视为没有被授权。","categories":[{"name":"Docker","slug":"Docker","permalink":"http://blog.crazylaw.cn/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://blog.crazylaw.cn/tags/Docker/"}]},{"title":"【kubernetes】测试集群部署","slug":"k8s/k8s测试集群部署","date":"2019-05-28T03:28:30.000Z","updated":"2021-03-20T16:25:01.807Z","comments":true,"path":"2019/05/28/k8s/k8s测试集群部署/","link":"","permalink":"http://blog.crazylaw.cn/2019/05/28/k8s/k8s%E6%B5%8B%E8%AF%95%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/","excerpt":"前言为了在公司推广 docker 和 k8s，方便我们开发人员去更好的维护自己的对应的生产环境目前用 k8s 构建我们自己的测试环境，用于接口测试和功能测试专用。本文记录一下集群部署的情况","text":"前言为了在公司推广 docker 和 k8s，方便我们开发人员去更好的维护自己的对应的生产环境目前用 k8s 构建我们自己的测试环境，用于接口测试和功能测试专用。本文记录一下集群部署的情况 机器准备最小化分布式集群安装，一台master，一台node。后续可以调整为多master，解决单点问题，node的话也可以增加以加入集群。 hostname IP 配置 操作系统 matser 192.168.8.171 4核8G CentOS Linux release 7.2.1511 (Core)，Linux version 3.10.0-327.el7.x86_64，gcc version 4.8.3 20140911 (Red Hat 4.8.3-9) (GCC) node1 192.168.8.174 4核8G CentOS Linux release 7.2.1511 (Core) ，Linux version 3.10.0-327.el7.x86_64，gcc version 4.8.3 20140911 (Red Hat 4.8.3-9) (GCC) 安装k8s集群\b### 检查各机器防火墙状态 1systemctl list-unit-files | grep firewalld.service 如果防火墙的状态是 enabled 的话，暂时先关闭防火墙，为了不影响我们的测试部署 关闭防火墙 &amp;&amp; 禁止随开机启动 1systemctl stop firewalld.service &amp;&amp; systemctl disable firewalld.service 关闭各机器上的SELINUX1setenforce 0 &amp;&amp; sed -i 's/SELINUX=enforcing/SELINUX=disabled/' /etc/selinux/config 同步各机器时间差1yum install -y ntpdate &amp;&amp; ntpdate -u ntp.api.bz 修改各个机器的hostname和hosts文件修改 etc/hosts 1master_ip=192.168.8.171;node1_ip=192.168.8.174;echo -e \"\\n$&#123;master_ip&#125; master\\n$&#123;node1_ip&#125; node1\" &gt;&gt; /etc/hosts 修改 master 的 hostname 1hostnamectl set-hostname master 修改 node1 的 hostname 1hostnamectl set-hostname node1 配置相关的yum仓库配置阿里云docker-ce仓库1wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo 配置阿里云k8s仓库12345678cat &gt; /etc/yum.repos.d/kubernetes.repo &lt;&lt; EOF[kubernetes]name=Kubernetes Repobaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/gpgcheck=1gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgenable=1EOF 安装docker kubelet kubeadm kubectl当前最新版：1.14.2 为了稳定性考虑，目前暂时考虑最新版的前一个版本1.14.1 kubeadm kubectl kubelet kubeadm-1.14.1-0.x86_64 kubectl-1.14.1-0.x86_64 kubelet-1.14.1-0.x86_64 由于利用kubeadm部署集群，查看kubeadm所有版本，因为kuadm和kubectl和kubelet都要对应好版本，所以如果版本不对应的话，可能需要降级或者升级操作 1yum list --showduplicates | grep kubeadm 12345yum install -y docker kubelet-1.14.1-0 kubeadm-1.14.1-0 kubectl-1.14.1-0``` ```shellrpm -qa | grep kubeadm &amp;&amp; rpm -qa | grep kubelet &amp;&amp; rpm -qa | grep kubectl 修改系统内核参数由于docker随后会大量的操作iptables，所有nf-call的值要设置为1，尽量不使用swap 12345cat &gt; /etc/sysctl.d/k8s-sysctl.conf &lt;&lt;EOFnet.bridge.bridge-nf-call-iptables = 1net.bridge.bridge-nf-call-ip6tables = 1vm.swappiness = 0EOF 刷新内核配置参数 1sysctl --system 查看参数信息 1cat /proc/sys/net/bridge/bridge-nf-call-iptables 设置随开机启动1systemctl enable docker &amp;&amp; systemctl enable kubelet 设置kubelet忽略swap1echo 'KUBELET_EXTRA_ARGS=\"--fail-swap-on=false\"' &gt; /etc/sysconfig/kubelet 关闭swap1swapoff -a 启动docker1234567systemctl start docker``` 确保docker已启动```shellsystemctl status docker 下载k8s核心组件镜像由于k8s的镜像在谷歌云，所以我们需要在国内的镜像源中获取镜像，下面是为从国内镜像源中找到的镜像源，后续，我们可以做一个我们自己的私有仓库，保存这里镜像 1234567891011121314151617181920212223242526272829303132333435cat &gt; deploy.sh &lt;&lt;EOF#/usr/bin/env bash## k8s-coreimages=( kube-proxy:v1.14.1 kube-apiserver:v1.14.1 kube-controller-manager:v1.14.1 kube-scheduler:v1.14.1 etcd:3.3.10 pause:3.1)for imageName in \\$&#123;images[@]&#125;; do docker pull mirrorgooglecontainers/\\$&#123;imageName&#125; docker tag mirrorgooglecontainers/\\$&#123;imageName&#125; k8s.gcr.io/\\$&#123;imageName&#125; docker rmi mirrorgooglecontainers/\\$&#123;imageName&#125;done## k8s-network-managerimages=( coredns:1.3.1)for imageName in \\$&#123;images[@]&#125;; do docker pull coredns/\\$&#123;imageName&#125; docker tag coredns/\\$&#123;imageName&#125; k8s.gcr.io/\\$&#123;imageName&#125; docker rmi coredns/\\$&#123;imageName&#125;done ## k8s-web-ui## docker pull gcrxio/kubernetes-dashboard-amd64:v1.10.1 &amp;&amp; docker tag gcrxio/kubernetes-dashboard-amd64:v1.10.1 k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1 &amp;&amp; docker rmi gcrxio/kubernetes-dashboard-amd64:v1.10.1## nginx-ingress-controller## docker pull bitnami/nginx-ingress-controller:0.24.1 &amp;&amp; docker tag bitnami/nginx-ingress-controller:0.24.1 quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.24.1 &amp;&amp; docker rmi bitnami/nginx-ingress-controller:0.24.1EOF 1chmod +x deploy.sh 执行 deploy.sh 脚本 等待下载安装，完毕之后，查看一下镜像是否已经下载好了 123456789[root@master ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEk8s.gcr.io/kube-proxy v1.14.1 20a2d7035165 7 weeks ago 82.1 MBk8s.gcr.io/kube-apiserver v1.14.1 cfaa4ad74c37 7 weeks ago 210 MBk8s.gcr.io/kube-scheduler v1.14.1 8931473d5bdb 7 weeks ago 81.6 MBk8s.gcr.io/kube-controller-manager v1.14.1 efb3887b411d 7 weeks ago 158 MBk8s.gcr.io/coredns 1.3.1 eb516548c180 4 months ago 40.3 MBk8s.gcr.io/etcd 3.3.10 2c4adeb21b4f 5 months ago 258 MBk8s.gcr.io/pause 3.1 da86e6ba6ca1 17 months ago 742 kB 下载好了之后，我们就开始部署我们的k8s集群 初始化k8s-matser在 master 节点初始化操作 1kubeadm init --kubernetes-version=v1.14.1 --pod-network-cidr=10.244.0.0/16 --ignore-preflight-errors=Swap 过程如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465[init] Using Kubernetes version: v1.14.1[preflight] Running pre-flight checks[preflight] Pulling images required for setting up a Kubernetes cluster[preflight] This might take a minute or two, depending on the speed of your internet connection[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'[kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\"[kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\"[kubelet-start] Activating the kubelet service[certs] Using certificateDir folder \"/etc/kubernetes/pki\"[certs] Generating \"ca\" certificate and key[certs] Generating \"apiserver\" certificate and key[certs] apiserver serving cert is signed for DNS names [master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.8.171][certs] Generating \"apiserver-kubelet-client\" certificate and key[certs] Generating \"front-proxy-ca\" certificate and key[certs] Generating \"front-proxy-client\" certificate and key[certs] Generating \"etcd/ca\" certificate and key[certs] Generating \"etcd/server\" certificate and key[certs] etcd/server serving cert is signed for DNS names [master localhost] and IPs [192.168.8.171 127.0.0.1 ::1][certs] Generating \"etcd/healthcheck-client\" certificate and key[certs] Generating \"etcd/peer\" certificate and key[certs] etcd/peer serving cert is signed for DNS names [master localhost] and IPs [192.168.8.171 127.0.0.1 ::1][certs] Generating \"apiserver-etcd-client\" certificate and key[certs] Generating \"sa\" key and public key[kubeconfig] Using kubeconfig folder \"/etc/kubernetes\"[kubeconfig] Writing \"admin.conf\" kubeconfig file[kubeconfig] Writing \"kubelet.conf\" kubeconfig file[kubeconfig] Writing \"controller-manager.conf\" kubeconfig file[kubeconfig] Writing \"scheduler.conf\" kubeconfig file[control-plane] Using manifest folder \"/etc/kubernetes/manifests\"[control-plane] Creating static Pod manifest for \"kube-apiserver\"[control-plane] Creating static Pod manifest for \"kube-controller-manager\"[control-plane] Creating static Pod manifest for \"kube-scheduler\"[etcd] Creating static Pod manifest for local etcd in \"/etc/kubernetes/manifests\"[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \"/etc/kubernetes/manifests\". This can take up to 4m0s[apiclient] All control plane components are healthy after 25.053527 seconds[upload-config] storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace[kubelet] Creating a ConfigMap \"kubelet-config-1.14\" in namespace kube-system with the configuration for the kubelets in the cluster[upload-certs] Skipping phase. Please see --experimental-upload-certs[mark-control-plane] Marking the node master as control-plane by adding the label \"node-role.kubernetes.io/master=''\"[mark-control-plane] Marking the node master as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule][bootstrap-token] Using token: f1agmt.o01kvgt0slzlj7y6[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster[bootstrap-token] creating the \"cluster-info\" ConfigMap in the \"kube-public\" namespace[addons] Applied essential addon: CoreDNS[addons] Applied essential addon: kube-proxyYour Kubernetes control-plane has initialized successfully!To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/configYou should now deploy a pod network to the cluster.Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/Then you can join any number of worker nodes by running the following on each as root:kubeadm join 192.168.8.171:6443 --token f1agmt.o01kvgt0slzlj7y6 \\ --discovery-token-ca-cert-hash sha256:a6bc1cbed5084d136237f7bfb469f82c3dfbfbdef0f967602d015b5fb5a6447d 需要用到kubectl的用户都需要执行如下命令 1mkdir -p $HOME/.kube &amp;&amp; sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config &amp;&amp; sudo chown $(id -u):$(id -g) $HOME/.kube/config 安装网络组件-flannel这个时候可以看一下master节点的状态 123[root@master ~]# kubectl get nodesNAME STATUS ROLES AGE VERSIONmaster NotReady master 15h v1.14.1 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667[root@master ~]# kubectl describe nodes masterName: masterRoles: masterLabels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/os=linux kubernetes.io/arch=amd64 kubernetes.io/hostname=master kubernetes.io/os=linux node-role.kubernetes.io/master=Annotations: kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock node.alpha.kubernetes.io/ttl: 0 volumes.kubernetes.io/controller-managed-attach-detach: trueCreationTimestamp: Tue, 28 May 2019 18:14:37 +0800Taints: node.kubernetes.io/not-ready:NoExecute node-role.kubernetes.io/master:NoSchedule node.kubernetes.io/not-ready:NoScheduleUnschedulable: falseConditions: Type Status LastHeartbeatTime LastTransitionTime Reason Message ---- ------ ----------------- ------------------ ------ ------- MemoryPressure False Wed, 29 May 2019 09:15:07 +0800 Tue, 28 May 2019 18:14:28 +0800 KubeletHasSufficientMemory kubelet has sufficient memory available DiskPressure False Wed, 29 May 2019 09:15:07 +0800 Tue, 28 May 2019 18:14:28 +0800 KubeletHasNoDiskPressure kubelet has no disk pressure PIDPressure False Wed, 29 May 2019 09:15:07 +0800 Tue, 28 May 2019 18:14:28 +0800 KubeletHasSufficientPID kubelet has sufficient PID available Ready False Wed, 29 May 2019 09:15:07 +0800 Tue, 28 May 2019 18:14:28 +0800 KubeletNotReady runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitializedAddresses: InternalIP: 192.168.8.171 Hostname: masterCapacity: cpu: 4 ephemeral-storage: 51175Mi hugepages-2Mi: 0 memory: 8009192Ki pods: 110Allocatable: cpu: 4 ephemeral-storage: 48294789041 hugepages-2Mi: 0 memory: 7906792Ki pods: 110System Info: Machine ID: fae837ec0d8a402ab8085d2e0ae4624f System UUID: 421D57E8-3C84-52D4-17F1-7D87F3BF8FF8 Boot ID: b6c7d535-ad5f-41b1-9636-4e46e96adaf0 Kernel Version: 3.10.0-957.el7.x86_64 OS Image: CentOS Linux 7 (Core) Operating System: linux Architecture: amd64 Container Runtime Version: docker://1.13.1 Kubelet Version: v1.14.1 Kube-Proxy Version: v1.14.1PodCIDR: 10.244.0.0/24Non-terminated Pods: (5 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits AGE --------- ---- ------------ ---------- --------------- ------------- --- kube-system etcd-master 0 (0%) 0 (0%) 0 (0%) 0 (0%) 14h kube-system kube-apiserver-master 250m (6%) 0 (0%) 0 (0%) 0 (0%) 14h kube-system kube-controller-manager-master 200m (5%) 0 (0%) 0 (0%) 0 (0%) 14h kube-system kube-proxy-nz9h6 0 (0%) 0 (0%) 0 (0%) 0 (0%) 15h kube-system kube-scheduler-master 100m (2%) 0 (0%) 0 (0%) 0 (0%) 14hAllocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 550m (13%) 0 (0%) memory 0 (0%) 0 (0%) ephemeral-storage 0 (0%) 0 (0%)Events: &lt;none&gt; 我们看到这里告诉我们网络插件并没有准备好。所以我们接下来安装网络插件 1kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 过一会儿我们再来看一下节点的信息 123[root@master ~]# kubectl get nodesNAME STATUS ROLES AGE VERSIONmaster Ready master 16h v1.14.1 发现我们的 master节点已经是 Ready 状态了。 加入node工作节点还记得我们再初始化master的时候有，有如下提示： 1234Then you can join any number of worker nodes by running the following on each as root:kubeadm join 192.168.8.171:6443 --token f1agmt.o01kvgt0slzlj7y6 \\ --discovery-token-ca-cert-hash sha256:a6bc1cbed5084d136237f7bfb469f82c3dfbfbdef0f967602d015b5fb5a6447d 这个需要我们在node 节点执行的命令，一定要保存好，以后扩展node节点的时候，都要用这个 token 和 discovery-token-ca-cert-hash 在node1的机器上执行如下命令 12kubeadm join 192.168.8.171:6443 --token f1agmt.o01kvgt0slzlj7y6 \\ --discovery-token-ca-cert-hash sha256:a6bc1cbed5084d136237f7bfb469f82c3dfbfbdef0f967602d015b5fb5a6447d 过程如下 1234567891011121314[preflight] Running pre-flight checks[preflight] Reading configuration from the cluster...[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'[kubelet-start] Downloading configuration for the kubelet from the \"kubelet-config-1.14\" ConfigMap in the kube-system namespace[kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\"[kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\"[kubelet-start] Activating the kubelet service[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...This node has joined the cluster:* Certificate signing request was sent to apiserver and a response was received.* The Kubelet was informed of the new secure connection details.Run 'kubectl get nodes' on the control-plane to see this node join the cluster. 在 master 的机器上执行如下命令 1234[root@master ~]# kubectl get nodesNAME STATUS ROLES AGE VERSIONmaster Ready master 16h v1.14.1node1 Ready &lt;none&gt; 2m39s v1.14.1 发现node1节点已经加入进来了，并且状态已经是 Ready 123456789101112[root@master ~]# kubectl get pods -n kube-system -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATEScoredns-fb8b8dccf-cr7t4 1/1 Running 0 16h 10.244.0.2 master &lt;none&gt; &lt;none&gt;coredns-fb8b8dccf-hn5kh 1/1 Running 0 16h 10.244.0.3 master &lt;none&gt; &lt;none&gt;etcd-master 1/1 Running 0 16h 192.168.8.171 master &lt;none&gt; &lt;none&gt;kube-apiserver-master 1/1 Running 0 16h 192.168.8.171 master &lt;none&gt; &lt;none&gt;kube-controller-manager-master 1/1 Running 0 16h 192.168.8.171 master &lt;none&gt; &lt;none&gt;kube-flannel-ds-amd64-27cts 1/1 Running 0 3m29s 192.168.8.174 node1 &lt;none&gt; &lt;none&gt;kube-flannel-ds-amd64-wk4c9 1/1 Running 0 91m 192.168.8.171 master &lt;none&gt; &lt;none&gt;kube-proxy-mfjlr 1/1 Running 0 3m29s 192.168.8.174 node1 &lt;none&gt; &lt;none&gt;kube-proxy-nz9h6 1/1 Running 0 16h 192.168.8.171 master &lt;none&gt; &lt;none&gt;kube-scheduler-master 1/1 Running 0 16h 192.168.8.171 master &lt;none&gt; &lt;none&gt; 这里我们可以看到所有的k8s组件pods都已经准备就绪，到此位置，一个master一个node的k8s已经部署完毕。 遇到的坑SELinux is not supported with the overlay2 graph driver on this kernel意思是： 此linux的内核中的SELinux不支持 overlay2 graph driver ，解决方法有两个，要么启动一个新内核，要么就在docker里禁用selinux，–selinux-enabled=false 打开docker配置文件 vim /etc/sysconfig/docker 1234OPTIONS='--selinux-enabled --log-driver=journald --signature-verification=false'if [ -z \"$&#123;DOCKER_CERT_PATH&#125;\" ]; then DOCKER_CERT_PATH=/etc/dockerfi 改为如下: 1234OPTIONS='--selinux-enabled=false --log-driver=journald --signature-verification=false'if [ -z \"$&#123;DOCKER_CERT_PATH&#125;\" ]; then DOCKER_CERT_PATH=/etc/dockerfi \u001e改完之后再启动docker即可。 [kubelet-check] connection refused[kubelet-check] The HTTP call equal to ‘curl -sSL http://localhost:10255/healthz&#39; failed with error: Get http://localhost:10255/healthz: dial tcp [::1]:10255: connection refused 初始化过程中，master初始化的过程中，发现总是在校验kubelet服务的时候失败，会报如上内容。 查看kubelet服务是否启动了 1systemctl status kubelet 发现服务是没有启动的。 由于没有详细的信息，所以执行如下命令： 1journalctl -xeu kubelet 得到如下结果： 1Failed to find subsystem mount for required subsystem: pid 我们的 cgroup 不支持 pids，所以运行不起来。 查看当前系统支持哪些subsystem 123456789101112cat /proc/cgroups#subsys_name hierarchy num_cgroups enabledcpuset 5 11 1cpu 4 104 1cpuacct 4 104 1memory 6 104 1devices 3 104 1freezer 2 11 1net_cls 8 11 1blkio 9 104 1perf_event 10 11 1hugetlb 7 11 1 发现确实没有pids。查看当前系统内核。 12uname -r3.10.0-327.el7.x86_64 这个内核正是开头所说的内核版本。那我们只能升级内核版本了。看一下有什么内核版本可以升级。 123456789101112131415yum list kernel.x86_64 --showduplicates | sort -r* updates: ap.stykers.moeLoading mirror speeds from cached hostfileLoaded plugins: fastestmirror, langpackskernel.x86_64 3.10.0-957.el7 base kernel.x86_64 3.10.0-957.5.1.el7 updates kernel.x86_64 3.10.0-957.1.3.el7 updates kernel.x86_64 3.10.0-957.10.1.el7 updates kernel.x86_64 3.10.0-327.el7 @anacondaInstalled Packages * extras: mirrors.huaweicloud.com * epel: mirrors.aliyun.com * elrepo: mirrors.tuna.tsinghua.edu.cn * base: ap.stykers.moeAvailable Packages 1yum install kernel-3.10.0-957.el7.x86_64 -y CentOS 7使用grub2作为引导程序，查看有哪些内核选项 12345678cat /boot/grub2/grub.cfg |grep menuentry ##查看有哪些内核选项if [ x\"$&#123;feature_menuentry_id&#125;\" = xy ]; then menuentry_id_option=\"--id\" menuentry_id_option=\"\"export menuentry_id_optionmenuentry 'CentOS Linux (3.10.0-957.el7.x86_64) 7 (Core)' --class centos --class gnu-linux --class gnu --class os --unrestricted $menuentry_id_option 'gnulinux-3.10.0-327.el7.x86_64-advanced-7787952f-c2d4-4216-ae09-5188e7fd88b8' &#123;menuentry 'CentOS Linux (3.10.0-327.el7.x86_64) 7 (Core)' --class centos --class gnu-linux --class gnu --class os --unrestricted $menuentry_id_option 'gnulinux-3.10.0-327.el7.x86_64-advanced-7787952f-c2d4-4216-ae09-5188e7fd88b8' &#123;menuentry 'CentOS Linux (0-rescue-d918a8d2df0e481a820b4e5554fed3b5) 7 (Core)' --class centos --class gnu-linux --class gnu --class os --unrestricted $menuentry_id_option 'gnulinux-0-rescue-d918a8d2df0e481a820b4e5554fed3b5-advanced-7787952f-c2d4-4216-ae09-5188e7fd88b8' &#123; 查看默认启动内核 1grub2-editenv list 重启系统，更换内核版本 1reboot 重启后在查看内核版本换了没，subsystem中是否有pids 1uname -r 1234567891011121314cat /proc/cgroups#subsys_name hierarchy num_cgroups enabledcpuset 5 11 1cpu 4 110 1cpuacct 4 110 1memory 3 110 1devices 6 110 1freezer 7 11 1net_cls 2 11 1blkio 10 110 1perf_event 8 11 1hugetlb 9 11 1pids 11 110 1net_prio 2 11 1 ok，这个时候支持了pids，所以这个时候，这个问题就解决了。 每次重启都需要关闭swap1swapoff -a","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://blog.crazylaw.cn/categories/kubernetes/"},{"name":"Docker","slug":"kubernetes/Docker","permalink":"http://blog.crazylaw.cn/categories/kubernetes/Docker/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"http://blog.crazylaw.cn/tags/kubernetes/"}]},{"title":"PHP-CS-Fixer 配置详细对比","slug":"php-cs-fixer配置项详细对比说明","date":"2019-05-21T01:50:00.000Z","updated":"2021-03-20T16:25:01.812Z","comments":true,"path":"2019/05/21/php-cs-fixer配置项详细对比说明/","link":"","permalink":"http://blog.crazylaw.cn/2019/05/21/php-cs-fixer%E9%85%8D%E7%BD%AE%E9%A1%B9%E8%AF%A6%E7%BB%86%E5%AF%B9%E6%AF%94%E8%AF%B4%E6%98%8E/","excerpt":"前言由于我们项目比较多，而且每个人的编写代码的风格也不太一致，我们希望尽可能在格式上得到统一，这样子的好处有如下几点： 遵循 PSR2 codereview 的时候可以免除格式差异所带来的干扰 统一格式有利于大家在这个基础上写出更为优雅的代码格式 因此，我们实现了用于的统一格式化配套的工具链： husky-php：用于实现客户端 githook 钩子，用于执行触发我们的 husky 项目（默认支持自动格式化，冲突校验） composer-husky-plugin：专门由 husky-php 定制的 composer 插件，用于自动部署husky-php php-cs-fixer-config：专门定义 php-cs-fixer 格式配置的组件","text":"前言由于我们项目比较多，而且每个人的编写代码的风格也不太一致，我们希望尽可能在格式上得到统一，这样子的好处有如下几点： 遵循 PSR2 codereview 的时候可以免除格式差异所带来的干扰 统一格式有利于大家在这个基础上写出更为优雅的代码格式 因此，我们实现了用于的统一格式化配套的工具链： husky-php：用于实现客户端 githook 钩子，用于执行触发我们的 husky 项目（默认支持自动格式化，冲突校验） composer-husky-plugin：专门由 husky-php 定制的 composer 插件，用于自动部署husky-php php-cs-fixer-config：专门定义 php-cs-fixer 格式配置的组件 PHP_CS_Fixer这是一个代码格式化工具，并且支持很多高级格式内容。 PHP_CS_FIXER 配置项@PSR2遵循 PSR2 的标准 123$rules = [ '@PSR2' =&gt; true]; 原始格式： 1234function A($args1,$args2=0)&#123; $a=2; var_dump($args1,$args2);&#125; 格式化后： 12345function A($args1, $args2=0)&#123; $a=2; var_dump($args1, $args2);&#125; align_multiline_comment [@PhpCsFixer]每行多行 DocComments 必须有一个星号（PSR-5），并且必须与第一行对齐。 可选配置项 comment_type phpdocs_only （默认） phpdocs_like all_multiline 暂时未知三个选项的具体区别，目前来看效果都是一致的，例子如下 12345$rules = [ 'align_multiline_comment' =&gt; [ 'comment_type' =&gt; 'phpdocs_only' ]]; 原始格式： 12345678/** * @param $args1@param int $args2 */function A($args1, $args2=0)&#123; $a=2;&#125; 格式化后： 12345678/** * @param $args1 * @param int $args2 */function A($args1, $args2=0)&#123; $a=2;&#125; array_indentation [@PhpCsFixer]数组的每个元素必须缩进一次。 123$rules = [ 'array_indentation'=&gt;true]; 原始格式： 12345678910/** * @param $args1 * @param int $args2 */function A($args1, $args2=0)&#123; $array=[ '1' ];&#125; 格式化后： 12345678910/** * @param $args1 * @param int $args2 */function A($args1, $args2=0)&#123; $array=[ '1' ];&#125; array_syntax [@PhpCsFixer]应使用配置的语法声明 PHP 数组。 可选配置项 syntax long (默认，用array关键字来定义数组) short (用[]关键字来定义数组) 12345$rules = [ 'array_syntax' =&gt; [ 'syntax' =&gt; 'short', ]]; 原始格式： 12345678/** * @param $args1 * @param int $args2 */function A($args1, $args2=0)&#123; $array=array();&#125; 格式化后： 12345678/** * @param $args1 * @param int $args2 */function A($args1, $args2=0)&#123; $array=[];&#125; backtick_to_shell_exec将反引号运算符转换为 shell_exec 调用。 123$rules = [ 'backtick_to_shell_exec' =&gt; true]; 原始格式： 1234567function A()&#123; $out = `echo Hello`; var_dump($out);&#125;A(); 格式化后： 1234567function A()&#123; $out = shell_exec(\"echo Hello\"); var_dump($out);&#125;A(); binary_operator_spaces [@Symfony, @PhpCsFixer]“=”, ““, “/“, “%”, “&lt;”, “&gt;”, “|”, “^”, “+”, “-“, “&amp;”, “&amp;=”, “&amp;&amp;”, “||”, “.=”, “/=”, “=&gt;”, “==”, “&gt;=”, “===”, “!=”, “&lt;&gt;”, “!==”, “&lt;=”, “and”, “or”, “xor”, “-=”, “%=”, “=”, “|=”, “+=”, “&lt;&lt;”, “&lt;&lt;=”, “&gt;&gt;”, “&gt;&gt;=”, “^=”, ““, “=”, “&lt;=&gt;”, “??”这一类的二进制运算符应按配置包含的空格。 可选配置项 align_double_arrow (双箭头居中格式，将会为废弃，用 default/operators 来代替更详细的配置) false null true align_equals (等于号居中格式，将会为废弃，用 default/operators 来代替更详细的配置) false null true default (默认) align （居中） align_single_space （默认，单空格居中） align_single_space_minimal (和 align_single_space 的区别是如果空格大于 1 个的时候，会缩减到 1 个) no_space （没有空格） single_space (单空格，不居中) null (不做任何改变) operators（array） （针对特殊的操作符做特殊的处理） key ：”=”, ““, “/“, “%”, “&lt;”, “&gt;”, “|”, “^”, “+”, “-“, “&amp;”, “&amp;=”, “&amp;&amp;”, “||”, “.=”, “/=”, “=&gt;”, “==”, “&gt;=”, “===”, “!=”, “&lt;&gt;”, “!==”, “&lt;=”, “and”, “or”, “xor”, “-=”, “%=”, “=”, “|=”, “+=”, “&lt;&lt;”, “&lt;&lt;=”, “&gt;&gt;”, “&gt;&gt;=”, “^=”, ““, “=”, “&lt;=&gt;”, “??” value 同 default 这里就不测试 align_double_arrow 和 align_equals 了，因为这 2 个参数会被废弃 12345$rules = [ 'binary_operator_spaces' =&gt; [ 'default' =&gt; 'align' ]]; 原始格式： 12345678function A()&#123; $a = 1&gt;&gt;2; $a = 12&gt;&gt;2; $a = 12&lt;&lt;1; $a = 12&amp;1; $bb=1112|1;&#125; 格式化后： 12345678function A()&#123; $a = 1 &gt;&gt;2; $a = 12&gt;&gt;2; $a = 12&lt;&lt;1; $a = 12&amp;1; $bb=1112|1;&#125; 12345$rules = [ 'binary_operator_spaces' =&gt; [ 'default' =&gt; 'align_single_space' ]]; 原始格式： 12345678function A()&#123; $a = 1&gt;&gt;2; $a = 12&gt;&gt;2; $a = 12&lt;&lt;1; $a = 12&amp;1; $bb=1112|1;&#125; 格式化后： 12345678function A()&#123; $a = 1 &gt;&gt; 2; $a = 12 &gt;&gt; 2; $a = 12 &lt;&lt; 1; $a = 12 &amp; 1; $bb = 1112 | 1;&#125; 12345$rules = [ 'binary_operator_spaces' =&gt; [ 'default' =&gt; 'align_single_space_minimal' ]]; 原始格式： 12345678function A()&#123; $a = 1 &gt;&gt; 2; $a = 12 &gt;&gt; 2; $a = 12 &lt;&lt; 1; $a = 12 &amp; 1; $bb = 1112 | 1;&#125; 格式化后： 12345678function A()&#123; $a = 1 &gt;&gt; 2; $a = 12 &gt;&gt; 2; $a = 12 &lt;&lt; 1; $a = 12 &amp; 1; $bb = 1112 | 1;&#125; 12345$rules = [ 'binary_operator_spaces' =&gt; [ 'default' =&gt; 'no_space' ]]; 原始格式： 12345678function A()&#123; $a = 1 &gt;&gt; 2; $a = 12 &gt;&gt; 2; $a = 12 &lt;&lt; 1; $a = 12 &amp; 1; $bb = 1112 | 1;&#125; 格式化后： 12345678function A()&#123; $a=1&gt;&gt;2; $a=12&gt;&gt;2; $a=12&lt;&lt;1; $a=12&amp;1; $bb=1112|1;&#125; 12345$rules = [ 'binary_operator_spaces' =&gt; [ 'default' =&gt; 'single_space' ]]; 原始格式： 12345678function A()&#123; $a=1&gt;&gt;2; $a=12&gt;&gt;2; $a=12&lt;&lt;1; $a=12&amp;1; $bb=1112|1;&#125; 格式化后： 12345678function A()&#123; $a = 1 &gt;&gt; 2; $a = 12 &gt;&gt; 2; $a = 12 &lt;&lt; 1; $a = 12 &amp; 1; $bbbb = 1112 | 1;&#125; 1234567$rules = [ 'binary_operator_spaces' =&gt; [ 'operators' =&gt; [ '=' =&gt; 'no_space' ] ]]; 原始格式： 12345678function A()&#123; $array = [ 'a'=&gt;1, 'bb'=&gt;2, 'ccc'=&gt;3 ];&#125; 格式化后： 12345678function A()&#123; $array=[ 'a' =&gt; 1, 'bb' =&gt; 2, 'ccc' =&gt; 3 ];&#125; blank_line_after_namespace [@PSR2, @Symfony, @PhpCsFixer]命名空间之后空一行 123$rules = [ 'blank_line_after_namespace' =&gt; true]; 原始格式： 1234namespace Test;function A()&#123;&#125; 格式化后： 12345namespace Test;function A()&#123;&#125; blank_line_after_opening_tag [@Symfony, @PhpCsFixer]&lt;?php 后面加一个空行 123$rules = [ 'blank_line_after_namespace' =&gt; true]; 原始格式： 123456&lt;?phpnamespace Test;function A()&#123;&#125; 格式化后： 1234567&lt;?phpnamespace Test;function A()&#123;&#125; blank_line_before_returnreturn 前面加一个空白行（弃用！用 blank_line_before_statement 代替） 123$rules = [ 'blank_line_before_return' =&gt; true]; 原始格式： 12345function A()&#123; $a = '123' return $a;&#125; 格式化后： 123456function A()&#123; $a = '123' return $a;&#125; blank_line_before_statement [@Symfony, @PhpCsFixer]空行换行必须在任何已配置的语句之前 1234567891011$rules = [ 'blank_line_before_statement' =&gt; [ 'statements' =&gt; [ 'break', 'continue', 'declare', 'return', 'throw', 'try' ]]; 可选配置项 statements (赋值数组) break case continue declare default die do exit for foreach goto if include include_once require require_once return switch throw try while yield 默认配置项的值为：[‘break’, ‘continue’, ‘declare’, ‘return’, ‘throw’, ‘try’] 原始格式： 123456789101112131415161718function A()&#123; $a = 1; switch ($a) &#123; case 1: break; default: &#125; while (true) &#123; if ($a == true)&#123; $a = 2; continue; &#125; &#125; throw new \\Exception(); return 1;&#125; 格式化后： 123456789101112131415161718192021function A()&#123; $a = 1; switch ($a) &#123; case 1: break; default: &#125; while (true) &#123; if ($a == true) &#123; $a = 2; continue; &#125; &#125; throw new \\Exception(); return 1;&#125; braces [@PSR2, @Symfony, @PhpCsFixer]每个结构的主体必须用大括号括起来。大括号应妥善放置。大括号应适当缩进。 可选配置项 allow_single_line_closure (是否应该允许单行 lambda 表示法) true false (默认) position_after_anonymous_constructs (在匿名构造（匿名类和 lambda 函数）之后是否应将开括号放在“next”或“same”行) next same (默认) position_after_functions_and_oop_constructs 在优雅结构（非匿名类，接口，特征，方法和非 lambda 函数）之后，是否应将开括号放在“next”或“same”行上 next （默认） same 这个规则就不写了，因为这个就是最佳的了，因为是 PSR2 标准（偷懒） 原始格式： 12345class A&#123; public function __construct()&#123; $a = function()&#123; echo 1;&#125;; &#125;&#125; 格式化后： 123456789class A&#123; public function __construct() &#123; $a = function () &#123; echo 1; &#125;; &#125;&#125; cast_spaces [@Symfony, @PhpCsFixer]类型转换的时候，是否需要在中间加空格 可选配置项 space none single (默认) 123456$rules = [ 'cast_spaces' =&gt; [ 'space' =&gt; [ 'single', ]]; 原始格式： 123function A()&#123; echo (int)1;&#125; 格式化后： 1234function A()&#123; echo (int) 1;&#125; class_attributes_separation [@Symfony, @PhpCsFixer]Class, trait 和 interface 的属性是否需要一个空行隔开 可选配置项 elements （数组） const method property 默认配置项的值为：[‘const’, ‘method’, ‘property’] 12345$rules = [ 'class_attributes_separation' =&gt; [ 'elements' =&gt; ['const', 'method', 'property'] ]]; 原始格式： 12345class A&#123; private $a; private $b;&#125; 格式化后： 123456class A&#123; private $a; private $b;&#125; class_definition [@PSR2, @Symfony, @PhpCsFixer]class,trait,interface 关键字周围是否只有一个空格 可选配置项 multi_line_extends_each_single_line true false (默认) single_item_single_line true false (默认) single_line true false (默认) PSR2 标准的就不具体配置了 原始格式： 12345class A&#123; private $a; private $b;&#125; 格式化后： 123456class A&#123; private $a; private $b;&#125; class_keyword_remove::class 关键字移除，转成字符串 123$rules = [ 'class_keyword_remove' =&gt; true]; 原始格式： 12345class A&#123;&#125;echo A::class; 格式化后： 12345class A&#123;&#125;echo 'A'; combine_consecutive_issets [@PhpCsFixer]当多个 isset 通过&amp;&amp;连接的时候，合并处理 123$rules = [ 'combine_consecutive_issets' =&gt; true]; 原始格式： 1isset($a) &amp;&amp; isset($b) &amp;&amp; isset($c); 格式化后： 1isset($a, $b, $c); combine_consecutive_unsets [@PhpCsFixer]当多个 unset 使用的时候，合并处理 123$rules = [ 'combine_consecutive_unsets' =&gt; true]; 原始格式： 123unset($a);unset($b);unset($c); 格式化后： 1unset($a, $b, $c); combine_nested_dirname [@PHP70Migration:risky, @PHP71Migration:risky]Replace multiple nested calls of dirname by only one call with second $level parameter. Requires PHP &gt;= 7.0. Risky rule: risky when the function dirname is overridden. comment_to_phpdoc [@PhpCsFixer:risky]Comments with annotation should be docblock when used on structural elements. Risky rule: risky as new docblocks might mean more, e.g. a Doctrine entity might have a new column in database. compact_nullable_typehint [@PhpCsFixer] 暂时不太清楚什么区别 concat_space [@Symfony, @PhpCsFixer]连接字符是否需要空格 可选配置项 spacing none (默认) one 12345$rules = [ 'concat_space' =&gt; [ 'spacing' =&gt; 'one' ]]; 原始格式： 123$a = '';$c = 'c';$b = $a.$c; 格式化后： 123$a = '';$c = 'c';$b = $a . $c; date_time_immutableClass DateTimeImmutable should be used instead of DateTime. Risky rule: risky when the code relies on modifying DateTime objects or if any of the date_create* functions are overridden. declare_equal_normalize [@Symfony, @PhpCsFixer]declare 语句中的等于号是否需要空格 可选配置项 space none single（默认） 12345$rules = [ 'declare_equal_normalize' =&gt; [ 'space' =&gt; 'single' ]]; 原始格式： 12declare(ticks=1) &#123;&#125; 格式化后： 12declare(ticks = 1) &#123;&#125; declare_strict_types [@PHP70Migration:risky, @PHP71Migration:risky]Force strict types declaration in all files. Requires PHP &gt;= 7.0. Risky rule: forcing strict types will stop non strict code from working. dir_constant [@Symfony:risky, @PhpCsFixer:risky]Replaces dirname(__FILE__) expression with equivalent __DIR__ constant. Risky rule: risky when the function dirname is overridden. doctrine_annotation_array_assignment [@DoctrineAnnotation]Doctrine annotations must use configured operator for assignment in arrays. Configuration options: ignored_tags (array): list of tags that must not be treated as Doctrine Annotations; defaults to [‘abstract’, ‘access’, ‘code’, ‘deprec’, ‘encode’, ‘exception’, ‘final’, ‘ingroup’, ‘inheritdoc’, ‘inheritDoc’, ‘magic’, ‘name’, ‘toc’, ‘tutorial’, ‘private’, ‘static’, ‘staticvar’, ‘staticVar’, ‘throw’, ‘api’, ‘author’, ‘category’, ‘copyright’, ‘deprecated’, ‘example’, ‘filesource’, ‘global’, ‘ignore’, ‘internal’, ‘license’, ‘link’, ‘method’, ‘package’, ‘param’, ‘property’, ‘property-read’, ‘property-write’, ‘return’, ‘see’, ‘since’, ‘source’, ‘subpackage’, ‘throws’, ‘todo’, ‘TODO’, ‘usedBy’, ‘uses’, ‘var’, ‘version’, ‘after’, ‘afterClass’, ‘backupGlobals’, ‘backupStaticAttributes’, ‘before’, ‘beforeClass’, ‘codeCoverageIgnore’, ‘codeCoverageIgnoreStart’, ‘codeCoverageIgnoreEnd’, ‘covers’, ‘coversDefaultClass’, ‘coversNothing’, ‘dataProvider’, ‘depends’, ‘expectedException’, ‘expectedExceptionCode’, ‘expectedExceptionMessage’, ‘expectedExceptionMessageRegExp’, ‘group’, ‘large’, ‘medium’, ‘preserveGlobalState’, ‘requires’, ‘runTestsInSeparateProcesses’, ‘runInSeparateProcess’, ‘small’, ‘test’, ‘testdox’, ‘ticket’, ‘uses’, ‘SuppressWarnings’, ‘noinspection’, ‘package_version’, ‘enduml’, ‘startuml’, ‘fix’, ‘FIXME’, ‘fixme’, ‘override’]operator (‘:’, ‘=’): the operator to use; defaults to ‘=’ doctrine_annotation_braces [@DoctrineAnnotation]Doctrine annotations without arguments must use the configured syntax. Configuration options: ignored_tags (array): list of tags that must not be treated as Doctrine Annotations; defaults to [‘abstract’, ‘access’, ‘code’, ‘deprec’, ‘encode’, ‘exception’, ‘final’, ‘ingroup’, ‘inheritdoc’, ‘inheritDoc’, ‘magic’, ‘name’, ‘toc’, ‘tutorial’, ‘private’, ‘static’, ‘staticvar’, ‘staticVar’, ‘throw’, ‘api’, ‘author’, ‘category’, ‘copyright’, ‘deprecated’, ‘example’, ‘filesource’, ‘global’, ‘ignore’, ‘internal’, ‘license’, ‘link’, ‘method’, ‘package’, ‘param’, ‘property’, ‘property-read’, ‘property-write’, ‘return’, ‘see’, ‘since’, ‘source’, ‘subpackage’, ‘throws’, ‘todo’, ‘TODO’, ‘usedBy’, ‘uses’, ‘var’, ‘version’, ‘after’, ‘afterClass’, ‘backupGlobals’, ‘backupStaticAttributes’, ‘before’, ‘beforeClass’, ‘codeCoverageIgnore’, ‘codeCoverageIgnoreStart’, ‘codeCoverageIgnoreEnd’, ‘covers’, ‘coversDefaultClass’, ‘coversNothing’, ‘dataProvider’, ‘depends’, ‘expectedException’, ‘expectedExceptionCode’, ‘expectedExceptionMessage’, ‘expectedExceptionMessageRegExp’, ‘group’, ‘large’, ‘medium’, ‘preserveGlobalState’, ‘requires’, ‘runTestsInSeparateProcesses’, ‘runInSeparateProcess’, ‘small’, ‘test’, ‘testdox’, ‘ticket’, ‘uses’, ‘SuppressWarnings’, ‘noinspection’, ‘package_version’, ‘enduml’, ‘startuml’, ‘fix’, ‘FIXME’, ‘fixme’, ‘override’]syntax (‘with_braces’, ‘without_braces’): whether to add or remove braces; defaults to ‘without_braces’ doctrine_annotation_indentation [@DoctrineAnnotation]Doctrine annotations must be indented with four spaces. Configuration options: ignored_tags (array): list of tags that must not be treated as Doctrine Annotations; defaults to [‘abstract’, ‘access’, ‘code’, ‘deprec’, ‘encode’, ‘exception’, ‘final’, ‘ingroup’, ‘inheritdoc’, ‘inheritDoc’, ‘magic’, ‘name’, ‘toc’, ‘tutorial’, ‘private’, ‘static’, ‘staticvar’, ‘staticVar’, ‘throw’, ‘api’, ‘author’, ‘category’, ‘copyright’, ‘deprecated’, ‘example’, ‘filesource’, ‘global’, ‘ignore’, ‘internal’, ‘license’, ‘link’, ‘method’, ‘package’, ‘param’, ‘property’, ‘property-read’, ‘property-write’, ‘return’, ‘see’, ‘since’, ‘source’, ‘subpackage’, ‘throws’, ‘todo’, ‘TODO’, ‘usedBy’, ‘uses’, ‘var’, ‘version’, ‘after’, ‘afterClass’, ‘backupGlobals’, ‘backupStaticAttributes’, ‘before’, ‘beforeClass’, ‘codeCoverageIgnore’, ‘codeCoverageIgnoreStart’, ‘codeCoverageIgnoreEnd’, ‘covers’, ‘coversDefaultClass’, ‘coversNothing’, ‘dataProvider’, ‘depends’, ‘expectedException’, ‘expectedExceptionCode’, ‘expectedExceptionMessage’, ‘expectedExceptionMessageRegExp’, ‘group’, ‘large’, ‘medium’, ‘preserveGlobalState’, ‘requires’, ‘runTestsInSeparateProcesses’, ‘runInSeparateProcess’, ‘small’, ‘test’, ‘testdox’, ‘ticket’, ‘uses’, ‘SuppressWarnings’, ‘noinspection’, ‘package_version’, ‘enduml’, ‘startuml’, ‘fix’, ‘FIXME’, ‘fixme’, ‘override’]indent_mixed_lines (bool): whether to indent lines that have content before closing parenthesis; defaults to false doctrine_annotation_spaces [@DoctrineAnnotation]Fixes spaces in Doctrine annotations. Configuration options: after_argument_assignments (null, bool): whether to add, remove or ignore spaces after argument assignment operator; defaults to falseafter_array_assignments_colon (null, bool): whether to add, remove or ignore spaces after array assignment : operator; defaults to trueafter_array_assignments_equals (null, bool): whether to add, remove or ignore spaces after array assignment = operator; defaults to truearound_argument_assignments (bool): whether to fix spaces around argument assignment operator; defaults to true. DEPRECATED: use options before_argument_assignments and after_argument_assignments insteadaround_array_assignments (bool): whether to fix spaces around array assignment operators; defaults to true. DEPRECATED: use options before_array_assignments_equals, after_array_assignments_equals, before_array_assignments_colon and after_array_assignments_colon insteadaround_commas (bool): whether to fix spaces around commas; defaults to truearound_parentheses (bool): whether to fix spaces around parentheses; defaults to truebefore_argument_assignments (null, bool): whether to add, remove or ignore spaces before argument assignment operator; defaults to falsebefore_array_assignments_colon (null, bool): whether to add, remove or ignore spaces before array : assignment operator; defaults to truebefore_array_assignments_equals (null, bool): whether to add, remove or ignore spaces before array = assignment operator; defaults to trueignored_tags (array): list of tags that must not be treated as Doctrine Annotations; defaults to [‘abstract’, ‘access’, ‘code’, ‘deprec’, ‘encode’, ‘exception’, ‘final’, ‘ingroup’, ‘inheritdoc’, ‘inheritDoc’, ‘magic’, ‘name’, ‘toc’, ‘tutorial’, ‘private’, ‘static’, ‘staticvar’, ‘staticVar’, ‘throw’, ‘api’, ‘author’, ‘category’, ‘copyright’, ‘deprecated’, ‘example’, ‘filesource’, ‘global’, ‘ignore’, ‘internal’, ‘license’, ‘link’, ‘method’, ‘package’, ‘param’, ‘property’, ‘property-read’, ‘property-write’, ‘return’, ‘see’, ‘since’, ‘source’, ‘subpackage’, ‘throws’, ‘todo’, ‘TODO’, ‘usedBy’, ‘uses’, ‘var’, ‘version’, ‘after’, ‘afterClass’, ‘backupGlobals’, ‘backupStaticAttributes’, ‘before’, ‘beforeClass’, ‘codeCoverageIgnore’, ‘codeCoverageIgnoreStart’, ‘codeCoverageIgnoreEnd’, ‘covers’, ‘coversDefaultClass’, ‘coversNothing’, ‘dataProvider’, ‘depends’, ‘expectedException’, ‘expectedExceptionCode’, ‘expectedExceptionMessage’, ‘expectedExceptionMessageRegExp’, ‘group’, ‘large’, ‘medium’, ‘preserveGlobalState’, ‘requires’, ‘runTestsInSeparateProcesses’, ‘runInSeparateProcess’, ‘small’, ‘test’, ‘testdox’, ‘ticket’, ‘uses’, ‘SuppressWarnings’, ‘noinspection’, ‘package_version’, ‘enduml’, ‘startuml’, ‘fix’, ‘FIXME’, ‘fixme’, ‘override’] elseif [@PSR2, @Symfony, @PhpCsFixer]用elseif来代替else if 由于是 PSR2 的标准，所以就不配置了 原始格式： 123if ($a = 1) &#123;&#125; else if (true) &#123;&#125; 格式化后： 123if ($a = 1) &#123;&#125; elseif (true) &#123;&#125; encoding [@PSR1, @PSR2, @Symfony, @PhpCsFixer]PHP 代码必须只使用没有 BOM 的 UTF-8（删除 BOM）。 由于是 PSR2 的标准，所以就不配置了 这个也没什么好演示的了 ereg_to_preg [@Symfony:risky, @PhpCsFixer:risky]Replace deprecated ereg regular expression functions with preg. Risky rule: risky if the ereg function is overridden. error_suppression [@Symfony:risky, @PhpCsFixer:risky]Error control operator should be added to deprecation notices and/or removed from other cases. Risky rule: risky because adding/removing @ might cause changes to code behaviour or if trigger_error function is overridden. Configuration options: mute_deprecation_error (bool): whether to add @ in deprecation notices; defaults to truenoise_remaining_usages (bool): whether to remove @ in remaining usages; defaults to falsenoise_remaining_usages_exclude (array): list of global functions to exclude from removing @; defaults to [] escape_implicit_backslashes [@PhpCsFixer]是否需要自动帮忙添加转义字符 可选配置项 double_quoted (双引号) true （默认） false heredoc_syntax （heredoc 的语法） true （默认） false single_quoted (单引号) true false （默认） 1234567$rules = [ 'escape_implicit_backslashes' =&gt; [ 'double_quoted' =&gt; true, 'heredoc_syntax' =&gt; true, 'single_quoted' =&gt; false ]]; 原始格式： 12345$a = \"\\d\";$a = &lt;&lt;&lt;HEREDOC\\dHEREDOC;$a = '\\d'; 格式化后： 12345$a = \"\\\\d\";$a = &lt;&lt;&lt;HEREDOC\\\\dHEREDOC;$a = '\\d'; explicit_indirect_variable [@PhpCsFixer]Add curly braces to indirect variables to make them clear to understand. Requires PHP &gt;= 7.0. explicit_string_variable [@PhpCsFixer]把双引号或者 heredoc 字符串内部的隐形变量转成显性 123$rules = [ 'explicit_string_variable' =&gt; true]; 原始格式： 12345$b = 'b';$a = \"\\d $b\";$a = &lt;&lt;&lt;HEREDOC\\d $bHEREDOC; 格式化后： 12345$b = 'b';$a = \"\\d $&#123;b&#125;\";$a = &lt;&lt;&lt;HEREDOC\\d $&#123;b&#125;HEREDOC; final_classAll classes must be final, except abstract ones and Doctrine entities. Risky rule: risky when subclassing non-abstract classes. final_internal_class [@PhpCsFixer:risky]Internal classes should be final. Risky rule: changing classes to final might cause code execution to break. Configuration options: annotation-black-list (array): class level annotations tags that must be omitted to fix the class, even if all of the white list ones are used as well. (case insensitive); defaults to [‘@final’, ‘@Entity’, ‘@ORM\\Entity’]annotation-white-list (array): class level annotations tags that must be set in order to fix the class. (case insensitive); defaults to [‘@internal’]consider-absent-docblock-as-internal-class (bool): should classes without any DocBlock be fixed to final?; defaults to false fopen_flag_order [@Symfony:risky, @PhpCsFixer:risky]Order the flags in fopen calls, b and t must be last. Risky rule: risky when the function fopen is overridden. fopen_flags [@Symfony:risky, @PhpCsFixer:risky]The flags in fopen calls must omit t, and b must be omitted or included consistently. Risky rule: risky when the function fopen is overridden. Configuration options: b_mode (bool): the b flag must be used (true) or omitted (false); defaults to true fopen_flags [@Symfony:risky, @PhpCsFixer:risky]The flags in fopen calls must omit t, and b must be omitted or included consistently. Risky rule: risky when the function fopen is overridden. Configuration options: b_mode (bool): the b flag must be used (true) or omitted (false); defaults to true full_opening_tag [@PSR1, @PSR2, @Symfony, @PhpCsFixer]php 代码必须用 &lt;?php 或者 &lt;?= 不能是其他 这个在以前的前后段代码一起的时候可能需要注意的问题，在这里不做过多的演示 fully_qualified_strict_types [@PhpCsFixer]Transforms imported FQCN parameters and return types in function arguments to short version. function_declaration [@PSR2, @Symfony, @PhpCsFixer]必包函数关键字function后面是否需要空格 可选配置项 closure_function_spacing （闭包函数 function 是否需要空格） none one (默认) 12345$rules = [ 'function_declaration' =&gt; [ 'closure_function_spacing' =&gt; 'one' ]]; 原始格式： 12$a = function($a, $b) &#123;&#125;; 格式化后： 12$a = function ($a, $b) &#123;&#125;; function_to_constant [@Symfony:risky, @PhpCsFixer:risky]Replace core functions calls returning constants with the constants. Risky rule: risky when any of the configured functions to replace are overridden. Configuration options: functions (a subset of [‘get_called_class’, ‘get_class’, ‘php_sapi_name’, ‘phpversion’, ‘pi’]): list of function names to fix; defaults to [‘get_class’, ‘php_sapi_name’, ‘phpversion’, ‘pi’] function_typehint_space [@Symfony, @PhpCsFixer]在闭包函数的参数类型约束的时候，是否需要空格 123$rules = [ 'function_typehint_space' =&gt; true]; 原始格式： 12$a = function(array$a, $b) &#123;&#125;; 格式化后： 12$a = function (array $a, $b) &#123;&#125;; general_phpdoc_annotation_remove在 phpdoc 中应该忽略的注解 可选配置项 annotations(数组) author … 默认可选配置项的值：[] 1234567$rules = [ 'general_phpdoc_annotation_remove' =&gt; [ 'annotations' =&gt; [ 'author' ] ]]; 原始格式： 123456/** * @copyright 471113744@qq.com * @author caiwenhui */$a = '1'; 格式化后： 12345/** * @copyright caiwenhui */$a = '1'; hash_to_slash_commentSingle line comments should use double slashes // and not hash #. DEPRECATED: use single_line_comment_style instead. header_commentAdd, replace or remove header comment. Configuration options: comment_type (‘comment’, ‘PHPDoc’): comment syntax type; defaults to ‘comment’; DEPRECATED alias: commentTypeheader (string): proper header content; requiredlocation (‘after_declare_strict’, ‘after_open’): the location of the inserted header; defaults to ‘after_declare_strict’separate (‘both’, ‘bottom’, ‘none’, ‘top’): whether the header should be separated from the file content with a new line; defaults to ‘both’ heredoc_indentation [@PHP73Migration]Heredoc/nowdoc content must be properly indented. Requires PHP &gt;= 7.3. heredoc_to_nowdoc [@PhpCsFixer]当一个 heredoc 里面没有变量当时候，可以转成 nowdoc 123$rules = [ 'heredoc_to_nowdoc' =&gt; true]; 原始格式： 12345678$b = 'a';$a = &lt;&lt;&lt;HEREDOC$&#123;b&#125;HEREDOC;$c = &lt;&lt;&lt;CNOWDOC\\$bCNOWDOC; 格式化后： 12345678$b = 'a';$a = &lt;&lt;&lt;HEREDOC$&#123;b&#125;HEREDOC;$c = &lt;&lt;&lt;'CNOWDOC'$bCNOWDOC; implode_call [@Symfony:risky, @PhpCsFixer:risky]Function implode must be called with 2 arguments in the documented order. Risky rule: risky when the function implode is overridden. include [@Symfony, @PhpCsFixer]Include/Require 的时候，不应该用括号扩起来，应该用空格分割 123$rules = [ 'include' =&gt; true]; 原始格式： 1include('test.php'); 格式化后： 1include 'test.php'; increment_style [@Symfony, @PhpCsFixer]Pre- or post-increment and decrement operators should be used if possible. Configuration options: style (‘post’, ‘pre’): whether to use pre- or post-increment and decrement operators; defaults to ‘pre’ indentation_type [@PSR2, @Symfony, @PhpCsFixer]Code MUST use configured indentation type. is_null [@Symfony:risky, @PhpCsFixer:risky]Replaces is_null($var) expression with null === $var. Risky rule: risky when the function is_null is overridden. Configuration options: use_yoda_style (bool): whether Yoda style conditions should be used; defaults to true. DEPRECATED: use yoda_style fixer instead line_ending [@PSR2, @Symfony, @PhpCsFixer]所有的 PHP 文件编码必须一致 linebreak_after_opening_tag在&lt;?php 标签所在的行不允许存在代码 123$rules = [ 'linebreak_after_opening_tag' =&gt; true]; 原始格式： 123&lt;?php $a = 2;$a = 0; 格式化后： 12345&lt;?php$a = 2;$a = 0; list_syntaxList (array destructuring) assignment should be declared using the configured syntax. Requires PHP &gt;= 7.1. Configuration options: syntax (‘long’, ‘short’): whether to use the long or short list syntax; defaults to ‘long’ logical_operators [@PhpCsFixer:risky]Use &amp;&amp; and || logical operators instead of and and or. Risky rule: risky, because you must double-check if using and/or with lower precedence was intentional. lowercase_cast [@Symfony, @PhpCsFixer]数据类型转换必须小写 123$rules = [ 'lowercase_cast' =&gt; true]; 原始格式： 123&lt;?php$a = (INT)2; 格式化后： 123&lt;?php$a = (int)2; lowercase_constants [@PSR2, @Symfony, @PhpCsFixer]true, false, null 这几个 php 常量必须为小写 123$rules = [ 'lowercase_constants' =&gt; true]; 原始格式： 123$a = TRUE;$a = FALSE;$a = NULL; 格式化后： 123$a = true;$a = false;$a = null; lowercase_keywords [@PSR2, @Symfony, @PhpCsFixer]PHP 关键字必须小写 123$rules = [ 'lowercase_keywords' =&gt; true]; 原始格式： 123CLASS A &#123;&#125; 格式化后： 123class A &#123;&#125; lowercase_static_reference [@Symfony, @PhpCsFixer]静态调用必须小写,例如：self, static, parent 123$rules = [ 'lowercase_static_reference' =&gt; true]; 原始格式： 123456789class A&#123; public static $b; public function __construct() &#123; STATIC::$b; &#125;&#125; 格式化后： 123456789class A&#123; public static $b; public function __construct() &#123; static::$b; &#125;&#125; magic_constant_casing [@Symfony, @PhpCsFixer]Magic constants should be referred to using the correct casing. magic_method_casing [@Symfony, @PhpCsFixer]Magic method definitions and calls must be using the correct casing. mb_str_functionsReplace non multibyte-safe functions with corresponding mb function. Risky rule: risky when any of the functions are overridden. method_argument_space [@PSR2, @Symfony, @PhpCsFixer]在方法参数和方法调用中，每个逗号之前不能有空格，每个逗号后必须有一个空格。参数列表可以分为多行，每行后续行缩进一次。这样做时，列表中的第一项必须在下一行，并且每行必须只有一个参数。 可选配置项 after_heredoc (是否应删除 heredoc end 和逗号之间的空格) true false (默认) ensure_fully_multiline (确保多行参数列表的每个参数都在其自己的行上,废弃，on_multiline 改为使用选项) true false （默认） keep_multiple_spaces_after_comma （逗号后是否保留多个空格） true false（默认） on_multiline （定义如何处理包含换行符函数的参数列表） ensure_fully_multiline ensure_single_line ignore （默认） 这个也是比较重要，比较繁琐，但是容易理解，这里就不具体举例子了 method_chaining_indentation [@PhpCsFixer]Method chaining MUST be properly indented. Method chaining with different levels of indentation is not supported. modernize_types_casting [@Symfony:risky, @PhpCsFixer:risky]Replaces intval, floatval, doubleval, strval and boolval function calls with according type casting operator. multiline_comment_opening_closing [@PhpCsFixer]DocBlocks 必须以两个星号开头，多行注释必须以单个星号开头，在开头的斜线后面。两者必须在结束斜杠之前以单个星号结尾 multiline_whitespace_before_semicolons [@PhpCsFixer]在结束分号之前禁止多行空格或将分号移动到链接调用的新行。 12345$rules = [ 'multiline_whitespace_before_semicolons' =&gt; [ 'strategy' =&gt; 'no_multi_line' ]]; 原始格式： 123$a = 1; 格式化后： 1$a = 1; native_constant_invocation [@Symfony:risky, @PhpCsFixer:risky]Add leading \\ before constant invocation of internal constant to speed up resolving. Constant name match is case-sensitive, except for null, false and true. Risky rule: risky when any of the constants are namespaced or overridden. Configuration options: exclude (array): list of constants to ignore; defaults to [‘null’, ‘false’, ‘true’]fix_built_in (bool): whether to fix constants returned by get_defined_constants. User constants are not accounted in this list and must be specified in the include one; defaults to trueinclude (array): list of additional constants to fix; defaults to []scope (‘all’, ‘namespaced’): only fix constant invocations that are made within a namespace or fix all; defaults to ‘all’ native_function_casing [@Symfony, @PhpCsFixer]Function defined by PHP should be called using the correct casing. native_function_invocation [@Symfony:risky, @PhpCsFixer:risky]Add leading \\ before function invocation to speed up resolving. Risky rule: risky when any of the functions are overridden. Configuration options: exclude (array): list of functions to ignore; defaults to []include (array): list of function names or sets to fix. Defined sets are @internal (all native functions), @all (all global functions) and @compiler_optimized (functions that are specially optimized by Zend); defaults to [‘@internal’]scope (‘all’, ‘namespaced’): only fix function calls that are made within a namespace or fix all; defaults to ‘all’strict (bool): whether leading \\ of function call not meant to have it should be removed; defaults to false native_function_type_declaration_casing [@Symfony, @PhpCsFixer]Native type hints for functions should use the correct case. new_with_braces [@Symfony, @PhpCsFixer]使用 new 关键字创建的所有实例必须后跟括号。 123$rules = [ 'new_with_braces' =&gt; true]; 原始格式： 12345class A&#123;&#125;$a = new A; 格式化后： 12345class A&#123;&#125;$a = new A(); no_alias_functions [@Symfony:risky, @PhpCsFixer:risky]Master functions shall be used instead of aliases. Risky rule: risky when any of the alias functions are overridden. Configuration options: sets (a subset of [‘@internal’, ‘@IMAP’, ‘@mbreg’, ‘@all’]): list of sets to fix. Defined sets are @internal (native functions), @IMAP (IMAP functions), @mbreg (from ext-mbstring) @all (all listed sets); defaults to [‘@internal’, ‘@IMAP’] no_alternative_syntax [@PhpCsFixer]一般是结合在 html 页面写 php 的时候才用到 (alternative_syntax)[https://www.php.net/manual/zh/control-structures.alternative-syntax.php] no_binary_string [@PhpCsFixer]There should not be a binary flag before strings. no_blank_lines_after_class_opening [@Symfony, @PhpCsFixer]class 开标签后面不应该有空 123$rules = [ 'no_blank_lines_after_class_opening' =&gt; true]; 原始格式： 12345class A&#123; private $d;&#125; 格式化后： 1234class A&#123; private $d;&#125; no_blank_lines_after_phpdoc [@Symfony, @PhpCsFixer]phpdoc 后面不应该有空行 123$rules = [ 'no_blank_lines_after_phpdoc' =&gt; true]; 123456789/** * A class */class A&#123; private $d;&#125; 格式化后： 12345678/** * A class */class A&#123; private $d;&#125; no_blank_lines_before_namespace命名空间之前不应该有空行 123$rules = [ 'no_blank_lines_before_namespace' =&gt; true]; 12345678&lt;?phpnamespace caiwenhuiclass A&#123; private $d;&#125; 格式化后： 1234567&lt;?phpnamespace caiwenhuiclass A&#123; private $d;&#125; no_break_comment [@PSR2, @Symfony, @PhpCsFixer]There must be a comment when fall-through is intentional in a non-empty case body. Configuration options: comment_text (string): the text to use in the added comment and to detect it; defaults to ‘no break’ no_closing_tag [@PSR2, @Symfony, @PhpCsFixer]?&gt; 关闭标签必须在 PHP 文件中去掉 no_empty_comment [@Symfony, @PhpCsFixer]不应该存在空注释 no_empty_phpdoc [@Symfony, @PhpCsFixer]不应该存在空的 phpdoc no_empty_statement [@Symfony, @PhpCsFixer]不应该存在空的结构体 no_extra_blank_lines [@Symfony, @PhpCsFixer]移除额外的空行，在[‘break’, ‘case’, ‘continue’, ‘curly_brace_block’, ‘default’, ‘extra’, ‘parenthesis_brace_block’, ‘return’, ‘square_brace_block’, ‘switch’, ‘throw’, ‘use’, ‘useTrait’, ‘use_trait’中 1234567$rules = [ 'no_extra_blank_lines' =&gt; [ 'tokens' =&gt; [ 'return' ] ]]; 可选配置项 tokens （数组） break case continue curly_brace_block default extra ( 默认) parenthesis_brace_block return square_brace_block switch throw use useTrait use_trait 可选配置项默认值：[‘extra’] 原始格式： 12345678910function a()&#123; echo 1; return 1;&#125; 格式化后： 123456function a()&#123; echo 1; return 1;&#125; no_homoglyph_names [@Symfony:risky, @PhpCsFixer:risky]Replace accidental usage of homoglyphs (non ascii characters) in names. Risky rule: renames classes and cannot rename the files. You might have string references to renamed code ($$name). no_leading_import_slash [@Symfony, @PhpCsFixer]在 use 语句中，取消前置斜杠 123$rules = [ 'no_leading_import_slash' =&gt; true]; 原始格式： 1use \\A; 格式化后： 1use A; no_leading_namespace_whitespace [@Symfony, @PhpCsFixer]在声明命令空间的时候，不允许有前置空格 123$rules = [ 'no_leading_namespace_whitespace' =&gt; true]; 原始格式： 1namespace A; 格式化后： 1namespace A; no_mixed_echo_print [@Symfony, @PhpCsFixer]不允许混合使用echo和print语句 可选配置项 use echo (默认) print 12345$rules = [ 'no_mixed_echo_print' =&gt; [ 'use' =&gt; 'echo' ]]; 原始格式： 12echo 1;print 1; 格式化后： 12echo 1;echo 1; no_multiline_whitespace_around_double_arrow [@Symfony, @PhpCsFixer]运算符 =&gt; 不应被多行空格包围。 123$rules = [ 'no_multiline_whitespace_around_double_arrow' =&gt; true]; 原始格式： 123456$a = [ 1 =&gt; '33',]; 格式化后： 123$a = [ 1 =&gt; '33',]; no_null_property_initialization [@PhpCsFixer]属性不能用显式初始化 null 123$rules = [ 'no_null_property_initialization' =&gt; true]; 原始格式： 1234class A&#123; private $dd = null;&#125; 格式化后： 1234class A&#123; private $dd;&#125; no_php4_constructorConvert PHP4-style constructors to __construct. Risky rule: risky when old style constructor being fixed is overridden or overrides parent one. no_short_bool_cast [@Symfony, @PhpCsFixer]Short cast bool using double exclamation mark should not be used. no_short_echo_tag [@PhpCsFixer]用 &lt;?php echo 来代替 &lt;?= no_singleline_whitespace_before_semicolons [@Symfony, @PhpCsFixer]禁止在关闭分号前使用单行空格。 no_spaces_after_function_name [@PSR2，@Symfony，@PhpCsFixer]在函数或者方法定义的时候，不允许函数和左括号之间有空格 123$rules = [ 'no_spaces_after_function_name' =&gt; true]; 原始格式： 1234function a ()&#123;&#125; 格式化后： 1234function a()&#123;&#125; no_spaces_around_offset [@Symfony, @PhpCsFixer]There MUST NOT be spaces around offset braces. Configuration options: positions (a subset of [‘inside’, ‘outside’]): whether spacing should be fixed inside and/or outside the offset braces; defaults to [‘inside’, ‘outside’] no_spaces_inside_parenthesis [@PSR2, @Symfony, @PhpCsFixer]在左括号后面不能有空格。在右括号之前不能有空格。 no_superfluous_elseif [@PhpCsFixer]Replaces superfluous elseif with if. no_superfluous_phpdoc_tags删除没有提供有效信息的@param 和@return 注解 \b 可选配置项 allow_mixed （是否允许 mixed 注解存在） true false （默认） 12345$rules = [ 'no_superfluous_phpdoc_tags' =&gt; [ 'allow_mixed' =&gt; false ]]; 原始格式： 123456/** * @param $c */function a($b)&#123;&#125; 格式化后： 12345/** */function a($b)&#123;&#125; no_trailing_comma_in_list_call [@Symfony，@ PhpCsFixer]Remove trailing commas in list function calls. no_trailing_comma_in_singleline_array [@Symfony, @PhpCsFixer]PHP 单行数组不应该有逗号。 123$rules = [ 'no_trailing_comma_in_singleline_array' =&gt; true]; 原始格式： 1$array = [1,2,]; 格式化后： 1$array = [1,2]; no_trailing_whitespace [@PSR2，@Symfony，@PhpCsFixer]删除非空行末尾的尾随空格。 意思如描述 no_trailing_whitespace_in_comment [@PSR2，@Soundfony，@PhpCsFixer]注释或 PHPDoc 中必须没有尾随空格。 意思如描述 no_unneeded_control_parentheses [@Symfony，@PhpCsFixer]删除控制语句周围不需要的括号。 可选配置项 statements （数组） break clone continue echo_print return switch_case yield 可选配置项默认值：[‘break’, ‘clone’, ‘continue’, ‘echo_print’, ‘return’, ‘switch_case’, ‘yield’] 12345$rules = [ 'no_unneeded_curly_braces' =&gt; [ 'statements' =&gt; ['break', 'clone', 'continue', 'echo_print', 'return', 'switch_case', 'yield'] ]]; 原始格式： 1234function a()&#123; return(1);&#125; 格式化后： 1234function a()&#123; return 1;&#125; no_unneeded_curly_braces [@Symfony，@PhpCsFixer]删除不需要的花括号，这些花括号是多余的，不属于控制结构的主体。 123$rules = [ 'no_unneeded_curly_braces' =&gt; true]; 1234&lt;?php&#123; echo 1;&#125; 格式化后： 123&lt;?php echo 1; no_unneeded_final_method [@Symfony，@PhpCsFixer]终态类一定不能有终态方法 123$rules = [ 'no_unneeded_final_method' =&gt; true]; 123456final class A&#123; final public function echoA() &#123; &#125;&#125; 格式化后： 123456final class A&#123; public function echoA() &#123; &#125;&#125; no_unreachable_default_argument_value [@PhpCsFixer:risky]In function arguments there must not be arguments with default values before non-default ones. Risky rule: modifies the signature of functions; therefore risky when using systems (such as some Symfony components) that rely on those (for example through reflection). no_unset_cast [@PhpCsFixer]必须设置变量 null 而不是使用(unset)强制转换。 123$rules = [ 'no_unset_cast' =&gt; true]; 1$a = (unset)$b; 格式化后： 1$a = null; no_unset_on_property [@PhpCsFixer:risky]Properties should be set to null instead of using unset. Risky rule: changing variables to null instead of unsetting them will mean they still show up when looping over class variables. no_unused_imports [@Symfony, @PhpCsFixer]引入 use 后但是没有用到的类要删除 如描述 no_useless_else [@PhpCsFixer]不需要没有用的 else 分支 123$rules = [ 'no_useless_else' =&gt; true]; 12345678function a($a, $b)&#123; if ($a) &#123; return $a; &#125; else &#123; return $b; &#125;&#125; 格式化后： 12345678function a($a, $b)&#123; if ($a) &#123; return $a; &#125; return $b;&#125; no_whitespace_before_comma_in_array [@Symfony，@PhpCsFixer]在数组声明中，每个逗号前不得有空格。 可选配置项 after_heredoc （是否应删除 heredoc end 和逗号之间的空格） true false （默认） 123$rules = [ 'no_whitespace_before_comma_in_array' =&gt; true]; 1$a = [1 , 2 , 3]; 格式化后： 1$a = [1, 2, 3]; no_whitespace_in_blank_line [@Symfony，@PhpCsFixer]删除空行中的空格 如描述 non_printable_character [@Symfony:risky, @PhpCsFixer:risky, @PHP70Migration:risky, @PHP71Migration:risky]Remove Zero-width space (ZWSP), Non-breaking space (NBSP) and other invisible unicode symbols. Risky rule: risky when strings contain intended invisible characters. Configuration options: use_escape_sequences_in_strings (bool): whether characters should be replaced with escape sequences in strings; defaults to false normalize_index_brace [@Symfony, @PhpCsFixer]Array index should always be written by using square braces. not_operator_with_space逻辑 NOT 运算符（!）应该具有前导和尾随空格。 如描述 not_operator_with_successor_space逻辑 NOT 运算符（!）应该有一个尾随空格。 如描述 phpdoc_align [@Symfony，@PhpCsFixer]给定 phpdoc 标签的所有项目必须左对齐或（默认情况下）垂直对齐。 配置选项： align（’left’，’vertical’）：对齐评论; 默认为’vertical’tags（子集[‘param’, ‘property’, ‘return’, ‘throws’, ‘type’, ‘var’, ‘method’]）：应该对齐的标签; 默认为 [‘param’, ‘return’, ‘throws’, ‘type’, ‘var’ phpdoc_annotation_without_dot [@Symfony，@PhpCsFixer]PHPDoc 注释描述不应该是一个句子。 phpdoc_indent [@Symfony，@PhpCsFixer]Docblock 应与文档主题具有相同的缩进。 phpdoc_inline_tag [@Symfony，@PhpCsFixer]修复 PHPDoc 内联标记，使@inheritdoc 内联始终。 phpdoc_no_empty_return [@Symfony，@ PhpCsFixer]@return void 和@return null 注释应该 PHPDoc 的被省略。 phpdoc_no_package [@Symfony，@ PhpCsFixer]@package 和@subpackage 注释应该 PHPDoc 的被省略。 phpdoc_order [@PhpCsFixer]应该对 PHPDoc 中的@param 注释进行排序，以便首先@throws 注释，然后是@return 注释，然后是注释。 phpdoc_return_self_reference [@Symfony，@ PhpCsFixer]@return 返回对自身的引用的方法的注释类型必须是已配置的注释。 配置选项： replacements（array）：替换的返回类型与新的返回类型之间的映射; 默认为[‘this’ =&gt; ‘$this’, ‘@this’ =&gt; ‘$this’, ‘$self’ =&gt; ‘self’, ‘@self’ =&gt; ‘self’, ‘$static’ =&gt; ‘static’, ‘@static’ =&gt; ‘static’] phpdoc_scalar [@Symfony，@PhpCsFixer]标量类型应始终以相同的形式编写。int 不 integer，bool 不 boolean，float 不是 real 或 double。 配置选项： types（子集[‘boolean’, ‘callback’, ‘double’, ‘integer’, ‘real’, ‘str’]）：要修复的类型的映射; 默认为[‘boolean’, ‘double’, ‘integer’, ‘real’, ‘str’] phpdoc_separation [@Symfony，@ PhpCsFixer]PHPDoc 中的注释应该组合在一起，以便相同类型的注释紧跟在一起，并且不同类型的注释由单个空行分隔。 phpdoc_single_line_var_spacing [@Symfony，@ PhpCsFixer]单行@varPHPDoc 应该有适当的间距。 phpdoc_summary [@Symfony，@ PhpCsFixer]PHPDoc 摘要应以句号，感叹号或问号结尾。 phpdoc_to_comment [@Symfony，@ PhpCsFixer]Docblock 只应用于结构元素。 phpdoc_trim [@Symfony，@ PhpCsFixer]PHPDoc 应该以内容开头和结尾，不包括 docblock 的第一行和最后一行。 phpdoc_trim_consecutive_blank_line_separation [@PhpCsFixer]在摘要之后和 PHPDoc 中的描述之后删除额外的空白行。 phpdoc_var_annotation_correct_order [@PhpCsFixer]@var 和@type 注释必须具有正确顺序的类型和名称。 phpdoc_var_without_name [@Symfony，@ PhpCsFixer]@var 和@type 注释不应包含变量名称。 protected_to_private [@Symfony，@ PhpCsFixer]尽可能将 protected 变量和方法转换 private。 single_blank_line_at_eof [@ PSR2，@ Soundfony，@ PhpCsFixer]没有结束标记的 PHP 文件必须始终以单个空行换头结束。 single_import_per_statement [@ PSR2，@ Symfony，@ PhpCsFixer]每个声明必须有一个 use 关键字。(针对 PHP7 可以多个 use) single_line_after_imports [@ PSR2，@ Symfony，@ PhpCsFixer]每个命名空间使用必须在它自己的行上，并且在 use 语句块之后必须有一个空行。 single_line_comment_style [@Symfony，@PhpCsFixer]只有一行实际内容的单行注释和多行注释应使用//语法。 配置选项： comment_types（子集[‘asterisk’, ‘hash’]）：要修复的注释类型列表; 默认为[‘asterisk’, ‘hash’] space_after_semicolon [@Symfony，@PhpCsFixer]分号后修复空格 可选配置项 remove_in_empty_for_expressions （是否应删除空 for 表达式的空格） true false (默认) 12345$rules = [ 'space_after_semicolon' =&gt; [ 'remove_in_empty_for_expressions' =&gt; true ]]; 原始格式： 123for ($i = 0; $i &lt; 10; $i++) &#123; echo $i;&#125; 格式化后： 123for ($i = 0; $i &lt; 10; $i++) &#123; echo $i;&#125; standardize_increment [@Symfony，@PhpCsFixer]如果可能，应使用递增和递减运算符。 123$rules = [ 'standardize_increment' =&gt; true]; 原始格式： 123$a = 0;$a += 1;$a -= 1; 格式化后： 123$a = 0;++$a;--$a; switch_case_semicolon_to_colon [@PSR2，@Symfony，@PhpCsFixer]case 之后应该是冒号而不是分号。 123$rules = [ 'switch_case_semicolon_to_colon' =&gt; true]; 原始格式： 123456789$type = 1;switch ($type) &#123; case 1; echo 2; break; default; echo 1;&#125; 格式化后： 123456789$type = 1;switch ($type) &#123; case 1: echo 2; break; default: echo 1;&#125; switch_case_space [@PSR2，@Symfony，@PhpCsFixer]删除 case 冒号和大小写值之间的额外空格。 123$rules = [ 'switch_case_space' =&gt; true]; 原始格式： 123456789$type = 1;switch ($type) &#123; case 1 : echo 2; break; default : echo 1;&#125; 格式化后： 123456789$type = 1;switch ($type) &#123; case 1: echo 2; break; default: echo 1;&#125; ternary_operator_spaces [@Symfony, @PhpCsFixer]三元操作符周围有标准空格 123$rules = [ 'ternary_operator_spaces' =&gt; true]; 原始格式： 1$a = empty($e)?true:false; 格式化后： 1$a = empty($e) ? true : false; ternary_to_null_coalescing [@PHP70Migration, @PHP71Migration, @PHP73Migration]Use null coalescing operator ?? where possible. Requires PHP &gt;= 7.0. trailing_comma_in_multiline_array [@Symfony，@PhpCsFixer]PHP 多行数组应该有一个尾随逗号。 配置选项： after_heredoc（bool）：是否也应该在 heredoc 结束后放置一个尾随逗号; 默认为 false trim_array_spaces [@Symfony，@ PhpCsFixer]数组应该像函数/方法参数一样格式化，不带前导或尾随单行空格。 unary_operator_spaces [@Symfony，@ PhpCsFixer]一元运算符应放在其操作数旁边。 whitespace_after_comma_in_array [@Symfony，@ PhpCsFixer]在数组声明中，每个逗号后必须有一个空格。 总结以上均为 php-cs-fixer 的配置详，有一些风险的配置，我们不提倡使用，所以并没有翻译，而且一些没有给出例子。情况有 2 种，一种是：暂时不了解具体用法。一种是字面意思可懂。 每个配置项后面的中括号中的@xxx的写法代表这个配置在什么 tag 中生效，具体相关的可以查看 php-cs-fixer 的官网文档。","categories":[{"name":"PHP","slug":"PHP","permalink":"http://blog.crazylaw.cn/categories/PHP/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"http://blog.crazylaw.cn/tags/PHP/"}]},{"title":"k8s の 基础知识","slug":"k8s知识点一","date":"2019-05-16T06:24:00.000Z","updated":"2021-03-20T16:25:01.807Z","comments":true,"path":"2019/05/16/k8s知识点一/","link":"","permalink":"http://blog.crazylaw.cn/2019/05/16/k8s%E7%9F%A5%E8%AF%86%E7%82%B9%E4%B8%80/","excerpt":"k8s 知识点一k8s 总架构由于 2 个概念构成 Master Node Master 和 Node 都是服务器物理机或者虚拟机, 都可以由于一台或者多台组成，如果 Master 不考虑HA(高可用)的情况下。一般都是 1 台 Master + N 台 Node。 Master 上所需要的服务 etcd Api Server Controller Manager Scheduler 后三个组件构成了 Kubernetes 的总控中心，这些进程实现了整个集群的资源管理、Pod 调度、弹性伸缩、安全控制、系统监控和纠错等管理功能，并且全都是自动完成。 Node 上所需要的服务 kubelet proxy","text":"k8s 知识点一k8s 总架构由于 2 个概念构成 Master Node Master 和 Node 都是服务器物理机或者虚拟机, 都可以由于一台或者多台组成，如果 Master 不考虑HA(高可用)的情况下。一般都是 1 台 Master + N 台 Node。 Master 上所需要的服务 etcd Api Server Controller Manager Scheduler 后三个组件构成了 Kubernetes 的总控中心，这些进程实现了整个集群的资源管理、Pod 调度、弹性伸缩、安全控制、系统监控和纠错等管理功能，并且全都是自动完成。 Node 上所需要的服务 kubelet proxy 负责对本节点上的 Pod 的生命周期进行管理，以及实现服务代理的功能 流程通过 Kubectl 提交一个创建 RC 的请求，该请求通过 API Server 被写入 etcd 中，此时 Controller Manager 通过 API Server 的监听资源变化的接口监听到这个 RC 事件，分析之后，发现当前集群中还没有它所对应的 Pod 实例，于是根据 RC 里的 Pod 模板定义生成一个 Pod 对象，通过 API Server 写入 etcd，接下来，此事件被 Scheduler 发现，它立即执行一个复杂的调度流程，为这个新 Pod 选定一个落户的 Node，然后通过 API Server 讲这一结果写入到 etcd 中，随后，目标 Node 上运行的 Kubelet 进程通过 API Server 监测到这个“新生的”Pod，并按照它的定义，启动该 Pod 并任劳任怨地负责它的下半生，直到 Pod 的生命结束。 随后，我们通过 Kubectl 提交一个新的映射到该 Pod 的 Service 的创建请求，Controller Manager 会通过 Label 标签查询到相关联的 Pod 实例，然后生成 Service 的 Endpoints 信息，并通过 API Server 写入到 etcd 中，接下来，所有 Node 上运行的 Proxy 进程通过 API Server 查询并监听 Service 对象与其对应的 Endpoints 信息，建立一个软件方式的负载均衡器来实现 Service 访问到后端 Pod 的流量转发功能。 etcd用于持久化存储集群中所有的资源对象，如 Node、Service、Pod、RC、Namespace 等；API Server 提供了操作 etcd 的封装接口 API，这些 API 基本上都是集群中资源对象的增删改查及监听资源变化的接口。 API Server提供了资源对象的唯一操作入口，其他所有组件都必须通过它提供的 API 来操作资源数据，通过对相关的资源数据“全量查询”+“变化监听”，这些组件可以很“实时”地完成相关的业务功能。 Controller Manager集群内部的管理控制中心，其主要目的是实现 Kubernetes 集群的故障检测和恢复的自动化工作，比如根据 RC 的定义完成 Pod 的复制或移除，以确保 Pod 实例数符合 RC 副本的定义；根据 Service 与 Pod 的管理关系，完成服务的 Endpoints 对象的创建和更新；其他诸如 Node 的发现、管理和状态监控、死亡容器所占磁盘空间及本地缓存的镜像文件的清理等工作也是由 Controller Manager 完成的。 Scheduler集群中的调度器，负责 Pod 在集群节点中的调度分配。 Kubelet负责本 Node 节点上的 Pod 的创建、修改、监控、删除等全生命周期管理，同时 Kubelet 定时“上报”本 Node 的状态信息到 API Server 里。 Proxy实现了 Service 的代理与软件模式的负载均衡器。 客户端通过 Kubectl 命令行工具或 Kubectl Proxy 来访问 Kubernetes 系统，在 Kubernetes 集群内部的客户端可以直接使用 Kuberctl 命令管理集群。Kubectl Proxy 是 API Server 的一个反向代理，在 Kubernetes 集群外部的客户端可以通过 Kubernetes Proxy 来访问 API Server。","categories":[{"name":"k8s","slug":"k8s","permalink":"http://blog.crazylaw.cn/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://blog.crazylaw.cn/tags/k8s/"}]},{"title":"rust-for-docker","slug":"rust-for-docker","date":"2019-04-02T12:49:59.000Z","updated":"2021-03-20T16:25:01.814Z","comments":true,"path":"2019/04/02/rust-for-docker/","link":"","permalink":"http://blog.crazylaw.cn/2019/04/02/rust-for-docker/","excerpt":"说明今天，我们开始学习一下 Rust，希望将来在公司可以推广 Rust 语言，并且用 Rust 语言做更多的事情。 在此，\b 进入我们的入学篇。 Rust for Docker拉取最新版本的 Rust 1docker pull rust:latest 由于官方上的命令有点问题，并且我希望我有一个交互的终端环境，所以，经过修改后，进入 Rust 容器的命令如下： 1docker run -it --rm -e &quot;USER&#x3D;$(whoami)&quot; -e &quot;RUST_BACKTRACE&#x3D;1&quot; -v &quot;$PWD&quot;:&#x2F;usr&#x2F;src&#x2F;myapp -w &#x2F;usr&#x2F;src&#x2F;myapp rust:latest bash USER: cargo 打包的时候会用这个命名 RUST_BACKTRACE：终端调试的时候，会打印出错误栈 进入到容器后，我们创建属于我们的第一个项目：Hello，World 利用 Rust 的包管理工具：Cargo 进行开发和编译 123cargo new hello_world&#x2F;&#x2F; car new --lib &lt;package_name&gt; 这个是创建类库的命令，暂时用不到，所以忽略先","text":"说明今天，我们开始学习一下 Rust，希望将来在公司可以推广 Rust 语言，并且用 Rust 语言做更多的事情。 在此，\b 进入我们的入学篇。 Rust for Docker拉取最新版本的 Rust 1docker pull rust:latest 由于官方上的命令有点问题，并且我希望我有一个交互的终端环境，所以，经过修改后，进入 Rust 容器的命令如下： 1docker run -it --rm -e &quot;USER&#x3D;$(whoami)&quot; -e &quot;RUST_BACKTRACE&#x3D;1&quot; -v &quot;$PWD&quot;:&#x2F;usr&#x2F;src&#x2F;myapp -w &#x2F;usr&#x2F;src&#x2F;myapp rust:latest bash USER: cargo 打包的时候会用这个命名 RUST_BACKTRACE：终端调试的时候，会打印出错误栈 进入到容器后，我们创建属于我们的第一个项目：Hello，World 利用 Rust 的包管理工具：Cargo 进行开发和编译 123cargo new hello_world&#x2F;&#x2F; car new --lib &lt;package_name&gt; 这个是创建类库的命令，暂时用不到，所以忽略先 然后创建好了之后。可以看到 目录结构如下： src：需要开发的源码目录 target: 利用 cargo build 运行之后的结果 tree 结构如下： 12345678910111213141516171819202122232425262728.├── Cargo.lock├── Cargo.toml├── src│ └── main.rs└── target └── debug ├── build ├── deps │ ├── hello_world-fe752e96abed129f │ └── hello_world-fe752e96abed129f.d ├── examples ├── hello_world ├── hello_world.d ├── incremental │ └── hello_world-a3apxnpw89q3 │ ├── s-fax5vd6s7z-mo2l8f-2iq1v3vohkxfu │ │ ├── 1lpqxcu6kfhnj0ty.o │ │ ├── 1z70i4nrjlg4y6gh.o │ │ ├── 2qa4nktsifidbpri.o │ │ ├── 3k2rwqf2cnq9zjxc.o │ │ ├── 4is0vkl128fgpc6l.o │ │ ├── dep-graph.bin │ │ ├── g5tl0es1ce46vid.o │ │ ├── query-cache.bin │ │ └── work-products.bin │ └── s-fax5vd6s7z-mo2l8f.lock └── native 在这里，我们可以执行执行二进制文件： 1.&#x2F;target&#x2F;debug&#x2F;hello_world 结果如下： 12root@66c4794c7528:&#x2F;usr&#x2F;src&#x2F;myapp&#x2F;hello_world# .&#x2F;target&#x2F;debug&#x2F;hello_worldHello, world 或者利用 cargo 运行启动: 1cargo run 结果如下: 1234root@66c4794c7528:&#x2F;usr&#x2F;src&#x2F;myapp&#x2F;hello_world# cargo run Finished dev [unoptimized + debuginfo] target(s) in 0.06s Running &#96;target&#x2F;debug&#x2F;hello_world&#96;Hello, world! 到此为止，我们的第一章节 Hello World 完成了，接下来要积累点才可以了","categories":[{"name":"Rust","slug":"Rust","permalink":"http://blog.crazylaw.cn/categories/Rust/"}],"tags":[{"name":"Rust","slug":"Rust","permalink":"http://blog.crazylaw.cn/tags/Rust/"}]},{"title":"【工具】oh-my-zsh 换行补偿","slug":"linux-shell-huanhangbuchong","date":"2018-11-15T04:06:01.000Z","updated":"2021-03-20T16:25:01.809Z","comments":true,"path":"2018/11/15/linux-shell-huanhangbuchong/","link":"","permalink":"http://blog.crazylaw.cn/2018/11/15/linux-shell-huanhangbuchong/","excerpt":"oh-my-zsh 换行补偿大家应该也是经常会用到 linux 或者 macOS，大家应该都有了解 macos 必备软件之间，oh-my-zsh，它可以为我们的终端提供丰富的主题，当然，这里就不详细介绍了。现在主要说一下其中一个问题就是，oh-my-zsh 的 换行补偿机制。 一般情况下，我们在终端有时候会发现会存在 %，例如： root 账号的时候是 # 号非 root 账号是 % 号","text":"oh-my-zsh 换行补偿大家应该也是经常会用到 linux 或者 macOS，大家应该都有了解 macos 必备软件之间，oh-my-zsh，它可以为我们的终端提供丰富的主题，当然，这里就不详细介绍了。现在主要说一下其中一个问题就是，oh-my-zsh 的 换行补偿机制。 一般情况下，我们在终端有时候会发现会存在 %，例如： root 账号的时候是 # 号非 root 账号是 % 号 后面会多一个百分号，显得是否不舒服，但是这个恰巧就是 zsh 的换行补偿机制。意义在于，当我们输出的字符串忘记在终端显示换行符号 \\n 的时候，他就会自动在输出到标准输出之前添加一个百分号来告诉我们，你的输出忘记换行了，我帮你换行了，用的是 % 符号。 看似挺友好，但是实在是不习惯。所以，我们现在的目的是要屏蔽。 需要用的是环境变量 PROMPT_EOL_MARK 我们只需要在 shell 中设置 1PROMPT_EOL_MARK&#x3D;&#39;&#39; 那么它的换行补偿机制的替换符号就会是为空，那就是不存在了。 这个环境变量加在哪里？加在~/.zshrc 里面也可以，或者你不想永久生效的话，就直接在终端输入就可以了。 友情提示：记得不要忘记 source 一下配置哦～","categories":[{"name":"Linux","slug":"Linux","permalink":"http://blog.crazylaw.cn/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.crazylaw.cn/tags/Linux/"},{"name":"Shell","slug":"Shell","permalink":"http://blog.crazylaw.cn/tags/Shell/"}]},{"title":"【Linux命令】ed命令","slug":"linux-shell-ed","date":"2018-10-25T12:22:00.000Z","updated":"2021-03-20T16:25:01.809Z","comments":true,"path":"2018/10/25/linux-shell-ed/","link":"","permalink":"http://blog.crazylaw.cn/2018/10/25/linux-shell-ed/","excerpt":"说明ed 是 linux 下原始的编辑器，类似于一个行为记录器 ed 的所有操作都可以用一个文本来记录 里面的内容是所有的操作，有点类似于 dockerfile 的形式","text":"说明ed 是 linux 下原始的编辑器，类似于一个行为记录器 ed 的所有操作都可以用一个文本来记录 里面的内容是所有的操作，有点类似于 dockerfile 的形式 使用123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105. 当前行$ 文本最后一行n 文本第 n 行（ n 为数字，下同；m 亦是 ）-n 从文本当前行数起，向前第 n 行+n 从文本当前行数起，向后第 n 行- 相当于 -1 行地址+ 相当于 +1 行地址m，n 文本的第 m 到 n 行, 文本的所有行; 文本当前行到最后一行&#x2F;reg&#x2F; 从文本当前行数起，下一个匹配 reg 的行?reg? 从文本当前行数起，上一个匹配 reg 的行&#39;x 由 k 命令标记的行（ x 为一小写字母 ）正则表达式如下：(只涉及 ed 所支持的正则表达式). 匹配任何单个字符。[char-class] 匹配任何一个在 char-class 里的单个字符。如果中间出现 &#39;-&#39; ，则意为其左边的字符和其右边的字符之间的所有字符。例如，[abc] 匹配 a 或 b 或 c；[a-z] 匹配任意一个小写字母(a、b、c、...、z)，[0-9] 匹配任意一个数字(0、1、2、...、9)。char-class 也可以为一些字符集。如下：[:alpha:] 相当于 [a-zA-Z][:lower:] 相当于 [a-z][:upper:] 相当于 [A-Z][:digit:] 相当于 [0-9][:alnum:] 相当于 [a-zA-Z0-9][:blank:] 匹配 &#39; &#39;(空格)、 &#39;\\t&#39;(制表符)[:space:] 匹配 &#39; &#39;(空格)、&#39;\\t&#39;(制表符)、&#39;\\n&#39;(新行)、&#39;\\f&#39;()、&#39;\\v&#39;(垂直制表符)、&#39;\\r&#39;(回车符)[:cntrl:] 匹配控制字符。在 ASCII 码中，这些控制字符是从八进制数字 000 到 037, 和 177 (DEL)[:print:] 匹配 相当于 [:alnum:]、[:punct:] 和 空格[:graph:] 匹配 相当于 [:alnum:] 、 [:punct:][:punct:] 匹配 &#96;! &quot; # $ % &amp; &#39; ( ) * + , - . &#x2F; : ; &lt; &#x3D; &gt; ? @ [ \\ ] ^ _ &#96; &#123; | &#125; ~ &#39; 等标点符号[:xdigit:] 匹配十六进制字符 &#39;0 1 2 3 4 5 6 7 8 9 A B C D E F a b c d e f&#39;[^char-class] 匹配 [char-class] 的补集，即匹配任意一个不在 char-class 里的单个字符* 匹配前面的子表达式零次或多次。例如，&#39;ab*&#39; 能匹配 &quot;a&quot; 以及 &quot;abb&quot;。 * 等价于&#39;\\&#123;0,\\&#125;&#39;\\+ 匹配前面的子表达式一次或多次。例如，&#39;ab\\+&#39; 能匹配 &quot;ab&quot; 以及 &quot;abb&quot;，但不能匹配 &quot;a&quot;。\\+ 等价于 \\&#123;1,\\&#125;\\? 匹配前面的子表达式零次或一次。例如，&quot;word(s)\\?&quot; 可以匹配 &quot;word&quot; 或 &quot;words&quot; 。\\? 等价于\\&#123;0,1\\&#125;\\&#123;n,m\\&#125; m 和 n 均为非负整数，其中n &lt;&#x3D; m。最少匹配 n 次且最多匹配 m 次。 &quot;a&#123;1,3&#125;&quot; 将匹配 &quot;baaaaab&quot;中的前三个 a。&#39;a&#123;0,1&#125;&#39; 等价于 &#39;a\\?&#39;。请注意在逗号和两个数之间不能有空格\\&#123;n,\\&#125; n 是一个非负整数。至少匹配n 次。例如，&#39;a\\&#123;2,\\&#125;&#39; 不能匹配 &quot;abc&quot; 中的 &#39;a&#39;，但能匹配&quot;baaaaab&quot;中的所有 a。&#39;a\\&#123;1,\\&#125;&#39; 等价于 &#39;a\\+&#39;。&#39;a\\&#123;0,\\&#125;&#39; 则等价于 &#39;a*&#39;\\&#123;n\\&#125; n 是一个非负整数。匹配确定的 n 次。例如，&#39;a\\&#123;2\\&#125;&#39; 不能匹配 &quot;bab&quot; 中的 &#39;a&#39;，但是能匹配&quot;baab&quot;中的两个 a^ 匹配输入字符串的开始位置$ 匹配输入字符串的结束位置\\&lt; 匹配一个单词的前边界。例如，&#39;\\&lt;el&#39; 匹配 &quot;element&quot;,但不能匹配 &quot;help&quot;\\&gt; 匹配一个单词的后边界。例如，&#39;ly\\&gt;&#39; 匹配 &quot;lovely&quot;，但不匹配 &quot;lying&quot;\\b 匹配一个单词边界，也就是指单词和空格间的位置。例如， &#39;er\\b&#39; 可以匹配&quot;never&quot; 中的 &#39;er&#39;，但不能匹配&quot;verb&quot; 中的 &#39;er&#39;\\B 匹配非单词边界。&#39;er\\B&#39; 能匹配 &quot;verb&quot; 中的 &#39;er&#39;，但不能匹配 &quot;never&quot; 中的 &#39;er&#39;\\w 匹配任何非单词字符。等价于 &#39;[^A-Za-z0-9_]&#39;\\W 匹配任何非单词字符。等价于 &#39;[^A-Za-z0-9_]&#39;\\&#96; 匹配一个句子的边界 定义向后引用。&#39;\\n&#39;(n为一正整数)代表第 n 个括号中匹配的字符串ed 命令：ed 命令都是单个字符，其中一些命令有一些选项。如果一命令超过一行，应使 &#39;\\&#39; 结束每一行。命令如下：(括号内为默认地址)(.)a 切换到输入模式,将新输入的文本追加到指定行的后面，当前行被设为输入文本的最后一行(.)i 切换到输入模式,将新输入的文本插入到指定行的前面，当前行被设为输入文本的最后一行(.,.)c 切换到输入模式,将新输入的文本替换成指定行，当前行被设为输入文本的最后一行(.,.)d 删除指定行，如果被删除的文本后还有文本行，则当前行被设为该行，否则设为被删除的文本的上一行(.+1) 无命令时，默认 p 命令，但打印下一行内容，当前行被设为打印行(.+1)zn 一次跳动 n 行，如果未指出 n ，默认当前终端屏幕大小，当前行被设为最后被打印的行(.,.)p 打印指定行，当前行被设为打印行的最后一行P ed 命令模式下提示符开关命令，默认提示符为 &#39;*&#39;(.,.)l 在每行最后加一 &#39;$&#39; 符号指定结尾，并打印输出(.,.)n 打印指定行号和内容，行号与行内容用制表符分割,当前行被设为打印行的最后一行($)&#x3D; 打印指定行行号(.,.)# 注释行，将被忽略(.)k char 用一小写字母标记指定行(.,.)s&#x2F;reg&#x2F;replacement&#x2F;(.,.)s&#x2F;reg&#x2F;replacement&#x2F;g(.,.)s&#x2F;reg&#x2F;replacement&#x2F;n替换指定行命令(.,.)s 重复上一次替换命令，当前行被设为最后一个被改变的行(1,$)g&#x2F;reg&#x2F;cmd-list 所有匹配 &#39;&#x2F;reg&#x2F;&#39; 的行执行 cmd-list 命令，在命令执行前，当前行被设为匹配行。当所有匹配行执行完命令后，当前行被设定为最后一个匹配行。cmd-list 中每一行只能有一个命令，但有多个命令时，应以 &#39;\\&#39; 结束每一行(1,$)G&#x2F;reg&#x2F; 与 g&#x2F;reg&#x2F;cmd-list 相似，但匹配的每一行所执行的命令由用户各个定义。(1,$)v&#x2F;reg&#x2F;cmd-list 与 g&#x2F;reg&#x2F;cmd-list 相反，指不匹配行(1,$)V&#x2F;reg&#x2F; 与 G&#x2F;reg&#x2F; 相反，指不匹配行(.,.+1)j 合并指定行内容,当前行被设为合并行(.,.)m(.) 移动左边源指定行到右边目的指定行后,当前行被设为移动行的最后一行(.,.)t(.) 复制左边源指定行到右边目的指定行后，当前行被设为复制行的最后一行(.,.)y 复制指定行到缓存，当前行不改变(.)x 复制缓存内容到指定行后，当前行被设为复制行的最后一行u 撤销上一次命令，当前地址被设为上一次地址h 打印最后一个错误说明H 错误说明开关，默认不输出e file 编辑文件并设定文件名E file 强制编辑文件，同 e file，但丢失以前的修改，不做警告!cmd 执行 shell 命令 cmde !cmd 先将ed 缓冲区清除，替换 cmd 命令的输出f file 设置文件名，如果每给出 file 参数，则打印文件名($)r file 把指定文件内容追加到指定行后，当前行被设为追加文本的最后一行($)r !cmd 把命令的输出追加到指定行后，当前行被设为追加文本的最后一行(1,$)w file 保存指定文本内容到指定文件(覆盖保存)(1,$)W file 保存指定文本内容到指定文件(追加保存)，当前行不改变(1,$)w !cmd 输出指定文本内容到 cmd 的标准输入，当前行不改变(1,$)wq flie 保存指定文本内容到指定文件(覆盖保存)，并退出编辑器q 退出 ed 编辑器，退出前若所作的修改没保存，发出警告Q 强制退出 ed 编辑器，同 q 命令，但退出前若所作的修改没保存，不警告P ed 命令提示符显示开关。 &#39;*&#39; 为 ed 默认提示符，利用 ed 命令 -p 选项，其可被更改为任意字符更为详细之处请参阅 man info 手册 12345678910111213$ ed &lt;- 激活 ed 命令a &lt;- 告诉 ed 我要编辑新文件My name is Titan. &lt;- 输入第一行内容And I love Perl very much. &lt;- 输入第二行内容. &lt;- 返回 ed 的命令行状态i &lt;- 告诉 ed 我要在最后一行之前插入内容I am 24. &lt;- 将“I am 24.”插入“My name is Titan.”和“And I love Perl very much.”之间. &lt;- 返回 ed 的命令行状态c &lt;- 告诉 ed 我要替换最后一行输入内容I am 24 years old. &lt;- 将“I am 24.”替换成“I am 24 years old.”（注意：这里替换的是最后输的内容）. &lt;- 返回 ed 的命令行状态w readme.text &lt;- 将文件命名为“readme.text”并保存（注意：如果是编辑已经存在的文件，只需要敲入 w 即可）q &lt;- 完全退出 ed 编辑器 1234$ cat readme.textMy name is Titan.I am 24 years old.And I love Perl vrey much.","categories":[{"name":"Linux","slug":"Linux","permalink":"http://blog.crazylaw.cn/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.crazylaw.cn/tags/Linux/"}]},{"title":"【Linux命令】diff - colordiff","slug":"linux-shell-colordiff","date":"2018-10-25T11:44:00.000Z","updated":"2021-03-20T16:25:01.809Z","comments":true,"path":"2018/10/25/linux-shell-colordiff/","link":"","permalink":"http://blog.crazylaw.cn/2018/10/25/linux-shell-colordiff/","excerpt":"前言我们经常用 git 的客户端来对比文件差异，或者用一下其他的对比文件的差异的软件，例如常用的 Beyond Compare，但是如果我们不想借助其他的工具，用 linux 自带的命令要如何实现呢。 这里和大家说一下 diff 命令 使用diff 分析两个文件，并输出两个文件的不同的行。diff 的输出结果表明需要对一个文件做怎样的操作之后才能与第二个文件相匹配。diff 并不会改变文件的内容，但是 diff 可以输出一个 ed 脚本来应用这些改变。现在让我们来看一下 diff 是如何工作的，假设有两个文件：","text":"前言我们经常用 git 的客户端来对比文件差异，或者用一下其他的对比文件的差异的软件，例如常用的 Beyond Compare，但是如果我们不想借助其他的工具，用 linux 自带的命令要如何实现呢。 这里和大家说一下 diff 命令 使用diff 分析两个文件，并输出两个文件的不同的行。diff 的输出结果表明需要对一个文件做怎样的操作之后才能与第二个文件相匹配。diff 并不会改变文件的内容，但是 diff 可以输出一个 ed 脚本来应用这些改变。现在让我们来看一下 diff 是如何工作的，假设有两个文件： 123456789101112131415161718192021222324&#x2F;&#x2F;file1.txtI need to buy apples.I need to run the laundry.I need to wash the dog.I need to get the car detailed.&#x2F;&#x2F;file2.txtI need to buy apples.I need to do the laundry.I need to wash the car.I need to get the dog detailed.我们使用diff比较他们的不同：diff file1.txt file2.txt输出如下结果：2,4c2,4&lt; I need to run the laundry.&lt; I need to wash the dog.&lt; I need to get the car detailed.---&gt; I need to do the laundry.&gt; I need to wash the car.&gt; I need to get the dog detailed. 我们来说明一下该输出结果的含义，要明白 diff 比较结果的含义，我们必须牢记一点，diff 描述两个文件不同的方式是告诉我们怎么样改变第一个文件之后与第二个文件匹配。我们看看上面的比较结果中的第一行 2,4c2,4 前面的数字 2,4 表示第一个文件中的行，中间有一个字母 c 表示需要在第一个文件上做的操作(a=add,c=change,d=delete)，后面的数字 2,4 表示第二个文件中的行。 2,4c2,4 的含义是：第一个文件中的第[2,4]行(注意这是一个闭合区间，包括第 2 行和第 4 行)需要做出修改才能与第二个文件中的[2,4]行相匹配。接下来的内容则告诉我们需要修改的地方，前面带 &lt; 的部分表示左边文件的第[2,4]行的内容，而带&gt; 的部分表示右边文件的第[2,4]行的内容，中间的 — 则是两个文件内容的分隔符号。 模式 Normal （默认） Context（计算机模式）【-c】 Unified （类似 github）【-u】 Gui (类似于客户端) 【-y】 diff 还提供了一些有用的参数来控制比较行为与输出结果，一些常用的参数如下： -b –ignore-space-change 忽略空格，如果两行进行比较，多个连续的空格会被当作一个空格处理，同时会忽略掉行尾的空格差异。 -w –ignore-all-space 忽略所有空格，忽略范围比-b 更大，包括很多不可见的字符都会忽略。 -B 忽略空白行。 -y 输出两列，一个文件一列，有点类似 GUI 的输出外观了，这种方式输出更加直观。 -W 大写 W，当指定-y 的时候设置列的宽度，默认是 130 -x, –exclude=PAT 比较目录的时候排除指定 PAT 模式的文件名的比较 -i, –ignore-case 忽略两个文件中大小写的不同 -e 将比较的结果保存成一个 ed 脚本，之后 ed 程序可以执行该脚本文件，从而将 file1 修改成与 file2 的内容相同，这一般在 patch 的时候有用。 -r 如果比较两个目录，-r 参数会比较其下同名的子目录 -q 输出结果中，只指出两个文件不同，而不输出两个文件具体内容的比较，这在比较两个目录的时候很好用。我们只需要知道两个目录下那些文件做了修改，而不需要知道每个文件具体修改了那些内容。特别是当两个目录文件很多的时候。 diff -e 1.txt 2.txt &gt; script.txt 这样就是生成了一个 ed 可以执行的脚本文件 script.txt，生成脚本文件之后我们还需要做一个操作， 在脚本文件末尾添加 ed 的 write 指令，只需要执行echo &quot;w&quot; &gt;&gt;script.txt 将 w 指令附加到脚本文件的最后一行即可。那么如何应用该脚本文件呢，可以这样使用：ed - 1.txt &lt; script.txt注意中间的 – 符号表示从标准输入中读取，而 &lt; script.txt 则重定向 script.txt 的内容到标准输入。这样执行之后 1.txt 的内容将与 2.txt 完全相同。","categories":[{"name":"Linux","slug":"Linux","permalink":"http://blog.crazylaw.cn/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.crazylaw.cn/tags/Linux/"},{"name":"Shell","slug":"Shell","permalink":"http://blog.crazylaw.cn/tags/Shell/"}]},{"title":"docker-python-httpruner","slug":"docker-python-httpruner","date":"2018-08-22T11:56:00.000Z","updated":"2021-03-20T16:25:01.805Z","comments":true,"path":"2018/08/22/docker-python-httpruner/","link":"","permalink":"http://blog.crazylaw.cn/2018/08/22/docker-python-httpruner/","excerpt":"Httprunner最近，由于某些原因，了解到了 httprunner。 先贴上我的本次文章的主题： 支持 docker 环境 支持 jsonschema 粗略封装成了应用型框架 import python-file 的时候，请写基于项目根目录的路径 目标：[Github:httprunner-docker] [HubDocker:httprunner-docker]","text":"Httprunner最近，由于某些原因，了解到了 httprunner。 先贴上我的本次文章的主题： 支持 docker 环境 支持 jsonschema 粗略封装成了应用型框架 import python-file 的时候，请写基于项目根目录的路径 目标：[Github:httprunner-docker] [HubDocker:httprunner-docker] 前言源自于：[httprunner] 可以看到，它是一个由 python 写的测试框架。 HttpRunner 是一款面向 HTTP(S) 协议的通用测试框架，只需编写维护一份 YAML/JSON 脚本，即可实现自动化测试、性能测试、线上监控、持续集成等多种测试需求。 作为一名后端语言开发人员，python 能力不算太强，一边写，一遍学习。在本机上开发上遇到了各种瓶颈，可以说是看得懂和自己写是二回事。于是，这里可以把我当作一个测试新手人员，和大家一起记录学习的过程。 首先，我选择了 pycharm 这一款 JB 的产品作为我的 ide。 我发现它有一些智能的地方，比如自带 python 等环境，但是这也是致命麻烦的地方（可能是我孤陋寡闻了），就是想着如果我有一个项目，用到 httprunner 作为测试框架，那么我的目录不固定，我要到处引入 __init__.py的文件，显得很痛苦。而且，python 我所了解到的是 3.x 和 2.x 还是有一些区别的。 默认它安装的是 2.x，但是作为开发人员，在学习的过程中，当然期待是最新的，后续，由于需求的原因，我又去看了一下 2.x 和 3.x 兼容的问题，但是这里就不做叙述了。 我们大家平时用的环境比较多的是 2 中，一种是window，一种是maxOS，万一我在 ide 环境下运行得好好的，去到我的生成环境却无法运行了怎么办？或者说，想要模拟服务器上运行的话，或者方便移植的话，怎么处理？答案是：docker。是的，我们可以用 docker 来解决这个问题。 但是我翻看了一下httprunner的官方，并没有提供出docker文件，于是那好吧，那我就自己撸一个吧，再加上参考了几个文章，构建出了一套基于httprunner的测试开发框架，整套体系就在我的 github 中，大家可以去看看。 项目目录结构如下： 123456789101112131415161718192021222324252627├── .env ## demo所使用的环境变量├── .env.example├── Dockerfile ## 本项目的Dockerfile├── README.MD├── config│ ├── demo_data.ini ## 可变数据的设置│ └── test_data.ini ## 多个可变数据的设置├── docker-entrypoint.sh ## Dockerfile的入口点├── jsonschemas ## 用于支持json-schema│ └── demo-jsonschema│ └── user.schema.json├── lib ## 存放自己测试框架所需要用到的类库（例如生成token的算法）├── parameters ## 用于解析ini数据│ ├── common.py│ └── header.py├── reports ## 报告默认生成的地方│ └── 1534933506.html├── requirements.txt ## 项目初始化依赖安装列表├── run-all.sh ## 大家批量运行用例写的一个十分简单的shell脚本├── testcases ## （重点）存放我们的用例│ ├── debugtalk.py│ └── demo.json└── util ## 本项目官方所用的工具类和自定义公共函数 ├── common_config.py ├── env.py ├── function.py └── validation_json_schema.py 这里，我在上面已经大致写了每个目录所干什么的了。 起步你需要安装好 docker，docker 作为虚拟化技术，建议大家都多学学。即使是测试的同学，也要积极拥抱各种前沿的技术哦。加油～！ 1. 安装 docker略过…. 2. 下载项目1git clone https:&#x2F;&#x2F;github.com&#x2F;whiteCcinn&#x2F;httprunner-docker.git 3. 进入项目1cd httprunner-docker 4. 编译 dockerfile 生成项目镜像1docker build -t httprunner . 5. 通过镜像运行容器代替 httprunner1docker run -it --rm -v &quot;$PWD&quot;:&#x2F;usr&#x2F;src&#x2F;myapp httprunner 为了帮助不熟悉 docker 的同学在这里强调一下参数 -it 让 docker 容器拥有交互的能力。（重要的是加了这个参数，你可以通过 /bin/sh 进入容器执行也是可以的，而且加了这个参数，容器内部的颜色样式才会被打印出来，不然会全部白色，十分难以辨别 ） --rm 可以理解为临时创建一个容器，容器运行结束后便会自行销毁，这个大家可以不用理解，但是加了这个参数字面的意思就是它不会重复 new 很多随机命名的容器。 -v 这个是 docker 中相对来说比较重要的命令，用了这个命令，你的 docker 容器和你的本机目录将会变成共享卷，通俗一点就是加了这个命令，你新加的用例 json 文件，才会和 docker 容器里面的目录同步，并且你所生成的测试报告，才可以在本地直接访问。由于我们是在项目下执行的，所以我们用$PWD命令来快速获取当前目录的路径，然后映射到我们的容器里面的路径/usr/src/myapp（因为我们的 docker 项目的工作路径就是在这里，这个不需要修改） httprunner 这个就是我们刚才用docker build -t httprunner .打印出来的httprunner的名字。 6. 结果大家可以看到，这个时候，默认的，我们可以看到hrun提供给我们的命令提示信息。具体的大家，可以观看 httprunner 官方文档。 运行官方 demo执行如下命令: 1docker run -it --rm -v &quot;$PWD&quot;:&#x2F;usr&#x2F;src&#x2F;myapp httprunner .&#x2F;testcases&#x2F;demo.json 这里可以看到，我们的官方 demo 成功运行了，并且生成的测试报告就在reports目录中。 可以看到，demo 中，运行了 2 个用例。 /user - use jsonschema （该用例运用了 jsonschema 来校验具体数据是否正常） /favicon.ico - not use jsonschema （该用例没有用 jsonschema 来校验具体数据，仅仅用了 httprunner 提供的校验） 并且我们可以看到，用到 json-schema 来作为校验的时候，我们的 Validators 中 jsonschema_error为空的时候，就是代表数据信息通过了json-schema的校验。 尝试一下不通过 json-schema 的结果12345678910&quot;message&quot;: &#123; &quot;$id&quot;: &quot;#&#x2F;properties&#x2F;message&quot;, &quot;type&quot;: &quot;string&quot;, ## 把这里改成&quot;object&quot; &quot;title&quot;: &quot;The Message Schema&quot;, &quot;default&quot;: &quot;&quot;, &quot;examples&quot;: [ &quot;Requires authentication&quot; ], &quot;pattern&quot;: &quot;^(.*)$&quot;&#125;, 终端： 这里可以我们可以看到终端会抛出一丢红色的错误信息。 我们提取一段关键信息 1234567891011ERROR validate: jsonschema_error equals (str) &#x3D;&#x3D;&gt; failTraceback (most recent call last):jsonschema.exceptions.ValidationError: &#39;Requires authentication&#39; is not of type &#39;object&#39;Failed validating &#39;type&#39; in schema[&#39;properties&#39;][&#39;message&#39;]: &#123;&#39;$id&#39;: &#39;#&#x2F;properties&#x2F;message&#39;, &#39;default&#39;: &#39;&#39;, &#39;examples&#39;: [&#39;Requires authentication&#39;], &#39;pattern&#39;: &#39;^(.*)$&#39;, &#39;title&#39;: &#39;The Message Schema&#39;, &#39;type&#39;: &#39;object&#39;&#125; 这段信息告诉我们，message 不是 object 类型，所以报错了。 测试报告的结果： 这里我们看到这个测试报告，变成了我们所期待的错误的样子，并且也可以具体的信息。 总结start1docker build -t httprunner . 运行单独测试用例：12345docker run -it --rm -v &quot;$PWD&quot;:&#x2F;usr&#x2F;src&#x2F;myapp httprunner testcases&#x2F;you-json-file.jsondocker run -it --rm -v &quot;$PWD&quot;:&#x2F;usr&#x2F;src&#x2F;myapp httprunner .&#x2F;testcases&#x2F;you-json-file.jsondocker run -it --rm -v &quot;$PWD&quot;:&#x2F;usr&#x2F;src&#x2F;myapp httprunner hrun testcases&#x2F;you-json-file.json 批量运行：1docker run -it --rm -v &quot;$PWD&quot;:&#x2F;usr&#x2F;src&#x2F;myapp httprunner run-all.sh 进入容器内部操作：1docker run -it --rm -v &quot;$PWD&quot;:&#x2F;usr&#x2F;src&#x2F;myapp httprunner &#x2F;bin&#x2F;sh 本项目的命令都做了兼容处理，避免了一些命令错误，请大家放心使用，作为一名开发人员，希望本文章对大家的测试有所帮助。如果使用过程中遇到问题，环境在本 github 项目中提 issue。 我也相当于是个测试新手，大家觉得文章好的话，记得在 github 给项目 Star 一个哦。 后续，我会针对 httprunner 开发者模式，进行研究，看看 httprunner 还可以怎么优化。接下来大家一起期待吧～","categories":[{"name":"Docker","slug":"Docker","permalink":"http://blog.crazylaw.cn/categories/Docker/"},{"name":"Python","slug":"Docker/Python","permalink":"http://blog.crazylaw.cn/categories/Docker/Python/"},{"name":"测试","slug":"Docker/Python/测试","permalink":"http://blog.crazylaw.cn/categories/Docker/Python/%E6%B5%8B%E8%AF%95/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://blog.crazylaw.cn/tags/Docker/"},{"name":"Python","slug":"Python","permalink":"http://blog.crazylaw.cn/tags/Python/"},{"name":"测试","slug":"测试","permalink":"http://blog.crazylaw.cn/tags/%E6%B5%8B%E8%AF%95/"},{"name":"jsonschema","slug":"jsonschema","permalink":"http://blog.crazylaw.cn/tags/jsonschema/"}]},{"title":"【Laravel】Service Providers 服务提供者","slug":"php-laravel-2","date":"2018-07-08T06:10:46.000Z","updated":"2021-03-20T16:25:01.812Z","comments":true,"path":"2018/07/08/php-laravel-2/","link":"","permalink":"http://blog.crazylaw.cn/2018/07/08/php-laravel-2/","excerpt":"前言服务提供者和服务容器是两个不同的概念，在我看来，服务提供者是 Laravel 应用启动的中心，你自己的应用以及所有 Laravel 的核心服务都是通过服务提供者启动。 正文里面有 2 个核心的方法，分别是 register（一般用于将服务的对象绑定到服务容器中，不做其他事情。） boot （在 register 之后，将会调用这个 boot 方法，这个 boot 方法允许依赖注入。） 里面 2 个核心的属性，分别是 bindings singletons 具体的用处就是用于服务器容器依赖注入用的。","text":"前言服务提供者和服务容器是两个不同的概念，在我看来，服务提供者是 Laravel 应用启动的中心，你自己的应用以及所有 Laravel 的核心服务都是通过服务提供者启动。 正文里面有 2 个核心的方法，分别是 register（一般用于将服务的对象绑定到服务容器中，不做其他事情。） boot （在 register 之后，将会调用这个 boot 方法，这个 boot 方法允许依赖注入。） 里面 2 个核心的属性，分别是 bindings singletons 具体的用处就是用于服务器容器依赖注入用的。 官方提供的例子如下： 1234567891011121314151617181920class AppServiceProvider extends ServiceProvider&#123; /** * All of the container bindings that should be registered. * * @var array */ public $bindings = [ ServerProvider::class =&gt; DigitalOceanServerProvider::class, ]; /** * All of the container singletons that should be registered. * * @var array */ public $singletons = [ DowntimeNotifier::class =&gt; PingdomDowntimeNotifier::class, ];&#125; 我们定义好了之后需要注册到系统中，找到config/app.php 1234'providers' =&gt; [ // 其它服务提供者 App\\Providers\\ComposerServiceProvider::class,], 这样子就注册好了。 注册好了之后，系统会在启动的时候就初始化服务提供者。 当然，如果你觉得这样子十分眼中影响性能的话，也有一个延迟加载的属性 123456/** * 服务提供者加是否延迟加载. * * @var bool */protected $defer = true; 定义了这个属性之后，在你用到这些服务提供者的时候才会去加载。","categories":[{"name":"PHP","slug":"PHP","permalink":"http://blog.crazylaw.cn/categories/PHP/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"http://blog.crazylaw.cn/tags/PHP/"},{"name":"Laravel","slug":"Laravel","permalink":"http://blog.crazylaw.cn/tags/Laravel/"}]},{"title":"【Laravel】Service Container 服务容器用法","slug":"php-laravel","date":"2018-07-08T04:50:00.000Z","updated":"2021-03-20T16:25:01.813Z","comments":true,"path":"2018/07/08/php-laravel/","link":"","permalink":"http://blog.crazylaw.cn/2018/07/08/php-laravel/","excerpt":"前言Laravel 是一款 PHP 开源框架，最近学习了一下 Symfony，现在来了解一下 Laravel 的最新版本的一些东西。 其实说到服务器容器，相信大家都会直接提到依赖注入的概念，其实服务容器的概念，就和我们设计模式中的对象池差不多，把所有的对象都放在一个池子里面去，而不必一个个去 new 了。而且支持每次都新建和单例，等等。 在这些的基础之上，衍生出了 2 个概念Ioc：控制反转，Di：依赖注入，这些概念，在我的认知里，早起出自 java 框架之中。主要的目的就是实现对象依赖解耦。具体的设计模式的理念，可以参考我博客中的设计模式一系列的文章。 下述主要参考了谋篇 laravel 服务容器介绍摘录+部分自我实践整理。对比了一下和官网的介绍差不多，更多的主要是例子的说明。 正题Laravel 中有一大堆访问 Container 实例的姿势，比如最简单的： 1$container = app();","text":"前言Laravel 是一款 PHP 开源框架，最近学习了一下 Symfony，现在来了解一下 Laravel 的最新版本的一些东西。 其实说到服务器容器，相信大家都会直接提到依赖注入的概念，其实服务容器的概念，就和我们设计模式中的对象池差不多，把所有的对象都放在一个池子里面去，而不必一个个去 new 了。而且支持每次都新建和单例，等等。 在这些的基础之上，衍生出了 2 个概念Ioc：控制反转，Di：依赖注入，这些概念，在我的认知里，早起出自 java 框架之中。主要的目的就是实现对象依赖解耦。具体的设计模式的理念，可以参考我博客中的设计模式一系列的文章。 下述主要参考了谋篇 laravel 服务容器介绍摘录+部分自我实践整理。对比了一下和官网的介绍差不多，更多的主要是例子的说明。 正题Laravel 中有一大堆访问 Container 实例的姿势，比如最简单的： 1$container = app(); 但我们还是先关注下 Container 类本身。 1Laravel 官方文档中一般使用 $this-&gt;app 代替 $container。它是 Application 类的实例，而 Application 类继承自 Container 类。 用法一：基本用法，用type hint (类型提示) 注入 依赖：123456789101112131415161718192021222324&lt;?phpinclude './vendor/autoload.php';use Illuminate\\Container\\Container;$container = Container::getInstance();class MyClass&#123; private $dependency; public function __construct(AnotherClass $dependency) &#123; $this-&gt;dependency = $dependency; &#125;&#125;class AnotherClass&#123;&#125;$instance = $container-&gt;make(MyClass::class);var_dump($instance) 接下来用 Container 的 make 方法来代替 new MyClass: 1$instance = $container-&gt;make(MyClass::class); Container 会自动实例化依赖的对象，所以它等同于： 1$instance = new MyClass(new AnotherClass()); 如果 AnotherClass 也有 依赖，那么 Container 会递归注入它所需的依赖。 Container 使用 Reflection (反射) 来找到并实例化构造函数参数中的那些类。 用法二：Binding Interfaces to Implementations (绑定接口到实现)用 Container 可以轻松地写一个接口，然后在运行时实例化一个具体的实例。 首先定义接口： 12interface MyInterface &#123; /* ... */ &#125;interface AnotherInterface &#123; /* ... */ &#125; 然后声明实现这些接口的具体类。下面这个类不但实现了一个接口，还依赖了实现另一个接口的类实例： 12345678910class MyClass implements MyInterface&#123; private $dependency; // 依赖了一个实现 AnotherInterface 接口的类的实例 public function __construct(AnotherInterface $dependency) &#123; $this-&gt;dependency = $dependency; &#125;&#125; 现在用 Container 的 bind() 方法来让每个 接口 和实现它的类一一对应起来： 12$container-&gt;bind(MyInterface::class, MyClass::class);$container-&gt;bind(AnotherInterface::class, AnotherClass::class); 最后，用接口名 而不是 类名 来传给 make(): 1$instance = $container-&gt;make(MyInterface::class); 注意：如果你忘记绑定它们，会导致一个 Fatal Error:”Uncaught ReflectionException: Class MyInterface does not exist”。 实战 下面是可封装的 Cache 层： 123456789101112131415161718192021222324252627282930313233343536interface Cache&#123; public function get($key); public function put($key, $value);&#125;class Worker&#123; private $cache; public function __construct(Cache $cache) &#123; $this-&gt;cache = $cache; &#125; public function result() &#123; // 去缓存里查询 $result = $this-&gt;cache-&gt;get('worker'); if ($result === null) &#123; // 如果缓存里没有，就去别的地方查询，然后再放进缓存中 $result = do_something_slow(); $this-&gt;cache-&gt;put('worker', $result); &#125; return $result; &#125;&#125;use Illuminate\\Container\\Container;$container = Container::getInstance();$container-&gt;bind(Cache::class, RedisCache::class);$result = $container-&gt;make(Worker::class)-&gt;result();// 这里用 Redis 做缓存，如果改用其他缓存，只要把 RedisCache 换成别的就行了，easy! 用法三：Binding Abstract &amp; Concret Classes （绑定抽象类和具体类）绑定还可以用在抽象类： 1$container-&gt;bind(MyAbstract::class, MyConcreteClass::class); 或者继承的类中： 1$container-&gt;bind(MySQLDatabase::class, CustomMySQLDatabase::class); 用法四：自定义绑定如果类需要一些附加的配置项，可以把 bind() 方法中的第二个参数换成 Closure (闭包函数)： 123$container-&gt;bind(Database::class, function (Container $container) &#123; return new MySQLDatabase(MYSQL_HOST, MYSQL_PORT, MYSQL_USER, MYSQL_PASS);&#125;); 闭包也可用于定制 具体类 的实例化方式： 12345$container-&gt;bind(GitHub\\Client::class, function (Container $container) &#123; $client = new GitHub\\Client; $client-&gt;setEnterpriseUrl(GITHUB_HOST); return $client;&#125;); 用法五：Resolving Callbacks (回调)可用 resolveing()方法来注册一个 callback (回调函数)，而不是直接覆盖掉之前的 绑定。 这个函数会在绑定的类解析完成之后调用。 注意此时的回调函数中，第一个参数是对应被解析的对象，第二个参数是容器(container)&lt;=&gt;应用(app). 123$container-&gt;resolving(GitHub\\Client::class, function ($client, Container $container) &#123; $client-&gt;setEnterpriseUrl(GITHUB_HOST);&#125;); 如果有一大堆 callbacks，他们全部都会被调用。对于 接口 和 抽象类 也可以这么用： 1234567891011$container-&gt;resolving(Logger::class, function (Logger $logger) &#123; $logger-&gt;setLevel('debug');&#125;);$container-&gt;resolving(FileLogger::class, function (FileLogger $logger) &#123; $logger-&gt;setFilename('logs/debug.log');&#125;);$container-&gt;bind(Logger::class, FileLogger::class);$logger = $container-&gt;make(Logger::class); 更 diao 的是，还可以注册成「什么类解析完之后都调用」： 123$container-&gt;resolving(function ($object, Container $container) &#123; &#x2F;&#x2F; ...&#125;); 但这个估计只有 logging和 debugging 才会用到。 用法六：Extending a Class (扩展一个类)使用 extend() 方法，可以封装一个类然后返回一个不同的对象 (代理模式)： （为什么不是装饰器模式？，因为装饰器模式不需要实现一样的接口，但是代理模式下，代理类需要和原来的类一样实现同一接口）. 123$container-&gt;extend(APIClient::class, function ($client, Container $container) &#123; return new APIClientProxy($client);&#125;); 注意：这两个类要实现相同的 接口，不然用类型提示的时候会出错：. 12345678910111213141516171819202122232425262728293031323334353637383940414243interface Getable&#123; public function get();&#125;class APIClient implements Getable&#123; public function get() &#123; return 'yes!'; &#125;&#125;class APIClentProxy implements Getable&#123; private $client; public function __construct(APIClient $client) &#123; $this-&gt;client = $client; &#125; public function get() &#123; return 'no!'; &#125;&#125;class User&#123; private $client; public function __construct(Getable $client) &#123; $this-&gt;client = $client; &#125;&#125;$container-&gt;extend(APIClient::class, function ($client, Container $container) &#123; return new APIClentProxy($client);&#125;);$container-&gt;bind(Getable::class, APIClient::class);// 此时 $instance 的 $client 属性已经是 APIClentProxy 类型了$instance = $container-&gt;make(User::class); 用法七：单例使用 bind() 方法绑定后，每次解析时都会新实例化一个对象(或重新调用闭包)，如果想获取 单例 ，则用 singleton() 方法代替 bind()： 1$container-&gt;singleton(Cache::class, RedisCache::class); 绑定单例 闭包 123$container-&gt;singleton(Database::class, function (Container $container) &#123; return new MySQLDatabase('localhost', 'testdb', 'user', 'pass');&#125;); 绑定 具体类 的时候，不需要第二个参数： 1$container-&gt;singleton(MySQLDatabase::class); 在每种情况下，单例 对象将在第一次需要时创建，然后在后续重复使用。 如果你已经有一个 实例 并且想重复使用，可以用 instance() 方法。 1$container-&gt;instance(Container::class, $container); Laravel 就是用这种方法来确保每次获取到的都是同一个 Container 实例： 用法七：Arbitrary Binding Names (任意绑定名称)Container 还可以绑定任意字符串而不是 类/接口名称。但这种情况下不能使用类型提示，并且只能用 make() 来获取实例。 12$container-&gt;bind('database', MySQLDatabase::class);$db = $container-&gt;make('database'); 为了同时支持类/接口名称和短名称，可以使用 alias()： 1234567$container-&gt;singleton(Cache::class, RedisCache::class);$container-&gt;alias(Cache::class, 'cache');$cache1 = $container-&gt;make(Cache::class);$cache2 = $container-&gt;make('cache');assert($cache1 === $cache2); 用法八：保存任何值Container 还可以用来保存任何值，例如 configuration 数据： 12$container-&gt;instance('database.name', 'testdb');$db_name = $container-&gt;make('database.name'); 它支持数组访问语法，这样用起来更自然： 12$container['database.name'] = 'testdb';$db_name = $container['database.name']; 这是因为 Container 实现了 PHP 的 ArrayAccess 接口。 当处理 Closure 绑定的时候，你会发现这个方式非常好用： 12345678$container-&gt;singleton('database', function (Container $container) &#123; return new MySQLDatabase( $container['database.host'], $container['database.name'], $container['database.user'], $container['database.pass'] );&#125;); Laravel 自己没有用这种方式来处理配置项，它使用了一个单独的 Config 类本身。 PHP-DI 用了。 数组访问语法还可以代替 make() 来实例化对象：. 1$db = $container['database']; 用法九：Dependency Injection for Functions &amp; Methods (给函数或方法注入依赖)除了给构造函数注入依赖，Laravel 还可以往任意函数中注入： 12function do_something(Cache $cache) &#123; /* ... */ &#125;$result = $container-&gt;call('do_something'); 函数的附加参数可以作为索引或关联数组传递： 123456789function show_product(Cache $cache, $id, $tab = 'details') &#123; /* ... */ &#125;// show_product($cache, 1)$container-&gt;call('show_product', [1]);$container-&gt;call('show_product', ['id' =&gt; 1]);// show_product($cache, 1, 'spec')$container-&gt;call('show_product', [1, 'spec']);$container-&gt;call('show_product', ['id' =&gt; 1, 'tab' =&gt; 'spec']); 除此之外，闭包： 12$closure = function (Cache $cache) &#123; /* ... */ &#125;;$container-&gt;call($closure); 静态方法： 1234567class SomeClass&#123; public static function staticMethod(Cache $cache) &#123; /* ... */ &#125;&#125;$container-&gt;call(['SomeClass', 'staticMethod']);// or:$container-&gt;call('SomeClass::staticMethod'); 实例的方法： 123456789class PostController&#123; public function index(Cache $cache) &#123; /* ... */ &#125; public function show(Cache $cache, $id) &#123; /* ... */ &#125;&#125;$controller = $container-&gt;make(PostController::class);$container-&gt;call([$controller, 'index']);$container-&gt;call([$controller, 'show'], ['id' =&gt; 1]); 都可以注入。 用法十: 调用实例方法的快捷方式使用 ClassName@methodName 语法可以快捷调用实例中的方法： 12$container-&gt;call('PostController@index');$container-&gt;call('PostController@show', ['id' =&gt; 4]); 因为 Container 被用来实例化类。意味着： 依赖 被注入进构造函数（或者方法）；如果需要复用实例，可以定义为单例；可以用接口或任何名称来代替具体类。所以这样调用也可以生效： 1234567class PostController&#123; public function __construct(Request $request) &#123; /* ... */ &#125; public function index(Cache $cache) &#123; /* ... */ &#125;&#125;$container-&gt;singleton('post', PostController::class);$container-&gt;call('post@index'); 最后，还可以传一个「默认方法」作为第三个参数。如果第一个参数是没有指定方法的类名称，则将调用默认方法。 Laravel 用这种方式来处理 event handlers: 1234$container-&gt;call(MyEventHandler::class, $parameters, 'handle');// 相当于:$container-&gt;call('MyEventHandler@handle', $parameters); 用法十一：Method Call Bindings (方法调用绑定)bindMethod() 方法可用来覆盖方法，例如用来传递其他参数： 12345$container-&gt;bindMethod('PostController@index', function ($controller, $container) &#123; $posts = get_posts(...); return $controller-&gt;index($posts);&#125;); 下面的方式都有效，调用闭包来代替调用原始的方法： 123$container-&gt;call('PostController@index');$container-&gt;call('PostController', [], 'index');$container-&gt;call([new PostController, 'index']); 但是，call() 的任何其他参数都不会传递到闭包中，因此不能使用它们。 1$container-&gt;call('PostController@index', ['Not used :-(']); 用法十二：Contextual Bindings (上下文绑定)有时候你想在不同的地方给接口不同的实现。这里有 Laravel 文档 里的一个例子： 123456789$container -&gt;when(PhotoController::class) -&gt;needs(Filesystem::class) -&gt;give(LocalFilesystem::class);$container -&gt;when(VideoController::class) -&gt;needs(Filesystem::class) -&gt;give(S3Filesystem::class); 现在 PhotoController 和 VideoController 都依赖了 Filesystem 接口，但是收到了不同的实例。 可以像 bind() 那样，给 give() 传闭包： 12345-&gt;when(VideoController::class)-&gt;needs(Filesystem::class)-&gt;give(function () &#123; return Storage::disk('s3');&#125;); 或者短名称： 123456$container-&gt;instance('s3', $s3Filesystem);$container -&gt;when(VideoController::class) -&gt;needs(Filesystem::class) -&gt;give('s3'); 用法十三：Binding Parameters to Primitives (绑定初始数据)当有一个类不仅需要接受一个注入类，还需要注入一个基本值（比如整数）。还可以通过将变量名称 (而不是接口) 传递给 needs() 并将值传递给 give() 来注入需要的任何值 (字符串、整数等) ： 1234$container -&gt;when(MySQLDatabase::class) -&gt;needs('$username') -&gt;give(DB_USER); 还可以使用闭包实现延时加载，只在需要的时候取回这个 值 。 123456$container -&gt;when(MySQLDatabase::class) -&gt;needs('$username') -&gt;give(function () &#123; return config('database.user'); &#125;); 这种情况下，不能传递类或命名的依赖关系（例如，give(‘database.user’)），因为它将作为字面值返回。所以需要使用闭包： 123456$container -&gt;when(MySQLDatabase::class) -&gt;needs('$username') -&gt;give(function (Container $container) &#123; return $container['database.user']; &#125;); 用法十四: Tagging (标记)Container 可以用来「标记」有关系的绑定： 12$container-&gt;tag(MyPlugin::class, 'plugin');$container-&gt;tag(AnotherPlugin::class, 'plugin'); 这样会以数组的形式取回所有「标记」的实例： 123foreach ($container-&gt;tagged('plugin') as $plugin) &#123; $plugin-&gt;init();&#125; tag() 方法的两个参数都可以接受数组： 12$container-&gt;tag([MyPlugin::class, AnotherPlugin::class], 'plugin');$container-&gt;tag(MyPlugin::class, ['plugin', 'plugin.admin']);","categories":[{"name":"PHP","slug":"PHP","permalink":"http://blog.crazylaw.cn/categories/PHP/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"http://blog.crazylaw.cn/tags/PHP/"},{"name":"Laravel","slug":"Laravel","permalink":"http://blog.crazylaw.cn/tags/Laravel/"}]},{"title":"【OpenResty】 带你入门高性能http代理服务器应用","slug":"openresty-1","date":"2018-07-07T16:23:00.000Z","updated":"2021-03-20T16:25:01.812Z","comments":true,"path":"2018/07/08/openresty-1/","link":"","permalink":"http://blog.crazylaw.cn/2018/07/08/openresty-1/","excerpt":"前言OpenResty® 是一个基于 Nginx 与 Lua 的高性能 Web 平台，其内部集成了大量精良的 Lua 库、第三方模块以及大多数的依赖项。用于方便地搭建能够处理超高并发、扩展性极高的动态 Web 应用、Web 服务和动态网关。 OpenResty® 通过汇聚各种设计精良的 Nginx 模块（主要由 OpenResty 团队自主开发），从而将 Nginx 有效地变成一个强大的通用 Web 应用平台。这样，Web 开发人员和系统工程师可以使用 Lua 脚本语言调动 Nginx 支持的各种 C 以及 Lua 模块，快速构造出足以胜任 10K 乃至 1000K 以上单机并发连接的高性能 Web 应用系统。 OpenResty® 的目标是让你的 Web 服务直接跑在 Nginx 服务内部，充分利用 Nginx 的非阻塞 I/O 模型，不仅仅对 HTTP 客户端请求,甚至于对远程后端诸如 MySQL、PostgreSQL、Memcached 以及 Redis 等都进行一致的高性能响应。 以上都是官方说辞","text":"前言OpenResty® 是一个基于 Nginx 与 Lua 的高性能 Web 平台，其内部集成了大量精良的 Lua 库、第三方模块以及大多数的依赖项。用于方便地搭建能够处理超高并发、扩展性极高的动态 Web 应用、Web 服务和动态网关。 OpenResty® 通过汇聚各种设计精良的 Nginx 模块（主要由 OpenResty 团队自主开发），从而将 Nginx 有效地变成一个强大的通用 Web 应用平台。这样，Web 开发人员和系统工程师可以使用 Lua 脚本语言调动 Nginx 支持的各种 C 以及 Lua 模块，快速构造出足以胜任 10K 乃至 1000K 以上单机并发连接的高性能 Web 应用系统。 OpenResty® 的目标是让你的 Web 服务直接跑在 Nginx 服务内部，充分利用 Nginx 的非阻塞 I/O 模型，不仅仅对 HTTP 客户端请求,甚至于对远程后端诸如 MySQL、PostgreSQL、Memcached 以及 Redis 等都进行一致的高性能响应。 以上都是官方说辞 实践这里我使用都 mac 系统都 brew 安装都 openresty，类似于 centos 下都 yum 安装。默认都情况下，openresty 已经帮忙安装好了 luajit（lua 解析器），和 MySQL、PostgreSQL、Memcached 以及 Redis 等扩展的 lua 文件，具体如下。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364├── cjson.so├── ngx│ ├── balancer.lua│ ├── base64.lua│ ├── errlog.lua│ ├── ocsp.lua│ ├── process.lua│ ├── re.lua│ ├── resp.lua│ ├── semaphore.lua│ ├── ssl│ │ └── session.lua│ └── ssl.lua├── redis│ └── parser.so└── resty ├── aes.lua ├── core │ ├── base.lua │ ├── base64.lua │ ├── ctx.lua │ ├── exit.lua │ ├── hash.lua │ ├── misc.lua │ ├── phase.lua │ ├── regex.lua │ ├── request.lua │ ├── response.lua │ ├── shdict.lua │ ├── time.lua │ ├── uri.lua │ ├── var.lua │ └── worker.lua ├── core.lua ├── dns │ └── resolver.lua ├── limit │ ├── conn.lua │ ├── count.lua │ ├── req.lua │ └── traffic.lua ├── lock.lua ├── lrucache │ └── pureffi.lua ├── lrucache.lua ├── md5.lua ├── memcached.lua ├── mysql.lua ├── random.lua ├── redis.lua ├── sha.lua ├── sha1.lua ├── sha224.lua ├── sha256.lua ├── sha384.lua ├── sha512.lua ├── string.lua ├── upload.lua ├── upstream │ └── healthcheck.lua └── websocket ├── client.lua ├── protocol.lua └── server.lua 我们需要做的步骤很简单。找到配置文件。 然后需要关注的主要目录只有 1 个。 lualib ，上图中的 tree 结构就是由这个目录下显示的。 我们需要找到对应 openresty 的 nginx 启动的可执行文件。 我们进入到bin目录。 发现了 openrestry 这个软连接，指向到是我们上级目录 nginx 中到 sbin 中到 nginx 可执行文件。 我们需要找到对应到配置文件和路径。 1.&#x2F;openresty -t 从结果中，我们找到了配置文件到路径，并且打开配置文件，进行添加部分内容。 （其实就是 nginx 到配置，只不过需要加上一些内容来加载 lua 相关的包和扩展） 在这里，我们需要加上这 2 行代码，这是 lua 的标准配置。 123# lua配置lua_package_path &quot;&#x2F;usr&#x2F;local&#x2F;opt&#x2F;openresty&#x2F;lualib&#x2F;?.lua;;&quot;; #lua 模块lua_package_cpath &quot;&#x2F;usr&#x2F;local&#x2F;opt&#x2F;openresty&#x2F;lualib&#x2F;?.so;;&quot;; #c模块 里面的路径就是刚才我们找到的lualib的路径，至于为什么是;;呢，这里代表着找的是 2 个路径，一个是默认路径，一个是指定路径。 写完之后，就可以写我们的 lua 代码了。 这里，我们设置了一个server段，端口设置为8080，需要注意的是charset utf-8;，这个配置项务必写上，否则在你 lua 代码中输出的中文，将会乱码。 然后我们配置了 location，设置了默认的响应类型为 text/html. lua_code_cache off这个也是十分重要的，在开发模式下，我们添加上这句话，我们就不必每次写完 lua 文件都重启一遍 nginx，这里如果没有 cache 都话，每次都会去由于 luajit 来从头解析一遍 lua，类似于我们 php 中都 opcache 都作用。所以，如果在生产环境中，务必设置为 on，或者删掉这句话（默认为:lua_code_cache on）。 接下来就是 nginx 配置中，挂载 lua 文件都一个重要语句了，那就是content_by_lua_file，这里我们都脚本语言 lua 就可以嵌入到 nginx 中去了。 我们来看看我写到一个和redis通信到一个例子。 12345678910111213141516171819202122232425262728local redis &#x3D; require &quot;resty.redis&quot;local red &#x3D; redis:new()red:set_timeout(1000)local ok, err &#x3D; red:connect(&quot;127.0.0.1&quot;, 6379)if not ok then ngx.say(&quot;failed to connect: &quot;, err) returnendlocal clientIp &#x3D; ngx.req.get_headers()[&quot;X-Real-Ip&quot;]if clientIP &#x3D;&#x3D; nil then clientIp &#x3D; ngx.req.get_headers()[&quot;x_forwarded_for&quot;]endif clientIP &#x3D;&#x3D; nil then clientIP &#x3D; ngx.var.remote_addrendlocal incrKey &#x3D; &quot;user:&quot;..clientIP..&quot;:freq&quot;local blockKey &#x3D; &quot;user:&quot;..clientIP..&quot;:block&quot;local is_block,err &#x3D; red:get(blockKey)if tonumber(is_block) &#x3D;&#x3D; 1 then ngx.say(blockKey..&quot;你被限流了！&quot;) ngx.exit(ngx.HTTP_FORBIDDEN)endngx.say(blockKey..&quot;调用成功！&quot;) 具体到 lua 语法就不详细说明了，这里需要大家去学一下 lua 的基本语法。 当我们执行的时候，就可以看到了这么一串东西了，当我在 redis 中设置对应当 key 当时候。 我们再来访问一下，这个时候就会发现，有变化了。 经过这个简单的例子，我们就可以想到，我们经常说在要 http 代理服务器中对请求进行拦截，或者一些高并发例如秒杀等拦截的时候，我们就可以借助 openresty 来进行一个流量削峰，等请求真正到达我们等下游服务器等时候，爆炸了请求已经被过滤掉了 80%无用的请求。","categories":[{"name":"Nginx","slug":"Nginx","permalink":"http://blog.crazylaw.cn/categories/Nginx/"}],"tags":[{"name":"Nginx","slug":"Nginx","permalink":"http://blog.crazylaw.cn/tags/Nginx/"},{"name":"Openresty","slug":"Openresty","permalink":"http://blog.crazylaw.cn/tags/Openresty/"}]},{"title":"【进制】数据库应用","slug":"jinzhi-1","date":"2018-05-10T15:05:00.000Z","updated":"2021-03-20T16:25:01.806Z","comments":true,"path":"2018/05/10/jinzhi-1/","link":"","permalink":"http://blog.crazylaw.cn/2018/05/10/jinzhi-1/","excerpt":"概要 最近加了一个月的班，很忙，博客已经有一段时间没有写了，心里很不是滋味，今晚决心，再晚，也要写一篇。 我们平时各种系统最常打交道的系统就是数据库了，我们在数据库中，可能就是最直观，最直接的用法，很少用到一些进制相关的知识在里面，其实进制在我们的生活中处处都在渗透，今天我们重点整理一下数据库和进制相关的内容。 分库分表我们都知道，数据库的优化策略之一就是分库分表，但是这个分库分表需要怎么分才算好呢，这里我也没有打包票说有一个固定的模式，因为这种所谓的“最优”是要结合具体的需求来说明的。 在这里我会列举其中的一些场景来具体说一下。大部分公司的订单号所以没有一个好的架构师或者比较资深的程序员，订单号一般都是最容易忽略的一个关键，一般毕业生或者初学者都会把订单号设置成简单的 int 并且 autoincrease，这样子的做法，简单粗暴有效，定义好了之后就可以撸起袖子干了。","text":"概要 最近加了一个月的班，很忙，博客已经有一段时间没有写了，心里很不是滋味，今晚决心，再晚，也要写一篇。 我们平时各种系统最常打交道的系统就是数据库了，我们在数据库中，可能就是最直观，最直接的用法，很少用到一些进制相关的知识在里面，其实进制在我们的生活中处处都在渗透，今天我们重点整理一下数据库和进制相关的内容。 分库分表我们都知道，数据库的优化策略之一就是分库分表，但是这个分库分表需要怎么分才算好呢，这里我也没有打包票说有一个固定的模式，因为这种所谓的“最优”是要结合具体的需求来说明的。 在这里我会列举其中的一些场景来具体说一下。大部分公司的订单号所以没有一个好的架构师或者比较资深的程序员，订单号一般都是最容易忽略的一个关键，一般毕业生或者初学者都会把订单号设置成简单的 int 并且 autoincrease，这样子的做法，简单粗暴有效，定义好了之后就可以撸起袖子干了。 我们一般的订单结构都会面临几个需求 通过 orderId 查询订单，绝大多数都是这类需求（where orderId = :orderId） 某个用户要查找自己的订单信息(where userId = :userId) 搜索订单数据是一个高频的需求，数据量大了之后，很容易产生慢查询，在高吞吐量，又要保证服务高可用，我们有没有一些手段可以优化一下？答案有：分库分表。 简单的分库分表方法 （在索引表和详情表都在一起的情况下）通过 OrderId 求余，例如 OrderId%10，那么的出来的结果就是[0-9]十位数，然后我们的库或者表就通过这么一个[0-9 加相应的后缀]，这样子我们在查询的时候，就可以通过简单的 OrderId%10 之后，找到对应的库或者表来进行查询，非科学的比喻一下，这里我们就把一个耗时 100s 的查询速度，最后用了 10s 来完成，在查询上我们通过类似于分区的概念，来提升了我们查询 sql 的性能。 或许乐于思考的同学开始发现问题了，这样子做的结果就是如果我要找“我的订单”呢？怎么办？没错，这就是分库或者分表的一个最大的弊端，这个时候，可能你就要通过遍历所有的表来查询“我的订单”了，因为基于 OrderId 的分库分表在 UserId 上一点关系都没有。 那么我反过来思考一下，那我换一种思维，用 UserId 分库分表呢，这个时候确实是可以快速查询到了“我的订单”，但是这个时候，如果是要通过 OrderId 来查数据呢？这个时候就惨了，你可能还是需要遍历全量数据来找到这个数据，运气好的话，可能就在第一次查询的时候刚好就查到了，运气不好的话，可能就是查全量数据了。 基因法顾名思义，我们接下来要将的就是和“基因”有关，为什么这么说呢，因为我们需要把 UserId 和 OrderId 通过某种方式让他们产生关联。 举个具体的例子： OrderId = 32 ， UserId = 666（这个时候，我们的分库标志是 UserId 的最后一个十进制位，也就是 6） 我们的 orderId 是不是可以*100，然后+666？ 结果：32666，那么如果我的需求的通过 UserId 来的话，那就截取最后一位，是不是就可以快速查询？是的，这个时候可以满足“我的订单”，“对应的 OrderId”，落在了同一个表里面了。 或许！！！你以为这就完了，善于思考的同学这个时候发现问题了，这样子的做法，基本上就是会让我的 OrderId 的字段迅速达到最大值，或许，这个时候，你就要考虑 bigint 了，再不济，有可能碎玉 OrderId 和 UserID 的增加，这个值有可能连 bigint 都存不下了。这简直就是个灾难。 这个时候，或许，我们是不是应该换一种思维，利用二进制呢？我们要考虑到这一点的前提是十进制和二进制的关系，十进制和二进制有什么区别呢，十进制是逢十进一，二进制是逢二进一，或许这个时候你还好没有发现奇妙的地方。 举个例子：一个十进制的 9，转成二进制就是 1001 接着上面的例子。 OrderId = 32 ， UserId = 17 OrderId 二进制就是：100000 UserId 二进制就是：10001 我们对 UserId%16，这个时候，UserId 得到的结果又 16 种可能[0-15] 这个时候，我们知道 16 一共是 4bit，我们把这 4bit 拼接到 100000 后面，那就是：10 0000 0001 ，十进制的写法也就是说(32 &lt;&lt; 4) | 1 结果等于 513。 (32 &lt;&lt; 4) | 1 详解：&lt;&lt; 4 代表向左移动 4 位，往高位移动，| 1 是因为 0001 的十进制是 1，并且我们相加是通过逻辑或完成。 我们试一下： 513 % 16 = 1 17 % 16 = 1 这个时候，我们就完成了二进制的基因法，并且我们可以省下了很多的空间，因为一个 int 是 4 个字节，32 位。最大值就是 2 的 32 次方-1，如果不放心的话用 bitint，基本上是肯定够了。 这个时候，我们的标志位都是 1，那么久可以在同一个表中查询我们想要的数据！ 如果在加多一种属性，那么可能就需要采用冗余的方法来了，一般基因法用于 1 对多的场景，常在多的一方嵌入 1 的基因。 单字段存储多个 0-1 关系的属性…后续补充","categories":[{"name":"算法","slug":"算法","permalink":"http://blog.crazylaw.cn/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"进制","slug":"进制","permalink":"http://blog.crazylaw.cn/tags/%E8%BF%9B%E5%88%B6/"},{"name":"算法","slug":"算法","permalink":"http://blog.crazylaw.cn/tags/%E7%AE%97%E6%B3%95/"}]},{"title":"【SSL】使用certbot工具申请letsencrypt的ssl证书","slug":"nginx-certbot-ssl","date":"2018-04-08T02:55:00.000Z","updated":"2021-03-20T16:25:01.811Z","comments":true,"path":"2018/04/08/nginx-certbot-ssl/","link":"","permalink":"http://blog.crazylaw.cn/2018/04/08/nginx-certbot-ssl/","excerpt":"","text":"申请 ssl 证书之前，先梳理几个要点：1、确定好拟申请 ssl 证书的域名，这里为了便于说明假设申请 ssl 的域名为：a.baicai.com，www.baicai.com、account.baicai.com 等其他域名（和主机），没有本质上的区别，仅名称上的差异； 2、确定拟申请 ssl 证书的域名正确解析到当前主机，本例也就是通过http://blog.crazylaw.com 能够正确解析到当前主机（注意是正确解析，至于是不是能正确访问依据 certbot 的模式不同要求也不同）； 3、确定好拟申请 ssl 证书域名解析到当前主机后的 web 根目录，假设为：/www/blog/ Github 上拉取 cerbot1git clone https:&#x2F;&#x2F;github.com&#x2F;certbot&#x2F;certbot 然后进入 1cd cerbot 接着看到cerbot-auto的可执行文件 然后生成证书。 注意，如果你第一次使用 letsencrypy 的话，一般需要指定邮箱，并且 letsencrypy 会发送证书到你的邮箱，请确认订阅 letsencrypy，下面的命令是带上了邮箱信息的！ 注意，需要暂时停止 nginx！ standalone 模式申请 ssl 证书standalone 模式不需要上述梳理出的 3 个要点中的第三条，也就意味着如果你的主机需要关停 80 端口（或和 443 端口）的 web 服务，譬如正在运行的 nginx、Apache 需要关停。 123##standalone模式，其中-d参数指定拟申请ssl证书的域名 #可以通过多个-d参数指定多个域名，但你得确保这些指定的多个域名均能正常解析到当前主机.&#x2F;certbot-auto certonly --standalone --email you@email.com -d example.com -d www.example.com -d other.example.net 非 standalone 模式申请 ssl 证书这种模式需要使用-w 参数（或者–webroot-path 参数）指定当前正在运行的 web 服务器的根目录。–webroot-path 参数指定 web 根目录后，certbot工具会自动在该目录下生成.well-known的隐藏目录，以及用于效验域名所有者的特定文件，此文件 Let’s Encrypt 的服务器会主动发起 http 请求去读取从达到效验域名所有者或者管理者就是本次操作 certbot 工具的人；certbot 工具自动生成的完整目录为：-w 参数指定的根目录/.well-known/acme-challenge，对于 nginx 而言 web 根目录下的隐藏目录默认情况下是不允许访问的，所以 nginx 情况下再执行非 standalone 模式申请 ssl 证书之前，需要将nginx网站根目录下的.well-known隐藏目录设置成允许访问。 12345##nginx对应主机的配置文件中添加允许.well-known隐藏目录的访问#注意配置文件中禁止浏览隐藏文件的相关代码引起冲突location ~ &#x2F;.well-known &#123; allow all;&#125; 如果熟悉 nginx，甚至可以为.well-known 隐藏目录指定单独的 root 入口；这就是 nginx 有关的合理利用了，不再补充。 12##--webroot指定当前正在运行的web server的根目录certbot certonly --webroot -w &#x2F;var&#x2F;www&#x2F; -d c1c2.test.com 一般使用 standalone 模式，继续接着如果你没订阅过的话，会在邮箱收到邮件，然后你要选择订阅。接着你会看到如下信息，选择 a 和 y 之后。 默认路径如下： 1234&#x2F;etc&#x2F;letsencrypt&#x2F;live&#x2F;&#123;you.domain&#125;&#x2F;fullchain.pem&#x2F;etc&#x2F;letsencrypt&#x2F;live&#x2F;&#123;you.domain&#125;&#x2F;privkey.pem 如何更新？这里截图写得很清楚了，再执行一次 certbot-auto 命令+参数即可，如果你想全部重新执行一边的话，可以使用 certbot-auto renew。 nginx 配置然后我们重启 nginx，即可。！ 如何实现自动化？ 停止 nginx 执行 certbot-auto 重启 nginx","categories":[{"name":"Nginx","slug":"Nginx","permalink":"http://blog.crazylaw.cn/categories/Nginx/"}],"tags":[{"name":"Nginx","slug":"Nginx","permalink":"http://blog.crazylaw.cn/tags/Nginx/"},{"name":"ssl","slug":"ssl","permalink":"http://blog.crazylaw.cn/tags/ssl/"}]},{"title":"【正则表达式】- 贪婪和非贪婪模式","slug":"regular-3","date":"2018-03-12T06:11:56.000Z","updated":"2021-03-20T16:25:01.814Z","comments":true,"path":"2018/03/12/regular-3/","link":"","permalink":"http://blog.crazylaw.cn/2018/03/12/regular-3/","excerpt":"模式：在使用修饰匹配次数的特殊符号时，有几种表示方法可以使同一个表达式能够匹配不同的次数，比如：”{m,n}”, “{m,}”, “?”, “*”, “+”，具体匹配的次数随被匹配的字符串而定。这种重复匹配不定次数的表达式在匹配过程中，总是尽可能多的匹配。比如，针对文本 “dxxxdxxxd”，举例如下： 贪婪模式： 表达式 匹配结果 (d)(\\w+) “\\w+” 将匹配第一个 “d” 之后的所有字符 “xxxdxxxd” (d)(\\w+)(d) “\\w+” 将匹配第一个 “d” 和最后一个 “d” 之间的所有字符 “xxxdxxx”。 虽然 “\\w+” 也能够匹配上最后一个 “d”，但是为了使整个表达式匹配成功，“\\w+” 可以 “让出” 它本来能够匹配的最后一个 “d”","text":"模式：在使用修饰匹配次数的特殊符号时，有几种表示方法可以使同一个表达式能够匹配不同的次数，比如：”{m,n}”, “{m,}”, “?”, “*”, “+”，具体匹配的次数随被匹配的字符串而定。这种重复匹配不定次数的表达式在匹配过程中，总是尽可能多的匹配。比如，针对文本 “dxxxdxxxd”，举例如下： 贪婪模式： 表达式 匹配结果 (d)(\\w+) “\\w+” 将匹配第一个 “d” 之后的所有字符 “xxxdxxxd” (d)(\\w+)(d) “\\w+” 将匹配第一个 “d” 和最后一个 “d” 之间的所有字符 “xxxdxxx”。 虽然 “\\w+” 也能够匹配上最后一个 “d”，但是为了使整个表达式匹配成功，“\\w+” 可以 “让出” 它本来能够匹配的最后一个 “d” 由此可见，”\\w+” 在匹配的时候，总是尽可能多的匹配符合它规则的字符。虽然第二个举例中，它没有匹配最后一个 “d”，但那也是为了让整个表达式能够匹配成功。同理，带 “*” 和 “{m,n}” 的表达式都是尽可能地多匹配，带 “?” 的表达式在可匹配可不匹配的时候，也是尽可能的 “要匹配”。这 种匹配原则就叫作 “贪婪” 模式 。 非贪婪模式：在修饰匹配次数的特殊符号后再加上一个 “?” 号，则可以使匹配次数不定的表达式尽可能少的匹配，使可匹配可不匹配的表达式，尽可能的 “不匹配”。这种匹配原则叫作 “非贪婪” 模式，也叫作 “勉强” 模式。如果少匹配就会导致整个表达式匹配失败的时候，与贪婪模式类似，非贪婪模式会最小限度的再匹配一些，以使整个表达式匹配成功。举例如下，针对文本 “dxxxdxxxd” 举例： 表达式 匹配结果 (d)(\\w+?) “\\w+?” 将尽可能少的匹配第一个 “d” 之后的字符，结果是：”\\w+?” 只匹配了一个 “x” (d)(\\w+?)(d) 为了让整个表达式匹配成功，”\\w+?” 不得不匹配 “xxx” 才可以让后边的 “d” 匹配，从而使整个表达式匹配成功。因此，结果是：”\\w+?” 匹配 “xxx”","categories":[{"name":"正则","slug":"正则","permalink":"http://blog.crazylaw.cn/categories/%E6%AD%A3%E5%88%99/"}],"tags":[{"name":"正则","slug":"正则","permalink":"http://blog.crazylaw.cn/tags/%E6%AD%A3%E5%88%99/"}]},{"title":"【正则表达式】 - 超简单练习题","slug":"regular-2","date":"2018-03-12T05:58:00.000Z","updated":"2021-03-20T16:25:01.814Z","comments":true,"path":"2018/03/12/regular-2/","link":"","permalink":"http://blog.crazylaw.cn/2018/03/12/regular-2/","excerpt":"1. 匹配一段文本中的每行的邮箱12345$str &#x3D; &#39;123@qq.comaaa@163.combbb@126.comasdfasfs33333@adfcom&#39;;preg_match_all(&quot;&#x2F;\\w+@(?:qq|126|163)\\.com&#x2F;&quot;, $str, $matches);var_dump($matches);","text":"1. 匹配一段文本中的每行的邮箱12345$str &#x3D; &#39;123@qq.comaaa@163.combbb@126.comasdfasfs33333@adfcom&#39;;preg_match_all(&quot;&#x2F;\\w+@(?:qq|126|163)\\.com&#x2F;&quot;, $str, $matches);var_dump($matches); 2. 匹配一段文本中的每行的时间字符串，比如：‘1990-07-12’12345$str &#x3D; &#39;asfasf1990-07-12asdfAAAbbbb434241&#39;;preg_match_all(&quot;&#x2F;(?P&lt;year&gt;19[0-9]&#123;2&#125;)-(?P&lt;month&gt;\\d+)-(?P&lt;day&gt;\\d+)&#x2F;&quot;, $str, $matches);var_dump($matches); 3. 匹配一段文本中的 1-12 的值12345$str &#x3D; &#39;23123156865423150506087111009&#39;;preg_match_all(&#39;&#x2F;1[0-2]|[1-9]&#x2F;&#39;,$str, $matches);var_dump($matches);","categories":[{"name":"正则","slug":"正则","permalink":"http://blog.crazylaw.cn/categories/%E6%AD%A3%E5%88%99/"}],"tags":[{"name":"正则","slug":"正则","permalink":"http://blog.crazylaw.cn/tags/%E6%AD%A3%E5%88%99/"}]},{"title":"【正则表达式】- 分组概念","slug":"regular","date":"2018-03-12T05:56:00.000Z","updated":"2021-03-20T16:25:01.814Z","comments":true,"path":"2018/03/12/regular/","link":"","permalink":"http://blog.crazylaw.cn/2018/03/12/regular/","excerpt":"正则我们经常用，但是对于几个特殊的用途，可以记录一下。 (exp) 匹配 exp,并捕获文本到自动命名的组里 (?&lt;name&gt;exp) 匹配 exp,并捕获文本到名称为 name 的组里，也可以写成(?’name’exp) (?P&lt;name&gt;exp) 和 (?&lt;name&gt;exp)一个意思 (?:exp) 匹配 exp,不捕获匹配的文本 (?=exp) 匹配 exp 前面的位置 (?&lt;=exp) 匹配 exp 后面的位置 (?!exp) 匹配后面跟的不是 exp 的位置 (?&lt;!exp) 匹配前面不是 exp 的位置 (?#comment) 这种类型的组不对正则表达式的处理产生任何影响，只是为了提供让人阅读注释","text":"正则我们经常用，但是对于几个特殊的用途，可以记录一下。 (exp) 匹配 exp,并捕获文本到自动命名的组里 (?&lt;name&gt;exp) 匹配 exp,并捕获文本到名称为 name 的组里，也可以写成(?’name’exp) (?P&lt;name&gt;exp) 和 (?&lt;name&gt;exp)一个意思 (?:exp) 匹配 exp,不捕获匹配的文本 (?=exp) 匹配 exp 前面的位置 (?&lt;=exp) 匹配 exp 后面的位置 (?!exp) 匹配后面跟的不是 exp 的位置 (?&lt;!exp) 匹配前面不是 exp 的位置 (?#comment) 这种类型的组不对正则表达式的处理产生任何影响，只是为了提供让人阅读注释 例子比如 12345var m = \"abcabc\".match(/(?:a)(b)(c)/)//结果 [\"abc\", \"b\", \"c\"]// m[0] 是/(?:a)(b)(c)/匹配到的整个字符串，这里包括了a// m[1] 是捕获组1，即(b)匹配的子字符串substring or sub sequence// m[2] 是捕获组2，即(c)匹配到的 如果这样 12var m = \"abcabc\".match(/(a)(b)(c)/)//结果 [\"abc\", \"a\", \"b\", \"c\"] 第一小题应该是这样的正则表达式 1/(\\w)((?=111)(1))+/ 这里有一个知识点 zero-width positive lookahead，零宽断言，正向前瞻（反正我记不住意思是(?=X)匹配某个位置，右边（正向）是 X，它不真正匹配捕获子串。看几个匹配的测试例子 12/(\\w)((?=111)(1))+/.test(\"1111\") // true/(\\w)((?=111)(1))+/.test(\"2222\") // false 匹配重复 4 次以上的字母或数字可以这么写 12/(\\w)(?=\\1&#123;3,&#125;)/.test(\"AAAAAAAA\") //true/(\\w)(?=\\1&#123;3,&#125;)/.test(\"AAAB\") //false","categories":[{"name":"正则","slug":"正则","permalink":"http://blog.crazylaw.cn/categories/%E6%AD%A3%E5%88%99/"}],"tags":[{"name":"正则","slug":"正则","permalink":"http://blog.crazylaw.cn/tags/%E6%AD%A3%E5%88%99/"}]},{"title":"【Git】- 利用git-hook实现自动化部署","slug":"git-auto-deploy","date":"2018-03-12T03:03:37.000Z","updated":"2021-03-20T16:25:01.805Z","comments":true,"path":"2018/03/12/git-auto-deploy/","link":"","permalink":"http://blog.crazylaw.cn/2018/03/12/git-auto-deploy/","excerpt":"","text":"要求实现 git push 直接完成代码部署到服务器的目录实现方式 利用 git 的 hooks 中的post-receive 来实现代码提交完成之后的动作。将仓库指定一个--work-tree然后进行检出操作checkout --force目录结构 我自己的项目结构是这样的，每一个仓库对应一个项目，例如 public/wx 项目对应 repo/wx.git 仓库 123456789101112131415161718.├── public│ └── wx &#x2F;&#x2F; 这是我们的web代码部署目录│ ├── index.php│ ├── test2.php│ ├── test3.php│ └── test.php└── repo &#x2F;&#x2F; 这个是我们的仓库目录 └── wx.git &#x2F;&#x2F; 这个对应wx项目的仓库 ├── branches ├── config ├── description ├── HEAD ├── hooks &#x2F;&#x2F; post-receive钩子代码写在这里面 ├── index ├── info ├── objects └── refs 再看下 hooks 文件目录 123456789101112.├── applypatch-msg.sample├── commit-msg.sample├── post-commit.sample├── post-receive├── post-receive.sample├── post-update.sample├── pre-applypatch.sample├── pre-commit.sample├── prepare-commit-msg.sample├── pre-rebase.sample└── update.sample 我们将 post-receive.sample 复制一份 post-receive，并且编写代码如下 指定我的代码检出目录1234DIR=/www/public/wxgit --work-tree=$&#123;DIR&#125; clean -fd# 直接强制检出git --work-tree=$&#123;DIR&#125; checkout --force 如何生成目录上面看到的 repo 目录中的 wx.git 实际上是一个裸仓库，我们用下面的命令来生成这样一个仓库。 12cd &#x2F;www&#x2F;repogit init --bare wx.git 对于代码部署目录和仓库我们已经通过 post-receive 进行了关联了，因为我们一旦将代码 push 到仓库，那么会自动检出到 publish/wx 目录下。 远程部署在本地电脑上，我们添加远程仓库 12git initgit remote add origin root@xxx.xxx.xxx.xxx:&#x2F;www&#x2F;repo&#x2F;wx.git 这个时候我们添加了远程仓库，那么我们来测试下 push 操作 1234touch index.phpgit add .git commit -m &#39;test&#39;git push 可能会提示一个–set-upstream，直接执行下就好了。执行完之后我们登陆服务器，会发现文件已经出现在 public/wx/index.php 注意点 如果我们没有配置 ssh 免密码登陆的话，我们需要在 push 代码的时候输入密码 如果我们添加的远程仓库不是 root@xxx.xxx.xx.xx，例如是 abc@xx.xx.xx.xx，那么我们要确保 abc 用户对 wx.git 目录下的文件有 777 权限。 新增仓库 需要登陆远程服务器进行初始化 repo_name.git 仓库 需要手动创建 public/repo_name 文件夹，并且修改权限为 777 需要重新编写 hooks/post-recieve 文件，修改里面的 DIR 路径为 public/repo_name","categories":[{"name":"Git","slug":"Git","permalink":"http://blog.crazylaw.cn/categories/Git/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.crazylaw.cn/tags/Linux/"},{"name":"Git","slug":"Git","permalink":"http://blog.crazylaw.cn/tags/Git/"}]},{"title":"【LeeCode】- Add digital","slug":"算法/leetcode/leecode-1","date":"2018-03-09T10:13:43.000Z","updated":"2021-03-20T16:25:01.821Z","comments":true,"path":"2018/03/09/算法/leetcode/leecode-1/","link":"","permalink":"http://blog.crazylaw.cn/2018/03/09/%E7%AE%97%E6%B3%95/leetcode/leecode-1/","excerpt":"给定一个非负整数 num，重复添加其所有数字，直到结果只有一个数字。 例如： 鉴于 num = 38，这个过程就像：3 + 8 = 11，1 + 1 = 2。由于 2 只有一位数字，请将其返回。 后续工作：你可以在 O（1）运行时没有任何循环/递归吗？ 123function digital($num) &#123; return ($num - 1) % 9 +1;&#125;","text":"给定一个非负整数 num，重复添加其所有数字，直到结果只有一个数字。 例如： 鉴于 num = 38，这个过程就像：3 + 8 = 11，1 + 1 = 2。由于 2 只有一位数字，请将其返回。 后续工作：你可以在 O（1）运行时没有任何循环/递归吗？ 123function digital($num) &#123; return ($num - 1) % 9 +1;&#125; 解析： 假设现在有一个数:1000A+100B+10*C+D，分别表示了前位、百位、十位、个位（A、B、C、D），我们可以分解成 （A+B+C+D）+ （999A + 99B + 9C）。这样子的话，我们得到了 1y &#x3D; （A+B+C+D）+ （999A + 99B + 9C）; （999A + 99B + 9C + D）一定是 9 的倍数，所以 A+B+C+D = y % 9","categories":[{"name":"LeeCode","slug":"LeeCode","permalink":"http://blog.crazylaw.cn/categories/LeeCode/"},{"name":"算法","slug":"LeeCode/算法","permalink":"http://blog.crazylaw.cn/categories/LeeCode/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://blog.crazylaw.cn/tags/%E7%AE%97%E6%B3%95/"},{"name":"LeeCode","slug":"LeeCode","permalink":"http://blog.crazylaw.cn/tags/LeeCode/"}]},{"title":"【基础算法】- 排序 - 堆排序","slug":"算法/algorithm-base-heap-sort","date":"2018-03-02T04:52:14.000Z","updated":"2021-03-20T16:25:01.820Z","comments":true,"path":"2018/03/02/算法/algorithm-base-heap-sort/","link":"","permalink":"http://blog.crazylaw.cn/2018/03/02/%E7%AE%97%E6%B3%95/algorithm-base-heap-sort/","excerpt":"选择排序是一种不稳定的算法。基本思路是：他是将每次 2 个数据都比较完之后，记录一个最大值或者最小值，不急着交换，（这是和冒泡排序的差别，减少了交换的次数，所以效率会高），等一轮循环比较完毕之后，再把无序区最靠近有序区的那个数和当前记录的值交换，每次循环只交换一次。","text":"选择排序是一种不稳定的算法。基本思路是：他是将每次 2 个数据都比较完之后，记录一个最大值或者最小值，不急着交换，（这是和冒泡排序的差别，减少了交换的次数，所以效率会高），等一轮循环比较完毕之后，再把无序区最靠近有序区的那个数和当前记录的值交换，每次循环只交换一次。 1234567891011121314151617181920212223242526272829303132333435class Select&#123; public function __invoke(array $args) &#123; $length = count($args) - 1; for ($i = 0; $i &lt; $length; $i++) &#123; $k = $i; // 1 for ($j = $i + 1; $j &lt; $length; $j++) &#123; if ($args[$j] &lt; $args[$k]) &#123; $k = $j; &#125; &#125; if ($k != $i) &#123; $tmp = $args[$i]; $args[$i] = $args[$k]; $args[$k] = $tmp; &#125;// foreach ($args as $v) &#123;// echo $v . ' - ';// &#125;// echo PHP_EOL; &#125; return $args; &#125;&#125;$data = [3, 5, 1, 2, 8, 6, 7, 9];$sort = new Select();$sortData = $sort($data);print_r($sortData);","categories":[{"name":"算法","slug":"算法","permalink":"http://blog.crazylaw.cn/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://blog.crazylaw.cn/tags/%E7%AE%97%E6%B3%95/"}]},{"title":"【基础算法】- 排序 - 选择排序","slug":"算法/algorithm-base-select-sort","date":"2018-03-01T09:22:01.000Z","updated":"2021-03-20T16:25:01.821Z","comments":true,"path":"2018/03/01/算法/algorithm-base-select-sort/","link":"","permalink":"http://blog.crazylaw.cn/2018/03/01/%E7%AE%97%E6%B3%95/algorithm-base-select-sort/","excerpt":"选择排序是一种不稳定的算法。基本思路是：他是将每次 2 个数据都比较完之后，记录一个最大值或者最小值，不急着交换，（这是和冒泡排序的差别，减少了交换的次数，所以效率会高），等一轮循环比较完毕之后，再把无序区最靠近有序区的那个数和当前记录的值交换，每次循环只交换一次。","text":"选择排序是一种不稳定的算法。基本思路是：他是将每次 2 个数据都比较完之后，记录一个最大值或者最小值，不急着交换，（这是和冒泡排序的差别，减少了交换的次数，所以效率会高），等一轮循环比较完毕之后，再把无序区最靠近有序区的那个数和当前记录的值交换，每次循环只交换一次。 1234567891011121314151617181920212223242526272829303132333435class Select&#123; public function __invoke(array $args) &#123; $length = count($args) - 1; for ($i = 0; $i &lt; $length; $i++) &#123; $k = $i; // 1 for ($j = $i + 1; $j &lt; $length; $j++) &#123; if ($args[$j] &lt; $args[$k]) &#123; $k = $j; &#125; &#125; if ($k != $i) &#123; $tmp = $args[$i]; $args[$i] = $args[$k]; $args[$k] = $tmp; &#125;// foreach ($args as $v) &#123;// echo $v . ' - ';// &#125;// echo PHP_EOL; &#125; return $args; &#125;&#125;$data = [3, 5, 1, 2, 8, 6, 7, 9];$sort = new Select();$sortData = $sort($data);print_r($sortData);","categories":[{"name":"算法","slug":"算法","permalink":"http://blog.crazylaw.cn/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://blog.crazylaw.cn/tags/%E7%AE%97%E6%B3%95/"}]},{"title":"【基础算法】- 排序 - 二分插入排序","slug":"算法/algorithm-base-binary-sort","date":"2018-03-01T07:15:00.000Z","updated":"2021-03-20T16:25:01.820Z","comments":true,"path":"2018/03/01/算法/algorithm-base-binary-sort/","link":"","permalink":"http://blog.crazylaw.cn/2018/03/01/%E7%AE%97%E6%B3%95/algorithm-base-binary-sort/","excerpt":"二分插入排序可以说是直接插入排序的升级版，对于数据量比较大的时候，二分插入排序可以有效的减少比较的次数，从而提高效率。 这里借助了二分的思想，但是需要注意的是，这个只是二分思想，和二分搜索并非一致，如果需要用到二分查询的话，前提必须是有序数据，但是这里只是借助了这个思想，把最靠近有序区的一个通过二分查找需要插入的位置，并且一次性移动所有大于目标数的所有元素。","text":"二分插入排序可以说是直接插入排序的升级版，对于数据量比较大的时候，二分插入排序可以有效的减少比较的次数，从而提高效率。 这里借助了二分的思想，但是需要注意的是，这个只是二分思想，和二分搜索并非一致，如果需要用到二分查询的话，前提必须是有序数据，但是这里只是借助了这个思想，把最靠近有序区的一个通过二分查找需要插入的位置，并且一次性移动所有大于目标数的所有元素。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849class BinSearch&#123; public function __invoke(array $args) &#123; $length = count($args); for ($i = 1; $i &lt; $length; $i++) &#123; $j = $i - 1; $k = $args[$i]; $loc = $this-&gt;_binSearch($args, $k, 0, $j); while ($j &gt;= $loc) &#123; $args[$j + 1] = $args[$j]; $j--; &#125; $args[$j + 1] = $k; &#125; return $args; &#125; public function _binSearch( $args, $key, $low, $high ) &#123; while ($low &lt;= $high) &#123; $mid = ceil(($low + $high) / 2); if ($key &gt; $args[$mid]) &#123; $low = $mid + 1; &#125; else &#123; $high = $mid - 1; &#125; &#125; return $low; &#125;&#125;$data = [3, 5, 1, 2, 8, 6, 7, 9];$sort = new BinSearch();$sortData = $sort($data);print_r($sortData);","categories":[{"name":"算法","slug":"算法","permalink":"http://blog.crazylaw.cn/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://blog.crazylaw.cn/tags/%E7%AE%97%E6%B3%95/"}]},{"title":"【PHP】- php7的新特性","slug":"php7","date":"2018-02-28T15:58:00.000Z","updated":"2021-03-20T16:25:01.813Z","comments":true,"path":"2018/02/28/php7/","link":"","permalink":"http://blog.crazylaw.cn/2018/02/28/php7/","excerpt":"php7 出来的时候，多了哪些特性呢，在这里总结一下 为什么 PHP7 比 PHP5 性能提升了？ 变量存储字节减小，减少内存占用，提升变量操作速度 改善数组结构，数组元素和 hash 映射表被分配在同一块内存里，降低了内存占用、提升了 cpu 缓存命中率（以往的 zval 是一个 24 字节的结构体，现在的固定是 16 个字节） 改进了函数的调用机制，通过优化参数传递的环节，减少了一些指令，提高执行效率 太空船操作符1&lt;&#x3D;&gt; 左边大于右边的时候，返回 1 右边大于左边的时候，返回-1 两边相等的时候，返回 0","text":"php7 出来的时候，多了哪些特性呢，在这里总结一下 为什么 PHP7 比 PHP5 性能提升了？ 变量存储字节减小，减少内存占用，提升变量操作速度 改善数组结构，数组元素和 hash 映射表被分配在同一块内存里，降低了内存占用、提升了 cpu 缓存命中率（以往的 zval 是一个 24 字节的结构体，现在的固定是 16 个字节） 改进了函数的调用机制，通过优化参数传递的环节，减少了一些指令，提高执行效率 太空船操作符1&lt;&#x3D;&gt; 左边大于右边的时候，返回 1 右边大于左边的时候，返回-1 两边相等的时候，返回 0 12345$a = 1;$b = 2;$result = $a &lt;=&gt; $becho $result;// -1 null 合并符1$a = $var['var'] ?? 'null'; 相当于 isset($var[‘var’])?$var:’null’ 强制返回类型123public function myFunc(): int &#123; return 1;&#125; 当一个方法确认数据类型的时候，可以写上强制返回类型，这可以使得 php7 的解析代码逻辑更快。 define 定义数组1define('defineArray',['a' =&gt; 1, 'b' =&gt; 2]); 这个的出现，可以使得你一开始就定义全局静态数组 匿名类123456789$b = new class &#123; public function __invoke() &#123; echo '用后即焚'; &#125;&#125;$b();// 减少引用（还需要回收周期执行：gc_collect_cycles()，手动强制执行gc回收周期）unset($b); 和匿名函数差不多，都可以实现用后即焚，但是这个涉及到 gc 垃圾回收机制，里面有一个引用计数，当引用计数为 0 并且刚好到了 gc 回收周期的时候，该便将就将会销毁 use 集合化123456use some\\namespace\\A;use some\\namespace\\B;use some\\namespace\\C;use some\\namespace\\&#123;A,B,C&#125;; 捕抓致命错误类PHP 5 的 try … catch … finally 无法处理传统错误，如果需要，你通常会考虑用 set_error_handler() 来 Hack 一下。但是仍有很多错误类型是 set_error_handler() 捕捉不到的。PHP 7 引入 Throwable 接口，错误及异常都实现了 Throwable，无法直接实现 Throwable，但可以扩展 \\Exception 和 \\Error 类。可以用 Throwable 捕捉异常跟错误。\\Exception 是所有 PHP 及用户异常的基类；\\Error 是所有内部 PHP 错误的基类。 123456789101112131415161718$name = \"Tony\";try &#123; $name = $name-&gt;method();&#125; catch (\\Error $e) &#123; echo \"出错消息 --- \", $e-&gt;getMessage(), PHP_EOL;&#125;try &#123; $name = $name-&gt;method();&#125; catch (\\Throwable $e) &#123; echo \"出错消息 --- \", $e-&gt;getMessage(), PHP_EOL;&#125;try &#123; intdiv(5, 0);&#125; catch (\\DivisionByZeroError $e) &#123; echo \"出错消息 --- \", $e-&gt;getMessage(), PHP_EOL;&#125; 后续陆续提供 7.1 才有的特性….","categories":[{"name":"PHP","slug":"PHP","permalink":"http://blog.crazylaw.cn/categories/PHP/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"http://blog.crazylaw.cn/tags/PHP/"}]},{"title":"CGI - php-cgi - Fastcgi - php-fpm - nginx的关系","slug":"php-cgi-fastcgi","date":"2018-02-28T14:03:00.000Z","updated":"2021-03-20T16:25:01.812Z","comments":true,"path":"2018/02/28/php-cgi-fastcgi/","link":"","permalink":"http://blog.crazylaw.cn/2018/02/28/php-cgi-fastcgi/","excerpt":"","text":"CGI：CGI 的英文是（COMMON GATEWAY INTERFACE）公共网关接口，它的作用就是帮助服务器与语言通信，这里就是 nginx 和 php 进行通信，因为 nginx 和 php 的语言不通，因此需要一个沟通转换的过程，而 CGI 就是这个沟通的协议。 nginx 服务器在接受到浏览器传递过来的数据后，如果请求的是静态的页面或者图片等无需动态处理的则会直接根据请求的 url 找到其位置然后返回给浏览器，这里无需 php 参与，但是如果是一个动态的页面请求，这个时候 nginx 就必须与 php 通信，这个时候就会需要用到 cgi 协议，将请求数据转换成 php 能理解的信息，然后 php 根据这些信息返回的信息也要通过 cgi 协议转换成 nginx 可以理解的信息，最后 nginx 接到这些信息再返回给浏览器。 fast-cgi：传统的 cgi 协议在每次连接请求时，会开启一个进程进行处理，处理完毕会关闭该进程，因此下次连接，又要再次开启一个进程进行处理，因此有多少个连接就有多少个 cgi 进程，这也就是为什么传统的 cgi 会显得缓慢的原因，因此过多的进程会消耗资源和内存。 而 fast-cgi 则是一个进程可以处理多个请求，和上面的 cgi 协议完全不一样，cgi 是一个进程只能处理一个请求，这样就会导致大量的 cgi 程序，因此会给服务器带来负担。 php-cgi：php-cgi 是 php 提供给 web serve 也就是 http 前端服务器的 cgi 协议接口程序，当每次接到 http 前端服务器的请求都会开启一个 php-cgi 进程进行处理，而且开启的 php-cgi 的过程中会先要重载配置，数据结构以及初始化运行环境，如果更新了 php 配置，那么就需要重启 php-cgi 才能生效，例如 phpstudy 就是这种情况。 php-fpm:php-fpm 是 php 提供给 web serve 也就是 http 前端服务器的 fastcgi 协议接口程序，它不会像 php-cgi 一样每次连接都会重新开启一个进程，处理完请求又关闭这个进程，而是允许一个进程对多个连接进行处理，而不会立即关闭这个进程，而是会接着处理下一个连接。它可以说是 php-cgi 的一个管理程序，是对 php-cgi 的改进。 php-fpm 会开启多个 php-cgi 程序，并且 php-fpm 常驻内存，每次 web serve 服务器发送连接过来的时候，php-fpm 将连接信息分配给下面其中的一个子程序 php-cgi 进行处理，处理完毕这个 php-cgi 并不会关闭，而是继续等待下一个连接，这也是 fast-cgi 加速的原理，但是由于 php-fpm 是多进程的，而一个 php-cgi 基本消耗 7-25M 内存，因此如果连接过多就会导致内存消耗过大，引发一些问题，例如 nginx 里的 502 错误。 同时 php-fpm 还附带一些其他的功能： 例如平滑过渡配置更改，普通的 php-cgi 在每次更改配置后，需要重新启动才能初始化新的配置，而 php-fpm 是不需要，php-fpm 分将新的连接发送给新的子程序 php-cgi，这个时候加载的是新的配置，而原先正在运行的 php-cgi 还是使用的原先的配置，等到这个连接后下一次连接的时候会使用新的配置初始化，这就是平滑过渡。","categories":[{"name":"PHP","slug":"PHP","permalink":"http://blog.crazylaw.cn/categories/PHP/"},{"name":"Nginx","slug":"PHP/Nginx","permalink":"http://blog.crazylaw.cn/categories/PHP/Nginx/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"http://blog.crazylaw.cn/tags/PHP/"},{"name":"Nginx","slug":"Nginx","permalink":"http://blog.crazylaw.cn/tags/Nginx/"}]},{"title":"【协议】http协议1/2的区别","slug":"http-d","date":"2018-02-28T13:34:54.000Z","updated":"2021-03-20T16:25:01.806Z","comments":true,"path":"2018/02/28/http-d/","link":"","permalink":"http://blog.crazylaw.cn/2018/02/28/http-d/","excerpt":"http 协议属于应用层的协议，是我们日常经常打交道的一种协议，属于一种文本协议，http 协议经历了 3 个比较大的版本改变。 http 协议基于 tcp 协议实现（应用层协议基于传输层协议，这个是正常的嘛），为了数据的安全性，有时候可能建立在 tls 或者 ssl 协议之上。 http1.0http1.0 使用的是非持久化连接，就是每次请求，都是一个新的连接，网络 io 较高 非持久化连接 每次请求都是一个独立的连接 http1.1","text":"http 协议属于应用层的协议，是我们日常经常打交道的一种协议，属于一种文本协议，http 协议经历了 3 个比较大的版本改变。 http 协议基于 tcp 协议实现（应用层协议基于传输层协议，这个是正常的嘛），为了数据的安全性，有时候可能建立在 tls 或者 ssl 协议之上。 http1.0http1.0 使用的是非持久化连接，就是每次请求，都是一个新的连接，网络 io 较高 非持久化连接 每次请求都是一个独立的连接 http1.1 http1.1 提供了 Keep-Alive 的参数来设置持久化连接，并且一个连接可以进行多次请求（一个页面加载的时候，css、js 等请求可以在同一个连接下完成），无须重新建立连接，降低了网络 io。并且还允许客户端不用等待上一次请求结果返回，就可以发出下一次请求（但服务器端必须按照接收到客户端请求的先后顺序依次回送响应结果，以保证客户端能够区分出每次请求的响应内容，这样也显著地减少了整个下载过程所需要的时间）。http1.1 开始允许设置 host 请求头字段（使得 web 浏览器可以通过发送 host 来明确服务器要请求的是哪个站点项目），还提供了身份认证，Cache 管理等请求头字段 持久化连接（Keep-Alive） 每个连接可以接受多个请求（但是必须按照请求顺序返回内容） 设置了 host、cache、身份认证等请求头字段 http2.0这是一个质的改变，由文本协议转变程了二进制协议，并且必须建立在 tls 或者 ssl 之上。 二进制协议相对比文本协议来说，更加简洁，传输的成本降低了（带宽），对计算器更加友好，并且更加安全 二进制协议 必须建立在 tls 或者 ssl 之上 IO 多路复用 请求划分优先级 用 HPACK 算法对请求头信息进行了压缩 服务端主动推送消息到客户端","categories":[{"name":"网络","slug":"网络","permalink":"http://blog.crazylaw.cn/categories/%E7%BD%91%E7%BB%9C/"},{"name":"协议","slug":"网络/协议","permalink":"http://blog.crazylaw.cn/categories/%E7%BD%91%E7%BB%9C/%E5%8D%8F%E8%AE%AE/"}],"tags":[{"name":"协议","slug":"协议","permalink":"http://blog.crazylaw.cn/tags/%E5%8D%8F%E8%AE%AE/"},{"name":"网络","slug":"网络","permalink":"http://blog.crazylaw.cn/tags/%E7%BD%91%E7%BB%9C/"}]},{"title":"【Linux】- 定时任务 - crontab","slug":"linux-crontab","date":"2018-02-28T07:13:00.000Z","updated":"2021-03-20T16:25:01.808Z","comments":true,"path":"2018/02/28/linux-crontab/","link":"","permalink":"http://blog.crazylaw.cn/2018/02/28/linux-crontab/","excerpt":"","text":"crontab 定时任务通过 crontab 命令，我们可以在固定的间隔时间执行指定的系统指令或 shell script 脚本。时间间隔的单位可以是分钟、小时、日、月、周及以上的任意组合。这个命令非常适合周期性的日志分析或数据备份等工作。 在 Linux 系统中，Linux 任务调度的工作主要分为以下两类：1、系统执行的工作：系统周期性所要执行的工作，如备份系统数据、清理缓存2、个人执行的工作：某个用户定期要做的工作，例如每隔 10 分钟检查邮件服务器是否有新信，这些工作可由每个用户自行设置 一、/etc/crontab、/etc/cron.deny 、 /etc/cron.allow 文件介绍 系统调度的任务一般存放在/etc/crontab 这个文件下，里面存放了一些系统运行的调度程序，通过命令我们可以看一下里面的内容： 1234567891011121314151617[root@caiwh ~]# cat &#x2F;etc&#x2F;crontabSHELL&#x3D;&#x2F;bin&#x2F;bashPATH&#x3D;&#x2F;sbin:&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;binMAILTO&#x3D;rootHOME&#x3D;&#x2F;# For details see man 4 crontabs# Example of job definition:# .---------------- minute (0 - 59)# | .------------- hour (0 - 23)# | | .---------- day of month (1 - 31)# | | | .------- month (1 - 12) OR jan,feb,mar,apr ...# | | | | .---- day of week (0 - 6) (Sunday&#x3D;0 or 7) OR sun,mon,tue,wed,thu,fri,sat# | | | | |# * * * * * user-name command to be executed 一定要注意默认的环境变量，否则直接写在 crontab -e 写的命令，由于不在当前的默认的环境变量中。所以会可能找不到。. 这些任务都会是系统在运行起来后自动进行调度的。同时在/etc 目录下还存放了/etc/cron.deny 和 /etc/cron.allow 文件 /etc/cron.deny 表示不能使用 crontab 命令的用户 /etc/cron.allow 表示能使用 crontab 的用户。 如果两个文件同时存在，那么/etc/cron.allow 优先。 如果两个文件都不存在，那么只有 root 用户可以安排作业。 命令格式12crontab [-u user] filecrontab [-u user] [ -e | -l | -r ] 命令参数 -u user：用来设定某个用户的 crontab 服务； file：file 是命令文件的名字,表示将 file 做为 crontab 的任务列表文件并载入 crontab。如果在命令行中没有指定这个文件，crontab 命令将接受标准输入（键盘）上键入的命令，并将它们载入 crontab。 -e：编辑某个用户的 crontab 文件内容。如果不指定用户，则表示编辑当前用户的 crontab 文件。 -l：显示某个用户的 crontab 文件内容，如果不指定用户，则表示显示当前用户的 crontab 文件内容。 -r：从/var/spool/cron 目录中删除某个用户的 crontab 文件，如果不指定用户，则默认删除当前用户的 crontab 文件。 -i：在删除用户的 crontab 文件时给确认提示。 crontab 的文件格式分 时 日 月 星期 要运行的命令 第 1 列分钟 0 ～ 59 第 2 列小时 0 ～ 23（0 表示子夜） 第 3 列日 1 ～ 31 第 4 列月 1 ～ 12 第 5 列星期 0 ～ 7（0 和 7 表示星期天） 第 6 列要运行的命令 常用方法 创建一个新的 crontab 文件 向 cron 进程提交一个 crontab 文件之前，首先要设置环境变量 EDITOR。cron 进程根据它来确定使用哪个编辑器编辑 crontab 文件。99%的 UNIX 和 LINUX 用户都使用 vi，如果你也是这样，那么你就编辑$HOME 目录下的. profile 文件，在其中加入这样一行 1EDITOR&#x3D;vi; export EDITOR 然后保存并退出。不妨创建一个名为 cron 的文件，其中是用户名，例如， davecron。在该文件中加入如下的内容。 123# (put your own initials here)echo the date to the console every# 15minutes between 6pm and 6am0,15,30,45 18-06 * * * &#x2F;bin&#x2F;echo &#39;date&#39; &gt; &#x2F;dev&#x2F;console 保存并退出。注意前面 5 个域用空格分隔。 在上面的例子中，系统将每隔 15 分钟向控制台输出一次当前时间。如果系统崩溃或挂起，从最后所显示的时间就可以一眼看出系统是什么时间停止工作的。在有些系统中，用 tty1 来表示控制台，可以根据实际情况对上面的例子进行相应的修改。为了提交你刚刚创建的 crontab 文件，可以把这个新创建的文件作为 cron 命令的参数 1$ crontab davecron 现在该文件已经提交给 cron 进程，它将每隔 1 5 分钟运行一次。同时，新创建文件的一个副本已经被放在/var/spool/cron 目录中，文件名就是用户名(即 dave)。 列出 crontab 文件使用-l 参数列出 crontab 文件 12$ crontab -l0,15,30,45 18-06 * * * &#x2F;bin&#x2F;echo &#96;date&#96; &gt; dev&#x2F;tty1 可以使用这种方法在$HOME 目录中对 crontab 文件做一备份 1$ crontab -l &gt; $HOME&#x2F;mycron 这样，一旦不小心误删了 crontab 文件，可以用上一节所讲述的方法迅速恢复。 编辑 crontab 文件如果希望添加、删除或编辑 crontab 文件中的条目，而 EDITOR 环境变量又设置为 vi，那么就可以用 vi 来编辑 crontab 文件 1$ crontab -e 可以像使用 vi 编辑其他任何文件那样修改 crontab 文件并退出。如果修改了某些条目或添加了新的条目，那么在保存该文件时， cron 会对其进行必要的完整性检查。如果其中的某个域出现了超出允许范围的值，它会提示你。我们在编辑 crontab 文件时，没准会加入新的条目。例如，加入下面的一条 12# DT:delete core files,at 3.30am on 1,7,14,21,26,26 days of each month 30 3 1,7,14,21,26 * * &#x2F;bin&#x2F;find -name &#39;core&#39; -exec rm &#123;&#125; \\; 保存并退出。 note 最好在crontab文件的每一个条目之上加入一条注释，这样就可以知道它的功能、运行时间，更为重要的是，知道这是哪位用户的定时作业。删除 crontab 文件 1$crontab -r 使用实例实例 1：每 1 分钟执行一次 myCommand 1* * * * * myCommand 实例 2：每小时的第 3 和第 15 分钟执行 13,15 * * * * myCommand 实例 3：在上午 8 点到 11 点的第 3 和第 15 分钟执行 13,15 8-11 * * * myCommand 实例 4：每隔两天的上午 8 点到 11 点的第 3 和第 15 分钟执行 1234567 3,15 8-11 *&#x2F;2 * * myCommand&#96;&#96;&#96;实例 5：每周一上午 8 点到 11 点的第 3 和第 15 分钟执行&#96;&#96;&#96;shell 3,15 8-11 * * 1 myCommand 实例 6：每晚的 21:30 重启 smb 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293 30 21 * * * &#x2F;etc&#x2F;init.d&#x2F;smb restart&#96;&#96;&#96;实例7：每月1、10、22日的4 : 45重启smb&#96;&#96;&#96;&#96; 45 4 1,10,22 * * &#x2F;etc&#x2F;init.d&#x2F;smb restart&#96;&#96;&#96;实例 8：每周六、周日的 1 : 10 重启 smb&#96;&#96;&#96; 10 1 * * 6,0 &#x2F;etc&#x2F;init.d&#x2F;smb restart&#96;&#96;&#96;实例9：每天18 : 00至23 : 00之间每隔30分钟重启smb&#96;&#96;&#96; 0,30 18-23 * * * &#x2F;etc&#x2F;init.d&#x2F;smb restart&#96;&#96;&#96;实例 10：每星期六的晚上 11 : 00 pm 重启 smb&#96;&#96;&#96; 0 23 * * 6 &#x2F;etc&#x2F;init.d&#x2F;smb restart&#96;&#96;&#96;实例11：每一小时重启smb&#96;&#96;&#96; * *&#x2F;1 * * * &#x2F;etc&#x2F;init.d&#x2F;smb restart&#96;&#96;&#96;实例12：晚上11点到早上7点之间，每隔一小时重启smb&#96;&#96;&#96; 0 23-7 * * * &#x2F;etc&#x2F;init.d&#x2F;smb restart&#96;&#96;&#96;## 使用注意事项注意环境变量问题&#96;&#96;&#96;有时我们创建了一个crontab，但是这个任务却无法自动执行，而手动执行这个任务却没有问题，这种情况一般是由于在crontab文件中没有配置环境变量引起的。在crontab文件中定义多个调度任务时，需要特别注环境变量的设置，因为我们手动执行某个任务时，是在当前shell环境下进行的，程序当然能找到环境变量，而系统自动执行任务调度时，是不会加载任何环境变量的，因此，就需要在crontab文件中指定任务运行所需的所有环境变量，这样，系统执行任务调度时就没有问题了。不要假定cron知道所需要的特殊环境，它其实并不知道。所以你要保证在shelll脚本中提供所有必要的路径和环境变量，除了一些自动设置的全局变量。所以注意如下3点：1. 脚本中涉及文件路径时写全局路径；2. 脚本执行要用到java或其他环境变量时，通过source命令引入环境变量，如::&#96;&#96;&#96; cat start_cbp.sh !&#x2F;bin&#x2F;sh source &#x2F;etc&#x2F;profile export RUN_CONF&#x3D;&#x2F;home&#x2F;d139&#x2F;conf&#x2F;platform&#x2F;cbp&#x2F;cbp_jboss.conf &#x2F;usr&#x2F;local&#x2F;jboss-4.0.5&#x2F;bin&#x2F;run.sh -c mev &amp;&#96;&#96;&#96;3. 当手动执行脚本OK，但是crontab死活不执行时,很可能是环境变量惹的祸，可尝试在crontab中直接引入环境变量解决问题。&#96;&#96;&#96; 0 * * * * . &#x2F;etc&#x2F;profile;&#x2F;bin&#x2F;sh &#x2F;var&#x2F;www&#x2F;java&#x2F;audit_no_count&#x2F;bin&#x2F;restart_audit.sh&#96;&#96;&#96;注意清理系统用户的邮件日志每条任务调度执行完毕，系统都会将任务输出信息通过电子邮件的形式发送给当前系统用户，这样日积月累，日志信息会非常大，可能会影响系统的正常运行，因此，将每条任务进行重定向处理非常重要。例如，可以在 crontab 文件中设置如下形式，忽略日志输出::&#96;&#96;&#96; 0 *&#x2F;3 * * * &#x2F;usr&#x2F;local&#x2F;apache2&#x2F;apachectl restart &gt;&#x2F;dev&#x2F;null 2&gt;&amp;1&#96;&#96;&#96;&quot;&#x2F;dev&#x2F;null 2&gt;&amp;1&quot;表示先将标准输出重定向到&#x2F;dev&#x2F;null，然后将标准错误重定向到标准输出，由于标准输出已经重定向到了&#x2F;dev&#x2F;null，因此标准错误也会重定向到&#x2F;dev&#x2F;null，这样日志输出问题就解决了。系统级任务调度与用户级任务调度系统级任务调度主要完成系统的一些维护操作，用户级任务调度主要完成用户自定义的一些任务，可以将用户级任务调度放到系统级任务调度来完成（不建议这么做），但是反过来却不行，root用户的任务调度操作可以通过&quot;crontab –uroot –e&quot;来设置，也可以将调度任务直接写入&#x2F;etc&#x2F;crontab文件，需要注意的是，如果要定义一个定时重启系统的任务，就必须将任务放到&#x2F;etc&#x2F;crontab文件，即使在root用户下创建一个定时重启系统的任务也是无效的。其他注意事项新创建的cron job，不会马上执行，至少要过2分钟才执行。如果重启cron则马上执行。当crontab失效时，可以尝试&#x2F;etc&#x2F;init.d&#x2F;crond restart解决问题。或者查看日志看某个job有没有执行&#x2F;报错tail -f &#x2F;var&#x2F;log&#x2F;cron。千万别乱运行crontab -r。它从Crontab目录（&#x2F;var&#x2F;spool&#x2F;cron）中删除用户的Crontab文件。删除了该用户的所有crontab都没了。在crontab中%是有特殊含义的，表示换行的意思。如果要用的话必须进行转义\\%，如经常用的date ‘+%Y%m%d’在crontab里是不会执行的，应该换成date ‘+\\%Y\\%m\\%d’。更新系统时间时区后需要重启cron,在ubuntu中服务名为cron::&#96;&#96;&#96; $service cron restart&#96;&#96;&#96;ubuntu下启动、停止与重启cron::&#96;&#96;&#96; $sudo &#x2F;etc&#x2F;init.d&#x2F;cron start $sudo &#x2F;etc&#x2F;init.d&#x2F;cron stop $sudo &#x2F;etc&#x2F;init.d&#x2F;cron restart&#96;&#96;&#96;Centso系统下管理crond&#96;&#96;&#96;systemctl restart crond&#96;&#96;&#96;","categories":[{"name":"Linux","slug":"Linux","permalink":"http://blog.crazylaw.cn/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.crazylaw.cn/tags/Linux/"},{"name":"Crontab","slug":"Crontab","permalink":"http://blog.crazylaw.cn/tags/Crontab/"}]},{"title":"【Linux命令】- 性能优化","slug":"linux-shell-performance-2","date":"2018-02-28T07:10:06.000Z","updated":"2021-03-20T16:25:01.810Z","comments":true,"path":"2018/02/28/linux-shell-performance-2/","link":"","permalink":"http://blog.crazylaw.cn/2018/02/28/linux-shell-performance-2/","excerpt":"性能优化contents:: 目录 性能优化的核心是找出系统的瓶颈点，问题找到了，优化的工作也就完成了大半；这里介绍的性能优化主要从两个层面来介绍：系统层面和程序层面； 分析系统瓶颈系统响应变慢，首先得定位大致的问题出在哪里，是 IO 瓶颈、CPU 瓶颈、内存瓶颈还是程序导致的系统问题； 使用 top 工具能够比较全面的查看我们关注的点::","text":"性能优化contents:: 目录 性能优化的核心是找出系统的瓶颈点，问题找到了，优化的工作也就完成了大半；这里介绍的性能优化主要从两个层面来介绍：系统层面和程序层面； 分析系统瓶颈系统响应变慢，首先得定位大致的问题出在哪里，是 IO 瓶颈、CPU 瓶颈、内存瓶颈还是程序导致的系统问题； 使用 top 工具能够比较全面的查看我们关注的点:: 1234567891011$toptop - 09:14:56 up 264 days, 20:56, 1 user, load average: 0.02, 0.04, 0.00Tasks: 87 total, 1 running, 86 sleeping, 0 stopped, 0 zombieCpu(s): 0.0%us, 0.2%sy, 0.0%ni, 99.7%id, 0.0%wa, 0.0%hi, 0.0%si, 0.2%stMem: 377672k total, 322332k used, 55340k free, 32592k buffersSwap: 397308k total, 67192k used, 330116k free, 71900k cachedPID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND1 root 20 0 2856 656 388 S 0.0 0.2 0:49.40 init2 root 20 0 0 0 0 S 0.0 0.0 0:00.00 kthreadd3 root 20 0 0 0 0 S 0.0 0.0 7:15.20 ksoftirqd&#x2F;04 root RT 0 0 0 0 S 0.0 0.0 0:00.00 migration&#x2F; 进入交互模式后: - 输入 M，进程列表按内存使用大小降序排序，便于我们观察最大内存使用者使用有问题（检测内存泄漏问题）; - 输入 P，进程列表按 CPU 使用大小降序排序，便于我们观察最耗 CPU 资源的使用者是否有问题； top 第三行显示当前系统的，其中有两个值很关键: - %id：空闲 CPU 时间百分比，如果这个值过低，表明系统 CPU 存在瓶颈； - %wa：等待 I/O 的 CPU 时间百分比，如果这个值过高，表明 IO 存在瓶颈； 分析内存瓶颈查看内存是否存在瓶颈，使用 top 指令看比较麻烦，而 free 命令更为直观:: 1234567891011[&#x2F;home&#x2F;weber#]free total used free shared buffers cachedMem: 501820 452028 49792 37064 5056 136732-&#x2F;+ buffers&#x2F;cache: 310240 191580Swap: 0 0 0[&#x2F;home&#x2F;weber#]toptop - 17:52:17 up 42 days, 7:10, 1 user, load average: 0.02, 0.02, 0.05Tasks: 80 total, 1 running, 79 sleeping, 0 stopped, 0 zombie%Cpu(s): 0.0 us, 0.0 sy, 0.0 ni,100.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 stKiB Mem: 501820 total, 452548 used, 49272 free, 5144 buffersKiB Swap: 0 total, 0 used, 0 free. 136988 cached Mem top 工具显示了 free 工具的第一行所有信息，但真实可用的内存，还需要自己计算才知道;系统实际可用的内存为 free 工具输出第二行的 free+buffer+cached；也就是第三行的 free 值 191580；关于 free 命令各个值的详情解读，请参考这篇文章 :ref:free ; 如果是因为缺少内存，系统响应变慢很明显，因为这使得系统不停的做换入换出的工作; 进一步的监视内存使用情况，可使用 vmstat 工具，实时动态监视操作系统的内存和虚拟内存的动态变化。参考： :ref:vmstat ; 分析 IO 瓶颈如果 IO 存在性能瓶颈，top 工具中的%wa 会偏高； 进一步分析使用 iostat 工具:: 12345678&#x2F;root$iostat -d -x -k 1 1Linux 2.6.32-279.el6.x86_64 (colin) 07&#x2F;16&#x2F;2014 _x86_64_ (4 CPU)Device: rrqm&#x2F;s wrqm&#x2F;s r&#x2F;s w&#x2F;s rkB&#x2F;s wkB&#x2F;s avgrq-sz avgqu-sz await svctm %utilsda 0.02 7.25 0.04 1.90 0.74 35.47 37.15 0.04 19.13 5.58 1.09dm-0 0.00 0.00 0.04 3.05 0.28 12.18 8.07 0.65 209.01 1.11 0.34dm-1 0.00 0.00 0.02 5.82 0.46 23.26 8.13 0.43 74.33 1.30 0.76dm-2 0.00 0.00 0.00 0.01 0.00 0.02 8.00 0.00 5.41 3.28 0.00 如果%iowait 的值过高，表示硬盘存在 I/O 瓶颈。 如果 %util 接近 100%，说明产生的 I/O 请求太多，I/O 系统已经满负荷，该磁盘可能存在瓶颈。 如果 svctm 比较接近 await，说明 I/O 几乎没有等待时间； 如果 await 远大于 svctm，说明 I/O 队列太长，io 响应太慢，则需要进行必要优化。 如果 avgqu-sz 比较大，也表示有大量 io 在等待。 更多参数说明请参考 :ref:iostat ; 分析进程调用通过 top 等工具发现系统性能问题是由某个进程导致的之后，接下来我们就需要分析这个进程；继续查询问题在哪； 这里我们有两个好用的工具：pstack 和 pstrace pstack 用来跟踪进程栈，这个命令在排查进程问题时非常有用，比如我们发现一个服务一直处于 work 状态（如假死状态，好似死循环），使用这个命令就能轻松定位问题所在；可以在一段时间内，多执行几次 pstack，若发现代码栈总是停在同一个位置，那个位置就需要重点关注，很可能就是出问题的地方； 示例：查看 bash 程序进程栈:: 123456789101112131415161718&#x2F;opt&#x2F;app&#x2F;tdev1$ps -fe| grep bashtdev1 7013 7012 0 19:42 pts&#x2F;1 00:00:00 -bashtdev1 11402 11401 0 20:31 pts&#x2F;2 00:00:00 -bashtdev1 11474 11402 0 20:32 pts&#x2F;2 00:00:00 grep bash&#x2F;opt&#x2F;app&#x2F;tdev1$pstack 7013#0 0x00000039958c5620 in __read_nocancel () from &#x2F;lib64&#x2F;libc.so.6#1 0x000000000047dafe in rl_getc ()#2 0x000000000047def6 in rl_read_key ()#3 0x000000000046d0f5 in readline_internal_char ()#4 0x000000000046d4e5 in readline ()#5 0x00000000004213cf in ?? ()#6 0x000000000041d685 in ?? ()#7 0x000000000041e89e in ?? ()#8 0x00000000004218dc in yyparse ()#9 0x000000000041b507 in parse_command ()#10 0x000000000041b5c6 in read_command ()#11 0x000000000041b74e in reader_loop ()#12 0x000000000041b2aa in main () 而 strace 用来跟踪进程中的系统调用；这个工具能够动态的跟踪进程执行时的系统调用和所接收的信号。是一个非常有效的检测、指导和调试工具。系统管理员可以通过该命令容易地解决程序问题。 参考： :ref:strace ; 优化程序代码优化自己开发的程序，建议采用以下准则:: 二八法则：在任何一组东西中，最重要的只占其中一小部分，约 20%，其余 80%的尽管是多数，却是次要的；在优化实践中，我们将精力集中在优化那 20%最耗时的代码上，整体性能将有显著的提升；这个很好理解。函数 A 虽然代码量大，但在一次正常执行流程中，只调用了一次。而另一个函数 B 代码量比 A 小很多，但被调用了 1000 次。显然，我们更应关注 B 的优化。 编完代码，再优化；编码的时候总是考虑最佳性能未必总是好的；在强调最佳性能的编码方式的同时，可能就损失了代码的可读性和开发效率； gprof 使用步骤 用 gcc、g++、xlC 编译程序时，使用-pg 参数，如：g++ -pg -o test.exe test.cpp 编译器会自动在目标代码中插入用于性能测试的代码片断，这些代码在程序运行时采集并记录函数的调用关系和调用次数，并记录函数自身执行时间和被调用函数的执行时间。 执行编译后的可执行程序，如：./test.exe。该步骤运行程序的时间会稍慢于正常编译的可执行程序的运行时间。程序运行结束后，会在程序所在路径下生成一个缺省文件名为 gmon.out 的文件，这个文件就是记录程序运行的性能、调用关系、调用次数等信息的数据文件。 使用 gprof 命令来分析记录程序运行信息的 gmon.out 文件，如：gprof test.exe gmon.out 则可以在显示器上看到函数调用相关的统计、分析信息。上述信息也可以采用 gprof test.exe gmon.out&gt; gprofresult.txt 重定向到文本文件以便于后续分析。 关于 gprof 的使用案例，请参考 [f1]_ ; 其它工具调试内存泄漏的工具 valgrind，感兴趣的朋友可以 google 了解； OProfile: Linux 平台上的一个功能强大的性能分析工具,使用参考 [f2]_ ; 除了上面介绍的工具，还有一些比较全面的性能分析工具，比如 sar（Linux 系统上默认不安装，需要手动安装下）；将 sar 的常驻监控工具打开后，能够收集比较全面的性能分析数据； 关于 sar 的使用，参考 :ref:sar ; .. [f1] C++的性能优化实践 http://www.cnblogs.com/me115/archive/2013/06/05/3117967.html.. [f2] 用 OProfile 彻底了解性能 http://www.ibm.com/developerworks/cn/linux/l-oprof/.. [f3] Linux 上的 free 命令详解 http://www.cnblogs.com/coldplayerest/archive/2010/02/20/1669949.html","categories":[{"name":"Linux","slug":"Linux","permalink":"http://blog.crazylaw.cn/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.crazylaw.cn/tags/Linux/"},{"name":"Shell","slug":"Shell","permalink":"http://blog.crazylaw.cn/tags/Shell/"}]},{"title":"【Linux命令】- 性能监控","slug":"linux-shell-performance","date":"2018-02-28T07:00:50.000Z","updated":"2021-03-20T16:25:01.810Z","comments":true,"path":"2018/02/28/linux-shell-performance/","link":"","permalink":"http://blog.crazylaw.cn/2018/02/28/linux-shell-performance/","excerpt":"使用操作系统的过程中，我们经常需要查看当前的性能如何，需要了解 CPU、内存和硬盘的使用情况；本节介绍的这几个工具能满足日常工作要求； 监控 CPU1top 查询内存1free -m","text":"使用操作系统的过程中，我们经常需要查看当前的性能如何，需要了解 CPU、内存和硬盘的使用情况；本节介绍的这几个工具能满足日常工作要求； 监控 CPU1top 查询内存1free -m 当系统中 sar 不可用时(sar 最好用)，可以使用以下工具替代：linux 下有 vmstat、Unix 系统有 prstat eg：查看 cpu、内存、使用情况：vmstat n m （n 为监控频率、m 为监控次数） 123456[&#x2F;home&#x2F;weber#]vmstat 1 3procs -----------memory---------- ---swap-- -----io---- -system-- ----cpu----r b swpd free buff cache si so bi bo in cs us sy id wa0 0 86560 42300 9752 63556 0 1 1 1 0 0 0 0 99 01 0 86560 39936 9764 63544 0 0 0 52 66 95 5 0 95 00 0 86560 42168 9772 63556 0 0 0 20 127 231 13 2 84 0 使用 watch 工具监控变化当需要持续的监控应用的某个数据变化时，watch 工具能满足要求；执行 watch 命令后，会进入到一个界面，输出当前被监控的数据，一旦数据变化，便会高亮显示变化情况； eg：操作 redis 时，监控内存变化： 123456789$watch -d -n 1 &#39;.&#x2F;redis-cli info | grep memory&#39;(以下为watch工具中的界面内容，一旦内存变化，即实时高亮显示变化）Every 1.0s: .&#x2F;redis-cli info | grep memory Mon Apr 28 16:10:36 2014used_memory:45157376used_memory_human:43.07Mused_memory_rss:47628288used_memory_peak:49686080used_memory_peak_human:47.38M 总结top / sar / free / watch","categories":[{"name":"Linux","slug":"Linux","permalink":"http://blog.crazylaw.cn/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.crazylaw.cn/tags/Linux/"},{"name":"Shell","slug":"Shell","permalink":"http://blog.crazylaw.cn/tags/Shell/"}]},{"title":"【Linux命令】- 进程管理工具","slug":"linux-shell-processor","date":"2018-02-28T06:50:53.000Z","updated":"2021-03-20T16:25:01.810Z","comments":true,"path":"2018/02/28/linux-shell-processor/","link":"","permalink":"http://blog.crazylaw.cn/2018/02/28/linux-shell-processor/","excerpt":"查询进程123456789101112ps -ef// 查看某用户的进程ps -u phper// 查看进程完整信息ps -ajx// 显示进程信息，并实时更新top","text":"查询进程123456789101112ps -ef// 查看某用户的进程ps -u phper// 查看进程完整信息ps -ajx// 显示进程信息，并实时更新top 分析线程栈1pmap PID","categories":[{"name":"Linux","slug":"Linux","permalink":"http://blog.crazylaw.cn/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.crazylaw.cn/tags/Linux/"},{"name":"Shell","slug":"Shell","permalink":"http://blog.crazylaw.cn/tags/Shell/"}]},{"title":"【Linux命令】- 磁盘管理","slug":"linux-shell-disk","date":"2018-02-28T06:15:00.000Z","updated":"2021-03-20T16:25:01.809Z","comments":true,"path":"2018/02/28/linux-shell-disk/","link":"","permalink":"http://blog.crazylaw.cn/2018/02/28/linux-shell-disk/","excerpt":"df 查看磁盘空间 -h 友好的方式显示（默认是 bytes） 1df -h du 查看目录所占空间大小 -s 递归目录总和 -h 友好的方式显示（默认是 bytes） 1du -sh","text":"df 查看磁盘空间 -h 友好的方式显示（默认是 bytes） 1df -h du 查看目录所占空间大小 -s 递归目录总和 -h 友好的方式显示（默认是 bytes） 1du -sh 查看当前目录所有“文件（包含目录）”的大小，并且按照大到小排序，显示前 3 个 12345du -s * | sort -nr | head -3ordu -s * | sort -n | tail -3 打包/压缩打包是将多个文件归并到一个文件:: 1tar -cvf etc.tar /etc &lt;==仅打包，不压缩! -c :打包选项 -v :显示打包进度 -f :使用档案文件注：有的系统中指定参数时不需要在前面加上-，直接使用 tar xvf 压缩1gzip demo.txt 解包 -z 解压 gz 文件 -j 解压 bz2 文件 -J 解压 xz 文件 1tar -xvf demo.tar 12常用：tar -zxvf demo.tar 总结查看磁盘空间 df -h 查看目录大小 du -sh 打包 tar -cvf 解包 tar -xvf(z/j/J) 压缩 gzip 解压缩 gunzip bzip","categories":[{"name":"Linux","slug":"Linux","permalink":"http://blog.crazylaw.cn/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.crazylaw.cn/tags/Linux/"},{"name":"Shell","slug":"Shell","permalink":"http://blog.crazylaw.cn/tags/Shell/"}]},{"title":"【Linux命令】- 文本处理","slug":"linux-shell-text","date":"2018-02-27T09:29:00.000Z","updated":"2021-03-20T16:25:01.810Z","comments":true,"path":"2018/02/27/linux-shell-text/","link":"","permalink":"http://blog.crazylaw.cn/2018/02/27/linux-shell-text/","excerpt":"find 文件查找find 默认是会把所有目录和文件都一起输出，除非加了type -f，此时就只会输出文件，type -d，此时就只会输出目录 查找 txt 和 pdf 文件 1234find ./ \\( -name \"*.php\" -o -name \"*.p\" \\)find ./ \\( -name \"*.php\" -or -name \"*.p\" \\)// 同时包含命名find ./ \\( -name \"*.php\" -and -name \"*.p\" \\) 正则方式查找.txt 和.pdf 1find ./ -regex \".*\\(php\\|p\\)\" iregex：忽略大小写的正则 否定参数，查找所有非 txt 文件 1find ./ -name \"*.txtx\" 指定搜索深度,打印出当前目录的文件（深度为 1，默认为 1）: 1find ./ -maxdepth 1 -type f","text":"find 文件查找find 默认是会把所有目录和文件都一起输出，除非加了type -f，此时就只会输出文件，type -d，此时就只会输出目录 查找 txt 和 pdf 文件 1234find ./ \\( -name \"*.php\" -o -name \"*.p\" \\)find ./ \\( -name \"*.php\" -or -name \"*.p\" \\)// 同时包含命名find ./ \\( -name \"*.php\" -and -name \"*.p\" \\) 正则方式查找.txt 和.pdf 1find ./ -regex \".*\\(php\\|p\\)\" iregex：忽略大小写的正则 否定参数，查找所有非 txt 文件 1find ./ -name \"*.txtx\" 指定搜索深度,打印出当前目录的文件（深度为 1，默认为 1）: 1find ./ -maxdepth 1 -type f 定制搜索 按类型搜索 1find . -type d -print //只列出所有目录 -type f 文件 / l 符号链接 / d 目录 find 支持的文件检索类型可以区分普通文件和符号链接、目录等，但是二进制文件和文本文件无法直接通过 find 的类型区分出来； 按大小搜索：w 字 k M G寻找大于 2k 的文件 1find . -type f -size +2k 按权限查找 1find . -type f -perm 644 -print //找具有可执行权限的所有文件 按用户查找 1find . -type f -user weber -print// 找用户weber所拥有的文件 执行动作（强大的 exec）将当前目录下的所有权变更为 weber:: 1find . -type f -user root -exec chown weber &#123;&#125; \\; 注：{}是一个特殊的字符串，对于每一个匹配的文件，{}会被替换成相应的文件名； 将找到的文件全都 copy 到另一个目录:: 1find . -type f -mtime +10 -name \"*.txt\" -exec cp &#123;&#125; OLD \\; grep 文本搜索1grep match_patten file // 默认访问匹配行 常用参数 -o 只输出匹配的文本行 VS -v 只输出没有匹配的文本行 -c 统计文件中包含文本的次数grep -c “text” filename -n 打印匹配的行号 -i 搜索时忽略大小写 -l 只打印文件名 -A 打印搜索到目标的前几行 -B 打印搜索到目标的后几行 -C 打印搜索到目标的前后几行 在多级目录中对文本递归搜索(程序员搜代码的最爱） 12// 在多级目录下搜索文本内容包含class的，并且打印行数，并且显示前后3行grep \"class\" . -R -n -C 3 综合应用：将日志中的所有带 where 条件的 sql 查找查找出来 1cat *sql.log | tr [a-z] [A-Z] | grep \"FROM\" | grep \"WHERE\" &gt; temp.log xargs 命令行参数转换单行输出 1cat file.txt | xargs 单行之后再以 n 个元素为一行 1cat file.txt | -n 2 xargs 参数说明 -d 定义定界符 （默认为空格 多行的定界符为 \\n） -n 指定输出为多行 -I {} 指定替换字符串，这个字符串在 xargs 扩展时会被替换掉,用于待执行的命令需要多个参数时 -0：指定\\0 为输入定界符 sort 排序 -n 按数字进行排序 VS -d 按字典序进行排序 -r 逆序排序 -k N 指定按第 N 列排序 -R 随机排列 -f 忽略大小写 -b 忽略前置无用 12345// 升序排列sort a.p// 随机排列sort -R a.p uniq 重复行操作 -d 只显示重复行 -u 只显示不重复的行 -c 覆盖重复，但是显示每行记录的重复数，默认为：1 -i 忽略大小写 可指定每行中需要比较的重复内容：-s 开始位置 -w 比较字符数 1sort unsort.txt | uniq tr sed 简化版注意[set1]和[set2]是单字符一对一替换 PS：关键是默认是单字符操作，除非加了-s -d 删除某些字符 -s 删除某个字符串 -c 获取文本中指定的字符 123echo 12345 | tr '0-9' '9876543210' //加解密转换，替换对应字符cat text| tr '\\t' ' ' //制表符转空格 tr 中可用各种字符类： * alnum：字母和数字 * alpha：字母 * digit：数字 * space：空白字符 * lower：小写 * upper：大写 * cntrl：控制（非可打印）字符 * print：可打印字符使用方法：tr [:class:] [:class:] 12// 小写替换成大写tr '[:lower:]' '[:upper:]' sed 文本替换利器格式： 1sed [options] &#39;[command]&lt;content&gt;[command2]&#39; file options： -i：替换原本的内容 command： s：替换文本: s/{text}/{replatce_text}/[command2] command2： g：全局匹配 Ng：N = \\d：从第 N 列开始逐行匹配 12echo sksksksksksk | sed 's/sk/SK/2g'skSKSKSKSKSK d：删除操作 1234567891011删除空白行：sed '/^$/d' file删除文件的第2行：sed '2d' file删除文件的第2行到末尾所有行：sed '2,$d' file","categories":[{"name":"Linux","slug":"Linux","permalink":"http://blog.crazylaw.cn/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.crazylaw.cn/tags/Linux/"},{"name":"Shell","slug":"Shell","permalink":"http://blog.crazylaw.cn/tags/Shell/"}]},{"title":"【Linux命令】- 文件及目录管理","slug":"linux-shell-file-dir","date":"2018-02-27T08:04:00.000Z","updated":"2021-03-20T16:25:01.809Z","comments":true,"path":"2018/02/27/linux-shell-file-dir/","link":"","permalink":"http://blog.crazylaw.cn/2018/02/27/linux-shell-file-dir/","excerpt":"创建和删除 创建：mkdir 删除：rm 删除目录（递归+强制）：rm -rf dir 删除文件（强制） rm -f file 删除统一后缀 rm *log(等价于：find ./ -name &quot;*log&quot; -exec rm -f {}，find ./ -name &quot;*.log&quot; -exec rm -f {} \\;)(注意，\\;必须加，否则报错) 移动：mv file 复制（目录）：cp（-r） file[dir] 查看当前目录下的文件个数 find ./ | wc -l 复制目录：","text":"创建和删除 创建：mkdir 删除：rm 删除目录（递归+强制）：rm -rf dir 删除文件（强制） rm -f file 删除统一后缀 rm *log(等价于：find ./ -name &quot;*log&quot; -exec rm -f {}，find ./ -name &quot;*.log&quot; -exec rm -f {} \\;)(注意，\\;必须加，否则报错) 移动：mv file 复制（目录）：cp（-r） file[dir] 查看当前目录下的文件个数 find ./ | wc -l 复制目录： cp -r source_dir dest_dir 目录切换 跳转：cd 切换到上一个工作目录：cd - 切换到 home 目录：cd ~ 显示当前目录：pwd 列出目录项 现实当前目录下的所有文件：ls 按时间排序降序，以列表的方式现实目录项：ls -lrt（l：列表，t：时间升序，r：排列反序） 显示隐藏目录、列表显示方式、分页显示（限制 12 行）：ls -al | more -12 给每项文件前面增加一个 id 编号：ls -l | cat -n 查看文件内容 查看前面的内容(n 默认 10)：head [-n] &lt;file&gt; 查看后面的内容(n 默认 10)：tail [-n] &lt;file&gt; 查看 2 个文件的差别：diff &lt;file1&gt; &lt;file2&gt; 文件与目录权限修改 改变文件的拥有者：chown 改变文件读、写、执行等属性：chmod 递归子目录修改：chown -R tuxapp source/ 增加脚本可执行权限： chmod a+x myscript 给文件增加别名（软链接/硬链接）12ln cc ccAgain :硬连接；删除一个，将仍能找到；ln -s cc ccTo :符号链接(软链接)；删除源，另一个无法使用；（后面一个ccTo 为新建的文件） 重定向 重定向标准输出到文件：cat foo &gt; foo.txt 重定向标准错误到文件：cat foo 2&gt; foot.txt 重定向标准输出到标准错误：cat foo 1&gt;&amp;2 重定向标准错误到标准输出：cat foo 2&gt;&amp;1 重定向标准输出到标准错误到同一个文件：cat foo 1&gt;&amp;2 foo.txt 或者(也不能说或者，只是效果一样) cat foo &amp;&gt; foo.txt 清空文件：:&gt; foo.php 追加内容到文件：echo a &gt;&gt; foo.php 覆盖内容到文件：echo a &gt; foo.php 综合应用查找 record.log 中包含 AAA，但不包含 BBB 的记录的总数: 1234567# record.logAAABBBCCCAAACCCBBBCCCCCCAAABBB cat record.log | grep &quot;AAA&quot; | grep -v &quot;BBB&quot; | wc -l","categories":[{"name":"Linux","slug":"Linux","permalink":"http://blog.crazylaw.cn/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.crazylaw.cn/tags/Linux/"},{"name":"Shell","slug":"Shell","permalink":"http://blog.crazylaw.cn/tags/Shell/"}]},{"title":"【基础算法】- 排序 - 直接插入排序","slug":"算法/algorithm-base-insert","date":"2018-02-24T05:52:00.000Z","updated":"2021-03-20T16:25:01.820Z","comments":true,"path":"2018/02/24/算法/algorithm-base-insert/","link":"","permalink":"http://blog.crazylaw.cn/2018/02/24/%E7%AE%97%E6%B3%95/algorithm-base-insert/","excerpt":"说明直接插入排序算法 稳定排序 PHP 实现123456789101112131415161718192021222324class Insert&#123; public function __invoke(array $args) &#123; $length = count($args); for ($i = 1; $i &lt; $length; $i++) &#123; if ($args[$i] &lt; $args[$i - 1]) &#123; $k = $args[$i]; for ($j = $i - 1; $j &gt;= 0 &amp;&amp; $args[$j] &gt; $k; $j--) &#123; $args[$j + 1] = $args[$j]; &#125; $args[$j + 1] = $k; &#125; &#125; return $args; &#125;&#125;$data = [3, 5, 1, 2, 8, 6, 7, 9];$sort = new Insert();$sortData = $sort($data);print_r($sortData);","text":"说明直接插入排序算法 稳定排序 PHP 实现123456789101112131415161718192021222324class Insert&#123; public function __invoke(array $args) &#123; $length = count($args); for ($i = 1; $i &lt; $length; $i++) &#123; if ($args[$i] &lt; $args[$i - 1]) &#123; $k = $args[$i]; for ($j = $i - 1; $j &gt;= 0 &amp;&amp; $args[$j] &gt; $k; $j--) &#123; $args[$j + 1] = $args[$j]; &#125; $args[$j + 1] = $k; &#125; &#125; return $args; &#125;&#125;$data = [3, 5, 1, 2, 8, 6, 7, 9];$sort = new Insert();$sortData = $sort($data);print_r($sortData); 直接插入排序：顾名思义，直接插入排序，是从前往后排出一个有序区间，最靠近区间的一个数在有序区间中从后往前不断比较，如果待插入的数小于当前的数，不断向后移动一个元素。直到待插入的数大于当前有序区间中的数 我们现在有一批数字：[3, 5, 1, 2, 8, 6, 7, 9] 有序区间为： [|3| , 5 , 1 , 2 , 8 , 6 , 7 , 9] 然后拿到 5 去和|3|里面的数比较，发现 5&gt;3 所以无需动。 [|3 ,5| , 1 , 2 , 8, 6 , 7 , 9] 然后拿到 1 去和|3,5|比较，发现 5&gt;1，然后 5 向后移动一位，然后发现 3&gt;1，然后 3 向后移动一位，然后到了尽头了，所以有序区间固定了。 [|1 , 3 , 5| , 2 , 8, 6 , 7 , 9] 同理…（时间有限，不写了，逃！） 然后整个流程下来，就完成了排序了，可以看到我们有一个稳定的有序区间，所以我们把整个算法叫做稳定算法。","categories":[{"name":"算法","slug":"算法","permalink":"http://blog.crazylaw.cn/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://blog.crazylaw.cn/tags/%E7%AE%97%E6%B3%95/"}]},{"title":"【基础算法】- 排序 - 快排","slug":"算法/algorithm-base-quick","date":"2018-02-24T05:46:00.000Z","updated":"2021-03-20T16:25:01.821Z","comments":true,"path":"2018/02/24/算法/algorithm-base-quick/","link":"","permalink":"http://blog.crazylaw.cn/2018/02/24/%E7%AE%97%E6%B3%95/algorithm-base-quick/","excerpt":"说明快排算法，比普通的冒泡排序要快，原因说明将在时间复杂度和空间复杂度的文章中说明。 PHP 实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869&lt;?php/** * Created by PhpStorm. * User: admin * Date: 2018/2/24 * Time: 9:39 */class Quick&#123; public function __invoke(array $args) &#123; $length = count($args); if ($length &lt; 1) &#123; return; &#125; $this-&gt;quickSort($args, 0, $length - 1); return $args; &#125; public function quickSort(array &amp;$args, int $low, int $high) &#123; if ($low &gt; $high) &#123; return; &#125; $frist = $low; $last = $high; $k = $args[$low];// $i = 0; while ($frist &lt; $last) &#123;// $i++; while ($frist &lt; $last &amp;&amp; $args[$last] &gt;= $k) &#123; $last--; &#125; $args[$frist] = $args[$last];// echo '[' . $i . ']last -- : $frist ' . $frist . ' . $last ' . $last . PHP_EOL;// foreach ($args as $value)&#123;// echo $value . ' ';// &#125; echo PHP_EOL; while ($frist &lt; $last &amp;&amp; $args[$frist] &lt;= $k) &#123; $frist++; &#125; $args[$last] = $args[$frist];// echo '[' . $i . ']last -- : $frist ' . $frist . ' . $last ' . $last . PHP_EOL;// foreach ($args as $value)&#123;// echo $value . ' ';// &#125;// echo PHP_EOL; &#125; $args[$frist] = $k;// print_r($args);// exit; $this-&gt;quickSort($args, $low, $frist - 1); $this-&gt;quickSort($args, $frist + 1, $high); &#125;&#125;$data = [3, 5, 1, 2, 8, 6, 7, 9];$sort = new Quick();$sortData = $sort($data);print_r($sortData);","text":"说明快排算法，比普通的冒泡排序要快，原因说明将在时间复杂度和空间复杂度的文章中说明。 PHP 实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869&lt;?php/** * Created by PhpStorm. * User: admin * Date: 2018/2/24 * Time: 9:39 */class Quick&#123; public function __invoke(array $args) &#123; $length = count($args); if ($length &lt; 1) &#123; return; &#125; $this-&gt;quickSort($args, 0, $length - 1); return $args; &#125; public function quickSort(array &amp;$args, int $low, int $high) &#123; if ($low &gt; $high) &#123; return; &#125; $frist = $low; $last = $high; $k = $args[$low];// $i = 0; while ($frist &lt; $last) &#123;// $i++; while ($frist &lt; $last &amp;&amp; $args[$last] &gt;= $k) &#123; $last--; &#125; $args[$frist] = $args[$last];// echo '[' . $i . ']last -- : $frist ' . $frist . ' . $last ' . $last . PHP_EOL;// foreach ($args as $value)&#123;// echo $value . ' ';// &#125; echo PHP_EOL; while ($frist &lt; $last &amp;&amp; $args[$frist] &lt;= $k) &#123; $frist++; &#125; $args[$last] = $args[$frist];// echo '[' . $i . ']last -- : $frist ' . $frist . ' . $last ' . $last . PHP_EOL;// foreach ($args as $value)&#123;// echo $value . ' ';// &#125;// echo PHP_EOL; &#125; $args[$frist] = $k;// print_r($args);// exit; $this-&gt;quickSort($args, $low, $frist - 1); $this-&gt;quickSort($args, $frist + 1, $high); &#125;&#125;$data = [3, 5, 1, 2, 8, 6, 7, 9];$sort = new Quick();$sortData = $sort($data);print_r($sortData); 快速排序：顾名思义，快速排序的基本思路是分治的思路，为什么这么说呢，因为他是一个整体排序的过程，但是他把数据分成了子数据段的排序 相信很多人看到这里还是一头雾水，我们按照这个说法，看看上面的例子。 我们现在有一批数字：[3, 5, 1, 2, 8, 6, 7, 9] 我们拿到基本数据元素:3。从后开始往前看，直到找到比 3 小的数据，我从 9-7-6-8-2(下标索引 j:7-6-5-4-3。j=3)看到 2 比 3 小，这个时候 3、2 交换位置。 [2,5,1,3,8,6,7,9] 然后从前开始往后看，直到找到比 3 大的数据我从 2-5（下标索引 i：0，1。i=1）看到 5 比 3 大，这个时候 5、3 交换位置。 [2,3,1,5,8,6,7,9] 这个时候，我们发现 j 不等于 i，所以需要计算计算，直到 i=j 位置。 这个时候（j=3，i=1）。我们按照从后开始往前找的规律。从 5-1（下标索引 j：3-2。j=2）看到 1 比 3 小，这个时候，1，3 交换位置。 [2,1,3,5,8,6,7,9] 这个时候（j=2，i=1）。我们按照从前开始往后找的规律。当读 1 的时候（i=j=2），已经符合了我们的 j=i，这个时候，说明已经完成了一轮循环。 以上这个过程称之为一轮循环（从后面开始找小的，从前面开始找大的）,这个时候,会发现，基础数据 3 左边都是小于 3 的，右边都是大于 3 的。以此循环，就是快排的基本思路。 如果想看数据变动的过程，可以把注释删掉，就可以看到每一次数据变动的情况了。","categories":[{"name":"算法","slug":"算法","permalink":"http://blog.crazylaw.cn/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://blog.crazylaw.cn/tags/%E7%AE%97%E6%B3%95/"}]},{"title":"【基础算法】- 排序 - 冒泡","slug":"算法/algorithm-base-bubble","date":"2018-02-24T04:13:00.000Z","updated":"2021-03-20T16:25:01.820Z","comments":true,"path":"2018/02/24/算法/algorithm-base-bubble/","link":"","permalink":"http://blog.crazylaw.cn/2018/02/24/%E7%AE%97%E6%B3%95/algorithm-base-bubble/","excerpt":"说明感觉一些基础的排序算法，貌似我的博客并没有说明，虽然我也很多了，但是还会记录一下，和大家分享一下，并且终结一下我每次的写排序算法的心得。 先贴代码，再讲原理。 PHP 实现123456789101112131415161718192021222324class Bubble&#123; public function __invoke(array $args) &#123; $length = count($args); for ($i = 0; $i &lt; $length - 1; $i++) &#123; for ($j = 0; $j &lt; $length - 1 - $i; $j++) &#123; if ($args[$j] &gt; $args[$j + 1]) &#123; $tmp = $args[$j]; $args[$j] = $args[$j + 1]; $args[$j + 1] = $tmp; &#125; &#125; &#125; return $args; &#125;&#125;$data = [3, 5, 1, 2, 8, 6, 7, 9];$sort = new Bubble();$sortData = $sort($data);print_r($sortData);","text":"说明感觉一些基础的排序算法，貌似我的博客并没有说明，虽然我也很多了，但是还会记录一下，和大家分享一下，并且终结一下我每次的写排序算法的心得。 先贴代码，再讲原理。 PHP 实现123456789101112131415161718192021222324class Bubble&#123; public function __invoke(array $args) &#123; $length = count($args); for ($i = 0; $i &lt; $length - 1; $i++) &#123; for ($j = 0; $j &lt; $length - 1 - $i; $j++) &#123; if ($args[$j] &gt; $args[$j + 1]) &#123; $tmp = $args[$j]; $args[$j] = $args[$j + 1]; $args[$j + 1] = $tmp; &#125; &#125; &#125; return $args; &#125;&#125;$data = [3, 5, 1, 2, 8, 6, 7, 9];$sort = new Bubble();$sortData = $sort($data);print_r($sortData); 冒泡排序：顾名思义，这个意思就是像泡泡一样一层层的向上浮（针对升序：从小到大排序，首先确定的是最大值），基本思路就是两两比较，只记得两者之间大的值，直到最后一次比较，即可确定该值为最大值，这样子就完成了一次循环比较，剩下的数字也同样的思路循环下去即可。 按照这个说法，我们来看上面的例子。 我们现在有一批数字：[3, 5, 1, 2, 8, 6, 7, 9] 第一次比较:3,5。发现 5 比 3 大，无需交换位置。 3，5，1，2，8，6，7，9 第二次比较：5，1。发现 1 比 5 小，交换位置。 3，1，5，2，8，6，7，9 第三次比较：5，2。发现 2 比 5 小，交换位置。 3，1，2，5，8，6，7，9 第四次比较：5，8。发现 8 比 5 大，无需交换位置。 3，1，2，5，8，6，7，9 第五次比较：8，6。发现 6 比 8 小，交换位置。 3，1，2，5，6，8，7，9 第六次比较：8，7。发现 7 比 8 小，交换位置。 3，1，2，5，6，7，8，9 第 7 次比较：8，9。发现 9 比 8 大，无需交换位置。 完成了第一轮比较，从而得出了9是这批数据中的最大值。 第二轮（不用比较 9，已经确定最大值），第三轮（不用比较 8，9 已经确认最大值和第二大的值）…直到第 7 轮（少了一轮是因为，不用比较最小值，因为已经比较得出了最大的 7 位，）。得出了： 1，2，3，5，6，7，8，9 的升序数据。","categories":[{"name":"算法","slug":"算法","permalink":"http://blog.crazylaw.cn/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://blog.crazylaw.cn/tags/%E7%AE%97%E6%B3%95/"}]},{"title":"【Mongodb】- 知识整理","slug":"mogodb-info","date":"2018-02-23T09:07:00.000Z","updated":"2021-03-20T16:25:01.810Z","comments":true,"path":"2018/02/23/mogodb-info/","link":"","permalink":"http://blog.crazylaw.cn/2018/02/23/mogodb-info/","excerpt":"说明Mongodb 作为 nosql 数据库的代表，和传统的 mysql 的关系型数据库不同，他的数据结构只有一种，那就是 BSON，没有固定的 schema，所以他是 no schema 的数据存储方式。 需要注意的是，Mongodb 只支持单文档事务，所以用到的业务中不应该有事务性强的数据源。 这么说比较直白。具体举例几个应用场景： 游戏场景，使用 MongoDB 存储游戏用户信息，用户的装备、积分等直接以内嵌文档的形式存储，方便查询、更新 物流场景，使用 MongoDB 存储订单信息，订单状态在运送过程中会不断更新，以 MongoDB 内嵌数组的形式来存储，一次查询就能将订单所有的变更读取出来。 社交场景，使用 MongoDB 存储存储用户信息，以及用户发表的朋友圈信息，通过地理位置索引实现附近的人、地点等功能 等等","text":"说明Mongodb 作为 nosql 数据库的代表，和传统的 mysql 的关系型数据库不同，他的数据结构只有一种，那就是 BSON，没有固定的 schema，所以他是 no schema 的数据存储方式。 需要注意的是，Mongodb 只支持单文档事务，所以用到的业务中不应该有事务性强的数据源。 这么说比较直白。具体举例几个应用场景： 游戏场景，使用 MongoDB 存储游戏用户信息，用户的装备、积分等直接以内嵌文档的形式存储，方便查询、更新 物流场景，使用 MongoDB 存储订单信息，订单状态在运送过程中会不断更新，以 MongoDB 内嵌数组的形式来存储，一次查询就能将订单所有的变更读取出来。 社交场景，使用 MongoDB 存储存储用户信息，以及用户发表的朋友圈信息，通过地理位置索引实现附近的人、地点等功能 等等 Mongodb 的优势 更高的写负载默认情况下，对比事务安全，MongoDB 更关注高的插入速度。如果你需要加载大量低价值的业务数据，比如日志收集，那么 MongoDB 将很适合你的用例，但是必须避免在要求高事务安全的情景下使用 MongoDB，比如一个 1000 万美元的交易。 处理很大的规模的单表：数据库扩展是非常有挑战性的，当单表格大小达到 5-10GB 时，MySQL 表格性能会毫无疑问的降低。如果你需要分片并且分割你的数据库，MongoDB 将很容易实现这一点。 不可靠环境保证高可用性设置副本集（主-从服务器设置）不仅方便而且很快，此外，使用 MongoDB 还可以快速、安全及自动化的实现节点（或数据中心）故障转移。 使用基于位置的数据查询，查的更快MongoDB 支持二维空间索引，比如管道，因此可以快速及精确的从指定位置获取数据。MongoDB 在启动后会将数据库中的数据以文件映射的方式加载到内存中。如果内存资源相当丰富的话，这将极大地提高数据库的查询速度，毕竟内存的 I/O 效率比磁盘高多了 非结构化数据的爆发增长增加列在有些情况下可能锁定整个数据表，或者增加负载从而导致性能下降，这个问题通常发生在表格大于 1GB 的情况下。鉴于 MongoDB 的弱数据结构模式，添加 1 个新字段不会对旧表格有任何影响，整个过程会非常快速；因此，在应用程序发生改变时，你不需要专门的 1 个 DBA 去修改数据库模式。 总结 动态查询 全索引支持,扩展到内部对象和内嵌数组 查询记录分析 快速,就地更新 高效存储二进制大对象 (比如照片和视频) 复制和故障切换支持 Auto- Sharding 自动分片支持云级扩展性 MapReduce 支持复杂聚合 商业支持,培训和咨询 缺点： 不支持复杂的真正的事务 不支持多表联查 MongoDB 占用空间过大","categories":[{"name":"MongoDb","slug":"MongoDb","permalink":"http://blog.crazylaw.cn/categories/MongoDb/"}],"tags":[{"name":"MongoDb","slug":"MongoDb","permalink":"http://blog.crazylaw.cn/tags/MongoDb/"}]},{"title":"【Redis】- 配置管理","slug":"redis-config","date":"2018-02-23T07:28:00.000Z","updated":"2021-03-20T16:25:01.813Z","comments":true,"path":"2018/02/23/redis-config/","link":"","permalink":"http://blog.crazylaw.cn/2018/02/23/redis-config/","excerpt":"说明redis 的常用知识就不详细说了。记录一下我认为值得记录的知识点 基本常用的配置参数、命令 daemonize no配置 redis 是否以守护进程的方式运行，默认 nopidfile /var/run/redis.pid Redis 以守护进程方式运行时，pid 的写入文件。默认为/var/run/redis.pid port 6379监听端口,默认为 6379 bind 127.0.0.1绑定的主机地址 timeout 300客户端限制多长时间后关闭连接 loglevel verbose日志级别，支持 debug/verbos/notice/warning，默认为 verbose","text":"说明redis 的常用知识就不详细说了。记录一下我认为值得记录的知识点 基本常用的配置参数、命令 daemonize no配置 redis 是否以守护进程的方式运行，默认 nopidfile /var/run/redis.pid Redis 以守护进程方式运行时，pid 的写入文件。默认为/var/run/redis.pid port 6379监听端口,默认为 6379 bind 127.0.0.1绑定的主机地址 timeout 300客户端限制多长时间后关闭连接 loglevel verbose日志级别，支持 debug/verbos/notice/warning，默认为 verbose databases 16数据库的数量，默认为 0.可以使用 SELECT &lt;dbid&gt;命令在连接上指定数据库 id save指定在多长时间内，有多少次更新操作，就将数据同步到数据文件。（Redis 默认的持久化方式：RDB 方式）redis 默认配置文件中配置了三个条件: 123save 900 1save 300 10save 60 10000 表示 900 秒内有 1 个更改、300 秒内有 10 个更改以及 60 秒内有 10000 个更改。可以存在多个条件，条件之间是“或”的关系，只要满足其中一个条件，就会进行快照。 如果想要禁用自动快照，只需要将所有的 save 参数删除即可。 rdbcompression yes指定存储至本地数据库时是否压缩数据，默认 yes。redis 采用 LZF 压缩，若需节省 CPU 时间可以关闭，但会导致数据库文件变大。 dbfilenam dump.rdb制定本地数据库文件名 默认为 dump.rdb dir ./制定本地数据库存放目录 slaveof设置当本机为 slave 服务时，设置 master 服务的 IP 地址及端口，在 Redis 启动时，它会自动从 master 进行数据同步 masterauth当 master 服务设置了密码保护时，slave 服务连接 master 的密码 requirepass boobared设置 redis 连接密码，如果配置了连接密码，客户端在连接 redis 时需要通过AUTH &lt;password&gt;命令提供密码，默认关闭 maxclients 128最大连接数，默认无限制。0 表示不限制超过限制时，redis 会遍历新的连接并向客户端返回 max number of clients reached maxmemory最大内存限制达到最大内存后，Redis 会先尝试清除已到期活即将到期的 key。当此方法处理后，仍然到达最大内存设置，将无法再进行写入操作，仍可以进行读取操作。Redis 新的 vm 机制会把 key 存放在内存，value 存放在 swap 区 appendonly no是否每次更新操作后进行日志记录(持久化：AOF)redis 默认情况下异步把数据写入磁盘，如果不开启，可能会在断电时丢失一部分数据。 appendfilename appendonly.aof更新日志文件名 appendfsync everysec日志更新条件，可选 no：等操作系统进行数据缓存同步到磁盘（30s 一次） - 快 always：每次更新操作后都会系统调用 fsync()将数据写 到磁盘 - 慢，安全 everysec ：每秒一次 - 折中，默认 auto-aof-rewrite-percentage 100当目前的 AOF 文件大小超过上一次重写时的 AOF 文件大小的百分之多少时会再次进行重写，如果之前没有重写过，则以启动时的 AOF 文件大小为依据 auto-aof-rewrite-min-size 64mb允许重写的最小 AOF 文件大小 vm-enabled no指定是否启用虚拟内存机制，默认 no vm-swap-file /tmp/redis.swap虚拟内存文件路径，默认为/temp/redis.swap，不可多个 Redis 实例共享 vm-max-memory 0将所有大于该值的数据存入虚拟内存，无论 vm-max-memory 设置多小，所有索引数据都是内存存储的（keys）。当 vm-max-memory 设置为 0 时，所有 value 都存储于磁盘中 vm-page-size 32redis swap 文件分成了很多的 page，一个对象可以保存在多个 page 上，但一个 page 上不能被多个对象共享，vm-page-size 要根据存储的数据大小来设定，如果存储很多小对象，page 大小最好设置为 32 或 64bytes；如果存储大对象，则可使用更大的 page 如果不确定，就使用默认值 vm-pages 134217728设置 swap 文件中的 page 数量，由于页表是存放在内存中的，在磁盘上没 9 个 pages 将消耗 1byte 的内存 vm-maxthreads 4swap 文件的线程数，最好不要超过机器的核心数，如果设置为 0，那么所有对 swap 文件的操作都是串行的，可能会造成比较长时间的延迟。默认为 4 glueoutputbuf yes设置在向客户端应答时，是否把较小的包合并为一个包发送，默认开启 hash-max-zipmap-entries 64hash-max-zipmap-value 512在超过一定的数量或者最大的元素超过某一临界值时，采用一种特殊的哈希算法 include /path/to/local.conf指定包含其他的配置文件，可以在同一主机上多个 Redis 实例之间使用同一份配置文件，而同时各个实例又拥有自己的特定配置文件 五种基本数据类型1.String 类型 一个 key，一个 value 二进制安全 一个 key 最大存储 512mb 基本命令：set、get 应用场景：String 是最常用的一种数据类型，普通的 key/ value 存储都可以归为此类.即可以完全实现目前 Memcached 的功能，并且效率更高。还可以享受 Redis 的定时持久化，操作日志及 Replication 等功能。除了提供与 Memcached 一样的 get、set、incr、decr 等. 普通缓存的用处。 2.List 类型 简单的字符串列表，类似于队列，但是是双向的，因为是双链表 有序 不去重 基本命令：lpush、lpop、rpush、rpop、lrange 应用场景：Redis list 的应用场景非常多，也是 Redis 最重要的数据结构之一，比如 twitter 的关注列表，粉丝列表等都可以用 Redis 的 list 结构来实现。Lists 就是链表，相信略有数据结构知识的人都应该能理解其结构。使用 Lists 结构，我们可以轻松地实现最新消息排行等功能。Lists 的另一个应用就是消息队列，可以利用 Lists 的 PUSH 操作，将任务存在 Lists 中，然后工作线程再用 POP 操作将任务取出进行执行。Redis 还提供了操作 Lists 中某一段的 api，你可以直接查询，删除 Lists 中某一段的元素。 3.Set 类型 无序 去重 哈希表实现，添加，删除，查询的复杂度都是 O(1) 基本命令：sadd、smembers 应用场景：Redis set 对外提供的功能与 list 类似是一个列表的功能，特殊之处在于 set 是可以自动排重的，当你需要存储一个列表数据，又不希望出现重复数据时，set 是一个很好的选择，并且 set 提供了判断某个成员是否在一个 set 集合内的重要接口，这个也是 list 所不能提供的。Sets 集合的概念就是一堆不重复值的组合。利用 Redis 提供的 Sets 数据结构，可以存储一些集合性的数据，比如在微博应用中，可以将一个用户所有的关注人存在一个集合中，将其所有粉丝存在一个集合。Redis 还为集合提供了求交集、并集、差集等操作，可以非常方便的实现如共同关注、共同喜好、二度好友等功能，对上面的所有集合操作，你还可以使用不同的命令选择将结果返回给客户端还是存集到一个新的集合中。 4.Zset 类型 根据 Score 从小到大排序（有序） 去重（但是会更新 Score） 基本命令：zadd、zrangebyscore 使用场景：Redis sorted set 的使用场景与 set 类似，区别是 set 不是自动有序的，而 sorted set 可以通过用户额外提供一个优先级(score)的参数来为成员排序，并且是插入有序的，即自动排序。当你需要一个有序的并且不重复的集合列表，那么可以选择 sorted set 数据结构，比如 twitter 的 public timeline 可以以发表时间作为 score 来存储，这样获取时就是自动按时间排好序的。另外还可以用 Sorted Sets 来做带权重的队列，比如普通消息的 score 为 1，重要消息的 score 为 2，然后工作线程可以选择按 score 的倒序来获取工作任务。让重要的任务优先执行。 5.Hash 类型 类似于 nosql，一个 key 里面可以有多个 field 和 value 的映射 基本命令：hmset、hgetall 应用场景：在 Memcached 中，我们经常将一些结构化的信息打包成 HashMap，在客户端序列化后存储为一个字符串的值，比如用户的昵称、年龄、性别、积分等，这时候在需要修改其中某一项时，通常需要将所有值取出反序列化后，修改某一项的值，再序列化存储回去。这样不仅增大了开销，也不适用于一些可能并发操作的场合（比如两个并发的操作都需要修改积分）。而 Redis 的 Hash 结构可以使你像在数据库中 Update 一个属性一样只修改某一项属性值。 redis 的持久化 RDB 快照，全量备份 AOF append only file 增量备份 Redis 允许同时开启 AOF 和 RDB，既保证了数据安全又使得进行备份等操作十分容易。此时重新启动 Redis 后 Redis 会使用 AOF 文件来恢复数据，因为 AOF 方式的持久化可能丢失的数据更少 save 和 bgsave 的区别。 save 会阻塞主进程 bgsave 会 fork 一个子进程进行快照备份，不会阻塞主进程 aof 文件备份与 dump 文件备份不同。dump 文件的编码格式和存储格式与数据库一致，而且 dump 文件中备份的是数据库的当前快照，意思就是，不管数据之前什么样，只要 BGSAVE 了，dump 文件就会刷新成当前数据库数据。 当 redis 重启时，会按照以下优先级进行启动： 如果只配置 AOF,重启时加载 AOF 文件恢复数据；如果同时 配置了 RBD 和 AOF,启动是只加载 AOF 文件恢复数据;如果只配置 RBD,启动时将加载 dump 文件恢复数据。 注意：只要配置了 aof，但是没有 aof 文件，这个时候启动的数据库会是空的 redis 集群123456port 7001 #修改端口号，从7001到7006cluster-enabled yes #开启cluster，去掉注释cluster-config-file nodes.confcluster-node-timeout 15000appendonly yes 复制六份，修改对应的端口号 启动 6 个服务，分别加载不同的配置文件，即可实现 redis 集群。 客户端连接集群 redis-cli 需要带上 -c","categories":[{"name":"Redis","slug":"Redis","permalink":"http://blog.crazylaw.cn/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://blog.crazylaw.cn/tags/Redis/"}]},{"title":"【网络编程】socket阻塞与非阻塞，同步与异步、I/O模型，select与poll、epoll比较","slug":"network-model","date":"2018-02-23T05:54:00.000Z","updated":"2021-03-20T16:25:01.811Z","comments":true,"path":"2018/02/23/network-model/","link":"","permalink":"http://blog.crazylaw.cn/2018/02/23/network-model/","excerpt":"","text":"说明作为phper，其实很多不了解网络编程的人来说，很容易忽略一些概念，在这里，这些网络基本概念，有必要梳理一下。 Sync 同步 Async 异步 Block 阻塞 Unblock 非阻塞 同步、异步的概念，主要针对C端同步： 所谓同步，就是在c端发出一个功能调用时，在没有得到结果之前，该调用就不返回。也就是必须一件一件事做,等前一件做完了才能做下一件事。 例如普通http请求，B/S模式（同步）：提交请求-&gt;等待服务器处理-&gt;处理完毕返回 这个期间客户端浏览器不能干任何事 异步： 异步的概念和同步相对。当c端一个异步过程调用发出后，调用者不能立刻得到结果。实际处理这个调用的部件在完成后，通过状态、通知和回调来通知调用者。 例如 ajax请求（异步）: 请求通过事件触发-&gt;服务器处理（这是浏览器仍然可以作其他事情）-&gt;处理完毕 阻塞、非阻塞的概念，主要针对S端阻塞/非阻塞主要针对S端: 阻塞： 阻塞调用是指调用结果返回之前，当前线程会被挂起（线程进入非可执行状态，在这个状态下，cpu不会给线程分配时间片，即线程暂停运行）。函数只有在得到结果之后才会返回。 有人也许会把阻塞调用和同步调用等同起来，实际上他是不同的。对于同步调用来说，很多时候当前线程还是激活的，只是从逻辑上当前函数没有返回而已。 例如，我们在socket中调用recv函数，如果缓冲区中没有数据，这个函数就会一直等待，直到有数据才返回。而此时，当前线程还会继续处理各种各样的消息。 快递的例子：比如到你某个时候到A楼一层（假如是内核缓冲区）取快递，但是你不知道快递什么时候过来，你又不能干别的事，只能死等着。但你可以睡觉（进程处于休眠状态），因为你知道快递把货送来时一定会给你打个电话（假定一定能叫醒你）。 非阻塞： 非阻塞和阻塞的概念相对应，指在不能立刻得到结果之前，该函数不会阻塞当前线程，而会立刻返回。 还是等快递的例子：如果用忙轮询的方法，每隔5分钟到A楼一层(内核缓冲区）去看快递来了没有。如果没来，立即返回。而快递来了，就放在A楼一层，等你去取。 同步IO和异步IO的区别就在于：数据访问的时候进程是否阻塞！ 阻塞IO和非阻塞IO的区别就在于：应用程序的调用是否立即返回！ 同步和异步都只针对于本机SOCKET而言的。 同步和异步,阻塞和非阻塞,有些混用,其实它们完全不是一回事,而且它们修饰的对象也不相同。 阻塞和非阻塞是指当server端的进程访问的数据如果尚未就绪,进程是否需要等待,简单说这相当于函数内部的实现区别,也就是未就绪时是直接返回还是等待就绪; 而同步和异步是指client端访问数据的机制，同步一般指主动请求并等待I/O操作完毕的方式，当数据就绪后在读写的时候必须阻塞(区别就绪与读写二个阶段,同步的读写必须阻塞)，异步则指主动请求数据后便可以继续处理其它任务,随后等待I/O,操作完毕的通知,这可以使进程在数据读写时也不阻塞。(等待”通知”) 同步就是同步，异步就是异步! 目前应用中阻塞和非阻塞是针对同步应用而言。 Linux下的五种I/O模型 阻塞I/O (blocking I/O) 非阻塞I/O (nonblocking I/O) I/O复用 （select和poll）（I/O multplexing） 信号驱动I/O 异步I/O （前面四种都是同步，只有最后一种才是异步IO） 阻塞I/O模型 简介：进程会一直阻塞，直到数据拷贝完成 应用程序调用一个IO函数，导致应用程序阻塞，等待数据准备好。 如果数据没有准备好，一直等待….数据准备好了，从内核拷贝到用户空间,IO函数返回成功指示。 我们 第一次接触到的网络编程都是从 listen()、send()、recv()等接口开始的。使用这些接口可以很方便的构建服务器 /客户机的模型。 阻塞I/O模型图：在调用recv()/recvfrom（）函数时，发生在内核中等待数据和复制数据的过程。 当调用recv()函数时，系统首先查是否有准备好的数据。如果数据没有准备好，那么系统就处于等待状态。当数据准备好后，将数据从系统缓冲区复制到用户空间，然后该函数返回。在套接应用程序中，当调用recv()函数时，未必用户空间就已经存在数据，那么此时recv()函数就会处于等待状态。 当使用socket()函数创建套接字时，默认的套接字都是阻塞的。并不是所有 Sockets API以阻塞套接字为参数调用都会发生阻塞。例如，以阻塞模式的套接字为参数调用bind()、listen()函数时，函数会立即返回。 1．输入操作： recv()、recvfrom()函数。以阻塞套接字为参数调用该函数接收数据。如果此时套接字缓冲区内没有数据可读，则调用线程在数据到来前一直睡眠。 2．输出操作： send()、sendto()函数。以阻塞套接字为参数调用该函数发送数据。如果套接字缓冲区没有可用空间，线程会一直睡眠，直到有空间。 3．接受连接：accept()函数。以阻塞套接字为参数调用该函数，等待接受对方的连接请求。如果此时没有连接请求，线程就会进入睡眠状态。 4．外出连接：connect()函数。对于TCP连接，客户端以阻塞套接字为参数，调用该函数向服务器发起连接。该函数在收到服务器的应答前，不会返回。这意味着TCP连接总会等待至少到服务器的一次往返时间。 使用阻塞模式的套接字，开发网络程序比较简单，容易实现。当希望能够立即发送和接收数据，且处理的套接字数量比较少的情况下，使用阻塞模式来开发网络程序比较合适。 阻塞模式套接字的不足表现为，在大量建立好的套接字线程之间进行通信时比较困难。当使用“生产者-消费者”模型开发网络程序时，为每个套接字都分别分配一个读线程、一个处理数据线程和一个用于同步的事件，那么这样无疑加大系统的开销。其最大的缺点是当希望同时处理大量套接字时，将无从下手，其扩展性很差. 阻塞模式给网络编程带来了一个很大的问题，如在调用 send()的同时，线程将被阻塞，在此期间，线程将无法执行任何运算或响应任何的网络请求。这给多客户机、多业务逻辑的网络编程带来了挑战。这时，我们可能会选择多线程的方式来解决这个问题。 应对多客户机的网络应用，最简单的解决方式是在服务器端使用多线程（或多进程）。多线程（或多进程）的目的是让每个连接都拥有独立的线程（或进程），这样任何一个连接的阻塞都不会影响其他的连接。 具体使用多进程还是多线程，并没有一个特定的模式。传统意义上，进程的开销要远远大于线程，所以，如果需要同时为较多的客户机提供服务，则不推荐使用多进程；如果单个服务执行体需要消耗较多的 CPU 资源，譬如需要进行大规模或长时间的数据运算或文件访问，则进程较为安全。通常，使用 pthread_create () 创建新线程，fork() 创建新进程。 多线程/进程服务器同时为多个客户机提供应答服务。模型如下： 主线程持续等待客户端的连接请求，如果有连接，则创建新线程，并在新线程中提供为前例同样的问答服务。 上述多线程的服务器模型似乎完美的解决了为多个客户机提供问答服务的要求，但其实并不尽然。如果要同时响应成百上千路的连接请求，则无论多线程还是多进程都会严重占据系统资源，降低系统对外界响应效率，而线程与进程本身也更容易进入假死状态。 由此可能会考虑使用“线程池”或“连接池”。“线程池”旨在减少创建和销毁线程的频率，其维持一定合理数量的线程，并让空闲的线程重新承担新的执行任务。“连接池”维持连接的缓存池，尽量重用已有的连接、减少创建和关闭连接的频率。这两种技术都可以很好的降低系统开销，都被广泛应用很多大型系统，如apache，MySQL数据库等。 但是，“线程池”和“连接池”技术也只是在一定程度上缓解了频繁调用 IO 接口带来的资源占用。而且，所谓“池”始终有其上限，当请求大大超过上限时，“池”构成的系统对外界的响应并不比没有池的时候效果好多少。所以使用“池”必须考虑其面临的响应规模，并根据响应规模调整“池”的大小。. 对应上例中的所面临的可能同时出现的上千甚至上万次的客户端请求，“线程池”或“连接池”或许可以缓解部分压力，但是不能解决所有问题。 非阻塞IO模型：简介：非阻塞IO通过进程反复调用IO函数（多次系统调用，并马上返回）；在数据拷贝的过程中，进程是阻塞的。 我们把一个SOCKET接口设置为非阻塞就是告诉内核，当所请求的I/O操作无法完成时，不要将进程睡眠，而是返回一个错误。这样我们的I/O操作函数将不断的测试数据是否已经准备好，如果没有准备好，继续测试，直到数据准备好为止。在这个不断测试的过程中，会大量的占用CPU的时间。 把SOCKET设置为非阻塞模式，即通知系统内核：在调用Sockets API时，不要让线程睡眠，而应该让函数立即返回。在返回时，该函数返回一个错误代码。图所示，一个非阻塞模式套接字多次调用recv()函数的过程。前三次调用recv()函数时，内核数据还没有准备好。因此，该函数立即返回EWOULDBLOCK错误代码。第四次调用recv()函数时，数据已经准备好，被复制到应用程序的缓冲区中，recv()函数返回成功指示，应用程序开始处理数据。 当使用socket()函数和WSASocket()函数创建套接字时，默认都是阻塞的。在创建套接字之后，通过调用函数fcntl()，将该套接字设置为非阻塞模式。套接字设置为非阻塞模式后，在调Sockets API函数时，调用函数会立即返回。大多数情况下，这些函数调用都会调用“失败”，并返回EWOULDBLOCK错误代码。说明请求的操作在调用期间内没有时间完成。通常，应用程序需要重复调用该函数，直到获得成功返回代码。 需要说明的是并非所有的Sockets API在非阻塞模式下调用，都会返回EWOULDBLOCK错误。例如，以非阻塞模式的套接字为参数调用bind()函数时，就不会返回该错误代码。因为该函数是应用程序第一调用的函数，当然不会返回这样的错误代码。 由于使用非阻塞套接字在调用函数时，会经常返回EWOULDBLOCK错误。所以在任何时候，都应仔细检查返回代码并作好对“失败”的准备。应用程序连续不断地调用这个函数，直到它返回成功指示为止。上面的程序清单中，在While循环体内不断地调用recv()函数，以读入1024个字节的数据。这种做法很浪费系统资源。 要完成这样的操作，有人使用MSG_PEEK标志调用recv()函数查看缓冲区中是否有数据可读。同样，这种方法也不好。因为该做法对系统造成的开销是很大的，并且应用程序至少要调用recv()函数两次，才能实际地读入数据。较好的做法是，使用套接字的“I/O模型”来判断非阻塞套接字是否可读可写。 非阻塞模式套接字与阻塞模式套接字相比，不容易使用。使用非阻塞模式套接字，需要编写更多的代码，以便在每个Sockets API函数调用中，对收到的EWOULDBLOCK错误进行处理。因此，非阻塞套接字便显得有些难于使用。 但是，非阻塞套接字在控制建立的多个连接，在数据的收发量不均，时间不定时，明显具有优势。这种套接字在使用上存在一定难度，但只要排除了这些困难，它在功能上还是非常强大的。通常情况下，可考虑使用套接字的“I/O模型”，它有助于应用程序通过异步方式，同时对一个或多个套接字的通信加以管理。 IO复用模型(重要)：简介：主要是select和epoll；对一个IO端口，两次调用，两次返回，比阻塞IO并没有什么优越性；关键是能实现同时对多个IO端口进行监听. I/O复用模型会用到select、poll、epoll函数，这几个函数也会使进程阻塞，但是和阻塞I/O所不同的的，这几个函数可以同时阻塞多个I/O操作。而且可以同时对多个读操作，多个写操作的I/O函数进行检测，直到有数据可读或可写时，才真正调用I/O操作函数。 信号驱动IO模型： 简介：两次调用，两次返回； 首先我们允许套接口进行信号驱动I/O，并安装一个信号处理函数，进程继续运行并不阻塞。当数据准备好时，进程会收到一个SIGIO信号，可以在信号处理函数中调用I/O操作函数处理数据。 异步IO模型： 简介：数据拷贝的时候进程无需阻塞。 当一个异步过程调用发出后，调用者不能立刻得到结果。实际处理这个调用的部件在完成后，通过状态、通知和回调来通知调用者的输入输出操作 阶段性小结 同步IO引起进程阻塞，直至IO操作完成。异步IO不会引起进程阻塞。IO复用是先通过select调用阻塞。 5个I/O模型的比较：select、poll、epoll区别select： select本质上是通过设置或者检查存放fd标志位的数据结构来进行下一步处理。这样所带来的缺点是： 1、 单个进程可监视的fd数量被限制，即能监听端口的大小有限。 一般来说这个数目和系统内存关系很大，具体数目可以cat /proc/sys/fs/file-max察看。32位机默认是1024个。64位机默认是2048.2、 对socket进行扫描时是线性扫描，即采用轮询的方法，效率较低： 当套接字比较多的时候，每次select()都要通过遍历FD_SETSIZE个Socket来完成调度,不管哪个Socket是活跃的,都遍历一遍。这会浪费很多CPU时间。如果能给套接字注册某个回调函数，当他们活跃时，自动完成相关操作，那就避免了轮询，这正是epoll与kqueue做的。3、需要维护一个用来存放大量fd的数据结构，这样会使得用户空间和内核空间在传递该结构时复制开销大 poll： poll本质上和select没有区别，它将用户传入的数组拷贝到内核空间，然后查询每个fd对应的设备状态，如果设备就绪则在设备等待队列中加入一项并继续遍历，如果遍历完所有fd后没有发现就绪设备，则挂起当前进程，直到设备就绪或者主动超时，被唤醒后它又要再次遍历fd。这个过程经历了多次无谓的遍历。 它没有最大连接数的限制，原因是它是基于链表来存储的，但是同样有一个缺点： 1、大量的fd的数组被整体复制于用户态和内核地址空间之间，而不管这样的复制是不是有意义。 2、poll还有一个特点是“水平触发”，如果报告了fd后，没有被处理，那么下次poll时会再次报告该fd。 epoll: epoll支持水平触发和边缘触发，最大的特点在于边缘触发，它只告诉进程哪些fd刚刚变为就绪态，并且只会通知一次。还有一个特点是，epoll使用“事件”的就绪通知方式，通过epoll_ctl注册fd，一旦该fd就绪，内核就会采用类似callback的回调机制来激活该fd，epoll_wait便可以收到通知 epoll的优点： 1、没有最大并发连接的限制，能打开的FD的上限远大于1024（1G的内存上能监听约10万个端口）； 2、效率提升，不是轮询的方式，不会随着FD数目的增加效率下降。只有活跃可用的FD才会调用callback函数； 即Epoll最大的优点就在于它只管你“活跃”的连接（因为支持边缘触发），而跟连接总数无关，因此在实际的网络环境中，Epoll的效率就会远远高于select和poll。 3、 内存拷贝，利用mmap()文件映射内存加速与内核空间的消息传递；即epoll使用mmap减少复制开销。. 1、支持一个进程所能打开的最大连接数 select 单个进程所能打开的最大连接数有FD_SETSIZE宏定义，其大小是32个整数的大小（在32位的机器上，大小就是3232，同理64位机器上FD_SETSIZE为3264），当然我们可以对进行修改，然后重新编译内核，但是性能可能会受到影响，这需要进一步的测试。 poll poll本质上和select没有区别，但是它没有最大连接数的限制，原因是它是基于链表来存储的 epoll 虽然连接数有上限，但是很大，1G内存的机器上可以打开10万左右的连接，2G内存的机器可以打开20万左右的连接 2.剧增后带来的IO效率问题 | select || 因为每次调用时都会对连接进行线性遍历，所以随着FD的增加会造成遍历速度慢的“线性下降性能问题”。 || ———————————————————————————————- | ————————————————————————————————————————————————————————————————————————– || poll | 同上 || epoll | 因为epoll内核中实现是根据每个fd上的callback函数来实现的，只有活跃的socket才会主动调用callback，所以在活跃socket较少的情况下，使用epoll没有前面两者的线性下降的性能问题，但是所有socket都很活跃的情况下，可能会有性能问题。 | 3.消息传递方式 select 内核需要将消息传递到用户空间，都需要内核拷贝动作 poll 同上 epoll 通过内核和用户空间共享一块内存来实现的","categories":[{"name":"网络","slug":"网络","permalink":"http://blog.crazylaw.cn/categories/%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"网络，异步","slug":"网络，异步","permalink":"http://blog.crazylaw.cn/tags/%E7%BD%91%E7%BB%9C%EF%BC%8C%E5%BC%82%E6%AD%A5/"}]},{"title":"【Nginx】- 负载均衡","slug":"nginx-load-balan","date":"2018-02-23T03:22:00.000Z","updated":"2021-03-20T16:25:01.811Z","comments":true,"path":"2018/02/23/nginx-load-balan/","link":"","permalink":"http://blog.crazylaw.cn/2018/02/23/nginx-load-balan/","excerpt":"说明在现在互联网爆发的时代，稳定性成为每个项目的首选，再次条件下，我们就要实现高可用的概念，实现高可用的最常用的一种手段就是负载均衡，分摊各个服务器的请求压力。 1.轮训（默认）按照请求时间的循序，把请求逐一分配到对应的服务器。如果其中一台服务器 down 了，nginx 会自动剔除 down 掉的服务器。 1234upstream backserver &#123; server 192.168.0.14; server 192.168.0.15;&#125; 2.权重(weight)其实就是轮训的升级版，给特定的服务器一个指定的权重，nginx 会根据权重分配请求访问。一般用于服务器性能不一致的情况下。 1234upstream backserver &#123; server 192.168.0.14 weight&#x3D;3; server 192.168.0.15 weight&#x3D;7;&#125;","text":"说明在现在互联网爆发的时代，稳定性成为每个项目的首选，再次条件下，我们就要实现高可用的概念，实现高可用的最常用的一种手段就是负载均衡，分摊各个服务器的请求压力。 1.轮训（默认）按照请求时间的循序，把请求逐一分配到对应的服务器。如果其中一台服务器 down 了，nginx 会自动剔除 down 掉的服务器。 1234upstream backserver &#123; server 192.168.0.14; server 192.168.0.15;&#125; 2.权重(weight)其实就是轮训的升级版，给特定的服务器一个指定的权重，nginx 会根据权重分配请求访问。一般用于服务器性能不一致的情况下。 1234upstream backserver &#123; server 192.168.0.14 weight&#x3D;3; server 192.168.0.15 weight&#x3D;7;&#125; 权重越高，访问的概率越大，分别是 30%,70%。 3. IP 哈希化（ip_hash）通过 ip 哈希化，可以让同一 ip 的请求分配到同一台机器上。 12345upstream backserver &#123; ip_hash; server 192.168.0.14:88; server 192.168.0.15:80;&#125; 这个做法的好处是可以保证在负载均衡的情况下，用户的 session 信息都保存在一台机器上，不会因为负载均衡导致用户 session 信息丢失。 4. fair（fair 模块，需要为 nginx 安装扩展）按后端服务器的响应时间来分配请求，响应时间短的优先分配。 12345upstream backserver &#123; server server1; server server2; fair;&#125; 这种方式有点类似于 epoll 方式。 特别说明1234567server 127.0.0.1:9090 down; (down 表示单前的server暂时不参与负载)server 127.0.0.1:8080 weight&#x3D;2; (weight 默认为1.weight越大，负载的权重就越大)server 127.0.0.1:6060;server 127.0.0.1:7070 backup; (其它所有的非backup机器down或者忙的时候，请求backup机器)","categories":[{"name":"Nginx","slug":"Nginx","permalink":"http://blog.crazylaw.cn/categories/Nginx/"}],"tags":[{"name":"Nginx","slug":"Nginx","permalink":"http://blog.crazylaw.cn/tags/Nginx/"}]},{"title":"【nginx】- 常用配置例子","slug":"nginx-example","date":"2018-02-23T02:53:30.000Z","updated":"2021-03-20T16:25:01.811Z","comments":true,"path":"2018/02/23/nginx-example/","link":"","permalink":"http://blog.crazylaw.cn/2018/02/23/nginx-example/","excerpt":"说明以下整理一下一些 nginx 常用例子。 例子 目录对换 123&#x2F;123456&#x2F;xxxx -&gt; &#x2F;xxxx?id&#x3D;123456rewrite ^&#x2F;(\\d+)&#x2F;(.+)&#x2F; &#x2F;$2?id&#x3D;$1 last; 例如下面设定 nginx 在用户使用 ie 的使用重定向到/nginx-ie 目录下： 123if ($http_user_agent ~ MSIE) &#123; rewrite ^(.*)$ &#x2F;nginx-ie&#x2F;$1 break;&#125;","text":"说明以下整理一下一些 nginx 常用例子。 例子 目录对换 123&#x2F;123456&#x2F;xxxx -&gt; &#x2F;xxxx?id&#x3D;123456rewrite ^&#x2F;(\\d+)&#x2F;(.+)&#x2F; &#x2F;$2?id&#x3D;$1 last; 例如下面设定 nginx 在用户使用 ie 的使用重定向到/nginx-ie 目录下： 123if ($http_user_agent ~ MSIE) &#123; rewrite ^(.*)$ &#x2F;nginx-ie&#x2F;$1 break;&#125; 目录自动加“/” 123if (-d $request_filename)&#123; rewrite ^&#x2F;(.*)([^&#x2F;])$ http:&#x2F;&#x2F;$host&#x2F;$1$2&#x2F; permanent; &#125; 禁止 htaccess 123location ~&#x2F;\\.ht &#123; deny all;&#125; 禁止多个目录 1234location ~ ^&#x2F;(cron|templates)&#x2F; &#123; deny all; break;&#125; 只充许固定 ip 访问网站，并加上密码 1234567root &#x2F;opt&#x2F;htdocs&#x2F;www;allow 208.97.167.194;allow 222.33.1.2;allow 231.152.49.4;deny all;auth_basic “C1G_ADMIN”;auth_basic_user_file htpasswd; 文件和目录不存在的时候重定向： 123if (!-e $request_filename) &#123; proxy_pass http:&#x2F;&#x2F;127.0.0.1;&#125; 多域名转向 123456server_name www.test.com&#x2F; www.test2.com&#x2F;;index index.html index.htm index.php;root &#x2F;opt&#x2F;ccinn&#x2F;;if ($host ~ “test3\\.cn”) &#123; rewrite ^(.*) http:&#x2F;&#x2F;www.test.com$1&#x2F; permanent;&#125;","categories":[{"name":"Nginx","slug":"Nginx","permalink":"http://blog.crazylaw.cn/categories/Nginx/"}],"tags":[{"name":"Nginx","slug":"Nginx","permalink":"http://blog.crazylaw.cn/tags/Nginx/"}]},{"title":"【Nginx】 - 变量","slug":"nginx-var","date":"2018-02-23T02:26:00.000Z","updated":"2021-03-20T16:25:01.811Z","comments":true,"path":"2018/02/23/nginx-var/","link":"","permalink":"http://blog.crazylaw.cn/2018/02/23/nginx-var/","excerpt":"说明Nginx配置文件管理者整个nginx服务的运作，有一些逻辑我们可以通过nginx内置变量或者自定义变量来选择性配置。 内置常见变量参见下表： 名称 说明 $arg_name 请求中的name参数 $args 请求中的参数 $content_length HTTP请求里面的”Content-Length” $content_type HTTP请求里面的”Content-Type” $document_root 配置里面设置的 root 赋值 $host 请求信息种的”HOST”,如果请求头不存在Host的话，那么久等于server_name 赋值 $http_cookie cookie信息 $http_referer 引用地址，就是前一个链接地址 $http_user_agent 客户端代理信息 $is_args 如果请求行带有参数，返回“？”，否则返回空字符串 $limit_rate 当前连接速率的限制 $pid nginx当前worker进程的PID $query_string 与$args基本一致 $remote_addr 客户端IP地址 $remote_port 客户端端口号 $scheme 当前所用的协议，比如http或者https $request_method 请求方法，比如“GET”，“POST”等 $request_uri 请求的URI，带参数，比如：http://localhost:8080/uuu","text":"说明Nginx配置文件管理者整个nginx服务的运作，有一些逻辑我们可以通过nginx内置变量或者自定义变量来选择性配置。 内置常见变量参见下表： 名称 说明 $arg_name 请求中的name参数 $args 请求中的参数 $content_length HTTP请求里面的”Content-Length” $content_type HTTP请求里面的”Content-Type” $document_root 配置里面设置的 root 赋值 $host 请求信息种的”HOST”,如果请求头不存在Host的话，那么久等于server_name 赋值 $http_cookie cookie信息 $http_referer 引用地址，就是前一个链接地址 $http_user_agent 客户端代理信息 $is_args 如果请求行带有参数，返回“？”，否则返回空字符串 $limit_rate 当前连接速率的限制 $pid nginx当前worker进程的PID $query_string 与$args基本一致 $remote_addr 客户端IP地址 $remote_port 客户端端口号 $scheme 当前所用的协议，比如http或者https $request_method 请求方法，比如“GET”，“POST”等 $request_uri 请求的URI，带参数，比如：http://localhost:8080/uuu 自定义变量可以使用set关键字来设置，例如 set $name caiwh 在对应的结构体之内就用$name来代表caiwh了","categories":[{"name":"Nginx","slug":"Nginx","permalink":"http://blog.crazylaw.cn/categories/Nginx/"}],"tags":[{"name":"Nginx","slug":"Nginx","permalink":"http://blog.crazylaw.cn/tags/Nginx/"}]},{"title":"【Nginx】- Rewrite功能","slug":"nginx-rewrite","date":"2018-02-23T01:57:00.000Z","updated":"2021-03-20T16:25:01.811Z","comments":true,"path":"2018/02/23/nginx-rewrite/","link":"","permalink":"http://blog.crazylaw.cn/2018/02/23/nginx-rewrite/","excerpt":"说明Nginx 作为一个 http 服务器，其中最重要的一个特点就是 rewrite 功能，这个功能算是 nginx 必会内容了，接下来，我们详细说明一下 URL 重写配置以及信息详解 Nginx-rewrite 依赖 PCRE 软件的支持，支持 perl 兼容正则表达式语句进行规则匹配 rewrite 是实现 URL 重写的关键指令，根据 regex（正则表达式）部分内容，重定向到 replacement，结尾是 flag 标记。 语法123rewrite &lt;regex&gt; &lt;replacement&gt; [flag]关键字 正则表达式 替换内容 flag标记 关键字：固定 rewrite 开启的关键字 正则：perl 兼容正则表达式语句进行规则匹配 替代内容：将正则匹配的内容替换成 replacement flag 标记：rewrite 支持的 flag 标记","text":"说明Nginx 作为一个 http 服务器，其中最重要的一个特点就是 rewrite 功能，这个功能算是 nginx 必会内容了，接下来，我们详细说明一下 URL 重写配置以及信息详解 Nginx-rewrite 依赖 PCRE 软件的支持，支持 perl 兼容正则表达式语句进行规则匹配 rewrite 是实现 URL 重写的关键指令，根据 regex（正则表达式）部分内容，重定向到 replacement，结尾是 flag 标记。 语法123rewrite &lt;regex&gt; &lt;replacement&gt; [flag]关键字 正则表达式 替换内容 flag标记 关键字：固定 rewrite 开启的关键字 正则：perl 兼容正则表达式语句进行规则匹配 替代内容：将正则匹配的内容替换成 replacement flag 标记：rewrite 支持的 flag 标记 flag 标记说明： last 本条规则匹配完成后，继续向下匹配新的 location URI 规则 break 本条规则匹配完成即终止，不再匹配后面的任何规则 redirect 返回 302 临时重定向，浏览器地址会显示跳转后的 URL 地址 permanent 返回 301 永久重定向，浏览器地址会显示跳转后的 URL 地址 例子rewrite ^/(.*) http://usblog.crazylaw.cn/$1 permanent; 这个例子，无论说明请求过来，都会永久重定向到http://usblog.crazylaw.cn/$1上，返回状态吗 301 具体例子:当具体的 url 请求:https://nginx.crazylaw.cn/index.php/archives/333/被 nginx-rewirte 之后,链接变成如下https://usblog.crazylaw.cn/index.php/archives/333/ location另外有一些location关键字，需要用到以下特殊符号： ~ &lt;rule&gt; 为区分大小写匹配 ~* &lt;rule&gt; 为不区分大小写匹配 !~ &lt;rule&gt; 和 !~* &lt;rule&gt; 分别是区分大小写不匹配以及不区分大小写不匹配 逻辑判断专用符号 -f &lt;file&gt; 和 !-f &lt;file&gt; 判断是否存在文件 -d &lt;dir&gt; 和 !-d &lt;dir&gt; 判断是否存在目录 -e &lt;file|[dir]&gt; 和 !-e &lt;file|[dir]&gt; 判断是否存在文件或者目录 !x 和 !-x 判断文件是否可以执行 Proxy_pass该关键字和rewrite要做一点区别，proxy_pass用于反向代理，顾名思义，和重定向不同，proxy_pass并不会改变浏览器的 url，整个关键字是用于内部服务器请求转发的。 特点可以归纳如下： 不影响浏览器地址栏的 url 设置被代理 server 的协议和地址，URI 可选（可以有，也可以没有） 协议可以为 http 或 https 地址可以为域名或者 IP，端口可选；eg：proxy_pass http://localhost:8000/uri/; 如果一个域名可以解析到多个地址，那么这些地址会被轮流使用，此外，还可以把一个地址指定为 server group（如：nginx 的 upstream）, eg: 1234567891011121314upstream backend &#123; server backend1.example.com weight=5; server backend2.example.com:8080; server unix:/tmp/backend3; server backup1.example.com:8080 backup; server backup2.example.com:8080 backup;&#125;server &#123; location / &#123; proxy_pass http://backend; &#125;&#125; 如果 proxy_pass 的 URL 定向里包括 URI，那么请求中匹配到 location 中 URI 的部分会被 proxy_pass 后面 URL 中的 URI 替换，eg： 12345location &#x2F;name&#x2F; &#123; proxy_pass http:&#x2F;&#x2F;127.0.0.1&#x2F;remote&#x2F;;&#125;请求http:&#x2F;&#x2F;127.0.0.1&#x2F;name&#x2F;test.html 会被代理到http:&#x2F;&#x2F;example.com&#x2F;remote&#x2F;test.htm 如果 proxy_pass 的 URL 定向里不包括 URI，那么请求中的 URI 会保持原样传送给后端 server，eg： 12345location &#x2F;name&#x2F; &#123; proxy_pass http:&#x2F;&#x2F;127.0.0.1;&#125;请求http:&#x2F;&#x2F;127.0.0.1&#x2F;name&#x2F;test.html 会被代理到http:&#x2F;&#x2F;127.0.0.1&#x2F;name&#x2F;test.html","categories":[{"name":"Nginx","slug":"Nginx","permalink":"http://blog.crazylaw.cn/categories/Nginx/"}],"tags":[{"name":"Nginx","slug":"Nginx","permalink":"http://blog.crazylaw.cn/tags/Nginx/"}]},{"title":"MM|DMM-中文全文搜索引擎","slug":"fulltext-mm-dmm","date":"2018-01-23T09:20:49.000Z","updated":"2021-03-20T16:25:01.805Z","comments":true,"path":"2018/01/23/fulltext-mm-dmm/","link":"","permalink":"http://blog.crazylaw.cn/2018/01/23/fulltext-mm-dmm/","excerpt":"前言什么是中文分词 众所周知，英文是以 词为单位的，词和词之间是靠空格隔开，而中文是以字为单位，句子中所有的字连起来才能描述一个意思。例如，英文句子 I am a student，用中文则为：“我是一个学生”。计算机可以很简单通过空格知道 student 是一个单词，但是不能很容易明白“学”、“生”两个字合起来 才表示一个词。把中文的汉字序列切分成有意义的词，就是中文分词，有些人也称为切词。我是一个学生，分词的结果是：我 是 一个 学生。 知识补充最小匹配算法","text":"前言什么是中文分词 众所周知，英文是以 词为单位的，词和词之间是靠空格隔开，而中文是以字为单位，句子中所有的字连起来才能描述一个意思。例如，英文句子 I am a student，用中文则为：“我是一个学生”。计算机可以很简单通过空格知道 student 是一个单词，但是不能很容易明白“学”、“生”两个字合起来 才表示一个词。把中文的汉字序列切分成有意义的词，就是中文分词，有些人也称为切词。我是一个学生，分词的结果是：我 是 一个 学生。 知识补充最小匹配算法 在所有的分词算法中，最早研究的是最小匹配算法(Minimum Matching)，该算法从待比较字符串左边开始比较，先取前两个字符组成的字段与词典中的词进行比较，如果词典中有该词，则分出此词，继续从第三个字 符开始取两个字符组成的字段进行比较，如果没有匹配到，则取前 3 个字符串组成的字段进行比较，依次类推，直到取的字符串的长度等于预先设定的阈值，如果还 没有匹配成功，则从待处理字串的第二个字符开始比较，如此循环。 例如，“如果还没有匹配成功”，取出左边两个字 组成的字段与词典进行比较，分出“如果”；再从“还”开始，取“还没”，字典中没有此词，继续取“还没有”，依次取到字段“还没有匹配”(假设阈值为 5)，然后从“没”开始，取“没有”，如此循环直到字符串末尾为止。这种方法的优点是速度快，但是准确率却不是很高，比如待处理字符串为“中华人民共和 国”，此匹配算法分出的结果为：中华、人民、共和国，因此该方法基本上已经不被采用 。 最大匹配算法基于字符串的最大匹配，这种方法现在仍比较常用。最大匹配(Maximum Matching)分为正向和逆向两种最大匹配，正向匹配的基本思想是：假设词典中最大词条所含的汉字个数为 n 个，取待处理字符串的前 n 个字作为匹配字 段，查找分词词典。若词典中含有该词，则匹配成功，分出该词，然后从被比较字符串的 n+1 处开始再取 n 个字组成的字段重新在词典中匹配；如果没有匹配成 功，则将这 n 个字组成的字段的最后一位剔除，用剩下的 n 一 1 个字组成的字段在词典中进行匹配，如此进行下去，直到切分成功为止。 例 如，待处理字符串为“汉字多为表意文字”，取字符串“汉语多为表”(假设比较的步长为 5，本文步长 step 都取 5)与词典进行比较，没有与之对应的词，去 除“表”字，用字段“汉语多为”进行匹配，直至匹配到“汉语”为至，再取字符串“多为表意”，循环到切分出“文字”一词。目前，正向最大匹配方法作为一种 基本的方法已被肯定下来，但是由于错误比较大，一般不单独使用。如字符串“处理机器发生的故障”，在正向最大匹配方法中会出现歧义切分，该字符串被分为： 处理机、发生、故障，但是使用逆向匹配就能得到有效的切分。 逆向最大匹配 RMM(Reverse Directional Maximum Matching Method)的分词原理和过程与正向最大匹配相似，区别在于前者从文章或者句子(字串)的末尾开始切分，若不成功则减去最前面的一个字。比如对于字符串 “处理机器发生的故障”，第一步，从字串的右边取长度以步长为单位的字段“发生的故障”在词典中进行匹配，匹配不成功，再取字段“生的故障”进行匹配，依 次匹配，直到分出“故障”一词，最终使用 RMM 方法切分的结果为：故障、发生、机器、处理。该方法要求配备逆序词典。","categories":[{"name":"算法","slug":"算法","permalink":"http://blog.crazylaw.cn/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://blog.crazylaw.cn/tags/%E7%AE%97%E6%B3%95/"},{"name":"全文搜索","slug":"全文搜索","permalink":"http://blog.crazylaw.cn/tags/%E5%85%A8%E6%96%87%E6%90%9C%E7%B4%A2/"}]},{"title":"基于tire-tree的关键字匹配","slug":"数据结构/trie-tree","date":"2018-01-23T07:20:00.000Z","updated":"2021-03-20T16:25:01.818Z","comments":true,"path":"2018/01/23/数据结构/trie-tree/","link":"","permalink":"http://blog.crazylaw.cn/2018/01/23/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/trie-tree/","excerpt":"前言字典树，又称前缀树或 trie 树，是一种有序树，用于保存关联数组，其中的键通常是字符串。与二叉查找树不同，键不是直接保存在节点中，而是由节点在树中的位置决定。一个节点的所有子孙都有相同的前缀，也就是这个节点对应的字符串，而根节点对应空字符串。 开始早期的时候，我在 github 上写了一个，但是一直没有写博文。 https://github.com/whiteCcinn/tire-php 当时写的时候，比较复杂，因为汉字的编码问题，里面会设置一些 ascii 码和 utf-8 编码的转码代码，看上去比较复杂。接下来，会贴一段通过正则实现的代码。 那么 trie 树怎么实现关键字的匹配呢? 这里以一幅图来讲解 trie 树匹配的过程。","text":"前言字典树，又称前缀树或 trie 树，是一种有序树，用于保存关联数组，其中的键通常是字符串。与二叉查找树不同，键不是直接保存在节点中，而是由节点在树中的位置决定。一个节点的所有子孙都有相同的前缀，也就是这个节点对应的字符串，而根节点对应空字符串。 开始早期的时候，我在 github 上写了一个，但是一直没有写博文。 https://github.com/whiteCcinn/tire-php 当时写的时候，比较复杂，因为汉字的编码问题，里面会设置一些 ascii 码和 utf-8 编码的转码代码，看上去比较复杂。接下来，会贴一段通过正则实现的代码。 那么 trie 树怎么实现关键字的匹配呢? 这里以一幅图来讲解 trie 树匹配的过程。 其中要点：构造 trie 树 将关键词用上面介绍的 preg_split()函数拆分为单个字符。如科学家就拆分为科、学、家三个字符。 在最后一个字符后添加一个特殊字符 `，此字符作为一个关键词的结尾（图中的粉红三角），以此字符来标识查到了一个关键词（不然，我们不知道匹配到科、学两个字符时算不算匹配成功）。`，此字符作为一个关键词的结尾（图中的粉红三角），以此字符来标识查到了一个关键词（不然，我们不知道匹配到科、学两个字符时算不算匹配成功）。 检查根部是否有第一个字符(科)节点，如果有了此节点，到步骤 4。 如果还没有，在根部添加值为科的节点。 依次检查并添加学、家 两个节点。 在结尾添加`节点，并继续下一个关键词的插入。 匹配然后我们以 这位科学家很了不起！为例来发起匹配。 首先我们将句子拆分为单个字符 这、位、…； 从根查询第一个字符这，并没有以这个字符开头的关键词，将字符“指针”向后移，直到找到根下有的字符节点科; 接着在节点科下寻找值为 学节点，找到时，结果子树的深度已经到了 2，关键词的最短长度是 2，此时需要在学结点下查找是否有`，找到意味着匹配成功，返回关键词，并将字符“指针”后移，如果找不到则继续在此结点下寻找下一个字符。 如此遍历，直到最后，返回所有匹配结果。 PHP 代码实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283&lt;?phpclass Trie&#123; /** * node struct * * node = array( * val-&gt;word * next-&gt;array(node)/null * depth-&gt;int * ) */ private $root = array( 'depth' =&gt; 0, 'next' =&gt; array(), ); private $matched = array(); public function append($keyword) &#123; $words = preg_split('/(?&lt;!^)(?!$)/u', $keyword); array_push($words, '`'); $this-&gt;insert($this-&gt;root, $words); &#125; public function match($str) &#123; $this-&gt;matched = array(); $words = preg_split('/(?&lt;!^)(?!$)/u', $str); while (count($words) &gt; 0) &#123; $matched = array(); $res = $this-&gt;query($this-&gt;root, $words, $matched); if ($res) &#123; $this-&gt;matched[] = implode('', $matched); &#125; array_shift($words); &#125; return $this-&gt;matched; &#125; private function insert(&amp;$node, $words) &#123; if (empty($words)) &#123; return; &#125; $word = array_shift($words); if (isset($node['next'][$word])) &#123; $this-&gt;insert($node['next'][$word], $words); &#125; else &#123; $tmp_node = array( 'depth' =&gt; $node['depth'] + 1, 'next' =&gt; array(), ); $node['next'][$word] = $tmp_node; $this-&gt;insert($node['next'][$word], $words); &#125; &#125; private function query($node, $words, &amp;$matched) &#123; $word = array_shift($words); if (isset($node['next'][$word])) &#123; array_push($matched, $word); if (isset($node['next'][$word]['next']['`'])) &#123; return true; &#125; return $this-&gt;query($node['next'][$word], $words, $matched); &#125; else &#123; $matched = array(); return false; &#125; &#125;&#125;$tire = new Trie();$msg = '性派对';$tire-&gt;append('性');$tire-&gt;append('性爱');$tire-&gt;append('性爱2');$tire-&gt;append('性爱3');print_r($tire-&gt;match($msg)); 注意到，上面的这一段代码有一个弱点就是按照最短匹配。当你输入性爱排队的时候，无法搜索出性爱，只能搜索到性。 以下的代码进行了升级，全匹配。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485class Trie&#123; /** * node struct * * node = array( * val-&gt;word * next-&gt;array(node)/null * depth-&gt;int * ) */ private $root = array( 'depth' =&gt; 0, 'next' =&gt; array(), ); private $matched = array(); public function append($keyword) &#123; $words = preg_split('/(?&lt;!^)(?!$)/u', $keyword); array_push($words, '`'); $this-&gt;insert($this-&gt;root, $words); &#125; public function match($str) &#123; $this-&gt;matched = array(); $words = preg_split('/(?&lt;!^)(?!$)/u', $str); while (count($words) &gt; 0) &#123; $matched = array(); $res = $this-&gt;query($this-&gt;root, $words, $matched); if ($res) &#123; $this-&gt;matched[] = implode('', $matched); &#125; array_shift($words); &#125; return $this-&gt;matched; &#125; private function insert(&amp;$node, $words) &#123; if (empty($words)) &#123; return; &#125; $word = array_shift($words); if (isset($node['next'][$word])) &#123; $this-&gt;insert($node['next'][$word], $words); &#125; else &#123; $tmp_node = array( 'depth' =&gt; $node['depth'] + 1, 'next' =&gt; array(), ); $node['next'][$word] = $tmp_node; $this-&gt;insert($node['next'][$word], $words); &#125; &#125; private function query($node, $words, &amp;$matched) &#123; $word = array_shift($words); if (isset($node['next'][$word])) &#123; array_push($matched, $word); $keys = array_keys($node['next'][$word]['next']); if (!empty($keys) &amp;&amp; in_array('`', $keys)) &#123; if (count($keys) == 1) &#123; return true; &#125; else &#123; $this-&gt;matched[] = implode('', $matched); &#125; &#125; return $this-&gt;query($node['next'][$word], $words, $matched); &#125; else &#123; $matched = array(); return false; &#125; &#125;&#125;$tire = new Trie();$msg = '性爱派对';$tire-&gt;append('性');$tire-&gt;append('性爱');$tire-&gt;append('性爱派');print_r($tire-&gt;match($msg));","categories":[{"name":"算法","slug":"算法","permalink":"http://blog.crazylaw.cn/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://blog.crazylaw.cn/tags/%E7%AE%97%E6%B3%95/"},{"name":"树","slug":"树","permalink":"http://blog.crazylaw.cn/tags/%E6%A0%91/"}]},{"title":"基于geohash算法-搜索附近的人","slug":"redis-geohash","date":"2018-01-23T03:45:00.000Z","updated":"2021-03-20T16:25:01.814Z","comments":true,"path":"2018/01/23/redis-geohash/","link":"","permalink":"http://blog.crazylaw.cn/2018/01/23/redis-geohash/","excerpt":"前言目前空间索引的实现有 R 树和其变种 GIST 树、四叉树、网格索引等。 网格索引不再多提，使用普通的 hash 表存储地点和风格之间的映射来实现。今天要介绍的 GeoHash 算法实现的空间索引，虽然是以 B 树实现，但我认为它也借用网格索引的一部分思想。 php-geohash GeoHash 从横向上将整个方形纸分为左右两份，左侧部分为标记为 0， 右侧部分标记为 1； 再将红点所在的部分划分为左右两块，再对红点位置做同样的标识，最后得出红点在横向上的标识为 10; 在纵向上对方形纸做同样的划分，左侧标识为0，右侧标识为 1，得出红点位置在纵向上的标识为 01; 将横向标识和纵向标识合并，规则为 纵向在奇数位，横向在偶数位 (也可纵横相反，但要在整个系统内保持一致)，得出红点在方形纸上的标识为 1001;","text":"前言目前空间索引的实现有 R 树和其变种 GIST 树、四叉树、网格索引等。 网格索引不再多提，使用普通的 hash 表存储地点和风格之间的映射来实现。今天要介绍的 GeoHash 算法实现的空间索引，虽然是以 B 树实现，但我认为它也借用网格索引的一部分思想。 php-geohash GeoHash 从横向上将整个方形纸分为左右两份，左侧部分为标记为 0， 右侧部分标记为 1； 再将红点所在的部分划分为左右两块，再对红点位置做同样的标识，最后得出红点在横向上的标识为 10; 在纵向上对方形纸做同样的划分，左侧标识为0，右侧标识为 1，得出红点位置在纵向上的标识为 01; 将横向标识和纵向标识合并，规则为 纵向在奇数位，横向在偶数位 (也可纵横相反，但要在整个系统内保持一致)，得出红点在方形纸上的标识为 1001; 只标记一个方格显得看不出什么规律，如果我们把这些都空格都标识后会发现 被划分在角落里的四个方格会有同样的前缀. 同样的前缀意味着可以使用 B 树 索引查找有相同前缀的点作为附近的点，GeoHash 算法便是这些同样的前缀上面做文章。 墨卡托投影 墨卡托投影，是正轴等角圆柱投影。由荷兰地图学家墨卡托(G.Mercator)于 1569 年创立。假想一个与地轴方向一致的圆柱切或割于地球，按等角条件，将经纬网投影到圆柱面上，将圆柱面展为平面后，即得本投影。墨卡托投影在切圆柱投影与割圆柱投影中，最早也是最常用的是切圆柱投影。 墨卡托投影简单地说，就是可以 把整个地球平面作为一个正方形来处理，当然地球平面不是严格的正方形，此投影在两极附近的点会有误差，本文专注于原理，纠偏就不多提了。 实现按照墨卡托投影的平面，我们可以按照上面划分方格纸的方式来将整个地球表面划分为各个小方格。 如(116.276349, 40.040875)这个点的经度划分： 经度在 [-180,0) 范围内的标识为 0，经度范围在 [0, 180) 度的标识为 1; 继续划分，经度范围在 [0,90) 的标识为 0，经度范围在 [90,180) 的标识为 1; 这样，我们划分 20 次，方格的精度（见文末对照表）已达到 2m，得到经度的标识二进制串为11010010101011110111; 对纬度同样划分，得到纬度的标识二进制串为10111000111100100111; 我们对它组合，得到 40 位的二进制串11011 01110 00010 01110 11100 10111 01001 11111; 我们将这个二进制串使用 base32 编码（原理同 base64，可以见我的另一篇文章：WEB 开发中的字符集和编码，位编码映射表见下），得到 GeoHash 编码为 3OCO4XJ7; 那么 GeoHash 编码前缀为 3OCO4XJ7的地理点就是离 (116.276349, 40.040875)两米内的点。如果我们把地理位置点和其 GeoHash 编码存入数据库的话，我们要查找 附近两米点的点，只需要限定条件 geo_code like &#39;3OCO4XJ7%&#39;就行了; 边界值问题边界点问题 可是最简版的 GeoHash 还有一个弱点，如下图： 如果每个方格的精度为 2km，那么我们直接按照前缀查询红点附近 2km 的点是查找不到离它很近的黑点的。 要解决这个问题，我们就需要所其周边八个方格也考虑上，将自身方格和周边八个方格内的点都遍历一次，再返回符合要求的点。那么如何知道周边方格的前缀呢？ 仔细观察相邻方格，我们会发现两个小方格会在 经度或纬度的二进制码上相差1；我们通过 GeoHash 码反向解析出二进制码后，将其经度或纬度（或两者）的二进制码加一，再次组合为 GeoHash 码。 Redis 的 GEO 函数版本限制：Redis 的 Geo 系列函数需要&gt;3.2 版本才支持。 问题我们常见的需求是查找 n米 范围内的点，那么 n 米 与 GeoHash 码位数之间的映射如何实现呢？由于 GeoHash 码是由5位二进制码组成，每少一位，精度就会损失 2e(5/2)。 方法当然有的，我们将二进制 GeoHash 码直接索引就可以，但很长的索引长度会导致 B 树 索引查询效率会迅速下降。 方案于是我们接着寻找解决方案，既然使用 base32 转换为 32 进制码 会不好控制精度，保持二进制又导致索引长度过长，那么进制位数和索引长度有没有一个平衡呢？ 另外 Redis 的 sorted set 支持 64 位 的 double 类型的 score，我们把二进制的 GeoHash 码转为十进制放入 Redis 的 sorted set 中，不是可以实现 log(n)的查询效率了么。 redis 的 geo 也有一个很致命的弱点，那就是单一的搜索附近的人，还不能带其他的连带条件，所以需要二次过滤才可以，增加了额外的开销。 APIgeoaddgeoadd 用来增加地理位置的坐标，可以批量添加地理位置，命令格式为： GEOADD key longitude latitude member [longitude latitude member ...] key 标识一个地理位置的集合。longitude latitude member 标识了一个地理位置的坐标。longitude 是地理位置的经度，latitude 是地理位置的纬度。member 是该地理位置的名称。GEOADD 可以批量给集合添加一批地理位置。geopos geopos 可以获取地理位置的坐标，可以批量获取多个地理位置的坐标，命令格式为： GEOPOS key member [member ...] geodistgeodist 用来获取两个地理位置的距离，命令格式为： GEODIST key member1 member2 [m|km|ft|mi] 单位可以指定为以下四种类型： m：米，距离单位默认为米，不传递该参数则单位为米。 km：公里。 mi：英里。 ft：英尺。 georadiusgeoradius 可以根据给定地理位置坐标获取指定范围内的地理位置集合。命令格式为： GEORADIUS key longitude latitude radius [m|km|ft|mi] [WITHCOORD] [WITHDIST] [ASC|DESC] [WITHHASH] [COUNT count] longitude latitude 标识了地理位置的坐标，radius 表示范围距离，距离单位可以为 m|km|ft|mi，还有一些可选参数： WITHCOORD：传入 WITHCOORD 参数，则返回结果会带上匹配位置的经纬度。 WITHDIST：传入 WITHDIST 参数，则返回结果会带上匹配位置与给定地理位置的距离。 ASC|DESC：默认结果是未排序的，传入 ASC 为从近到远排序，传入 DESC 为从远到近排序。 WITHHASH：传入 WITHHASH 参数，则返回结果会带上匹配位置的 hash 值。 COUNT count：传入 COUNT 参数，可以返回指定数量的结果。 georadiusbymembergeoradiusbymember 可以根据给定地理位置获取指定范围内的地理位置集合。georadius 命令传递的是坐标，georadiusbymember 传递的是地理位置。georadius 更为灵活，可以获取任何坐标点范围内的地理位置。但是大多数时候，只是想获取某个地理位置附近的其他地理位置，使用 georadiusbymember 则更为方便。georadiusbymember 命令格式为（命令可选参数与 georadius 含义一样）： GEORADIUSBYMEMBER key member radius [m|km|ft|mi] [WITHCOORD] [WITHDIST] [ASC|DESC] [WITHHASH] [COUNT count] geohashgeohash 可以获取某个地理位置的 geohash 值。geohash 是将二维的经纬度转换成字符串 hash 值的算法，后面会具体介绍 geohash 原理。可以批量获取多个地理位置的 geohash 值。命令格式为： GEOHASH key member [member ...] 参考文献：https://www.cnblogs.com/zhenbianshu/p/6863405.html","categories":[{"name":"算法","slug":"算法","permalink":"http://blog.crazylaw.cn/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://blog.crazylaw.cn/tags/%E7%AE%97%E6%B3%95/"},{"name":"Redis","slug":"Redis","permalink":"http://blog.crazylaw.cn/tags/Redis/"}]},{"title":"docker安装php7+nginx事例","slug":"docker-php-nginx","date":"2017-11-02T01:34:00.000Z","updated":"2021-03-20T16:25:01.804Z","comments":true,"path":"2017/11/02/docker-php-nginx/","link":"","permalink":"http://blog.crazylaw.cn/2017/11/02/docker-php-nginx/","excerpt":"简介docker 是由 go 语言开发的的一个虚拟机容器，通过层的概念，把 fs(filesysytem)连接起来，形成一个可以独立的系统。其中涉及到了宿主机的概念，在这里，由于我所用的环境基本都是 centos7，所以这篇文章讲诉的就是 centos7 中使用 docker 安装 php7+nginx 的事例。后续我会发布一些镜像以及 dockerFile 提供大家学习参考。 需要注意的是：docker 运动的服务都需要以前台的模式运行 安装 docker1yum install -y docker 由于我们学习如何使用 docker，所以这里就选择了的直接采用 yum 安装了。 安装创建 docker 容器操作用户由于 docker 容器基于宿主机，所以我们可以在宿主机中创建好对容器操作的用户 123&#x2F;&#x2F; -g -u 参数分别指的的是gid，和uidgroupadd -g 2017 docker-groupadduser -g 2017 -u 2017 docker-user","text":"简介docker 是由 go 语言开发的的一个虚拟机容器，通过层的概念，把 fs(filesysytem)连接起来，形成一个可以独立的系统。其中涉及到了宿主机的概念，在这里，由于我所用的环境基本都是 centos7，所以这篇文章讲诉的就是 centos7 中使用 docker 安装 php7+nginx 的事例。后续我会发布一些镜像以及 dockerFile 提供大家学习参考。 需要注意的是：docker 运动的服务都需要以前台的模式运行 安装 docker1yum install -y docker 由于我们学习如何使用 docker，所以这里就选择了的直接采用 yum 安装了。 安装创建 docker 容器操作用户由于 docker 容器基于宿主机，所以我们可以在宿主机中创建好对容器操作的用户 123&#x2F;&#x2F; -g -u 参数分别指的的是gid，和uidgroupadd -g 2017 docker-groupadduser -g 2017 -u 2017 docker-user 创建几个和 docker 容器服务配置有关的目录1234567891011&#x2F;&#x2F; 用于docker内nginx服务访问项目目录mkdir -p &#x2F;docker-www&#x2F;&#x2F; 用于修改nginx配置mkdir -p &#x2F;docker-config&#x2F;nginx_config&#x2F;&#x2F; 用于修改php配置mkdir -p &#x2F;docker-config&#x2F;php_config&#x2F;&#x2F; 用于修改php-fpm配置mkdir -p &#x2F;docker-config&#x2F;php-fpm_config 创建一个 php 入口文件来检测 docker 中的 nginx 服务是否正常1touch &#x2F;docker-www&#x2F;index.php 123456789101112131415&lt;?php/** * * index.php * * @author Caiwh &lt;471113744@qq.com&gt; * @version 2017年10月31日 * @copyright Copyright caiwh's code **/echo 'This is Docker Index.php';echo PHP_EOL;echo 'Hello-World'; 从 hub.docker 中拉取 php 镜像hub.docker 地址：https://hub.docker.com/_/php/ 由于我们这里需要配合 nginx 使用，所以我们可以指选择 fpm 系列的。 123&#x2F;&#x2F; 这里我选择7.1.11-fpm版本，想要了解这个镜像的基系统的话，可以去查看dockerFile，官方提供的几乎全部都是基于debian系统的。docker pull php:7.1.11-fpm 从 php 官方中拉取对应版本的 php.ini 1curl -o &#x2F;docker-config&#x2F;php_config&#x2F;php.ini https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;php&#x2F;php-src&#x2F;php-7.1.11&#x2F;php.ini-production 新建一个 php-fpm 的 zz-docker.conf 用于重写监听端口 9008 1vim &#x2F;docker-config&#x2F;php-fpm_config&#x2F;zz-docker.conf 添加以下内容， 12345[global]daemonize &#x3D; no[www]listen &#x3D; [::]:9008 运行 docker 中的 php 服务12&#x2F;&#x2F; 这里需要注意的是，由于centos7系统主机下有selinux的存在，导致用户操作权限严格受到限制，所以这里我们需要加上--privileged&#x3D;true这一项，如果是其他宿主机的话，可能不存在这个问题。不加也可以。docker run --name php -v &#x2F;docker-www:&#x2F;home&#x2F;wwwroot -v &#x2F;docker-config&#x2F;php_config&#x2F;php.ini:&#x2F;usr&#x2F;local&#x2F;etc&#x2F;php&#x2F;php.ini -v &#x2F;docker-config&#x2F;php-fpm_config&#x2F;zz-docker.conf:&#x2F;usr&#x2F;local&#x2F;etc&#x2F;php-fpm.d&#x2F;zz-docker.conf --privileged&#x3D;true -p 9008:9000 -d php:7.1.11-fpm 修改 docker 内部的用户权限。（这一步貌似没什么意义） 123&#x2F;&#x2F; 设置成刚才我们的用户组docker exec -it php sed -i &quot;s&#x2F;33&#x2F;2017&#x2F;g&quot; &#x2F;etc&#x2F;passwddocker exec -it php sed -i &quot;s&#x2F;33&#x2F;2017&#x2F;g&quot; &#x2F;etc&#x2F;group 从 hub.docker 中拉取 nginx 镜像这里我们就不做版本的选择了，我们直接拉去，默认的 tag 会是 latest，最新版本。 1docker pull nginx 然后我们在/docker-config/nginx_config/里面新增一个 nginx 的项目配置 1234567891011121314151617181920server &#123; listen 80; server_name localhost.com; root /home/wwwroot; index index.html index.htm index.php; error_page 404 /404.html; error_page 500 502 503 504 /50x.html; # 至于php:9008为什么是9008端口呢，是为了可能为本机也有php服务，为了不和宿主机的php服务冲突。 # 至于这个php:9008这个是哪里来的php呢，等下启动nginx服务的时候，我们会用--link选项把nginx容器和php容器连接起来，用到的别名就叫php，这个php会解析称为php容器的ip地址。 location ~ \\.php$ &#123; fastcgi_pass php:9008; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; &#125;&#125; 运行 docker 中的 nginx 服务1docker run --name nginx -v &#x2F;docker-www:&#x2F;home&#x2F;wwwroot -v &#x2F;docker-config&#x2F;nginx_config:&#x2F;etc&#x2F;nginx&#x2F;conf.d --privileged&#x3D;true --link&#x3D;php:php -p 80:80 -d nginx 修改 docker 内部的用户权限。（同上） 123&#x2F;&#x2F; 设置成刚才我们的用户组docker exec -it nginx sed -i &quot;s&#x2F;104:107&#x2F;docker-user:2017&#x2F;g&quot; &#x2F;etc&#x2F;passwddocker exec -it nginx sed -i &quot;s&#x2F;107&#x2F;2017&#x2F;g&quot; &#x2F;etc&#x2F;group","categories":[{"name":"Docker","slug":"Docker","permalink":"http://blog.crazylaw.cn/categories/Docker/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"http://blog.crazylaw.cn/tags/PHP/"},{"name":"Docker","slug":"Docker","permalink":"http://blog.crazylaw.cn/tags/Docker/"},{"name":"Nginx","slug":"Nginx","permalink":"http://blog.crazylaw.cn/tags/Nginx/"}]},{"title":"Centos7安装swoole2.x协程系列","slug":"linux-make-swoole2","date":"2017-09-08T02:47:00.000Z","updated":"2021-03-20T16:25:01.808Z","comments":true,"path":"2017/09/08/linux-make-swoole2/","link":"","permalink":"http://blog.crazylaw.cn/2017/09/08/linux-make-swoole2/","excerpt":"说明对于一个合格的 php 后端开发者而言，swoole 的知名度，估计是大家都知道的，swoole 分为 2 个系列，一个是1.9.x系列，一个是2.x系列，两个系列的区别在于1.9系列是原 swoole 成员研发的主分支，2.x系列是由腾讯成员研发的副分支。1.9.x系列底层原生不支持协程的概念，跟着 php 原生态走，但是2.x底层采用的是协程的概念进行研发，基本上需要用到网络 io 的地方在特定的情况下都会去触发协程，记住当前的VM stack的信息，然后通过Eventloop事件去处理别的请求，形成一个相对于原生态的阻塞的执行过程来说，这种非阻塞的处理方式，把 1+2 = 3 的 IO 时间变成了 max(1,2) =2。从而在高并发的请求下，可以更好的承载更多的请求。 之前我有过一篇文章说的就是安装 v2.0.5 的。但是现在重新装一次，说明一下。 下载 git1git clone -b v.2.0.9 https:&#x2F;&#x2F;github.com&#x2F;swoole&#x2F;swoole-src.git","text":"说明对于一个合格的 php 后端开发者而言，swoole 的知名度，估计是大家都知道的，swoole 分为 2 个系列，一个是1.9.x系列，一个是2.x系列，两个系列的区别在于1.9系列是原 swoole 成员研发的主分支，2.x系列是由腾讯成员研发的副分支。1.9.x系列底层原生不支持协程的概念，跟着 php 原生态走，但是2.x底层采用的是协程的概念进行研发，基本上需要用到网络 io 的地方在特定的情况下都会去触发协程，记住当前的VM stack的信息，然后通过Eventloop事件去处理别的请求，形成一个相对于原生态的阻塞的执行过程来说，这种非阻塞的处理方式，把 1+2 = 3 的 IO 时间变成了 max(1,2) =2。从而在高并发的请求下，可以更好的承载更多的请求。 之前我有过一篇文章说的就是安装 v2.0.5 的。但是现在重新装一次，说明一下。 下载 git1git clone -b v.2.0.9 https:&#x2F;&#x2F;github.com&#x2F;swoole&#x2F;swoole-src.git 进入目录 1cd swoole-src 利用 phpize 提取出编译文件 1phpize 开始编译，注意需要开启几项必须的东西 1.&#x2F;configure --enable-coroutine --enable-openssl 这里我需要开启写协程和 ssl 证书 然后编译完毕之后。 添加到 php.ini 里面即可。 开启 swoole 高性能开发之旅 记住检查 swoole 是否安装了 1php -m | grep swoole 查看 swoole 版本 1php --ri swoole 异步 redis 编译添加1--enable-async-redis 需要安装依赖一个官方的 hredis 的动态 so 库 hiredis 下载地址：https://github.com/redis/hiredis/releases 安装完毕之后,如果 swoole 编译完成，之后，如果出现了如下问题，可以手动 ldconfig，添加搜索动态库目录 libhiredis.so.0.13: cannot open shared object file: No such file or directory in Unknown on line 0， 1231. vim &#x2F;etc&#x2F;ld.so.conf2. 添加 &#x2F;usr&#x2F;local&#x2F;lib3. sudo ldconfig 然后再重新编译 12make clean&#x2F;&#x2F; make &amp;&amp; make install","categories":[{"name":"Linux","slug":"Linux","permalink":"http://blog.crazylaw.cn/categories/Linux/"},{"name":"PHP","slug":"Linux/PHP","permalink":"http://blog.crazylaw.cn/categories/Linux/PHP/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.crazylaw.cn/tags/Linux/"},{"name":"PHP","slug":"PHP","permalink":"http://blog.crazylaw.cn/tags/PHP/"},{"name":"Swoole","slug":"Swoole","permalink":"http://blog.crazylaw.cn/tags/Swoole/"}]},{"title":"linux-make-tcpdump","slug":"linux-make-tcpdump","date":"2017-08-22T10:06:24.000Z","updated":"2021-03-20T16:25:01.809Z","comments":true,"path":"2017/08/22/linux-make-tcpdump/","link":"","permalink":"http://blog.crazylaw.cn/2017/08/22/linux-make-tcpdump/","excerpt":"说明TCPDUMP 可以帮写我们抓包，相当于 window 下的wireshark，为了我们更好的深入分析，我们把 tcpdump 也安装了。 下载 tcpdump-国内镜像 找到最新版,安装 tcpdump 需要把 libpcap 也安装了. 安装 libpcap下载 1cd &#x2F;usr&#x2F;local&#x2F;src &amp;&amp; sudo mkdir libpcap &amp;&amp; sudo wget http:&#x2F;&#x2F;www.tcpdump.org&#x2F;release&#x2F;libpcap-1.8.1.tar.gz &amp;&amp; sudo tar -zxvf &amp;&amp; sudo tar -zxvf libpcap-1.8.1.tar.gz &amp;&amp; cd libpcap-1.8.1","text":"说明TCPDUMP 可以帮写我们抓包，相当于 window 下的wireshark，为了我们更好的深入分析，我们把 tcpdump 也安装了。 下载 tcpdump-国内镜像 找到最新版,安装 tcpdump 需要把 libpcap 也安装了. 安装 libpcap下载 1cd &#x2F;usr&#x2F;local&#x2F;src &amp;&amp; sudo mkdir libpcap &amp;&amp; sudo wget http:&#x2F;&#x2F;www.tcpdump.org&#x2F;release&#x2F;libpcap-1.8.1.tar.gz &amp;&amp; sudo tar -zxvf &amp;&amp; sudo tar -zxvf libpcap-1.8.1.tar.gz &amp;&amp; cd libpcap-1.8.1 编译 1sudo .&#x2F;configure 发现告诉我们需要安装 flex 我的 yum 仓库没有 flex，所以就需要去找了。找了几个地方之后，发现只有 github 有最新的版本了。 1git clone https:&#x2F;&#x2F;github.com&#x2F;westes&#x2F;flex.git 进入到 flex 目录 安装 flex 1sudo .&#x2F;autogen.sh 发现又有安装依赖了，说我没有 libtoolize 好的，我又要去找了 给大家介绍一个国内 gun 库 国内 GUN 库 基本所有 GUN 库都可以在这里找到相关的信息 找到 libtoolize 1sudo mkdir -p &#x2F;usr&#x2F;local&#x2F;src&#x2F;libtoolize &amp;&amp; sudo wget http:&#x2F;&#x2F;ftp.gnu.org&#x2F;gnu&#x2F;libtool&#x2F;libtool-2.4.tar.gz &amp;&amp; cd &#x2F;usr&#x2F;local&#x2F;src&#x2F;libtoolize&#x2F;libtool-2.4","categories":[{"name":"协议","slug":"协议","permalink":"http://blog.crazylaw.cn/categories/%E5%8D%8F%E8%AE%AE/"},{"name":"网络","slug":"协议/网络","permalink":"http://blog.crazylaw.cn/categories/%E5%8D%8F%E8%AE%AE/%E7%BD%91%E7%BB%9C/"},{"name":"Linux","slug":"协议/网络/Linux","permalink":"http://blog.crazylaw.cn/categories/%E5%8D%8F%E8%AE%AE/%E7%BD%91%E7%BB%9C/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.crazylaw.cn/tags/Linux/"},{"name":"协议","slug":"协议","permalink":"http://blog.crazylaw.cn/tags/%E5%8D%8F%E8%AE%AE/"},{"name":"网络","slug":"网络","permalink":"http://blog.crazylaw.cn/tags/%E7%BD%91%E7%BB%9C/"},{"name":"TcpDump","slug":"TcpDump","permalink":"http://blog.crazylaw.cn/tags/TcpDump/"}]},{"title":"Centos7下自行编译PHP","slug":"linux-make-php","date":"2017-08-16T04:02:00.000Z","updated":"2021-03-20T16:25:01.808Z","comments":true,"path":"2017/08/16/linux-make-php/","link":"","permalink":"http://blog.crazylaw.cn/2017/08/16/linux-make-php/","excerpt":"说明为了满足我的一篇博文，我单独抽出来写一篇 PHP 的编译篇 下载最好不要去 github 上下载，因为你不知道哪一个是稳定版，我们最好去官网寻找最新的稳定版下载 官网下载 1234567891011cd &#x2F;usr&#x2F;local&#x2F;src&#x2F;sudo mkdir phpcd phpwget xxxxx.php -O php-xx.tar.gztar -zxvf php-xx.tar.gzcd php-xx","text":"说明为了满足我的一篇博文，我单独抽出来写一篇 PHP 的编译篇 下载最好不要去 github 上下载，因为你不知道哪一个是稳定版，我们最好去官网寻找最新的稳定版下载 官网下载 1234567891011cd &#x2F;usr&#x2F;local&#x2F;src&#x2F;sudo mkdir phpcd phpwget xxxxx.php -O php-xx.tar.gztar -zxvf php-xx.tar.gzcd php-xx 在这里，我的 php 版本是 php-7.1.8 修改源码目录所有者 1chown -R caiwh:caiwh php-7.1.8 进入目录 1cd php-7.1.8 最小化安装的模块 curl扩展 (使用 curl 的 API) –with-curl fpm扩展 （管理 PHP 进程的服务） –enable-fpm openssl扩展 (打开 ssl 服务，很多服务都依赖这个，必须安装，例如：composer、mysql 的 ssl 等等) –with-openssl mysqlnd扩展 （mysql native drive） –enable-mysqlnd –mysql-pdo-mysql=mysqlnd gd扩展 （可以操作图片） –enable-gd –with-jpeg-dir (GD 库默认不支持 jpeg 格式，需要额外的添加) zlib扩展 (压缩解压缩服务) –enable-zlib –with-zlib –with-libzip mb扩展 (可以使用于 mb 系列的函数，例如汉字的截止之类的) –enable-mbstring socket扩展 （可以使用 socket 系列的函数） –enable-sockets x-debug扩展 （断点调试用，但是在生产环境不应该安装这个扩展，因为会拖底性能） –enable-debug readline扩展 （可以在 CLI 模式下直接交互，例如 php -a） –enable-readline 最后的编译参数1.&#x2F;configure --prefix&#x3D;&#x2F;usr&#x2F;local&#x2F;php --with-config-file-path&#x3D;&#x2F;usr&#x2F;local&#x2F;php&#x2F;etc --with-config-file-scan-dir&#x3D;&#x2F;usr&#x2F;local&#x2F;php&#x2F;etc&#x2F;php.d --enable-mysqlnd --with-pdo-mysql&#x3D;mysqlnd --with-gd --with-jpeg-dir --with-curl --with-readline --with-openssl --with-zlib --with-libzip --enable-mbstring --enable-sockets --enable-bcmath --enable-fpm --with-fpm-user&#x3D;phper --with-fpm-group&#x3D;phper 好的，开始补类库的过程，这个时候，告诉我没有 libxm2，所以我又要去安装了，这里我们再次用 yum 安装 让我们先看看我们的源是否有这些包 1yum search libxml2 OK，开始安装 1sudo yum install libxml2-devel libxml2 -y 再次编译。 好的，我还没有安装 libcurl 类，同样的做法 1sudo yum install libcurl-devel libcurl -y 再次编译。 还有一些问题，针对对应的类库安装对应的，这里就不一一列出来的，一次过写出来了 1sudo yum install libjpeg-turbo-devel libjpeg-turbo libpng-devel libpng readline-devel readline -y 再次编译。 编译完成！！ 安装1sudo make &amp;&amp; make install 安装的速度取决你的服务器配置。 ….Loading….….Loading….….Loading…. ….Waiting….….Waiting….….Waiting…. 安装完毕！！ 配置 php-fpm1cd sapi&#x2F;fpm 开始调试 php-fpm 1.&#x2F;php-fpm -h 讲一下几个重要的参数 -c 指定 php.ini 配置文件的路径 -y 指定 php-fpm.conf 配置文件的路径 -n 不用 php.ini 运行 1sudo .&#x2F;php-fpm -t 好的，我们发现测试失败，说是没有配置文件，我们去这个目录下，发现又一个文件叫 php-fpm.conf.default copy 一份出来，把 default 去掉 1sudo cp &#x2F;usr&#x2F;local&#x2F;php&#x2F;etc&#x2F;php-fpm.conf.default &#x2F;usr&#x2F;local&#x2F;php&#x2F;etc&#x2F;php-fpm.conf 开启 pid 文件 详细的配置文件也要弄一份出来 1sudo cp &#x2F;usr&#x2F;local&#x2F;php&#x2F;etc&#x2F;php-fpm.d&#x2F;www.conf.default &#x2F;usr&#x2F;local&#x2F;php&#x2F;etc&#x2F;php-fpm.d&#x2F;www.conf 测试通过。 因为我们用了 user=phper 和 group=phper 的用户去启动 php-fpm 的 worker 进程。所以，我要创建这么一个不可登录的用户 1useradd -M -s &#x2F;sbin&#x2F;nologin -U phper OK。 启动成功。完美。 把服务加入到 Systemctl首先，需要这么一个 shell 脚本 123cd &#x2F;usr&#x2F;local&#x2F;php&#x2F;sbin&#x2F;sudo vim php-fpm.sh 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152#! /bin/sh### BEGIN INIT INFO# Provides: caiwh[471113744@qq.com]# Short-Description: starts php-fpm# Description: starts the PHP FastCGI Process Manager daemon### END INIT INFOprefix=/usr/local/phpexec_prefix=$&#123;prefix&#125;php_fpm_BIN=$&#123;exec_prefix&#125;/sbin/php-fpmphp_fpm_CONF=$&#123;prefix&#125;/etc/php-fpm.confphp_fpm_PID=$&#123;prefix&#125;/var/run/php-fpm.pidphp_opts=\"--fpm-config $php_fpm_CONF --pid $php_fpm_PID\"wait_for_pid () &#123; try=0 while test $try -lt 35 ; do case \"$1\" in 'created') if [ -f \"$2\" ] ; then try='' break fi ;; 'removed') if [ ! -f \"$2\" ] ; then try='' break fi ;; esac echo -n . try=`expr $try + 1` sleep 1 done&#125;case \"$1\" in start) echo -n \"Starting php-fpm \" $php_fpm_BIN --daemonize $php_opts if [ \"$?\" != 0 ] ; then echo \" failed\" exit 1 fi wait_for_pid created $php_fpm_PID if [ -n \"$try\" ] ; then echo \" failed\" exit 1 else echo \" done\" fi ;; stop) echo -n \"Gracefully shutting down php-fpm \" if [ ! -r $php_fpm_PID ] ; then echo \"warning, no pid file found - php-fpm is not running ?\" exit 1 fi kill -QUIT `cat $php_fpm_PID` wait_for_pid removed $php_fpm_PID if [ -n \"$try\" ] ; then echo \" failed. Use force-quit\" exit 1 else echo \" done\" fi ;; status) if [ ! -r $php_fpm_PID ] ; then echo \"php-fpm is stopped\" exit 0 fi PID=`cat $php_fpm_PID` if ps -p $PID | grep -q $PID; then echo \"php-fpm (pid $PID) is running...\" else echo \"php-fpm dead but pid file exists\" fi ;; force-quit) echo -n \"Terminating php-fpm \" if [ ! -r $php_fpm_PID ] ; then echo \"warning, no pid file found - php-fpm is not running ?\" exit 1 fi kill -TERM `cat $php_fpm_PID` wait_for_pid removed $php_fpm_PID if [ -n \"$try\" ] ; then echo \" failed\" exit 1 else echo \" done\" fi ;; restart) $0 stop $0 start ;; reload) echo -n \"Reload service php-fpm \" if [ ! -r $php_fpm_PID ] ; then echo \"warning, no pid file found - php-fpm is not running ?\" exit 1 fi kill -USR2 `cat $php_fpm_PID` echo \" done\" ;; configtest) $php_fpm_BIN -t ;; *) echo \"Usage: $0 &#123;start|stop|force-quit|restart|reload|status|configtest&#125;\" exit 1 ;;esac 保存退出，然后修改权限 1sudo chmod 755 php-fpm.sh 然后就可以写 systemctl 的 service 了。 1sudo vim &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;php-fpm.service 把以下代码拷贝进去 123456789101112131415[Unit]Description&#x3D;Belong caiwh - php-fpmDocumentation&#x3D; &#x2F;usr&#x2F;local&#x2F;php&#x2F;etc&#x2F;php-fpm&#x2F;After&#x3D;network.target remote-fs.target nss-lookup.target[Service]Type&#x3D;forkingExecStartPre&#x3D;&#x2F;usr&#x2F;local&#x2F;php&#x2F;sbin&#x2F;php-fpm.sh configtestExecStart&#x3D;&#x2F;usr&#x2F;local&#x2F;php&#x2F;sbin&#x2F;php-fpm.sh startExecReload&#x3D;&#x2F;usr&#x2F;local&#x2F;php&#x2F;sbin&#x2F;php-fpm.sh restartExecStop&#x3D;&#x2F;usr&#x2F;local&#x2F;php&#x2F;sbin&#x2F;php-fpm.sh stopPrivateTmp&#x3D;true[Install]WantedBy&#x3D;multi-user.target 1systemctl daemon-reload 完成 systemctl 重启 然后就可以用 123#1.systemctl start php-fpm 启动服务#2.systemctl stop php-fpm 停止服务#3.systemctl restart php-fpm 重启服务 完美结合","categories":[{"name":"Linux","slug":"Linux","permalink":"http://blog.crazylaw.cn/categories/Linux/"},{"name":"PHP","slug":"Linux/PHP","permalink":"http://blog.crazylaw.cn/categories/Linux/PHP/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.crazylaw.cn/tags/Linux/"},{"name":"PHP","slug":"PHP","permalink":"http://blog.crazylaw.cn/tags/PHP/"},{"name":"编译安装","slug":"编译安装","permalink":"http://blog.crazylaw.cn/tags/%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85/"}]},{"title":"Systemctl服务管理软件","slug":"linux-systemctl","date":"2017-08-16T03:21:00.000Z","updated":"2021-03-20T16:25:01.810Z","comments":true,"path":"2017/08/16/linux-systemctl/","link":"","permalink":"http://blog.crazylaw.cn/2017/08/16/linux-systemctl/","excerpt":"说明由于特殊原因!!刚才写的文章没保存下来!!!接下来简写.!!! Centos7 和 Centos7 之前的版本有几点比较重要的不同,一个是 firewall-cmd 代替了 iptables，然后就是 systemctl 代替了 service。这里，现在我就写一下如何使用 systemctl 管理服务。 服务的基本命令 systemctl status xx （服务状态） systemctl start xx （服务启动） systemctl stop xx （停止服务） systemctl restart xx（重启服务） systemctl enable xx （服务随开机启动） systemctl disable xx （服务取消随启动） systemctl cat xx （查看服务的配置信息） systemctl daemon-reload (每次修改了一个 service 配置或者新增了一个配置，都需要执行这个命令，让 systemctl 服务重启，然后重新加载服务配置文件) systemctl list-dependencies [target name]（查看依赖树状图，十分的直观好用）","text":"说明由于特殊原因!!刚才写的文章没保存下来!!!接下来简写.!!! Centos7 和 Centos7 之前的版本有几点比较重要的不同,一个是 firewall-cmd 代替了 iptables，然后就是 systemctl 代替了 service。这里，现在我就写一下如何使用 systemctl 管理服务。 服务的基本命令 systemctl status xx （服务状态） systemctl start xx （服务启动） systemctl stop xx （停止服务） systemctl restart xx（重启服务） systemctl enable xx （服务随开机启动） systemctl disable xx （服务取消随启动） systemctl cat xx （查看服务的配置信息） systemctl daemon-reload (每次修改了一个 service 配置或者新增了一个配置，都需要执行这个命令，让 systemctl 服务重启，然后重新加载服务配置文件) systemctl list-dependencies [target name]（查看依赖树状图，十分的直观好用） 添加服务我们的服务配置文件存放的路径为 1&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F; 在这个文件下，你可以看到这些服务，但是你的 nginx 服务不是 yum 等软件管理工具安装的话，这里是不会存在nginx.service这个文件的，这个文件需要我们手动添加。 这里，以添加 nginx 服务到 systemctl 为例子。 1sudo vim &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;nginx.service 以下是我 nginx 的配置文件的信息 123456789101112131415161718192021[Unit]Description=Belong caiwh - nginx - high performance web serverDocumentation= ----&gt; http://nginx.org/en/docs/After=network.target remote-fs.target nss-lookup.target[Service]Type=forkingPIDFile=/var/run/nginx.pidExecStartPre=/usr/sbin/nginx -t -c /etc/nginx/nginx.confExecStart=/usr/sbin/nginx -c /etc/nginx/nginx.confExecReload=/bin/kill -s HUP $MAINPIDExecStop=/bin/kill -s QUIT $MAINPID#if you don't like use ExecStop,you can use:#KillSignal=SIGQUIT##why KillMode use process? because the nginx master process will be control the child process#KillMode=processPrivateTmp=trueTimeoutStopSec=5[Install]WantedBy=multi-user.target 保存。然后执行如下命令： 1sudo systemctl daemon-reload 然后启动 nginx: 1sudo systemctl start nginx 接下来，要介绍，我们 systemctl 的重要内容了。 开启启动和不开启启动的区别开启启动的情况下，systemctl 只会启动以下路径的服务配置 1/etc/systemd/system 但是，一般我们的基本服务，都是配置在以下路径： 1&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system 当我们用到上述介绍到的命令 systemctl enable xx 的时候，其实就是在 /etc/systemd/system 目录里面创建以下软连接到 /usr/lib/systemd/system 里面 [Unit] 区块：启动顺序与依赖关系 Description 给出当前服务的简单描述 Documentation 给出文档位置 After 表示该服务要在当前目标组或者目标服务启动之后才能启动（在这里就是 nginx 服务需要在 network.target remote-fs.target nss-lookup.target 启动之后才可以启动） Before 与 After 相对 Wants 表示该服务和哪一些服务有 弱依赖 关系。意思就是，所依赖的服务如果启动失败的话或者异常了，也不会影响到该服务的启动和继续运行 Requires 与 Wants 相反 ，强依赖 [Service] 区块：启动行为 EnvironmentFile 表示该服务自定义的配置参数变量文件，文件已 KEY=VALUE 键值对的形式存在，后续可以用 $KEY 来读取 value。 ExecReload : 重启服务时执行的命令 ExecStop : 停止服务时执行的命令 ExecStartPre : 启动服务之前执行的命令 ExecStart : 启动服务执行的命令 ExecStartPost : 启动服务之后执行的命令 ExecStopPost : 停止服务之后执行的命令 所有的启动设置之前，都可以加上一个连词号（-），表示”抑制错误”，即发生错误的时候，不影响其他命令的执行。比如，EnvironmentFile=-/etc/my-conf（注意等号后面的那个连词号），就表示即使 /etc/my-conf 文件不存在，也不会抛出错误。 Type 启动类型 simple（默认值）：ExecStart 字段启动的进程为主进程 forking：ExecStart 字段将以 fork()方式启动，此时父进程将会退出，子进程将成为主进程 oneshot：类似于 simple，但只执行一次，Systemd 会等它执行完，才启动其他服务 dbus：类似于 simple，但会等待 D-Bus 信号后启动 notify：类似于 simple，启动结束后会发出通知信号，然后 Systemd 再启动其他服务 idle：类似于 simple，但是要等到其他任务都执行完，才会启动该服务。一种使用场合是为让该服务的输出，不与其他服务的输出相混合 KillMode 杀死进程模式 control-group（默认值）：当前控制组里面的所有子进程，都会被杀掉 process：只杀主进程 mixed：主进程将收到 SIGTERM 信号，子进程收到 SIGKILL 信号 none：没有进程会被杀掉，只是执行服务的 stop 命令。 Restar 重启进程模式 no（默认值）：退出后不会重启 on-success：只有正常退出时（退出状态码为 0），才会重启 on-failure：非正常退出时（退出状态码非 0），包括被信号终止和超时，才会重启 on-abnormal：只有被信号终止和超时，才会重启 on-abort：只有在收到没有捕捉到的信号终止时，才会重启 on-watchdog：超时退出，才会重启 always：不管是什么退出原因，总是重启 [Install] 区块 WantedBy 表示该服务所在的 Target 组 这个设置非常重要，因为执行 systemctl enable xx 命令时，xx.service 的一个符号链接，就会放在/etc/systemd/system 目录下面的 WantedBy 命名的目录的子目录之中。","categories":[{"name":"Linux","slug":"Linux","permalink":"http://blog.crazylaw.cn/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.crazylaw.cn/tags/Linux/"},{"name":"Centos7","slug":"Centos7","permalink":"http://blog.crazylaw.cn/tags/Centos7/"},{"name":"Systemctl","slug":"Systemctl","permalink":"http://blog.crazylaw.cn/tags/Systemctl/"}]},{"title":"字符编码深度剖析","slug":"ascii-code-2","date":"2017-07-27T14:44:00.000Z","updated":"2021-03-20T16:25:01.801Z","comments":true,"path":"2017/07/27/ascii-code-2/","link":"","permalink":"http://blog.crazylaw.cn/2017/07/27/ascii-code-2/","excerpt":"","text":"前言在这里，我们对如下4种编码进行对比 unicode编码 utf8编码 ascii编码 asni编码 ASCII编码和ASNI编码的区别字符内码(charcter code)指的是用来代表字符的内码.读者在输入和存储文档时都要使用内码,内码分为 单字节内码 – Single-Byte character sets (SBCS),可以支持255（ 8bits = (2^8)-1 = 255）个字符编码. 双字节内码 – Double-Byte character sets)(DBCS),可以支持65000个字符编码（最多支持16 bits = (2^16)-1 = 65535）. 前者即为ASCII编码，后者对应ANSI. 至于简体中文编码GB2312，实际上它是ANSI的一个代码页 UNICODE编码unicode 是一种编码表格，例如，给一个汉字规定一个代码。类似 GB2312-1980, GB18030等，只不过字集不同。 一个unicode码可能转成长度为一个BYTE,或两个，三个，四个BYTE（最多6个byte）的UTF8码，取决于unicode码的值。英文unicode码因为值小于0x80（单字节字符）,只要用一个BYTE的UTF8传送，比送unicode两个bytes快。 如上，ANSI有很多代码页，使用不同代码页的内码无法在其他代码也正常显示，这就是为什么日文版／繁体中文版游戏无法在简体中文平台直接显示的原因． Unicode也是一种字符编码方法，不过它是由国际组织设计，可以容纳全世界所有语言文字的编码方案．它是一种２字节编码，能够提供６５５３５个字符， 这个数字是不够表示所有的字符的（汉语就有５５０００多字符），所以，通过一个代理对的机制来实现附加的９１７，４７６个字符表示，以达到所有字符都具有唯一编码． Unicode和BigEndianUnicode这两者只是存储顺序不同，如＂A＂的unicode编码为65 00 其BigEndianUnicode编码为00 65 其实就是类比于我之前说过的大小端的介绍 UTF8编码UTF8全称：Unicode Transformation Format – 8 bit（unicode转化格式为8位的流数据） Unicode是一个字符集，而UTF-8是Unicode的其中一种，Unicode是定长的都为双字节，而UTF-8是可变的，对于汉字来说Unicode占有的字节比UTF-8占用的字节少1个字节。Unicode为双字节，而UTF-8中汉字占三个字节。 是Unicode传送格式。即把Unicode文件转换成BYTE的传送流。 其中UTF-16和Unicode编码大致一样, UTF-8就是以8位为单元对Unicode进行编码。 UTF-8编码字符理论上可以最多到6个字节长,然而16位BMP（Basic Multilingual Plane）字符最多只用到3字节长。从Unicode到UTF-8的编码方式如下： Unicode编码(16进制) UTF-8 字节流(二进制) 0000 - 007F 0xxxxxxx 0080 - 07FF 110xxxxx 10xxxxxx 0800 - FFFF 1110xxxx 10xxxxxx 10xxxxxx 100000 - 1FFFFF 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx 200000 - 3FFFFFF 111110xx 10xxxxxx 10xxxxxx 10xxxxxx 10xxxxxx 4000000 - 7FFFFFFF 1111110x 10xxxxxx 10xxxxxx 10xxxxxx 10xxxxxx 10xxxxxx 例如“我”字的Unicode编码是4f6b。4f6b在0800-FFFF之间，所以肯定要用3字节模板了：1110xxxx 10xxxxxx 10xxxxxx。将4f6b写成二进制是：100 111101 101011， 用这个比特流依次代替模板中的x，得到：11100100 10111101 10101011，即E4 9C AB。 用PHP实现unicode编码和utf8编码的转变123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051&lt;?php/** * UTF-8编码字符理论上可以最多到6个字节长,然而16位BMP（Basic Multilingual Plane）字符最多只用到3字节长。下面看一下UTF-8编码表： * * U-00000000 - U-0000007F: 0xxxxxxx * U-00000080 - U-000007FF: 110xxxxx 10xxxxxx * U-00000800 - U-0000FFFF: 1110xxxx 10xxxxxx 10xxxxxx * U-00010000 - U-001FFFFF: 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx * U-00200000 - U-03FFFFFF: 111110xx 10xxxxxx 10xxxxxx 10xxxxxx 10xxxxxx * U-04000000 - U-7FFFFFFF: 1111110x 10xxxxxx 10xxxxxx 10xxxxxx 10xxxxxx 10xxxxxx *//** * 单字符汉字转换unicode(16进制) * * @param $utf8_str * * @return string */function utf8_transform_unicode($utf8_str)&#123; $unicode = (ord($utf8_str&#123;0&#125;) &amp; 0xF) &lt;&lt; 12; $unicode |= (ord($utf8_str&#123;1&#125;) &amp; 0x3F) &lt;&lt; 6; $unicode |= (ord($utf8_str&#123;2&#125;) &amp; 0x3F); return dechex($unicode);&#125;/** * unicode编码转汉字 * @param $unicode * @return string */function unicode_transform_utf8($unicode)&#123; // 十六进制转十进制 $unicode = (int)hexdec($unicode); $ord_1 = decbin(0xe0 | ($unicode &gt;&gt; 12)); $ord_2 = decbin(0x80 | (($unicode &gt;&gt; 6) &amp; 0x3f)); $ord_3 = decbin(0x80 | ($unicode &amp; 0x3f)); $utf8_str = chr(bindec($ord_1)) . chr(bindec($ord_2)) . chr(bindec($ord_3)); return $utf8_str;&#125;$str = '我';echo utf8_transform_unicode($str);$unicode = '6211';echo unicode_transform_utf8($unicode); 后续，我会出一个用php直接实现编码转换和汉字字符串截取的PHP扩展，都会用到这次的知识，后续见。","categories":[{"name":"协议","slug":"协议","permalink":"http://blog.crazylaw.cn/categories/%E5%8D%8F%E8%AE%AE/"}],"tags":[{"name":"协议","slug":"协议","permalink":"http://blog.crazylaw.cn/tags/%E5%8D%8F%E8%AE%AE/"},{"name":"ascii","slug":"ascii","permalink":"http://blog.crazylaw.cn/tags/ascii/"}]},{"title":"PHP 实现 Promise+协程调用","slug":"ccp","date":"2017-07-27T14:13:00.000Z","updated":"2021-03-20T16:25:01.802Z","comments":true,"path":"2017/07/27/ccp/","link":"","permalink":"http://blog.crazylaw.cn/2017/07/27/ccp/","excerpt":"PHP 实现 Promise+协程调用 github 地址：ccp 觉得好记的给个 Start 喔。 说明在 PHP 当中，我们或多或少会使用到异步编程的思想，但是异步编程难免给我们带来回调地狱的感觉，并且代码可读性十分之差，在 ES6 规范里面，或者更早就可以推出了一个做 Promise 的东西，利用 Promise，你可以用同步的做法来实现异步的操作，代码可读性上大大提高，不仅如此，由于异步编程和异步 IO 的混合使用，导致代码的准确性难以提高，不可否认的是，Promise 的推出，大大的提高了你们编写异步代码的可靠性，虽然这也是会损耗一些十分微小的性能，但是任何取舍都是相对的。","text":"PHP 实现 Promise+协程调用 github 地址：ccp 觉得好记的给个 Start 喔。 说明在 PHP 当中，我们或多或少会使用到异步编程的思想，但是异步编程难免给我们带来回调地狱的感觉，并且代码可读性十分之差，在 ES6 规范里面，或者更早就可以推出了一个做 Promise 的东西，利用 Promise，你可以用同步的做法来实现异步的操作，代码可读性上大大提高，不仅如此，由于异步编程和异步 IO 的混合使用，导致代码的准确性难以提高，不可否认的是，Promise 的推出，大大的提高了你们编写异步代码的可靠性，虽然这也是会损耗一些十分微小的性能，但是任何取舍都是相对的。 参考 ECMAScript 2015 (6th Edition, ECMA-262) Promise ECMAScript Latest Draft (ECMA-262)Promise 支持 PromiseAPI promise-&gt;then primise-&gt;catch Promise::all Promise::race Promise::resolve Promise::reject Promise::warp Promise::co 等等 翻译切换 English.MD","categories":[{"name":"PHP","slug":"PHP","permalink":"http://blog.crazylaw.cn/categories/PHP/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"http://blog.crazylaw.cn/tags/PHP/"},{"name":"Promise","slug":"Promise","permalink":"http://blog.crazylaw.cn/tags/Promise/"},{"name":"ES6","slug":"ES6","permalink":"http://blog.crazylaw.cn/tags/ES6/"}]},{"title":"64位分布式自增发号器","slug":"id-sender","date":"2017-07-13T09:30:00.000Z","updated":"2021-03-20T16:25:01.806Z","comments":true,"path":"2017/07/13/id-sender/","link":"","permalink":"http://blog.crazylaw.cn/2017/07/13/id-sender/","excerpt":"IdCenterSender —PHP 源码实现-64 位分布式自增发号器PHP 实现 64 位分布式 ID 发号器, github 上也有 C 语言版本上的连接可以进去跳转 github 地址 : IdCenterSender 觉得好记的给个 Start 喔。 原理参考 Snowflake 算法,根据自身设计情况扩展了其中的细节","text":"IdCenterSender —PHP 源码实现-64 位分布式自增发号器PHP 实现 64 位分布式 ID 发号器, github 上也有 C 语言版本上的连接可以进去跳转 github 地址 : IdCenterSender 觉得好记的给个 Start 喔。 原理参考 Snowflake 算法,根据自身设计情况扩展了其中的细节 64bits 分成了 4 个部分。 最高位舍弃毫秒级的时间戳,有 41 个 bit.能够使用 139 年，当然这些是可以扩展的,可以通知指定起始时间来延长这个日期长度。也就是说服务启动开始之后就可以持续使用 139 年自定义分布式机器节点 id,占位 12 个 bit,能够支持 8191 个节点。部署的时候可以配置好服务器 id,也就是代码里面的 node_id 变量，每一台机器都需要用不同的 node_id 来标志，就像 mysql 的 server_id 一样进程（毫秒）自增序号。占位 10bit,一毫秒能产生 2047 个 id。 总结特点： 类 snowflake 算法 ID 发号器有效期可以延续从发布开始的 139 年 分布式支持 8191 台机器 单进程调用的情况下，并发每秒支持 200 万个 ID 生成 唯一性保证 同一毫秒内自增变量保证并发的唯一性(采用文件锁的方式对 cache 文件进行锁定)。 使用1234567include_once '../cckeyid/IdCenterSender.php';echo \\cckeyid\\IdCenterSender::getInstance()-&gt;ck_get_new_id(1);echo PHP_EOL;print_r(\\cckeyid\\IdCenterSender::getInstance(true)-&gt;ck_get_new_id(4));","categories":[{"name":"算法","slug":"算法","permalink":"http://blog.crazylaw.cn/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"C","slug":"C","permalink":"http://blog.crazylaw.cn/tags/C/"},{"name":"PHP","slug":"PHP","permalink":"http://blog.crazylaw.cn/tags/PHP/"},{"name":"算法","slug":"算法","permalink":"http://blog.crazylaw.cn/tags/%E7%AE%97%E6%B3%95/"},{"name":"ID发号器","slug":"ID发号器","permalink":"http://blog.crazylaw.cn/tags/ID%E5%8F%91%E5%8F%B7%E5%99%A8/"}]},{"title":"RabbitMQの关键知识整理","slug":"rabbit-mq","date":"2017-07-11T03:45:00.000Z","updated":"2021-03-20T16:25:01.813Z","comments":true,"path":"2017/07/11/rabbit-mq/","link":"","permalink":"http://blog.crazylaw.cn/2017/07/11/rabbit-mq/","excerpt":"说明RabbitMQ 作为一个转发服务器，里面涉及了一些实体的东西，如下： 生产者 (Producter) 消息 (Message) 交换机 (Exchange) 队列 (Queue) 消费者 (Consumer) 在这些实体的东西的基础上，RabbitMQ 重点的是 交换机,队列 交换机与队列说完实体的东西之后，在 交换机 和 队列 之间有许多的规则（玩法），我们增加了如下内容： 队列名 (Queue_name) 交换机类型 (Exchange_type) 路由键 (Routing_key)","text":"说明RabbitMQ 作为一个转发服务器，里面涉及了一些实体的东西，如下： 生产者 (Producter) 消息 (Message) 交换机 (Exchange) 队列 (Queue) 消费者 (Consumer) 在这些实体的东西的基础上，RabbitMQ 重点的是 交换机,队列 交换机与队列说完实体的东西之后，在 交换机 和 队列 之间有许多的规则（玩法），我们增加了如下内容： 队列名 (Queue_name) 交换机类型 (Exchange_type) 路由键 (Routing_key) 交换机类型、路由键的作用 路由键一般可以设置为某一类相同属性定义的集合 fanout (扇形类型) ： 忽略了 Routing_key，类比广播模式，也算是类比发布者、订阅者模式 direct (直接类型) ： 不忽略 Routing_key，为普通String，当 N 个 Consumer 绑定的 Routing_key 一致的时候，和fanout 类型没区别 topics （话题类型） ： 不忽略 Routing_key，Routing_key 额外加多以个规则，如果需要为单词列表的时候，需要用 . 号进行分割，例如(c.a.i)，除此之外，* 可以代表一个占位符，# 可以代表 0 或者多个占位符，例如(*.a.i)，(c.#) headers （头部类型） ： 略略略…日后补充 各类知识交换机分发请求到队列 一个队列对应 N 个消费者 交换机分发请求的时候，用到了 Queue_name 和Routing_key，默认情况下（同一个队列），交换机分发的策略是循环调度，例如，我现在有 2 个消费者，对应一个队列，分别叫C1,C2，第一次消息，队列会发送到C1，第二次消息，队列会发送到C2，以此循环调用。 这里会产生几个问题： 如果 C1 是一个耗时（sleep 1s）的操作，但是 C2 是一个立刻执行完毕的队列。当前队列接受了交换机转发过来的 14 个消息，那么交换机会以循环调度的方式进行执行，消息的分配那么平均，C1 处理了 7 个消息的时候，C2 处理了 7 个消息，只不过 C2 进程会先完成任务，快将近一半的时间。 对策：RabbitMQ 中有一个qos的概念，这个概念可以帮助类比网络模型里面的epoll模式，当其中一个消费者空闲的时候，就把消息转发到该消费者上，在这种情况下的话，大大提高了整体消息消费的速率。但是也有弊端，就是如果所有的消费者都处于忙碌的状态的话，那么久可能需要考虑一下增加多一些消费者了。 如果消费者消费不及时，那么在队列中就会大量堆积消息，占用大量内存，导致服务器宕机或者 RabbitMQ 宕机，并且宕机的时候，大量消息随时面临了丢失的可能，这将会产生是否可怕的严重的后果 对策：RabbitMq 中有一个持久化的概念，持久化支持队列持久化，消息持久化，为的就是防止宕机的时候，这些消息也跟之丢失。 消费者因为某些系统错误结束了，理论上，该消息会转发给另外一个正在运行的消费者去消费，如果这个时候，这个消费者也宕了，那么消息将会在队列中存储，直到下一个消费者连接的时候，会投递到消费者中。 对策：设定 ack 应答机制实现高可用的特性。","categories":[{"name":"MQ","slug":"MQ","permalink":"http://blog.crazylaw.cn/categories/MQ/"}],"tags":[{"name":"MQ","slug":"MQ","permalink":"http://blog.crazylaw.cn/tags/MQ/"}]},{"title":"大端和小端（Big endian and Little endian）","slug":"network-protocol-endian","date":"2017-06-28T06:43:00.000Z","updated":"2021-03-20T16:25:01.811Z","comments":true,"path":"2017/06/28/network-protocol-endian/","link":"","permalink":"http://blog.crazylaw.cn/2017/06/28/network-protocol-endian/","excerpt":"大端和小端（Big endian and Little endian）网络字节序网络字节顺序是TCP/IP中规定好的一种数据表示格式，它与具体的CPU类型、操作系统等无关，从而可以保证数据在不同主机之间传输时能够被正确解释。网络字节顺序采用big endian排序方式。 由此可知，网络字节序是大端序的。那么什么是Big endian呢？ 一、大端和小端的问题对于整型、长整型等数据类型，Big endian 认为第一个字节是最高位字节（按照从低地址到高地址的顺序存放数据的高位字节到低位字节）；而 Little endian 则相反，它认为第一个字节是最低位字节（按照从低地址到高地址的顺序存放数据的低位字节到高位字节）。 例如，假设从内存地址 0x0000 开始有以下数据： 0x0000 0x0001 0x0002 0x0003 0x12 0x34 0xab 0xcd","text":"大端和小端（Big endian and Little endian）网络字节序网络字节顺序是TCP/IP中规定好的一种数据表示格式，它与具体的CPU类型、操作系统等无关，从而可以保证数据在不同主机之间传输时能够被正确解释。网络字节顺序采用big endian排序方式。 由此可知，网络字节序是大端序的。那么什么是Big endian呢？ 一、大端和小端的问题对于整型、长整型等数据类型，Big endian 认为第一个字节是最高位字节（按照从低地址到高地址的顺序存放数据的高位字节到低位字节）；而 Little endian 则相反，它认为第一个字节是最低位字节（按照从低地址到高地址的顺序存放数据的低位字节到高位字节）。 例如，假设从内存地址 0x0000 开始有以下数据： 0x0000 0x0001 0x0002 0x0003 0x12 0x34 0xab 0xcd 如果我们去读取一个地址为 0x0000 的四个字节变量，若字节序为big-endian，则读出结果为0x1234abcd；若字节序为little-endian，则读出结果为0xcdab3412。 如果我们将0x1234abcd 写入到以 0x0000 开始的内存中，则Little endian 和 Big endian 模式的存放结果如下： 地址 0x0000 0x0001 0x0002 0x0003 big-endian 0x12 0x34 0xab 0xcd little-endian 0xcd 0xab 0x34 0x12 一般来说，x86 系列 CPU 都是 little-endian 的字节序，PowerPC 通常是 big-endian，网络字节顺序也是 big-endian还有的CPU 能通过跳线来设置 CPU 工作于 Little endian 还是 Big endian 模式。 对于0x12345678的存储： 小端模式：（从低字节到高字节）地位地址 0x78 0x56 0x34 0x12 大端模式：（从高字节到低字节）地位地址 0x12 0x34 0x56 0x78 二、C语言大端小端转换方法 Big-Endian转换成Little-Endian 123#define BigtoLittle16(A) ((((uint16)(A) &amp; 0xff00) &gt;&gt; 8) | (((uint16)(A) &amp; 0x00ff) &lt;&lt; 8))#define BigtoLittle32(A) ((((uint32)(A) &amp; 0xff000000) &gt;&gt; 24) | (((uint32)(A) &amp; 0x00ff0000) &gt;&gt; 8) | \\(((uint32)(A) &amp; 0x0000ff00) &lt;&lt; 8) | (((uint32)(A) &amp; 0x000000ff) &lt;&lt; 24)) 三、C语言大端小端检测方法123456int i = 1;char *p = (char *)&amp;i;if(*p == 1) printf(\"Little Endian\");else printf(\"Big Endian\"); 大小端存储问题，如果小端方式中（i占至少两个字节的长度）则i所分配的内存最小地址那个字节中就存着1，其他字节是0.大端的话则1在i的最高地址字节处存放，char是一个字节，所以强制将char型量p指向i则p指向的一定是i的最低地址，那么就可以判断p中的值是不是1来确定是不是小端。 联合体union的存放顺序是所有成员都从低地址开始存放，利用该特性就可以轻松地获得了CPU对内存采用Little-endian还是Big-endian模式读写。 1234567891011/*return 1: little-endian, return 0: big-endian*/int checkCPUendian()&#123;union &#123; unsigned int a; unsigned char b; &#125;c; c.a = 1; return (c.b == 1);&#125; 实现同样的功能，来看看Linux 操作系统中相关的源代码是怎么做的： 123static union &#123; char c[4]; unsigned long mylong; &#125; endian_test = &#123;&#123; 'l', '?', '?', 'b' &#125; &#125;;#define ENDIANNESS ((char)endian_test.mylong) Linux 的内核作者们仅仅用一个union 变量和一个简单的宏定义就实现了一大段代码同样的功能！（如果ENDIANNESS=’l’表示系统为little endian，为’b’表示big endian） 四、相关的一些问题问题一： 123char *sz = \"0123456789\";int *p = (int*)sz;printf(\"%x\\n\",*++p); 问：字符’0’对应的十六进制是0x30，请问在x86环境下程序输出是多少？ 解答： 假设 字符串sz地址 从 $0 开始，那么sz在内存的存储为 $0 $1 $2 $3 $4 $5 $6 $7 $8 $9 0x30 0x31 0x32 0x33 0x34 0x35 0x36 0x37 0x38 0x39 当你把char强制类型转化成int后，因为int占四个字节，那么p指向$0，并且p占有的地址是*$0$1$2$3，打印的时候 先进行++p操作，那么p指向$4，此时*p占有的地址是$4$5$6$7*，根据上面地地址存地位，little endian，那么p应该等于0x37363534 问题二： 123int a = 0x12345678;char *p = (char*)(&amp;a);printf(\"%x\\n\",*(p+1)); 例如对于0x12345678，网络字节顺序是这样0x12,0x34,0x56,0x78存储的，这种方式称为big-endianintel处理器是0x78 0x56 0x34 0x12这样来存储的，称为小尾little-endian在x86环境下题目中的p指向0x78，加1后指向0x56 问题三： 12345678910111213#include &lt;stdio.h&gt;union&#123; int i; char x[2];&#125;a;int main()&#123;a.x[0] = 10; a.x[1] = 1; printf(\"%d\",a.i); return 0;&#125; x86下输出答案： 266 （x86下：i内存里存的值是Ox010A，十进制为266） 问题四： 123456789101112131415161718int main()&#123;union &#123; int i; struct &#123; char first; char second; &#125;half; &#125;number; number.i=0x4241; printf(\"%c %c\\n\", number.half.first, number.half.second); number.half.first='a'; number.half.second='b'; printf(\"%x\\n\", number.i); return 0;&#125; x86下输出答案： A B (0x41对应’A’,是低位；Ox42对应’B’,是高位）6261 (number.i和number.half共用一块地址空间0x6261）","categories":[{"name":"协议","slug":"协议","permalink":"http://blog.crazylaw.cn/categories/%E5%8D%8F%E8%AE%AE/"}],"tags":[{"name":"协议","slug":"协议","permalink":"http://blog.crazylaw.cn/tags/%E5%8D%8F%E8%AE%AE/"},{"name":"进制","slug":"进制","permalink":"http://blog.crazylaw.cn/tags/%E8%BF%9B%E5%88%B6/"},{"name":"网络","slug":"网络","permalink":"http://blog.crazylaw.cn/tags/%E7%BD%91%E7%BB%9C/"}]},{"title":"进制转换相关知识","slug":"bin-hex-oct","date":"2017-06-28T02:43:00.000Z","updated":"2021-03-20T16:25:01.801Z","comments":true,"path":"2017/06/28/bin-hex-oct/","link":"","permalink":"http://blog.crazylaw.cn/2017/06/28/bin-hex-oct/","excerpt":"进制转换 好久都没有好好整理过进制之间转换的关系了。最近在研究 RPC 的内容，有一些理论知识必须要重新整理一下。 由于数据在计算机中的表示，最终以二进制的形式存在，所以有时候使用二进制，可以更直观地解决问题。 但是有一些时候，二进制表示的数据实在的太长了，所以我们可能需要一些更高进制的数来表示数据，而我们从小学习的数学，就是以 10 进制来代表的。 进制数越大，所需要数长度就越短。 我们常用的进制有 2、8、16 进制，分别是 2 的 1 一次方，2 的 3 次方，2 的 4 次方。（至于为什么没有 3 进制，4 进制诸如此类的，各位百度一下吧）","text":"进制转换 好久都没有好好整理过进制之间转换的关系了。最近在研究 RPC 的内容，有一些理论知识必须要重新整理一下。 由于数据在计算机中的表示，最终以二进制的形式存在，所以有时候使用二进制，可以更直观地解决问题。 但是有一些时候，二进制表示的数据实在的太长了，所以我们可能需要一些更高进制的数来表示数据，而我们从小学习的数学，就是以 10 进制来代表的。 进制数越大，所需要数长度就越短。 我们常用的进制有 2、8、16 进制，分别是 2 的 1 一次方，2 的 3 次方，2 的 4 次方。（至于为什么没有 3 进制，4 进制诸如此类的，各位百度一下吧） 进制规则如下： 2 进制，用两个阿拉伯数字：0、1； 8 进制，用八个阿拉伯数字：0、1、2、3、4、5、6、7； 10 进制，用十个阿拉伯数字：0 到 9； 16 进制，用十六个阿拉伯数字……等等，阿拉伯人或说是印度人，只发明了 10 个数字啊？ 16 进制就是逢 16 进 1，但我们只有 0~9 这十个数字，所以我们用 A，B，C，D，E，F 这六个字母来分别表示 10，11，12，13，14，15。字母不区分大小写。 十进制转二进制如果我有一个数字 3，想要转成 2 进制。但是只有 0、1 两个数字，如何表示 3 的呢。所以，这个时候就需要逢二进一，从 2 开始计算。2 的进制是 10，3/2=1，所以就是 11，4 的进制呢，同理：4/2 = 2。所以就从 10 变成了 100。所以是 3 变成 4 的话，就是低位+1.进一位变成 100. 二进制转八进制（从低位起算，每三位二进制数代表一位 8 进制位） 有人会问，为什么每三位二进制代表一个 8 进制位呢，那是因为 2 的三次方是 8 的关系。所以二进制的每 3 位，都要代表 8 进制的进一位。 例如我有（十进制：10）00001010,要转成 8 进制就是 012。 二进制转十六进制（同理转八进制可得，每 4 位二进制数代表一个十六进制位）例如我有（十进制：10）00001010，要转成 16 进制就是 0xA。 涉及 PHP在 PHP 当中，需要表示八进制的时候，需要用 0 开头的整数，输出出来的数据会被解析转成十进制的数据。而表示十六进制的时候，需要用到的是 0x 开头 十进制转二进制 =&gt; decbin($decVar) 十进制转十六进制 =&gt; dechex($decVar) 十六进制转十进制 =&gt; hexdec($hexVar) 二进制转十进制 =&gt; bindec($binVar)","categories":[{"name":"进制","slug":"进制","permalink":"http://blog.crazylaw.cn/categories/%E8%BF%9B%E5%88%B6/"}],"tags":[{"name":"进制","slug":"进制","permalink":"http://blog.crazylaw.cn/tags/%E8%BF%9B%E5%88%B6/"}]},{"title":"设计模式の行为型の策略模式","slug":"design-strategy-model","date":"2017-06-22T07:23:00.000Z","updated":"2021-03-20T16:25:01.804Z","comments":true,"path":"2017/06/22/design-strategy-model/","link":"","permalink":"http://blog.crazylaw.cn/2017/06/22/design-strategy-model/","excerpt":"基本概念策略模式是一个非常常用，且非常有用的设计模式。简单的说，它是当你使用大量 if else 逻辑时的救星。if else 就是一种判断上下文的环境所作出的策略，如果你把 if else 写死，那么在复杂逻辑的时候你会发现代码超级长，而且最蛋疼的是，当你以后要新增策略时，再写一个 elseif？万一这个逻辑要修改 20 个地方呢？一口老血吐在屏幕上…策略模式就是来解决这个问题的。举一个场景，商城的首页，男的进来看男性商品，女的进来看女性商品，不男不女…以此类推，各种条件下用不同策略展示不同商品。","text":"基本概念策略模式是一个非常常用，且非常有用的设计模式。简单的说，它是当你使用大量 if else 逻辑时的救星。if else 就是一种判断上下文的环境所作出的策略，如果你把 if else 写死，那么在复杂逻辑的时候你会发现代码超级长，而且最蛋疼的是，当你以后要新增策略时，再写一个 elseif？万一这个逻辑要修改 20 个地方呢？一口老血吐在屏幕上…策略模式就是来解决这个问题的。举一个场景，商城的首页，男的进来看男性商品，女的进来看女性商品，不男不女…以此类推，各种条件下用不同策略展示不同商品。 实现showStrategy . php 展示策略接口1234interface showStrategy&#123; public function showCategory();&#125; maleShowStrategy . php 男性用户展示策略1234567class maleShowStrategy implements showStrategy&#123; // 具体策略A public function showCategory() &#123; echo '展示男性商品目录'; &#125;&#125; femaleShowStrategy . php 女性用户展示策略1234567class femaleShowStrategy implements showStrategy&#123; // 具体策略B public function showCategory() &#123; echo '展示女性商品目录'; &#125;&#125; page . php 展示页面1234567891011121314class Page&#123; private $_strategy; public function __construct(showStrategy $strategy) &#123; $this-&gt;_strategy = $strategy; &#125; public function showPage() &#123; $this-&gt;_strategy-&gt;showCategory(); &#125;&#125; 使用12345678910111213$_GET['male'] = 1;if (isset($_GET['male']))&#123; $strategy = new maleShowStrategy();&#125; elseif (isset($_GET['female']))&#123; $strategy = new femaleShowStrategy();&#125;//注意看这里上下，Page类不再依赖一种具体的策略，而是只需要绑定一个抽象的接口，这就是传说中的控制反转（IOC）。$question = new Page($strategy);$question-&gt;showPage(); 总结仔细看上面的例子，不复杂，我们发现有 2 个好处： 它把 if else 抽离出来了，不需要在每个类里都写 if else； 它成功的实现了控制反转，Page 类里没有具体的依赖策略，这样我们就可以随时添加和删除 不同的策略。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"http://blog.crazylaw.cn/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式，PHP","slug":"设计模式，PHP","permalink":"http://blog.crazylaw.cn/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%EF%BC%8CPHP/"}]},{"title":"设计模式の行为型の责任链模式","slug":"design-chain-of-responsibility-model","date":"2017-06-22T07:06:00.000Z","updated":"2021-03-20T16:25:01.802Z","comments":true,"path":"2017/06/22/design-chain-of-responsibility-model/","link":"","permalink":"http://blog.crazylaw.cn/2017/06/22/design-chain-of-responsibility-model/","excerpt":"理解概念责任链是一种比较高级的行为设计模式，就是当你有一个请求，你不知道用那个方法(handler)来处理这个请求时，你可以把这个请求丢进一个责任链里（里面有很多方法），这个责任链会通过轮询的方式自动找到对应的方法。 比如我要翻译一个单词，我写这个代码的时候，根本不知道用户会输入什么语言，所以我干脆就不管了，无论用户输入什么语言，我把它输入的内容丢进一个责任链，这个责任链里有德语翻译器，英语翻译器，法语翻译器，汉语翻译器，日语翻译器等等等等，丢进去的时候它就会自动查找了，找到对应的语言就会自动翻译出来。","text":"理解概念责任链是一种比较高级的行为设计模式，就是当你有一个请求，你不知道用那个方法(handler)来处理这个请求时，你可以把这个请求丢进一个责任链里（里面有很多方法），这个责任链会通过轮询的方式自动找到对应的方法。 比如我要翻译一个单词，我写这个代码的时候，根本不知道用户会输入什么语言，所以我干脆就不管了，无论用户输入什么语言，我把它输入的内容丢进一个责任链，这个责任链里有德语翻译器，英语翻译器，法语翻译器，汉语翻译器，日语翻译器等等等等，丢进去的时候它就会自动查找了，找到对应的语言就会自动翻译出来。 实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071abstract class TranslationResponsibility&#123; // 抽象责任角色 protected $next; // 下一个责任角色 protected $translator; public function setNext(TranslationResponsibility $l) &#123; $this-&gt;next = $l; return $this; &#125; public function canTranslate($input) &#123; return $this-&gt;translator == $this-&gt;check($input); &#125; // 不允许被继承，用final public final function check($input) &#123; //写验证输入语言总类的逻辑 if ($input == '白菜') &#123; return 'English'; &#125; return 'French'; &#125; abstract public function translate($input); // 翻译方法&#125;class EnglishTranslator extends TranslationResponsibility&#123; public function __construct() &#123; $this-&gt;translator = 'English'; &#125; public function translate($input) &#123; //如果当前翻译器翻译不了，并且责任链上还有下一个翻译器可用，则让下一个翻译器试试 if (!is_null($this-&gt;next) &amp;&amp; !$this-&gt;canTranslate($input)) &#123; $this-&gt;next-&gt;translate($input); &#125; else &#123; echo '英语逻辑'; &#125; &#125;&#125;class FrenchTranslator extends TranslationResponsibility&#123; public function __construct() &#123; $this-&gt;translator = 'French'; &#125; public function translate($input) &#123; //如果当前翻译器翻译不了，并且责任链上还有下一个翻译器可用，则让下一个翻译器试试 if (!is_null($this-&gt;next) &amp;&amp; !$this-&gt;canTranslate($input)) &#123; $this-&gt;next-&gt;translate($input); &#125; else &#123; echo '法语逻辑'; &#125; &#125;&#125; 使用12345678//组建注册链$res_a = new EnglishTranslator();$res_b = new FrenchTranslator();$res_a-&gt;setNext($res_b);//使用$res_a-&gt;translate('白菜'); // 英语逻辑$res_a-&gt;translate('白菜2'); // 法语逻辑 结果就是，英语翻译器翻译不了，传递到法语翻译器翻译。 注意，这里为了简化说明，只展示了 2 个翻译器互为责任链的情况，如果你需要多个翻译器，还需要改造一下代码，让它能够轮询。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"http://blog.crazylaw.cn/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"http://blog.crazylaw.cn/tags/PHP/"},{"name":"设计模式","slug":"设计模式","permalink":"http://blog.crazylaw.cn/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"设计模式の行为型の观察者模式","slug":"design-observer-model","date":"2017-06-22T06:26:45.000Z","updated":"2021-03-20T16:25:01.804Z","comments":true,"path":"2017/06/22/design-observer-model/","link":"","permalink":"http://blog.crazylaw.cn/2017/06/22/design-observer-model/","excerpt":"大概说明如果我希望一个动作在发生的时候，希望订阅他这个动作的所有人都知道了有这么一件事的话，那么就采用观察者模式。 没用观察者模式的情况下1234567891011121314151617181920class Event&#123; function trigger() &#123; echo \"Event update!&lt;br/&gt;\"; //具体更新逻辑 echo \"update1&lt;br/&gt;\"; echo \"update2&lt;br/&gt;\"; // ... &#125;&#125;//使用$event = new Event;$event-&gt;trigger(); 这个事件的触发可以看到如果我不断的有新的人需要订阅的话，那么这个 trigger 方法不断的就是要添加新的逻辑和业务。违反了设计模式-开闭原则，就是对修改关闭，对扩展开放的原则。","text":"大概说明如果我希望一个动作在发生的时候，希望订阅他这个动作的所有人都知道了有这么一件事的话，那么就采用观察者模式。 没用观察者模式的情况下1234567891011121314151617181920class Event&#123; function trigger() &#123; echo \"Event update!&lt;br/&gt;\"; //具体更新逻辑 echo \"update1&lt;br/&gt;\"; echo \"update2&lt;br/&gt;\"; // ... &#125;&#125;//使用$event = new Event;$event-&gt;trigger(); 这个事件的触发可以看到如果我不断的有新的人需要订阅的话，那么这个 trigger 方法不断的就是要添加新的逻辑和业务。违反了设计模式-开闭原则，就是对修改关闭，对扩展开放的原则。 观察者模式12345678910111213141516171819202122//声明一个抽象的事件发生者基类abstract class EventGenerator&#123; private $observers = array(); //添加观察者方法 function addobserver(Observer $observer) &#123; $this-&gt;observers[] = $observer; &#125; //对每个添加的观察者进行事件通知 function notify() &#123; //对每个观察者逐个去更新 foreach ($this-&gt;observers as $observer) &#123; $observer-&gt;update(); &#125; &#125;&#125; 123456//声明一个观察者接口interface observer&#123; function update($event_info = null);&#125; 1234567891011121314151617//声明多个观察者class Observer1 implements observer&#123; function update($event_info = null) &#123; echo \"逻辑1&lt;br/&gt;\"; &#125;&#125;class Observer2 implements observer&#123; function update($event_info = null) &#123; echo \"逻辑2&lt;br/&gt;\"; &#125;&#125; 12345678910111213141516171819//使用$event = new Event;$event-&gt;addObserver(new Observer1);$event-&gt;addObserver(new Observer2);$event-&gt;trigger();//仔细观察代码其实很简单的，Event基类里的foreach，可以实现一个事件对应多个观察者；在这里我们搞明白了，所谓观察者其实就是事件的handler，它和事件怎么挂钩呢，其实是需要注册一下；$event-&gt;addObserver(new Observer1);$event-&gt;addObserver(new Observer2);//而这个步骤$event = new Event;$event-&gt;trigger(); 这样子，就只需要注册对应的 handler 到 listered 里面便可以实现主动推送小新给“订阅者”。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"http://blog.crazylaw.cn/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"http://blog.crazylaw.cn/tags/PHP/"},{"name":"设计模式","slug":"设计模式","permalink":"http://blog.crazylaw.cn/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"设计模式の装饰器模式和观察者模式的区别","slug":"design-d-p-d-model","date":"2017-06-22T06:18:00.000Z","updated":"2021-03-20T16:25:01.803Z","comments":true,"path":"2017/06/22/design-d-p-d-model/","link":"","permalink":"http://blog.crazylaw.cn/2017/06/22/design-d-p-d-model/","excerpt":"","text":"观察者模式观察者模式完美的将观察者和被观察对象分开，系统中的每个类将重点放在某一个功能上，而不是其他的方面（对象之间的交互），很好的体现了单一职责原则。观察者将自己注册到被观察者的容器中，被观察者不应该过问观察者的具体类型，而是使用观察者的接口。这样的优点是：假定程序中还有别的观察者，那么这个观察者是相同的接口即可，基于接口而不是具体的实现，这一点为程序提供了更大的灵活性。 现实生活中像移动的就业信息推送系统，希望得到业务的人（观察者）先到移动注册，然后如果有具体的信息，移动会主动的推送到预订业务的人，不需要预订业务的人去主动询问。 装饰者模式装饰者模式不在不改变原类文件的情况下动态的扩展一个对象的功能。它是通过创建一个包装对象，也就是装饰来包裹真实的对象。当我们需要为某个对象动态地增加一个功能的或职责的时候，可以考虑使用装饰者模式；当某个对象的职责经常发生变化或者经常需要动态的增加职责，避免为了适应变化而增加爱继承子类扩展的方式，因为这种扩展可能会造成子类膨胀的速度过快，难以控制，此时可以使用装饰着模式。 对于这中模式的实现，会有被装饰的具体对象，被装饰的抽象，装饰者的抽象，和若干个装饰着，这若干个装饰者并不是创建各种不同的对象（所以装饰者模式为结构型模式而不是创建型模式），而是每个装饰者都会有一个真实的对象的引用，然后在这个具体对象方法的前后添加一些新的功能，起到装饰的作用。例如有两个装饰 1，和装饰 2，那么可以把装饰 1 当作装饰 2 的具体对象作为参数传进去，这个时候就会产生另外一种新的装饰了，而且没有新的子类。 现实生活着的例子例如包饺子，步骤分为和陷，和面，杆皮，包饺子，煮饺子，可以在和陷这个方法的前面多加点配菜，也可以在和面这个方法的前面在面里面加个鸡蛋，也可以同时用这两个装饰先加菜后和面加鸡蛋，这样就可以用两个已经存在的装饰产生一个新的装饰了。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"http://blog.crazylaw.cn/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://blog.crazylaw.cn/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"设计模式の结构型の代理模式","slug":"design-proxy-model","date":"2017-06-22T03:50:00.000Z","updated":"2021-03-20T16:25:01.804Z","comments":true,"path":"2017/06/22/design-proxy-model/","link":"","permalink":"http://blog.crazylaw.cn/2017/06/22/design-proxy-model/","excerpt":"大概意思这个模式其实比较简单，就是你想访问一个类的时候，不直接访问，而是找这个类的一个代理。代理就是中介，有中介就意味着解耦。 在代理模式下，代理对象和被代理的对象，有个重要特点：必须继承同一个接口。 这里说下重点，之前说过的 适配器模式，和代理模式非常非常像，只不过是在适配器模式下，适配器和它要适配的类没有继承同一接口，适配器就是要把这个第三方类变成符合接口规范。适配器也是个中介，所以我说它们很像。实现 接口： 1234interface Image&#123; public function getWidth();&#125; 现在我们有一个 Image 接口类，接口定义 getWidth 方法。现在我们需要一个具体类（实现类）来实现这个接口。","text":"大概意思这个模式其实比较简单，就是你想访问一个类的时候，不直接访问，而是找这个类的一个代理。代理就是中介，有中介就意味着解耦。 在代理模式下，代理对象和被代理的对象，有个重要特点：必须继承同一个接口。 这里说下重点，之前说过的 适配器模式，和代理模式非常非常像，只不过是在适配器模式下，适配器和它要适配的类没有继承同一接口，适配器就是要把这个第三方类变成符合接口规范。适配器也是个中介，所以我说它们很像。实现 接口： 1234interface Image&#123; public function getWidth();&#125; 现在我们有一个 Image 接口类，接口定义 getWidth 方法。现在我们需要一个具体类（实现类）来实现这个接口。 实现类： 1234567class RawImage implements Image&#123; public function getWidth() &#123; return \"100 x 100\"; &#125;&#125; 代理类 1234567891011121314class ImageProxy implements Image&#123; private $img; public function __construct() &#123; $this-&gt;img = new RawImage(); &#125; public function getWidth() &#123; return $this-&gt;img-&gt;getWidth(); &#125;&#125; 实现了同一个接口的目的是，接口是对外开放的，设计模式的原则是对内封闭。也就是说，如果别人给到的模式就是这样子的，你通过实现一样的接口，或者说是继承了同一个接口的设计模式，就可以额外的添加业务逻辑在里面判断了，我们经常也用到这种模式，例如在控制器里面 1234567891011class A&#123; public function __construct()&#123;&#125;&#125;class B extends A&#123; public function __construct()&#123; echo '做我想做'; parents::__construct(); echo '做我想做'; &#125;&#125; 这样子就可以很好的用到了代理模式。 作用显而易见，解耦。因为代理和被代理对象 都实现同一接口，所以对于原真实对象，你无论怎么改都行。同样，在代理对象中，除了如实反映真实对象的方法逻辑，你还可以添加点别的逻辑，怎么添加都行，不会影响到真实对象，添加后可以在所有使用过代理对象的业务逻辑中瞬间更新。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"http://blog.crazylaw.cn/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"http://blog.crazylaw.cn/tags/PHP/"},{"name":"设计模式","slug":"设计模式","permalink":"http://blog.crazylaw.cn/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"设计模式の结构型の链式模式","slug":"design-link-model","date":"2017-06-22T03:36:00.000Z","updated":"2021-03-20T16:25:01.803Z","comments":true,"path":"2017/06/22/design-link-model/","link":"","permalink":"http://blog.crazylaw.cn/2017/06/22/design-link-model/","excerpt":"一个常见的非正统的设计模式fluent interface（流利接口）有一个更广为人知的名字『链式操作』，可能大多数人大概都是从 Jquery 最先熟悉的，在 laravel 中，ORM 的一系列 sql 操作，也是链式操作，特点是每次都返回一个 Query Builder 对象。","text":"一个常见的非正统的设计模式fluent interface（流利接口）有一个更广为人知的名字『链式操作』，可能大多数人大概都是从 Jquery 最先熟悉的，在 laravel 中，ORM 的一系列 sql 操作，也是链式操作，特点是每次都返回一个 Query Builder 对象。 实例1234567891011121314151617181920212223242526272829303132333435363738394041424344454647class Employee&#123; public $name; public $surName; public $salary; public function setName($name) &#123; $this-&gt;name = $name; return $this; &#125; public function setSurname($surname) &#123; $this-&gt;surName = $surname; return $this; &#125; public function setSalary($salary) &#123; $this-&gt;salary = $salary; return $this; &#125; public function __toString() &#123; $employeeInfo = 'Name: ' . $this-&gt;name . PHP_EOL; $employeeInfo .= 'Surname: ' . $this-&gt;surName . PHP_EOL; $employeeInfo .= 'Salary: ' . $this-&gt;salary . PHP_EOL; return $employeeInfo; &#125;&#125;//链式操作的效果$employee = (new Employee()) -&gt;setName('Tom') -&gt;setSurname('Smith') -&gt;setSalary('100');echo $employee;# 输出结果# Name: Tom# Surname: Smith# Salary: 100 链式操作的关键在于，每次都返回本对象 return $this，使得没一次操作之后都是可以可调用方法的对象。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"http://blog.crazylaw.cn/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"http://blog.crazylaw.cn/tags/PHP/"},{"name":"设计模式","slug":"设计模式","permalink":"http://blog.crazylaw.cn/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"设计模式の结构型の门面模式","slug":"design-facade-model","date":"2017-06-22T03:19:00.000Z","updated":"2021-03-20T16:25:01.803Z","comments":true,"path":"2017/06/22/design-facade-model/","link":"","permalink":"http://blog.crazylaw.cn/2017/06/22/design-facade-model/","excerpt":"概念用过 Laravel 的朋友的应该熟悉，Laravel 给我们科普了一个概念 Facade，然而 Laravel 中的 Facade 并不是真正设计模式中定义的 Facade，那么为什么它们都叫一个名字呢？ 我们还是先来了解一下 Facade 这个单词的意思吧。首先它的读音是[fəˈsɑːd]，源自法语 façade，法语这个词原意就是 frontage，意思是建筑的正面，门面，由于以前法国，意大利的建筑只注重修葺临街的一面，十分精美，而背后却比较简陋，所以这个词引申的意思是表象，假象。 先讲设计模式中的概念在设计模式中，其实 Facade 这个概念十分简单。 它主要讲的是设计一个接口来统领所有子系统的功能。看完下面这个例子就明白了：","text":"概念用过 Laravel 的朋友的应该熟悉，Laravel 给我们科普了一个概念 Facade，然而 Laravel 中的 Facade 并不是真正设计模式中定义的 Facade，那么为什么它们都叫一个名字呢？ 我们还是先来了解一下 Facade 这个单词的意思吧。首先它的读音是[fəˈsɑːd]，源自法语 façade，法语这个词原意就是 frontage，意思是建筑的正面，门面，由于以前法国，意大利的建筑只注重修葺临街的一面，十分精美，而背后却比较简陋，所以这个词引申的意思是表象，假象。 先讲设计模式中的概念在设计模式中，其实 Facade 这个概念十分简单。 它主要讲的是设计一个接口来统领所有子系统的功能。看完下面这个例子就明白了： 123456789101112131415161718class CPU&#123; public function freeze()&#123; /*...*/&#125; public function jump()&#123;/*...*/&#125; public function execute()&#123;/*...*/&#125;&#125;class HardDrive&#123; public function read($boot_sector, $sector_size)&#123;/*...*/&#125;&#125;class Memory&#123; public function load($boot_address, $hd_data)&#123;/*...*/&#125;&#125; 这是三个电脑中的子系统，我们需要写一个总系统来组织它们之间的关系，这其实就是 Facade： 12345678910111213141516171819202122class ComputerFacade&#123; private $cpu; private $ram; private $hd; public function __construct() &#123; $this-&gt;cpu = new CPU(); $this-&gt;ram = new Memory(); $this-&gt;hd = new HardDrive(); &#125; public function start() &#123; $this-&gt;cpu-&gt;freeze(); $this-&gt;ram-&gt;load(BOOT_ADDRESS, $this-&gt;hd-&gt;read(BOOT_SECTOR, SECTOR_SIZE)); $this-&gt;cpu-&gt;jump(BOOT_ADDRESS); $this-&gt;cpu-&gt;execute(); &#125;&#125; 使用：12$computer = new ComputerFacade();$computer-&gt;start(); 门面模式其实就是这么回事，由一个门面（入口）把所有子系统隐藏起来了，只需要操作门面就可以，也可以理解为对外的，别人只知道有个 start 开机键，但是里面是怎么跑的，别人压根不清楚。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"http://blog.crazylaw.cn/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"http://blog.crazylaw.cn/tags/PHP/"},{"name":"设计模式","slug":"设计模式","permalink":"http://blog.crazylaw.cn/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"设计模式の结构型の依赖注入模式","slug":"design-dependency-injection-model","date":"2017-06-22T03:11:00.000Z","updated":"2021-03-20T16:25:01.803Z","comments":true,"path":"2017/06/22/design-dependency-injection-model/","link":"","permalink":"http://blog.crazylaw.cn/2017/06/22/design-dependency-injection-model/","excerpt":"很简单的理解终于要讲到这个著名的设计原则，其实它比其他设计模式都简单。依赖注入的实质就是把一个类不可能更换的部分 和 可更换的部分 分离开来，通过注入的方式来使用，从而达到解耦的目的。 一个数据库连接类12345678910111213141516171819202122class Mysql&#123; private $host; private $port; private $username; private $password; private $db_name; public function __construct() &#123; $this-&gt;host = '127.0.0.1'; $this-&gt;port = 22; $this-&gt;username = 'root'; $this-&gt;password = ''; $this-&gt;db_name = 'my_db'; &#125; public function connect() &#123; return mysqli_connect($this-&gt;host, $this-&gt;username, $this-&gt;password, $this-&gt;db_name, $this-&gt;port); &#125;&#125;","text":"很简单的理解终于要讲到这个著名的设计原则，其实它比其他设计模式都简单。依赖注入的实质就是把一个类不可能更换的部分 和 可更换的部分 分离开来，通过注入的方式来使用，从而达到解耦的目的。 一个数据库连接类12345678910111213141516171819202122class Mysql&#123; private $host; private $port; private $username; private $password; private $db_name; public function __construct() &#123; $this-&gt;host = '127.0.0.1'; $this-&gt;port = 22; $this-&gt;username = 'root'; $this-&gt;password = ''; $this-&gt;db_name = 'my_db'; &#125; public function connect() &#123; return mysqli_connect($this-&gt;host, $this-&gt;username, $this-&gt;password, $this-&gt;db_name, $this-&gt;port); &#125;&#125; 使用12$db = new Mysql();$con = $db-&gt;connect(); 通常应该设计为单例，这里就先不搞复杂了。 依赖注入显然，数据库的配置是可以更换的部分，因此我们需要把它拎出来。 123456789101112131415161718192021222324252627282930313233343536373839404142class MysqlConfiguration&#123; private $host; private $port; private $username; private $password; private $db_name; public function __construct(string $host, int $port, string $username, string $password, string $db_name) &#123; $this-&gt;host = $host; $this-&gt;port = $port; $this-&gt;username = $username; $this-&gt;password = $password; $this-&gt;db_name = $db_name; &#125; public function getHost(): string &#123; return $this-&gt;host; &#125; public function getPort(): int &#123; return $this-&gt;port; &#125; public function getUsername(): string &#123; return $this-&gt;username; &#125; public function getPassword(): string &#123; return $this-&gt;password; &#125; public function getDbName(): string &#123; return $this-&gt;db_name; &#125;&#125; 然后不可替换的部分这样： 1234567891011121314class Mysql&#123; private $configuration; public function __construct(MysqlConfiguration $config) &#123; $this-&gt;configuration = $config; &#125; public function connect() &#123; return mysqli_connect($this-&gt;configuration-&gt;getHost(), $this-&gt;configuration-&gt;getUsername(), $this-&gt;configuration-&gt;getPassword, $this-&gt;configuration-&gt;getDbName(), $this-&gt;configuration-&gt;getPort()); &#125;&#125; 这样就完成了配置文件和连接逻辑的分离。 使用123$config = new MysqlConfiguration('127.0.0.1', 'root', '', 'my_db', 22);$db = new Mysql($config);$con = $db-&gt;connect(); $config 是注入 Mysql 的，这就是所谓的依赖注入。 备注还有一种做法就是将所有的组件都放在一个容器里，等到控制器需要的时候，再去动态注入进来，后续如果还要再使用的话，就从已经加载了的是实例中获取。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"http://blog.crazylaw.cn/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"http://blog.crazylaw.cn/tags/PHP/"},{"name":"设计模式","slug":"设计模式","permalink":"http://blog.crazylaw.cn/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"设计模式の结构型の装饰器模式","slug":"design-decorator-model","date":"2017-06-22T01:57:00.000Z","updated":"2021-03-20T16:25:01.803Z","comments":true,"path":"2017/06/22/design-decorator-model/","link":"","permalink":"http://blog.crazylaw.cn/2017/06/22/design-decorator-model/","excerpt":"大概的意思一个类中有一个方法，我需要经常改它，而且会反反复复，改完了又改回去。 一般要么我们直接改原来的类中的方法，要么继承一下类覆盖这个方法。 有没有一个办法可以不用继承，只需要增加一个类进去，就可以改掉那个方法。 有，装饰器模式。 场景1234567891011class plainCoffee&#123; public function makeCoffee() &#123; $this-&gt;addCoffee(); &#125; public function addCoffee() &#123; &#125;&#125; 这是一个煮咖啡的程序，现在我还想加点糖，一般做法：","text":"大概的意思一个类中有一个方法，我需要经常改它，而且会反反复复，改完了又改回去。 一般要么我们直接改原来的类中的方法，要么继承一下类覆盖这个方法。 有没有一个办法可以不用继承，只需要增加一个类进去，就可以改掉那个方法。 有，装饰器模式。 场景1234567891011class plainCoffee&#123; public function makeCoffee() &#123; $this-&gt;addCoffee(); &#125; public function addCoffee() &#123; &#125;&#125; 这是一个煮咖啡的程序，现在我还想加点糖，一般做法： 123456789101112class sweetCoffee extends plainCoffee&#123; public function makeCoffee() &#123; $this-&gt;addCoffee(); $this-&gt;addSugar(); &#125; public function addSugar() &#123; &#125;&#125; 好了，下面如果我还想加点奶，加点奶油，加点巧克力，加点海盐？会 extends 到崩溃。 装饰器要想使用装饰器，需要对最早那个类进行改造： 1234567891011class plainCoffee&#123; public function makeCoffee() &#123; $this-&gt;addCoffee(); &#125; public function addCoffee() &#123; &#125;&#125; 我们想改造 makeCoffee()这个方法，无非是在它前面或后面加点逻辑，于是： 123456789101112131415161718192021class plainCoffee&#123; private function before() &#123; &#125; private function after() &#123; &#125; public function makeCoffee() &#123; $this-&gt;before(); $this-&gt;addCoffee(); $this-&gt;after(); &#125; public function addCoffee() &#123; &#125;&#125; 那么我们怎么在 before 和 after 中加入逻辑呢： 123456789101112131415161718192021222324252627282930313233343536class plainCoffee&#123; private $decorators; public function addDecorator($decorator) &#123; $this-&gt;decorators[] = $decorator; &#125; private function before() &#123; foreach ($this-&gt;decorators as $decorator) &#123; $decorator-&gt;before(); &#125; &#125; private function after() &#123; foreach ($this-&gt;decorators as $decorator) &#123; $decorator-&gt;after(); &#125; &#125; public function makeCoffee() &#123; $this-&gt;before(); $this-&gt;addCoffee(); $this-&gt;after(); &#125; public function addCoffee() &#123; &#125;&#125; 改造好了，我们来看看怎么写装饰器： 123456789101112131415class sweetCoffeeDecorator&#123; public function before() &#123; &#125; public function after() &#123; $this-&gt;addSugar(); &#125; public function addSugar() &#123; &#125;&#125; 由于我们这里这里的逻辑只会写在后面，所以 before 就留空了。 使用12345$coffee = new plainCoffee();$coffee-&gt;addDecorator(new sweetCoffeeDecorator());$coffee-&gt;makeCoffee(); 这样就得到了加糖的 coffee，如果要加奶的，就再新建一个类似的修饰器： 1234567$coffee = new plainCoffee();$coffee-&gt;addDecorator(new sweetCoffeeDecorator());$coffee-&gt;addDecorator(new milkCoffeeDecorator());$coffee-&gt;makeCoffee(); 不难发现，在这里可以自由的新增或注释掉 不同的装饰器。是不是很灵活？ 当你 extends 用过后又遇到需要再次 extends 的情况时，不妨考虑一下装饰器模式。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"http://blog.crazylaw.cn/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"http://blog.crazylaw.cn/tags/PHP/"},{"name":"设计模式","slug":"设计模式","permalink":"http://blog.crazylaw.cn/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"动态规划算法の爬楼梯","slug":"算法/algorithm-dynatic","date":"2017-06-21T08:04:00.000Z","updated":"2021-03-20T16:25:01.821Z","comments":true,"path":"2017/06/21/算法/algorithm-dynatic/","link":"","permalink":"http://blog.crazylaw.cn/2017/06/21/%E7%AE%97%E6%B3%95/algorithm-dynatic/","excerpt":"题目有一座高度是 10 级台阶的楼梯，从下往上走，每跨一步只能向上 1 级或者 2 级台阶。要求用程序来求出一共有多少种走法。","text":"题目有一座高度是 10 级台阶的楼梯，从下往上走，每跨一步只能向上 1 级或者 2 级台阶。要求用程序来求出一共有多少种走法。 解法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758&lt;?php/** * 算法：动态规划. * 思路: 最优子结构,自低向上，空间换时间，关键一步，和备忘录算法区别 * User: 白菜 * Date: 2017/6/21 * Time: 15:45 * 题目 :有一座高度是10级台阶的楼梯，从下往上走，每跨一步只能向上1级或者2级台阶。要求用程序来求出一共有多少种走法。 *//** * 动态规划算法 * * @param $n * * @return int|mixed *//** * 动态规划写法（备忘录算法升级版） * 时间复杂度O(n),空间复杂度O(1) * * @param int $n 台阶数 * * @return int */function dp3(int $n)&#123; if ($n &lt; 0) &#123; return false; &#125; switch ($n) &#123; case 1: return 1; break; case 2: return 2; break; default: $a = 1; // 第n-1个 $b = 2; // 第n-2个 $temp = 0; // 临时变量,用于下面循环处交互$a,$b大小 for ($i = 3; $i &lt;= $n; $i++) &#123; $temp = $a + $b; $a = $b; $b = $temp; &#125; unset($a, $b); return $temp; &#125;&#125;$level = 4;echo dp($level); 时间对比123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144&lt;?php/** * 算法：动态规划. * 思路: 最优子结构,自低向上，空间换时间，关键一步，和备忘录算法区别 * User: 白菜 * Date: 2017/6/21 * Time: 15:45 * 题目 :有一座高度是10级台阶的楼梯，从下往上走，每跨一步只能向上1级或者2级台阶。要求用程序来求出一共有多少种走法。 *//** * 单纯的递归 * 时间复杂度O(2^n),空间复杂度O(1) * * @param int $n 台阶数 * * @return int */function dp1(int $n)&#123; if ($n &lt; 0) &#123; return false; &#125; // 单纯的递归，而非动态规划 switch ($n) &#123; case 1: return 1; break; case 2: return 2; break; default: return dp1($n - 1) + dp1($n - 2); &#125;&#125;/** * 备忘录算法(单纯的递归升级) * 时间复杂度O(n),空间复杂度O(n) * * @param int $n 台阶数 * * @return int|mixed */function dp2(int $n)&#123; if ($n &lt; 0) &#123; return false; &#125; // 备忘录算法 static $storage = [ 1 =&gt; 1, 2 =&gt; 2, ]; if (isset($storage[ $n ])) &#123; return $storage[ $n ]; &#125; // 递归，动态规划根基 f(1)=1，f(2)=2 &#123;1-&gt;1|2&#125; ,f(3)&#123;1-&gt;1-&gt;1|1-&gt;2|2-&gt;1&#125; $num = dp2($n - 1) + dp2($n - 2); $storage[ $n ] = $num; return $num;&#125;/** * 动态规划写法（备忘录算法升级版） * 时间复杂度O(n),空间复杂度O(1) * * @param int $n 台阶数 * * @return int */function dp3(int $n)&#123; if ($n &lt; 0) &#123; return false; &#125; switch ($n) &#123; case 1: return 1; break; case 2: return 2; break; default: $a = 1; // 第n-1个 $b = 2; // 第n-2个 $temp = 0; // 临时变量,用于下面循环处交互$a,$b大小 for ($i = 3; $i &lt;= $n; $i++) &#123; $temp = $a + $b; $a = $b; $b = $temp; &#125; unset($a, $b); return $temp; &#125;&#125;$level = 4;$start = microtime(true);$memory = memory_get_usage();for ($i = 0; $i &lt; 100000000; $i++)&#123; dp3($level);&#125;echo '动态规划耗时：' . round(microtime(true) - $start, 4);echo PHP_EOL;echo '动态规划占用内存：' . (memory_get_usage() - $memory) / 1024 . 'KB';echo PHP_EOL . PHP_EOL;$start = microtime(true);$memory = memory_get_usage();for ($i = 0; $i &lt; 100000000; $i++)&#123; dp2($level);&#125;echo '备忘录算法耗时：' . round(microtime(true) - $start, 4);echo PHP_EOL;echo '备忘录算法占用内存：' . (memory_get_usage() - $memory) / 1024 . 'KB';echo PHP_EOL . PHP_EOL;$start = microtime(true);$memory = memory_get_usage();for ($i = 0; $i &lt; 100000000; $i++)&#123; dp1($level);&#125;echo '直接递归耗时：' . round(microtime(true) - $start, 4);echo PHP_EOL;echo '直接递归占用内存：' . (memory_get_usage() - $memory) / 1024 . 'KB';echo PHP_EOL; 总结从算法上看，动态规划，不管是从空间还是时间上都是最优的，但是由于我们用到的 static 关键字，放这个结果变成了备忘录算法比动态规划还要快。所以这个算法和结果还是有点出入的，不过也好，大家也知道动态规划的原理了。","categories":[{"name":"算法","slug":"算法","permalink":"http://blog.crazylaw.cn/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"http://blog.crazylaw.cn/tags/PHP/"},{"name":"算法","slug":"算法","permalink":"http://blog.crazylaw.cn/tags/%E7%AE%97%E6%B3%95/"},{"name":"动态规划","slug":"动态规划","permalink":"http://blog.crazylaw.cn/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"}]},{"title":"HASH算法のBKDRHash","slug":"算法/algorithm-hash-bkrd","date":"2017-06-20T09:39:00.000Z","updated":"2021-03-20T16:25:01.821Z","comments":true,"path":"2017/06/20/算法/algorithm-hash-bkrd/","link":"","permalink":"http://blog.crazylaw.cn/2017/06/20/%E7%AE%97%E6%B3%95/algorithm-hash-bkrd/","excerpt":"说明今天和别人说一下 hash 算法，起源是因为字符串在数据库做索引的问题，我的想法是字符串通过 hash 算法，得到的 int 类型的数据，在 int 类型的数据库上做索引，但是 hash 算法有很多选择，就 PHP 而言，有人说 10 万数据内的话，用 PHP 内置的 CRC32 算法(abs(crc32($str)))就可以，但是我发现 hash 算法有很多种，其中一种就是 bkdrhash 算法。","text":"说明今天和别人说一下 hash 算法，起源是因为字符串在数据库做索引的问题，我的想法是字符串通过 hash 算法，得到的 int 类型的数据，在 int 类型的数据库上做索引，但是 hash 算法有很多选择，就 PHP 而言，有人说 10 万数据内的话，用 PHP 内置的 CRC32 算法(abs(crc32($str)))就可以，但是我发现 hash 算法有很多种，其中一种就是 bkdrhash 算法。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152&lt;?php/** * BKDRHash算法 * * C、C++版本 * unsigned int BKDRHash(char *str) * &#123; * unsigned int seed = 131313;//也可以乘以31、131、1313、13131、131313.. * unsigned int hash = 0; * while(*str) * &#123; * hash = hash*seed + (*str++); * &#125; * * return hash%32767;//最好对一个大的素数取余 * &#125; * * php实现 比c语言版本复杂的部分，是由于php中整型数的范围是，且一定是-2147483648 到2147483647，并且没有无符号整形数。 * 在算法中会出现大数溢出的问题，不能使用intval,需要用floatval，同时在运算过程中取余保证不溢出。 * 0x7FFFFFFF 是控制long int的最大值 */class BKDRHash&#123; private $seed = 131; private $hash = 0; public $str = ''; public function __construct($str) &#123; $this-&gt;str = md5($str); &#125; public function hash() &#123; $this-&gt;hash = 0; for ($i = 0; $i &lt; 32; $i++) &#123; $this-&gt;hash = ((floatval($this-&gt;hash * $this-&gt;seed) &amp; 0x7FFFFFFF) + ord($this-&gt;str&#123;$i&#125;)) &amp; 0x7FFFFFFF; // 表示控制在long int范围之内 &#125; return $this-&gt;hash &amp; 0x7FFFFFFF; &#125;&#125;$obj = new BKDRHash('白菜');var_dump($obj-&gt;hash());$obj-&gt;str = md5('白菜');var_dump($obj-&gt;hash());","categories":[{"name":"算法","slug":"算法","permalink":"http://blog.crazylaw.cn/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"算法","slug":"算法","permalink":"http://blog.crazylaw.cn/tags/%E7%AE%97%E6%B3%95/"},{"name":"Hash函数","slug":"Hash函数","permalink":"http://blog.crazylaw.cn/tags/Hash%E5%87%BD%E6%95%B0/"}]},{"title":"设计模式の结构型の组合模式","slug":"design-combination-model","date":"2017-06-20T08:19:00.000Z","updated":"2021-03-20T16:25:01.803Z","comments":true,"path":"2017/06/20/design-combination-model/","link":"","permalink":"http://blog.crazylaw.cn/2017/06/20/design-combination-model/","excerpt":"大概意思一个接口对于多个实现，并且这些实现中都拥有相同的方法（名）。 有时候你需要只运行一个方法，就让不同实现类的某个方法或某个逻辑全部执行一遍。在批量处理多个实现类时，感觉就像在使用一个类一样。 12345678910111213//先建立一个表单$form = new Form();//在表单中增加一个Email元素$form-&gt;addElement(new TextElement('Email:'));$form-&gt;addElement(new InputElement());//在表单中增加一个密码元素$form-&gt;addElement(new TextElement('Password:'));$form-&gt;addElement(new InputElement());//把表单渲染出来$form-&gt;render(); 这个例子形象的介绍了组合模式，表单的元素可以动态增加，但是只要渲染一次，就可以把整个表单渲染出来。","text":"大概意思一个接口对于多个实现，并且这些实现中都拥有相同的方法（名）。 有时候你需要只运行一个方法，就让不同实现类的某个方法或某个逻辑全部执行一遍。在批量处理多个实现类时，感觉就像在使用一个类一样。 12345678910111213//先建立一个表单$form = new Form();//在表单中增加一个Email元素$form-&gt;addElement(new TextElement('Email:'));$form-&gt;addElement(new InputElement());//在表单中增加一个密码元素$form-&gt;addElement(new TextElement('Password:'));$form-&gt;addElement(new InputElement());//把表单渲染出来$form-&gt;render(); 这个例子形象的介绍了组合模式，表单的元素可以动态增加，但是只要渲染一次，就可以把整个表单渲染出来。 实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263// 顶层渲染接口 RenderableInterface.phpinterface RenderableInterface&#123; public function render(): string;&#125;// 表单构造器 Form.php//必须继承顶层渲染接口class Form implements RenderableInterface&#123; private $elements; //这里很关键，相当于是批量处理接口实现类 public function render(): string &#123; $formCode = '&lt;form&gt;'; foreach ($this-&gt;elements as $element) &#123; $formCode .= $element-&gt;render(); &#125; $formCode .= '&lt;/form&gt;'; return $formCode; &#125; //这个方法用来注册 接口实现类 public function addElement(RenderableInterface $element) &#123; $this-&gt;elements[] = $element; &#125;&#125;// 具体实现类一 TextElement.phpclass TextElement implements RenderableInterface&#123; private $text; public function __construct(string $text) &#123; $this-&gt;text = $text; &#125; public function render(): string &#123; return $this-&gt;text; &#125;&#125;// 具体实现类二 InputElement.phpclass InputElement implements RenderableInterface&#123; public function render(): string &#123; return '&lt;input type=\"text\" /&gt;'; &#125;&#125;// 你还可以定义更多的元素，来构建表单。 输出结果当你使用 $form-&gt;render() 渲染之后，所有的元素都可以渲染出来。 1&lt;form&gt;Email:&lt;input type=\"text\" /&gt;Password:&lt;input type=\"text\" /&gt;&lt;/form&gt;","categories":[{"name":"设计模式","slug":"设计模式","permalink":"http://blog.crazylaw.cn/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"http://blog.crazylaw.cn/tags/PHP/"},{"name":"设计模式","slug":"设计模式","permalink":"http://blog.crazylaw.cn/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"设计模式の结构型の桥接模式","slug":"design-bridge-model","date":"2017-06-20T07:45:00.000Z","updated":"2021-03-20T16:25:01.802Z","comments":true,"path":"2017/06/20/design-bridge-model/","link":"","permalink":"http://blog.crazylaw.cn/2017/06/20/design-bridge-model/","excerpt":"目的我们知道一个类可以实现多个接口，一个接口对应多个实现。 在不同的实现类中，它实现接口方法的逻辑是不一样的。 有时候我们需要对这些抽象方法进行一些组合，修改，但是又能适用于所有实现类。 这时候我们需要做一个桥，连接不同的实现类并统一标准。 一个接口多个实现1234567891011121314151617181920212223// 格式化接口interface FormatterInterface&#123; public function format(string $text);&#125;// 文本格式化class PlainTextFormatter implements FormatterInterface&#123; public function format(string $text) &#123; return $text; &#125;&#125;// HTML格式化class HtmlFormatter implements FormatterInterface&#123; public function format(string $text) &#123; return sprintf('&lt;p&gt;%s&lt;/p&gt;', $text); &#125;&#125;","text":"目的我们知道一个类可以实现多个接口，一个接口对应多个实现。 在不同的实现类中，它实现接口方法的逻辑是不一样的。 有时候我们需要对这些抽象方法进行一些组合，修改，但是又能适用于所有实现类。 这时候我们需要做一个桥，连接不同的实现类并统一标准。 一个接口多个实现1234567891011121314151617181920212223// 格式化接口interface FormatterInterface&#123; public function format(string $text);&#125;// 文本格式化class PlainTextFormatter implements FormatterInterface&#123; public function format(string $text) &#123; return $text; &#125;&#125;// HTML格式化class HtmlFormatter implements FormatterInterface&#123; public function format(string $text) &#123; return sprintf('&lt;p&gt;%s&lt;/p&gt;', $text); &#125;&#125; 桥接核心12345678910111213141516171819abstract class Service&#123; protected $implementation; //初始化一个FormatterInterface的实现 public function __construct(FormatterInterface $printer) &#123; $this-&gt;implementation = $printer; &#125; // 可以跟换实现 public function setImplementation(FormatterInterface $printer) &#123; $this-&gt;implementation = $printer; &#125; //桥接抽象方法 abstract public function get();&#125; 具体桥接12345678class HelloWorldService extends Service&#123; //桥接抽象方法的实现，这个方法是关键，因为它不在受限于原有的接口方法，而是可以自由组合修改，并且你可以编写多个类似的方法，这样就和原接口解耦了。 public function get() &#123; return $this-&gt;implementation-&gt;format('Hello World') . '-这是修改的后缀'; &#125;&#125; 使用12345678$service = new HelloWorldService(new PlainTextFormatter());echo $service-&gt;get(); //Hello World-这是修改的后缀//在这里切换实现很轻松$service-&gt;setImplementation(new HtmlFormatter());echo $service-&gt;get(); //&lt;p&gt;Hello World&lt;/p&gt;-这是修改的后缀","categories":[{"name":"设计模式","slug":"设计模式","permalink":"http://blog.crazylaw.cn/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"http://blog.crazylaw.cn/tags/PHP/"},{"name":"设计模式","slug":"设计模式","permalink":"http://blog.crazylaw.cn/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"设计模式の结构型の适配器模式","slug":"design-adapter-model","date":"2017-06-20T03:54:00.000Z","updated":"2021-03-20T16:25:01.802Z","comments":true,"path":"2017/06/20/design-adapter-model/","link":"","permalink":"http://blog.crazylaw.cn/2017/06/20/design-adapter-model/","excerpt":"说明我们先来看看下面的代码。先来看看接口的作用。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051// 目标角色(对外一致的接口)interface Database&#123; public function connect(); public function query(); public function close();&#125;class Mysql implements Database&#123; public function connect() &#123; //mysql 的逻辑 &#125; public function query() &#123; //mysql 的逻辑 &#125; public function close() &#123; //mysql 的逻辑 &#125;&#125;class Pdo implements Database&#123; public function connect() &#123; //Pdo 的逻辑 &#125; public function query() &#123; //Pdo 的逻辑 &#125; public function close() &#123; //Pdo 的逻辑 &#125;&#125;//使用$database = new Mysql(); //切换数据库只要改这一行就行了，因为后面的都是标准接口方法，不管哪个数据库都一样。$database-&gt;connect();$database-&gt;query();$database-&gt;close();","text":"说明我们先来看看下面的代码。先来看看接口的作用。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051// 目标角色(对外一致的接口)interface Database&#123; public function connect(); public function query(); public function close();&#125;class Mysql implements Database&#123; public function connect() &#123; //mysql 的逻辑 &#125; public function query() &#123; //mysql 的逻辑 &#125; public function close() &#123; //mysql 的逻辑 &#125;&#125;class Pdo implements Database&#123; public function connect() &#123; //Pdo 的逻辑 &#125; public function query() &#123; //Pdo 的逻辑 &#125; public function close() &#123; //Pdo 的逻辑 &#125;&#125;//使用$database = new Mysql(); //切换数据库只要改这一行就行了，因为后面的都是标准接口方法，不管哪个数据库都一样。$database-&gt;connect();$database-&gt;query();$database-&gt;close(); 问题有些第三方的 数据库类并没有按照我的接口来实现，而是有自己不同的方法，这个时候我们就需要有一个适配器类，来先处理一下这个异类。 作用有点像把 110v 电源转换成为 220v（电源适配器）。 123456789101112131415161718//第三方数据库类(源角色)class Oracle&#123; public function oracleConnect() &#123; //Oracle 的逻辑 &#125; public function oracleQuery() &#123; //Oracle 的逻辑 &#125; public function oracleClose() &#123; //Oracle 的逻辑 &#125;&#125; 适配器模式1234567891011121314151617181920212223242526// 适配器角色class Adapter implements Database&#123; private $adaptee; function __construct($adaptee) &#123; $this-&gt;adaptee = $adaptee; &#125; //这里把异类的方法转换成了 接口标准方法，下同 public function connect() &#123; $this-&gt;adaptee-&gt;oracleConnect(); &#125; public function query() &#123; $this-&gt;adaptee-&gt;oracleQuery(); &#125; public function close() &#123; $this-&gt;adaptee-&gt;oracleClose(); &#125;&#125; 使用上： 12345678$adaptee = new Oracle();$adapter = new Adapter($adaptee);//只要改这个类就行了，后面的都可以不用改；// OR// $adapter = new Mysql();$database = $adapter;$database-&gt;connect();$database-&gt;query();$database-&gt;close(); 所以说，适配器对应不同的具体类,这个类实现目标角色的所有接口，但是由于源角色的操作个不一致,所以需要一个适配器适配一个源角色。很简单，也很实用。 注意，还可以结合抽象类和借口分别做文章，最终的目的就是目标角色的统一性。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"http://blog.crazylaw.cn/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"http://blog.crazylaw.cn/tags/PHP/"},{"name":"设计模式","slug":"设计模式","permalink":"http://blog.crazylaw.cn/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"设计模式の创建型の原型模式","slug":"design-object-pool-model","date":"2017-06-20T03:46:00.000Z","updated":"2021-03-20T16:25:01.804Z","comments":true,"path":"2017/06/20/design-object-pool-model/","link":"","permalink":"http://blog.crazylaw.cn/2017/06/20/design-object-pool-model/","excerpt":"简介 对象池（也称为资源池）被用来管理对象缓存。对象池是一组已经初始化过且可以直接使用的对象集合，用户在使用对象时可以从对象池中获取对象，对其进行操作处理，并在不需要时归还给对象池而非销毁它。 若对象初始化、实例化的代价高，且需要经常实例化，但每次实例化的数量较少的情况下，使用对象池可以获得显著的性能提升。常见的使用对象池模式的技术包括线程池、数据库连接池、任务队列池、图片资源对象池等。 当然，如果要实例化的对象较小，不需要多少资源开销，就没有必要使用对象池模式了，这非但不会提升性能，反而浪费内存空间，甚至降低性能。","text":"简介 对象池（也称为资源池）被用来管理对象缓存。对象池是一组已经初始化过且可以直接使用的对象集合，用户在使用对象时可以从对象池中获取对象，对其进行操作处理，并在不需要时归还给对象池而非销毁它。 若对象初始化、实例化的代价高，且需要经常实例化，但每次实例化的数量较少的情况下，使用对象池可以获得显著的性能提升。常见的使用对象池模式的技术包括线程池、数据库连接池、任务队列池、图片资源对象池等。 当然，如果要实例化的对象较小，不需要多少资源开销，就没有必要使用对象池模式了，这非但不会提升性能，反而浪费内存空间，甚至降低性能。 123456789101112131415161718192021222324252627282930313233343536373839404142class ObjectPool&#123; private $instances = []; public function get($key) &#123; if (isset($this-&gt;instances[ $key ])) &#123; return $this-&gt;instances[ $key ]; &#125; else &#123; $item = $this-&gt;make($key); $this-&gt;instances[ $key ] = $item; return $item; &#125; &#125; public function add($object, $key) &#123; $this-&gt;instances[ $key ] = $object; &#125; public function make($key) &#123; if ($key == 'mysql') &#123; return new Mysql(); &#125; elseif ($key == 'socket') &#123; return new Socket(); &#125; &#125;&#125;class ReusableObject&#123; public function doSomething() &#123; // ... &#125;&#125; 说明上面的例子其实只是一个最基础的例子，对象池的理念还有很多的升级版本，像对象个数的控制，就有动态扩展容器对象的做法，也有静态固定的做法，还有初始化和静态固定之间浮动的做法。所有的线程池之类的概念，其实都是设计模式的对象池的思想。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"http://blog.crazylaw.cn/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"http://blog.crazylaw.cn/tags/PHP/"},{"name":"设计模式","slug":"设计模式","permalink":"http://blog.crazylaw.cn/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"设计模式の创建型の原型模式","slug":"design-prototype-model","date":"2017-06-20T03:46:00.000Z","updated":"2021-03-20T16:25:01.804Z","comments":true,"path":"2017/06/20/design-prototype-model/","link":"","permalink":"http://blog.crazylaw.cn/2017/06/20/design-prototype-model/","excerpt":"实质就是 对象的复制对一些大型对象，每次去 new，初始化开销很大，这个时候我们 先 new 一个模版对象，然后其他实例都去 clone 这个模版， 这样可以节约不少性能。这个所谓的模版，就是原型（Prototype）；当然，原型模式比单纯的 Clone 要稍微升级一下。 普通 clonenew 和 clone 都是用来创建对象的方法。在 PHP 中， 对象间的赋值操作实际上是引用操作 （事实上，绝大部分的编程语言都是如此! 主要原因是内存及性能的问题) ，比如：","text":"实质就是 对象的复制对一些大型对象，每次去 new，初始化开销很大，这个时候我们 先 new 一个模版对象，然后其他实例都去 clone 这个模版， 这样可以节约不少性能。这个所谓的模版，就是原型（Prototype）；当然，原型模式比单纯的 Clone 要稍微升级一下。 普通 clonenew 和 clone 都是用来创建对象的方法。在 PHP 中， 对象间的赋值操作实际上是引用操作 （事实上，绝大部分的编程语言都是如此! 主要原因是内存及性能的问题) ，比如： 1234567891011class ProClass&#123; public $data;&#125;$obj1 = new ProClass();$obj1-&gt;data = \"aaa\";$obj2 = $obj1;$obj2-&gt;data = \"bbb\"; //$obj1-&gt;data的值也会变成\"bbb\"var_dump($obj1-&gt;data); 但是如果你不是直接引用，而是 clone，那么相当于做了一个独立的副本： 12$obj2 = clone $obj1;$obj2-&gt;data =\"bbb\"; //$obj1-&gt;data的值还是\"aaa\"，不会关联 这样就得到一个和被复制对象完全没有纠葛的新对象，但两个对象长得是一模一样的。 浅复制和深复制如果你以为你已经把它俩彻底分开了，你错了，没那么容易, 我们再看一个复杂点例子。继续接着上面的例子看: 12345678910111213141516171819202122232425262728293031323334353637class ProClass&#123; public $data; public $item;&#125;$obj1 = new ProClass();$obj1-&gt;data = \"aaa\";class itemObject&#123; public $count = 0; public function add() &#123; $this-&gt;count = ++$this-&gt;count; &#125;&#125;$item = new itemObject;$obj1-&gt;item = $item;$obj2 = clone $obj1;$obj2-&gt;data = \"bbb\";var_dump($obj1-&gt;data); // aaavar_dump($obj2-&gt;data); // bbb// 到目前为止,一切都没问题,已经隔离开了.// 但是到这里之后，运行以下代码$obj2-&gt;item-&gt;add();print_r($item); // count = 1print_r($obj1); // count = 1print_r($obj2); // count = 1// 发现$obj1,$obj2的count都变成了1,发现又依赖上了，并没有完全隔离开。这就是所谓的 浅复制 我们既然要 Clone，目的就是要把 两个对象 完全分离开。 所以我们来聊一下 深复制 的方法： 非常简单，在被复制对象中加一个魔术方法就可以了。 12345678class ProClass&#123; public $data; public $item; public function __clone() &#123; $this-&gt;item &#x3D; clone $this-&gt;item; &#125;&#125; 正解12345678910111213141516171819202122232425262728interface Prototype&#123; public function copy();&#125;class ConcretePrototype implements Prototype&#123; private $_name; public function __construct($name) &#123; $this-&gt;_name = $name; &#125; public function copy() &#123; return clone $this; &#125;&#125;class Demo&#123;&#125;// client$demo = new Demo();$object1 = new ConcretePrototype($demo);$object2 = $object1-&gt;copy(); 从上面的例子不难看出，所谓原型模式就是不直接用 clone 这种关键字写法，而是创建一个原型类。 把需要被复制的对象丢进 原型类里面，然后这个类就具有了 复制自己的能力（方法），并且可以继承原型的一些公共的属性和方法。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"http://blog.crazylaw.cn/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"http://blog.crazylaw.cn/tags/PHP/"},{"name":"设计模式","slug":"设计模式","permalink":"http://blog.crazylaw.cn/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"设计模式の创建型の单例模式","slug":"design-single-model","date":"2017-06-20T02:22:00.000Z","updated":"2021-03-20T16:25:01.804Z","comments":true,"path":"2017/06/20/design-single-model/","link":"","permalink":"http://blog.crazylaw.cn/2017/06/20/design-single-model/","excerpt":"简介很容易理解，也很简单。 最常见的场景就是一个数据库的链接，我们每次请求只需要连接一次，也就是说如果我们用类来写的话，只需要用一个实例就够了（多了浪费）。","text":"简介很容易理解，也很简单。 最常见的场景就是一个数据库的链接，我们每次请求只需要连接一次，也就是说如果我们用类来写的话，只需要用一个实例就够了（多了浪费）。 1234567891011121314151617181920212223242526272829303132333435363738&lt;?phpclass Mysql&#123; //该属性用来保存实例 private static $conn; //构造函数为private,防止创建对象 private function __construct() &#123; self::$conn = mysqli_connect('localhost', 'root', ''); &#125; //创建一个用来实例化对象的方法，如果不存在一个这个类的实例属性，就创建一个，否则就取这个实例属性。 public static function getInstance() &#123; if (!(self::$conn instanceof self)) &#123; self::$conn = new self; &#125; return self::$conn; &#125; //防止对象被复制 public function __clone() &#123; trigger_error('Clone is not allowed !'); &#125; //防止反序列化后创建对象 private function __wakeup() &#123; trigger_error('Unserialized is not allowed !'); &#125;&#125;//只能这样取得实例，不能new 和 clone$mysql = Mysql::getInstance(); 说明单例模式其实分为 2 种，PHP 中最常用的是懒汉模式，意识就是需要加载的时候才去实例化，还有一种就是叫饿汉模式，就是一开始就开始实例化了，但是由于 PHP 不支持在类定义时给类的成员变量赋予非基本类型的值。如表达式，new 操作等等，所以 PHP 中就不存在饿汉模式了。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"http://blog.crazylaw.cn/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式，PHP","slug":"设计模式，PHP","permalink":"http://blog.crazylaw.cn/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%EF%BC%8CPHP/"}]},{"title":"linuxの修改语言包","slug":"linux-language-package","date":"2017-06-19T08:32:00.000Z","updated":"2021-03-20T16:25:01.808Z","comments":true,"path":"2017/06/19/linux-language-package/","link":"","permalink":"http://blog.crazylaw.cn/2017/06/19/linux-language-package/","excerpt":"","text":"执行 locale -a 发现存在中文语言包，语言包存放路径应该是 /usr/share/locale/。 执行 echo $LANG 发现语言配置是 C，大概是 ANSI C 的意思。 修改 /etc/locale.conf 为 LANG=”zh_CN.UTF-8”，并执行 source /etc/locale.conf 使配置生效。 现在支持中文了，可是提示语也变中文了，我想保留英文作为系统语言，同时支持中文，所以重新修改 /etc/locale.conf LANG=”en_US.UTF-8”，并执行 source","categories":[{"name":"Linux","slug":"Linux","permalink":"http://blog.crazylaw.cn/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.crazylaw.cn/tags/Linux/"},{"name":"语言包","slug":"语言包","permalink":"http://blog.crazylaw.cn/tags/%E8%AF%AD%E8%A8%80%E5%8C%85/"}]},{"title":"linux の动态库","slug":"ldconfig","date":"2017-06-19T06:24:00.000Z","updated":"2021-03-20T16:25:01.807Z","comments":true,"path":"2017/06/19/ldconfig/","link":"","permalink":"http://blog.crazylaw.cn/2017/06/19/ldconfig/","excerpt":"1. /lib 和 /usr/lib 的区别/lib 里面给的是 root 和内核所需so(动态库)或者a(静态)之类的库文件，而/usr/lib 是普通用户能够使用的。 Linux 的程序有两种模式，这个你应该知道，是用户模式和内核模式，和这个也是有关系的，不再冗述。 简单说,/lib 是内核级的,/usr/lib 是系统级的,/usr/local/lib 是用户级的. /lib/ — 包含许多被 /bin/ 和 /sbin/ 中的程序使用的库文件。目录 /usr/lib/ 中含有更多用于用户程序的库文件。/lib 目录下放置的是/bin 和/sbin 目录下程序所需的库文件。/lib 目录下的文件的名称遵循下面的格式： 12libc.so.*ld*","text":"1. /lib 和 /usr/lib 的区别/lib 里面给的是 root 和内核所需so(动态库)或者a(静态)之类的库文件，而/usr/lib 是普通用户能够使用的。 Linux 的程序有两种模式，这个你应该知道，是用户模式和内核模式，和这个也是有关系的，不再冗述。 简单说,/lib 是内核级的,/usr/lib 是系统级的,/usr/local/lib 是用户级的. /lib/ — 包含许多被 /bin/ 和 /sbin/ 中的程序使用的库文件。目录 /usr/lib/ 中含有更多用于用户程序的库文件。/lib 目录下放置的是/bin 和/sbin 目录下程序所需的库文件。/lib 目录下的文件的名称遵循下面的格式： 12libc.so.*ld* 仅仅被/usr 目录下的程序所使用的共享库不必放到/lib 目录下。只有/bin 和/sbin 下的程序所需要的库有必要放到/lib 目录下。实际上，libm.so.*类型的库文件如果被是/bin 和/sbin 所需要的，也可以放到/usr/lib 下。 /bin/ — 用来贮存用户命令。目录 /usr/bin 也被用来贮存用户命令。 /sbin/ — 许多系统命令（例如 shutdown）的贮存位置。目录 /usr/sbin 中也包括了许多系统命令。 /root/ — 根用户（超级用户）的主目录。 /mnt/ — 该目录中通常包括系统引导后被挂载的文件系统的挂载点。譬如，默认的光盘挂载点是 /mnt/cdrom/. /boot/ — 包括内核和其它系统启动期间使用的文件。 /lost+found/ — 被 fsck 用来放置零散文件（没有名称的文件）。 /lib/ — 包含许多被 /bin/ 和 /sbin/ 中的程序使用的库文件。目录 /usr/lib/ 中含有更多用于用户程序的库文件。 /dev/ — 贮存设备文件。 /etc/ — 包含许多配置文件和目录。 /var/ — 用于贮存 variable（或不断改变的）文件，例如日志文件和打印机假脱机文件。 /usr/ — 包括与系统用户直接有关的文件和目录，例如应用程序及支持它们的库文件。 /proc/ — 一个虚拟的文件系统（不是实际贮存在磁盘上的），它包括被某些程序使用的系统信息。 /initrd/ — 用来在计算机启动时挂载 initrd.img 映像文件的目录以及载入所需设备模块的目录。 警告 不要删除 /initrd/ 目录。如果你删除了该目录后再重新引导 Red Hat Linux 时，你将无法引导你的计算机。 /tmp/ — 用户和程序的临时目录。 /tmp 给予所有系统用户读写权。 /home/ — 用户主目录的默认位置。 /opt/ — 可选文件和程序的贮存目录。该目录主要被第三方开发者用来简易地安装和卸装他们的软件包。 2.ldconfig ldconfig 是一个动态链接库管理命令，其目的为了让动态链接库为系统所共享。 2.1 ldconfig 的主要用途：默认搜寻/lilb 和/usr/lib，以及配置文件/etc/ld.so.conf 内所列的目录下的库文件。 搜索出可共享的动态链接库，库文件的格式为：lib*.so.，进而创建出动态装入程序(ld.so)所需的连接和缓存文件。 缓存文件默认为/etc/ld.so.cache，该文件保存已排好序的动态链接库名字列表。 ldconfig 通常在系统启动时运行，而当用户安装了一个新的动态链接库时，就需要手工运行这个命令。 2.2 ldconfig 需要注意的地方：往/lib 和/usr/lib 里面加东西，是不用修改/etc/ld.so.conf 文件的，但是添加完后需要调用下 ldconfig，不然添加的 library 会找不到。 如果添加的 library 不在/lib 和/usr/lib 里面的话，就一定要修改/etc/ld.so.conf 文件，往该文件追加 library 所在的路径，然后也需要重新调用下 ldconfig 命令。比如在安装 MySQL 的时候，其库文件/usr/local/mysql/lib，就需要追加到/etc/ld.so.conf 文件中。命令如下： 123echo &quot;&#x2F;usr&#x2F;local&#x2F;mysql&#x2F;lib&quot; &gt;&gt; &#x2F;etc&#x2F;ld.so.confldconfig -v | grep mysql 如果添加的 library 不在/lib 或/usr/lib 下，但是却没有权限操作写/etc/ld.so.conf 文件的话，这时就需要往 export 里写一个全局变量 LD_LIBRARY_PATH，就可以了。","categories":[{"name":"Linux","slug":"Linux","permalink":"http://blog.crazylaw.cn/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://blog.crazylaw.cn/tags/Linux/"}]},{"title":"Ascii码和字节字符的关系","slug":"ascii-code-1","date":"2017-05-27T10:20:00.000Z","updated":"2021-03-20T16:25:01.801Z","comments":true,"path":"2017/05/27/ascii-code-1/","link":"","permalink":"http://blog.crazylaw.cn/2017/05/27/ascii-code-1/","excerpt":"","text":"##关于 Ascii 码的整理 123456789101112131415- 0 &lt; ascii &lt; 128 单字节字符 &#x3D;&#x3D; 1xxx xxxx向右移动 7 位等于 0.符合范围内- 240 &lt;&#x3D; ascii 四字节字符 &#x3D;&#x3D; 1111 xxxx向右移动 4 位&#x3D;15，符合范围内- 224 &lt;&#x3D; ascii &lt; 240 3 字节字符 &#x3D;&#x3D; 111x xxxx向右移动 5 位&#x3D;7，符合范围内- 192 &lt;&#x3D; ascii &lt; 224 2 字节字符 11xx xxxx向右移动 6 位&#x3D;3，符合范围内","categories":[{"name":"协议","slug":"协议","permalink":"http://blog.crazylaw.cn/categories/%E5%8D%8F%E8%AE%AE/"}],"tags":[{"name":"协议","slug":"协议","permalink":"http://blog.crazylaw.cn/tags/%E5%8D%8F%E8%AE%AE/"},{"name":"Ascii","slug":"Ascii","permalink":"http://blog.crazylaw.cn/tags/Ascii/"}]},{"title":"ip_to_long","slug":"ip-to-long","date":"2017-03-15T06:20:00.000Z","updated":"2021-03-20T16:25:01.806Z","comments":true,"path":"2017/03/15/ip-to-long/","link":"","permalink":"http://blog.crazylaw.cn/2017/03/15/ip-to-long/","excerpt":"字符串 ip 转长整形原理和实现 php 内置方法：ip2long 但是这个方法存在 bug，就是 010.0.3.198 的时候，计算的长整形并非正确的长整形。并且掌握实现原理，其实并不困难。 原理：IPv4 地址我们知道的是这样子的形式： 192.168.2.10 这种形式我们称之为 点分十进制，当然还有我们的冒分十六进制，但是这个是适用于 IPv6 的 回到主题，点分十进制的写法其实是一个 4 个字节，32 位的整型组成的。 例如 : 10.0.3.198 二进制形式 ：00001010.00000000.00000011.11000001 说到这里，大家可能都知道了，为什么是 32 位了吧。那么二进制的 32 位转成十进制的时候，就是我们的长整形了。 转换结果为 ：167773121","text":"字符串 ip 转长整形原理和实现 php 内置方法：ip2long 但是这个方法存在 bug，就是 010.0.3.198 的时候，计算的长整形并非正确的长整形。并且掌握实现原理，其实并不困难。 原理：IPv4 地址我们知道的是这样子的形式： 192.168.2.10 这种形式我们称之为 点分十进制，当然还有我们的冒分十六进制，但是这个是适用于 IPv6 的 回到主题，点分十进制的写法其实是一个 4 个字节，32 位的整型组成的。 例如 : 10.0.3.198 二进制形式 ：00001010.00000000.00000011.11000001 说到这里，大家可能都知道了，为什么是 32 位了吧。那么二进制的 32 位转成十进制的时候，就是我们的长整形了。 转换结果为 ：167773121 那么怎么转换呢？？？ ip 转成长整形1234567// ip转成长整形public function ipToLong(string $ip)&#123; list($position1, $position2, $position3, $position4) = explode('.', $ip); return ($position1 &lt;&lt; 24) | ($position2 &lt;&lt; 16) | ($position3 &lt;&lt; 8) | ($position4);&#125; 可能看到这里，你会疑惑，为什么这里面会设计位运算和逻辑运行呢。 如果还不理解为什么涉及了位运算的话，那说明上面的 32 位的 10 进制的概念还不理解。 第一段如果要按满 32 位的话，那么他还需要填补 24 位，就是 24 个 0 在右边。第二段如果要按满 32 位的话，那么他还需要填补 24 位，分别是左边 8 位，右边 16 位。就是左边 8 个 0，右边 16 个 0。第三段如果要按满 32 位的话，那么他还需要填补 24 位，分别是左边 16 位，右边 8 位。就是左边 16 个 0，右边 8 个 0。第四段如果要按满 32 位的话，那么他还需要填补 24 位，就是 24 个 0 在左边。 那么为什么是逻辑运算呢，其实这里的话逻辑运行可以提高二进制的计算速度，在这里大家更倾向于逻辑运行的加法法则。所以这里你用+号的话，括号里面的值会由 2 进制先转换程 10 进制，再进行相加，多了一步繁琐的步骤，所以大家更倾向于逻辑或的运算。 长整形转 ip123456789101112131415161718// 整型转ip,三种方式，都可以，我喜欢没注释的那种function LongToIp(int $long)&#123;// return long2ip($long); $ret = []; for ($i = 3; $i &gt;= 0; $i--) &#123; $ret[] = ($long &gt;&gt; $i * 8) &amp; 0xff; &#125;// $ret[] = (($long &amp; 0xffffffff) &gt;&gt; 24);// $ret[] = (($long &amp; 0xffffff) &gt;&gt; 16);// $ret[] = (($long &amp; 0xffff) &gt;&gt; 8);// $ret[] = (($long &amp; 0xff) &gt;&gt; 0); return join('.', $ret);&#125; 讲到这里,转换的原理就是这样子的了，但是深入的理解，可能还要涉及到网络 ip 的知识，就是 256 进制在 ip 中的运作。","categories":[{"name":"协议","slug":"协议","permalink":"http://blog.crazylaw.cn/categories/%E5%8D%8F%E8%AE%AE/"},{"name":"网络","slug":"协议/网络","permalink":"http://blog.crazylaw.cn/categories/%E5%8D%8F%E8%AE%AE/%E7%BD%91%E7%BB%9C/"},{"name":"算法","slug":"协议/网络/算法","permalink":"http://blog.crazylaw.cn/categories/%E5%8D%8F%E8%AE%AE/%E7%BD%91%E7%BB%9C/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"协议","slug":"协议","permalink":"http://blog.crazylaw.cn/tags/%E5%8D%8F%E8%AE%AE/"},{"name":"PHP","slug":"PHP","permalink":"http://blog.crazylaw.cn/tags/PHP/"},{"name":"算法","slug":"算法","permalink":"http://blog.crazylaw.cn/tags/%E7%AE%97%E6%B3%95/"},{"name":"网络","slug":"网络","permalink":"http://blog.crazylaw.cn/tags/%E7%BD%91%E7%BB%9C/"},{"name":"IP","slug":"IP","permalink":"http://blog.crazylaw.cn/tags/IP/"}]},{"title":"模拟从1亿个ip中访问次数最多的IP--PHP","slug":"most-duplicated-ip-from-100-million","date":"2017-03-15T06:03:38.000Z","updated":"2021-03-20T16:25:01.810Z","comments":true,"path":"2017/03/15/most-duplicated-ip-from-100-million/","link":"","permalink":"http://blog.crazylaw.cn/2017/03/15/most-duplicated-ip-from-100-million/","excerpt":"前提： 存储在一个文件中 内存有限，时间无限。 思路： 采用类似分表的办法，将文件拆分到各个文件中再做统计运算，提高检索速度，分散内存压力 涉及到 ip 的，首先要考虑的就是将 ip 转程长整形，理由是整型省内存，并且支持一般都支持整形索引，比字符串索引速度快 数组存储的数据结构，采用冗余的策略，记录一组数组中重复最多的 ip 和数据 独立小文件胜出的，再和分出来的其他的 N-1 个小文件胜出的比较","text":"前提： 存储在一个文件中 内存有限，时间无限。 思路： 采用类似分表的办法，将文件拆分到各个文件中再做统计运算，提高检索速度，分散内存压力 涉及到 ip 的，首先要考虑的就是将 ip 转程长整形，理由是整型省内存，并且支持一般都支持整形索引，比字符串索引速度快 数组存储的数据结构，采用冗余的策略，记录一组数组中重复最多的 ip 和数据 独立小文件胜出的，再和分出来的其他的 N-1 个小文件胜出的比较 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182&lt;?php/** * 模拟从1亿个ip中访问次数最多的IP * 创建文件 * Created by PhpStorm. * User: admin * Date: 2017/3/14 * Time: 16:19 */class Test&#123; const MAX_NUM = 100000000; const HASH_NUM = 1000; const FILE_NAME = 'IpFile.txt'; public static $hashIps = []; public static function generateIp() &#123; // 大概15个字节byte $number = '192.168'; $number = $number . '.' . random_int(0, 255) . '.' . random_int(0, 255); return $number; &#125; public static function generateIpFiles() &#123; /** * fopen 的mode，如果加上b，例如wb的话，是说明强调说明这个文件是二进制文件 * 如果1亿次的话，那就是15byte*1E = 15E byte /1024 * 由于机器性能比较差，文件我只生成了700M的数据。可能只有5千万条数据 */ $handler = fopen(self::FILE_NAME, 'w'); for ($i = 0; $i &lt; self::MAX_NUM; $i++) &#123; $ip = self::generateIp(); fwrite($handler, $ip . PHP_EOL); &#125; fclose($handler); &#125; /** * ip转成长整形 * * @param string $ip * * @return int */ public static function ipToLong(string $ip) &#123; list($position1, $position2, $position3, $position4) = explode('.', $ip); return ($position1 &lt;&lt; 24) | ($position2 &lt;&lt; 16) | ($position3 &lt;&lt; 8) | ($position4); &#125; /** * 哈希取模 * * @param string $ip * * @return int */ public static function hash(string $ip) &#123; $longInt = self::ipToLong($ip); return $longInt % self::HASH_NUM; &#125; /** * 拆分文件 * 考虑： * 1.1E行数据，一次性加载文件这是不明智的做法，所以我们需要一行一行来读取，也可以理解为缓存读取。 * 2.假如1E个ip，我们直接一行行读写操作的话，需要操作的io就是1E次，所以这个是十分不明智的，所以我们可以根据自身服务器的内存比例来进行划分出预定的数组来保存 */ public static function divideIpsFiles() &#123; $handler = fopen(self::FILE_NAME, 'r'); $count = 0; while (($ip = fgets($handler)) !== false) &#123; $count++; $hashIp = self::hash($ip); self::$hashIps[ $hashIp ][] = $ip; // 当处理了50000万条数据的时候，就写入一次性写入文件。 if ($count == 50000) &#123; foreach (self::$hashIps as $key =&gt; $hip) &#123; $handler2 = fopen($key . '.txt', 'a'); while (($ip = current($hip)) !== false) &#123; $byte = fwrite($handler2, $ip); if ($byte === false) &#123; die('Shutdown'); &#125; next($hip); &#125; fclose($handler2); &#125; $count = 0; self::$hashIps = []; &#125; &#125; fclose($handler); &#125; public static function calus() &#123; $last = []; for ($i = 0; $i &lt; 1000; $i++) &#123; $first = [ 'item' =&gt; [], 'max' =&gt; ['ip' =&gt; 0, 'count' =&gt; 0] ]; $handler = fopen($i . '.txt', 'r'); while (($ip = fgets($handler)) !== false) &#123; if (array_key_exists($ip, $first['item'])) &#123; $first['item'][ $ip ]++; &#125; else &#123; $first['item'][ $ip ] = 1; &#125; if ($first['item'][ $ip ] &gt; $first['max']['count']) &#123; $first['max']['ip'] = $ip; $first['max']['count'] = $first['item'][ $ip ]; &#125; &#125; fclose($handler); $last[ $first['max']['ip'] ] = $first['max']['count']; unset($first); &#125; echo 'IP最多重复的是：' . array_search(($count = max($last)), $last) . ',重复次数为:' . $count; &#125; /** * 移除文件 */ public static function deleteFiles() &#123; for ($i = 0; $i &lt; 1000; $i++) &#123; if (file_exists($i . '.txt')) &#123; unlink($i . '.txt'); &#125; else &#123; return false; &#125; &#125; echo '移除完毕' . PHP_EOL; return true; &#125;&#125;if (!file_exists(Test::FILE_NAME))&#123; Test::generateIpFiles();&#125;////Test::deleteFiles();////Test::divideIpsFiles();Test::calus();","categories":[{"name":"PHP","slug":"PHP","permalink":"http://blog.crazylaw.cn/categories/PHP/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"http://blog.crazylaw.cn/tags/PHP/"},{"name":"算法","slug":"算法","permalink":"http://blog.crazylaw.cn/tags/%E7%AE%97%E6%B3%95/"},{"name":"大数据","slug":"大数据","permalink":"http://blog.crazylaw.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}]},{"title":"C语言数据结构のAVL树","slug":"数据结构/struct-study-6","date":"2017-02-20T03:03:43.000Z","updated":"2021-03-20T16:25:01.818Z","comments":true,"path":"2017/02/20/数据结构/struct-study-6/","link":"","permalink":"http://blog.crazylaw.cn/2017/02/20/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/struct-study-6/","excerpt":"","text":"实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210#ifndef _INCLUDE_BASE#define _INCLUDE_BASE#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#define true 1#define false 0#endiftypedef int avl_elementType;typedef struct avl&#123; avl_elementType data; struct avl *left; // 左节点 struct avl *right; // 右节点 avl_elementType hight; // 以此为根的高度&#125;AVL,*PAVL;// ================================================================ void RotateWithDoubleRight(PAVL *avl); // RR 旋转平衡算法 void RotateWithDoubleLeft(PAVL *avl); // LL旋转平衡算法 void RotateWithLeftRight(PAVL *avl); // LR旋转平衡算法 void RotateWithRightLeft(PAVL *avl); // RL旋转平衡算法 void InsertNode(PAVL *avl,int data); // AVL插入算法 int NodeHigh(PAVL avl); // 查询当前树节点的高度 int compareHigh(PAVL a,PAVL b); // 对比高度，取最大值 void LeftBalance(PAVL *avl); // 左平衡算法 void RightBalance(PAVL *avl); // 右平衡算法 void print_inorder(PAVL avl); // 中序排列打印 void print_preorder(PAVL avl); // 前序排列打印 void print_houorder(PAVL avl); // 后序排列打印 void print_inhight(PAVL avl); // 节点的高度// =================================================================// main funcvoid main()&#123; PAVL root = (PAVL)malloc(sizeof(PAVL));// root-&gt;data = 3;// InsertNode(&amp;root,2); InsertNode(&amp;root,2); InsertNode(&amp;root,1); InsertNode(&amp;root,-1);// InsertNode(&amp;root,0); InsertNode(&amp;root,6); InsertNode(&amp;root,7); InsertNode(&amp;root,5); printf(\"中序排列节点的高度：\"); print_inhight(root); printf(\"\\n\"); printf(\"中序排列：\"); print_inorder(root); printf(\"\\n\"); printf(\"前序排列：\"); print_preorder(root);&#125;void InsertNode(PAVL *avl,int data)&#123; if(!(*avl)) &#123; PAVL node = (PAVL)malloc(sizeof(PAVL)); node-&gt;data = data; node-&gt;hight = 0; *avl = node; return ; &#125; if(data &gt; (*avl)-&gt;data) &#123; InsertNode(&amp;((*avl)-&gt;right),data); //判断是否破坏AVL树的平衡性 if (NodeHeight((*avl)-&gt;right)-NodeHeight((*avl)-&gt;left)==2) RightBalance(avl); //右平衡处理 &#125;else&#123; InsertNode(&amp;((*avl)-&gt;left),data); //判断是否破坏AVL树的平衡性 printf(\"相减为:%d\",NodeHeight((*avl)-&gt;left)-NodeHeight((*avl)-&gt;right)); if (NodeHeight((*avl)-&gt;left)-NodeHeight((*avl)-&gt;right)==2) LeftBalance(avl); //左平衡处理 &#125; (*avl)-&gt;hight = compareHigh((*avl)-&gt;left,(*avl)-&gt;right) + 1 ; printf(\"当前节点:%d , 高度:%d , left-高度:%d,right-高度:%d \\n\",(*avl)-&gt;data,(*avl)-&gt;hight,NodeHeight((*avl)-&gt;left),NodeHeight((*avl)-&gt;right));&#125;// 空节点为-1，否则返回树节点高度int NodeHeight(PAVL avl)&#123; return avl == NULL ? -1 : avl-&gt;hight;&#125;int compareHigh(PAVL a,PAVL b)&#123; if(!a || !b) return 0; return a-&gt;hight &gt; b-&gt;hight ? a-&gt;hight : b-&gt;hight;&#125;// 左平衡处理void LeftBalance(PAVL *avl)&#123; if(NodeHeight((*avl)-&gt;left-&gt;left) - NodeHeight((*avl)-&gt;left-&gt;right) != -1) &#123; RotateWithDoubleLeft(avl); &#125;else &#123; RotateWithLeftRight(avl); &#125;&#125;// 右平衡处理void RightBalance(PAVL *avl)&#123; if(NodeHeight((*avl)-&gt;right-&gt;right) - NodeHeight((*avl)-&gt;right-&gt;left) != -1) &#123; RotateWithDoubleRight(avl); &#125;else &#123; RotateWithRightLeft(avl); &#125;&#125;// LL旋转算法void RotateWithDoubleLeft(PAVL *avl)&#123; PAVL p = *avl; PAVL q = p-&gt;left; p-&gt;left = q-&gt;right; q-&gt;right = p; p-&gt;hight = compareHigh(p-&gt;left,p-&gt;right) +1 ; q-&gt;hight = compareHigh(q-&gt;left,q-&gt;right) +1 ; *avl = q;&#125;// RR旋转算法void RotateWithDoubleRight(PAVL *avl)&#123; PAVL p = *avl; PAVL q = p-&gt;right; p-&gt;right = q-&gt;left; q-&gt;left = p; p-&gt;hight = compareHigh(p-&gt;left,p-&gt;right) +1 ; q-&gt;hight = compareHigh(q-&gt;left,q-&gt;right) +1 ; *avl = q;&#125;// LR旋转算法void RotateWithLeftRight(PAVL *avl)&#123; printf(\"\\nLR\\n\"); RotateWithDoubleRight(&amp;((*avl)-&gt;left)); RotateWithDoubleLeft(avl);&#125;// RL旋转算法void RotateWithRightLeft(PAVL *avl)&#123; printf(\"\\nRL\\n\"); RotateWithDoubleLeft(&amp;((*avl)-&gt;right)); RotateWithDoubleRight(avl);&#125;void print_inorder(PAVL avl)&#123; if(avl) &#123; print_inorder(avl-&gt;left); printf(\"%d - \",avl-&gt;data); print_inorder(avl-&gt;right); &#125;&#125;void print_inhight(PAVL avl)&#123; if(avl) &#123; print_inhight(avl-&gt;left); printf(\"%d - \",avl-&gt;hight); print_inhight(avl-&gt;right); &#125;&#125;void print_preorder(PAVL avl)&#123; if(avl) &#123; printf(\"%d - \",avl-&gt;data); print_preorder(avl-&gt;left); print_preorder(avl-&gt;right); &#125;&#125;void print_houorder(PAVL avl)&#123; if(avl) &#123; print_houorder(avl-&gt;left); print_houorder(avl-&gt;right); printf(\"%d - \",avl-&gt;data); &#125;&#125;","categories":[{"name":"数据结构","slug":"数据结构","permalink":"http://blog.crazylaw.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"C","slug":"C","permalink":"http://blog.crazylaw.cn/tags/C/"},{"name":"数据结构","slug":"数据结构","permalink":"http://blog.crazylaw.cn/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"BST树","slug":"BST树","permalink":"http://blog.crazylaw.cn/tags/BST%E6%A0%91/"}]},{"title":"C语言数据结构のBST树(写法二:指针引用，递归)","slug":"数据结构/struct-study-5","date":"2017-02-15T10:01:00.000Z","updated":"2021-03-20T16:25:01.818Z","comments":true,"path":"2017/02/15/数据结构/struct-study-5/","link":"","permalink":"http://blog.crazylaw.cn/2017/02/15/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/struct-study-5/","excerpt":"","text":"实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#define true 1#define false 0typedef int elementType;typedef struct bst&#123; elementType data; // 数据域 struct bst *left; // 左指针 struct bst *right; // 右指针&#125;BST,*PBST;PBST create_bst(int root_data); // create include the root node treeint insert_node(PBST *bst,int data); // insert into bstvoid print_inorder(PBST tree); void main()&#123; int in; printf(\"请输入根节点的值\"); scanf(\"%d\",&amp;in); PBST bst = create_bst(in); insert_node(&amp;bst,2); insert_node(&amp;bst,3); insert_node(&amp;bst,4); insert_node(&amp;bst,5); insert_node(&amp;bst,6);// printf(\"\\n%d\",bst-&gt;right == NULL);// printf(\"\\n%d\",bst-&gt;left-&gt;data); print_inorder(bst);&#125;PBST create_bst(int root_data)&#123; // 申请内存，并且返回地址 PBST bst = (PBST)malloc(sizeof(PBST)); if(bst == NULL) &#123; printf(\"malloc apply fail\"); exit(-1); &#125; bst-&gt;data = root_data; // 默认为0 bst-&gt;left = NULL; // 防止出现野指针 bst-&gt;right = NULL; // 防止出现野指针 return bst;&#125;void create_node(PBST child,int data)&#123; child = (PBST)malloc(sizeof(PBST)); child-&gt;data = data; child-&gt;left = child-&gt;right = NULL;&#125;// 中序排列（递增排列）int insert_node(PBST *bst,int data)&#123; if(!(*bst)) &#123; PBST temp = (PBST)malloc(sizeof(PBST)); temp-&gt;left = temp-&gt;right = NULL; temp-&gt;data = data; *bst = temp; return ; &#125; if (data &lt; (*bst)-&gt;data) &#123; insert_node(&amp;((*bst)-&gt;left),data); &#125;else if (data &gt; (*bst)-&gt;data) &#123; insert_node(&amp;((*bst)-&gt;right),data); &#125;&#125;void print_inorder(PBST tree) &#123; if(tree) &#123; print_inorder(tree-&gt;left); printf(\"%d\\n\",tree-&gt;data); print_inorder(tree-&gt;right); &#125;&#125;","categories":[{"name":"数据结构","slug":"数据结构","permalink":"http://blog.crazylaw.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"C","slug":"C","permalink":"http://blog.crazylaw.cn/tags/C/"},{"name":"数据结构","slug":"数据结构","permalink":"http://blog.crazylaw.cn/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"BST树","slug":"BST树","permalink":"http://blog.crazylaw.cn/tags/BST%E6%A0%91/"}]},{"title":"C语言数据结构のBST树(写法一:非指针引用，非递归)","slug":"数据结构/struct-study-4","date":"2017-02-15T10:00:00.000Z","updated":"2021-03-20T16:25:01.818Z","comments":true,"path":"2017/02/15/数据结构/struct-study-4/","link":"","permalink":"http://blog.crazylaw.cn/2017/02/15/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/struct-study-4/","excerpt":"","text":"实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#define true 1#define false 0typedef int elementType;typedef struct bst&#123; elementType data; // 数据域 struct bst *left; // 左指针 struct bst *right; // 右指针&#125;BST,*PBST;PBST create_bst(int root_data); // create include the root node treeint insert_node(PBST bst,int data); // insert into bstPBST create_node(PBST child,int data);void print_inorder(PBST tree);void main()&#123; int in; printf(\"请输入根节点的值\"); scanf(\"%d\",&amp;in); PBST bst = create_bst(in); insert_node(bst,2);// insert_node(bst,3); insert_node(bst,4); insert_node(bst,1);// insert_node(bst,6);// printf(\"\\n%d\",bst-&gt;right == NULL);// printf(\"\\n%d\",bst-&gt;left-&gt;data); print_inorder(bst);&#125;PBST create_bst(int root_data)&#123; // 申请内存，并且返回地址 PBST bst = (PBST)malloc(sizeof(PBST)); if(bst == NULL) &#123; printf(\"malloc apply fail\"); exit(-1); &#125; bst-&gt;data = root_data; // 默认为0 bst-&gt;left = NULL; // 防止出现野指针 bst-&gt;right = NULL; // 防止出现野指针 return bst;&#125;PBST create_node(PBST child,int data)&#123; child = (PBST)malloc(sizeof(PBST)); if(child==NULL) &#123; printf(\"malloc apply fail - child\"); exit(-1); &#125; child-&gt;data = data; child-&gt;left = child-&gt;right = NULL; return child;&#125;// 中序排列（递增排列）int insert_node(PBST bst,int data)&#123; PBST current_node = bst; if(current_node-&gt;data &gt; data) &#123; while(true)&#123; if(current_node-&gt;left !=NULL) &#123; current_node = current_node-&gt;left; &#125;else &#123; PBST child = (PBST)malloc(sizeof(PBST)); child = create_node(child,data); printf(\"\\nchild data : %d \\n\",child-&gt;data); current_node-&gt;left = child; break; &#125; &#125; &#125;else&#123; while(true)&#123; if(current_node-&gt;right !=NULL) &#123; current_node = current_node-&gt;right; &#125;else &#123; PBST child = (PBST)malloc(sizeof(PBST)); child = create_node(child,data); printf(\"\\nchild data : %d \\n\",child-&gt;data); current_node-&gt;right = child; break; &#125; &#125; &#125;&#125;void print_inorder(PBST tree) &#123; if(tree) &#123; print_inorder(tree-&gt;left); printf(\"%d\\n\",tree-&gt;data); print_inorder(tree-&gt;right); &#125;&#125;","categories":[{"name":"数据结构","slug":"数据结构","permalink":"http://blog.crazylaw.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"C","slug":"C","permalink":"http://blog.crazylaw.cn/tags/C/"},{"name":"数据结构","slug":"数据结构","permalink":"http://blog.crazylaw.cn/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"BST树","slug":"BST树","permalink":"http://blog.crazylaw.cn/tags/BST%E6%A0%91/"}]},{"title":"C语言数据结构の循环队列","slug":"数据结构/struct-study-3","date":"2017-02-15T06:08:17.000Z","updated":"2021-03-20T16:25:01.817Z","comments":true,"path":"2017/02/15/数据结构/struct-study-3/","link":"","permalink":"http://blog.crazylaw.cn/2017/02/15/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/struct-study-3/","excerpt":"","text":"实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;// because the C language does not has the type of boolean ,so , we use replace of which int,example 1 and 0 replace of the \"true\" and \"false\"#define true 1#define false 0// alias name by inttypedef int elementType;// design the queue data stucttypedef struct queue&#123; elementType *content; elementType front; elementType rear; elementType maxSize;&#125;QUEUE,*PQUEUE;// define the funcPQUEUE createQueue(); // create one queue , return the queueint isFull(PQUEUE queue); // is Fullint isEmpty(PQUEUE queue); // is Emptyint enQueue(PQUEUE queue,int data); // EnQueue , include isFullint deQueue(PQUEUE queue); // DeQueue, include isEmpty//void loop_print(PQUEUE queue); // Loop Print the queue;void main()&#123; // define the queue PQUEUE queue = NULL; printf(\" Please Input the queue of length :\"); int length; scanf(\"%d\",&amp;length); // invoke func queue = createQueue(length); while(true) &#123; printf(\" Please Input which your want to enQueue the data:\"); int data ; scanf(\"%d\",&amp;data); if(data == 0) &#123; break; &#125; // invoke func enQueue(queue,data); &#125; char *result = (char *)malloc(sizeof(char)); while(true) &#123; printf(\"deQueue :\"); int data = deQueue(queue); printf(\"data : %d\\n\",data); printf(\" Go on ? [yes|no]\\n\"); scanf(\"%s\",result); if(strcmp(result,\"yes\") != 0) &#123; break; &#125; &#125;&#125;PQUEUE createQueue(int maxSize)&#123; PQUEUE p = (PQUEUE)malloc(sizeof(PQUEUE)); if(p == NULL) &#123; printf(\"malloc apply fail\"); exit(-1); &#125; if(maxSize &lt; 1) &#123; printf(\"the queue size does not &lt; 1\"); exit(-1); &#125; maxSize++; // when the front end rear equals ,the queue is empty p-&gt;front = p-&gt;rear = 0; p-&gt;maxSize = maxSize; p-&gt;content = (elementType *)malloc(sizeof(elementType)); // init Mem // return the queue of point; return p;&#125;int enQueue(PQUEUE queue,int data)&#123; if(isFull(queue))&#123; printf(\"The queue is fullest,data : %d\\n\",data); exit(-1); &#125; queue-&gt;content[queue-&gt;rear] = data; queue-&gt;rear = (queue-&gt;rear+1)%queue-&gt;maxSize;&#125;int isFull(PQUEUE queue)&#123; if((((queue-&gt;rear)+1)%queue-&gt;maxSize == queue-&gt;front)) &#123; return true; &#125; return false;&#125;int isEmpty(PQUEUE queue)&#123; if(queue-&gt;rear == queue-&gt;front &amp;&amp; queue-&gt;maxSize &gt; 0) &#123; return true; &#125; return false;&#125;int deQueue(PQUEUE queue)&#123; if(isEmpty(queue)) &#123; printf(\"队列为空\"); exit(-1); &#125; int data = queue-&gt;content[queue-&gt;front]; queue-&gt;front = (queue-&gt;front+1)%queue-&gt;maxSize; return data;&#125;","categories":[{"name":"数据结构","slug":"数据结构","permalink":"http://blog.crazylaw.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"C","slug":"C","permalink":"http://blog.crazylaw.cn/tags/C/"},{"name":"数据结构","slug":"数据结构","permalink":"http://blog.crazylaw.cn/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"循环队列","slug":"循环队列","permalink":"http://blog.crazylaw.cn/tags/%E5%BE%AA%E7%8E%AF%E9%98%9F%E5%88%97/"}]},{"title":"PHP7扩展开发のHello Wold","slug":"php-extend-1","date":"2017-02-10T03:47:00.000Z","updated":"2021-03-20T16:25:01.812Z","comments":true,"path":"2017/02/10/php-extend-1/","link":"","permalink":"http://blog.crazylaw.cn/2017/02/10/php-extend-1/","excerpt":"简介想要突破自我，必须深入了解 PHP，今天我们讲讲 PHP 扩展的开发，此路长长，需要多多积累 C 语言的知识，废话不多说，直入主题。 资源先去 github 上下载 PHP 的源码，搜索 php-src https://github.com/php/php-src","text":"简介想要突破自我，必须深入了解 PHP，今天我们讲讲 PHP 扩展的开发，此路长长，需要多多积累 C 语言的知识，废话不多说，直入主题。 资源先去 github 上下载 PHP 的源码，搜索 php-src https://github.com/php/php-src 开始1.在 PHP 源码包里面，里面有一个 ext，和 Zend 目录，Zend 目录就是我们的 PHP 源码的所有头文件和执行文件，ext 则是我们存放扩展的目录。日后必读 Zend。目前只需要关注 ext。 1cd ext 在 ext 目录下可以看到很多 PHP 源码帮我们扩展好的一些扩展，例如 curl,xml,pdo 等等。我们现在要关注的是 ext-skel 工具。它可以帮我们生成扩展的标准目录格式。 1./ext-skel --help 1.extname 参数为我们需要自定义的扩展名称 1./ext-skel --extname=hello_world 以上代码会自动为你创建扩展名字叫hello_world的标准目录。 创建完毕之后 1cd hello_world 目前，我们需要关注的文件只有两个，分别是config.m4,hello_world.c 这个config.m4目录是我们生成编译文件的配置文件。 1vim config.m4 修改完毕之后，保存退出。 接下来就是我们的重点文件了，hello_world.c 1234vim hello_world.c#以下是vim搜索命令zend_module_entry/zend_module_entry 查看到如下部分： 这个函数就是我们扩展的入口文件。现在我们暂时用不上。 我们需要关心的 zend_function_entry 这个函数会初始化我们的自定义函数，也就是我们接下来的Hello_World()。 我们在zend_function_entry 里面写入一行代码: 1PHP_FE(Hello_World,NULL) 第一个参数为函数的名称，第二参数为传入函数的参数。 好了，定义完要初始化的函数之后，我们就要去写函数了。 找个空白的地方插入以下代码 123456PHP_FUNCTION(Hello_World)&#123; php_printf(\"I'm PHP's ext function Hello_World\"); /* this function php_printf Zend Engine defined*/ RETURN_TRUE; /* this const var also with top*/&#125; 保存退出 123phpize./configuremake &amp;&amp; make install 1ll modules&#x2F; ok，完成了 90%了。 剩下的就是把这个 so 文件，添加到php.ini里面吧。 Good Luck!","categories":[{"name":"PHP","slug":"PHP","permalink":"http://blog.crazylaw.cn/categories/PHP/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"http://blog.crazylaw.cn/tags/PHP/"},{"name":"PHP扩展","slug":"PHP扩展","permalink":"http://blog.crazylaw.cn/tags/PHP%E6%89%A9%E5%B1%95/"}]},{"title":"C语言数据结构の单链表","slug":"数据结构/struct-study-2","date":"2017-01-24T09:27:20.000Z","updated":"2021-03-20T16:25:01.817Z","comments":true,"path":"2017/01/24/数据结构/struct-study-2/","link":"","permalink":"http://blog.crazylaw.cn/2017/01/24/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/struct-study-2/","excerpt":"","text":"实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt; // 声明exit函数/* 单节点的数据结构 16个字节 */typedef struct Node&#123; int data; //数据域 struct Node * pNext; //指针域&#125;NODE,*PNODE;//函数声明PNODE createLinkList(void); //创建链表的函数void traverseLinkList(PNODE pHead); //遍历链表的函数/** * 主函数入口 main */void main()&#123; PNODE pHead = NULL; pHead = createLinkList(); traverseLinkList(pHead);&#125;/** * 创建链表 */PNODE createLinkList(void)&#123; int length; int i; int value; // 申请内存空间,头结点 PNODE phead = (PNODE)malloc(sizeof(NODE)); if(NULL == phead) &#123; printf(\"内存分配失败，程序退出！\\n\"); exit(-1); &#125; // 尾节点 PNODE pend = phead; // pend始终指向尾节点 pend-&gt;pNext = NULL; // 单链表尾节点指针域为NULL printf(\"请输入链表的长度,len =\"); scanf(\"%d\",&amp;length); // 创建链表长度 for(i= 0;i&lt; length;i++)&#123; printf(\"请输入第%d个节点的值 :\",i); scanf(\"%d\",&amp;value); PNODE pNew =(PNODE)malloc(sizeof(NODE)); if(NULL == pNew) &#123; printf(\"内存分配失败，程序退出！\\n\"); exit(-1); &#125; pNew-&gt;data = value; // 把新值放入节点 pend-&gt;pNext = pNew; // 把尾节点指向新节点 pNew-&gt;pNext = NULL; // 尾节点指针域为NULL pend = pNew; &#125; return phead;&#125;void traverseLinkList(PNODE pHead)&#123; PNODE p = pHead-&gt;pNext; while(NULL != p) &#123; printf(\"%d \", p-&gt;data); p = p-&gt;pNext; &#125; printf(\"\\n\");&#125;","categories":[{"name":"数据结构","slug":"数据结构","permalink":"http://blog.crazylaw.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"C","slug":"C","permalink":"http://blog.crazylaw.cn/tags/C/"},{"name":"数据结构","slug":"数据结构","permalink":"http://blog.crazylaw.cn/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"单链表","slug":"单链表","permalink":"http://blog.crazylaw.cn/tags/%E5%8D%95%E9%93%BE%E8%A1%A8/"}]},{"title":"线性存储结构","slug":"数据结构/struct-study-1","date":"2017-01-24T03:29:00.000Z","updated":"2021-03-20T16:25:01.817Z","comments":true,"path":"2017/01/24/数据结构/struct-study-1/","link":"","permalink":"http://blog.crazylaw.cn/2017/01/24/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/struct-study-1/","excerpt":"简介 线性结构是一个有序元素集合 常用的线性结构有：线性表，栈，队列，双队列，数组，串 关于广义表，是一种非线性数据结构 常见的非线性结构有：二维数组，多维数组，树（二叉树等），广义表，图。","text":"简介 线性结构是一个有序元素集合 常用的线性结构有：线性表，栈，队列，双队列，数组，串 关于广义表，是一种非线性数据结构 常见的非线性结构有：二维数组，多维数组，树（二叉树等），广义表，图。 线性表 顺序表 顺序表将元素存储在一组连续的单元中，在物理内存上是连续的，就是说内存地址是连续的。 顺序的密度比较大,节省空间。读取的时间复杂度为:O(1),查询的时间复杂度为 O((n-0)/2),插入或者删除的时间复杂度为 O((n-0)/2),删除的时间复杂度为 O((n-1)/2) 链表 链表拥有很多结点，每个结点前半部分是数据域，后半部分是指针域，指针域指针指向下一个结点；链表可分为单链表、循环链表和双链表。 单链表 从结构可以看出,单链表有一个头部节点,没有 data 域,直接指向第一个节点 p1,第一个节点分为两个部分,一个是 data 域,一个是指针域,指针域指向下一个节点 p2,最后一个节点的指针域为 NULL. 链表的密度比较散列。读取的时间复杂度为 O((n+1)/2),查询的时间复杂度为 O(n/2)，插入的时间复杂度为 O(1),删除的时间复杂为 O(1)","categories":[{"name":"数据结构","slug":"数据结构","permalink":"http://blog.crazylaw.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"数据结构","slug":"数据结构","permalink":"http://blog.crazylaw.cn/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"线性存储结构","slug":"线性存储结构","permalink":"http://blog.crazylaw.cn/tags/%E7%BA%BF%E6%80%A7%E5%AD%98%E5%82%A8%E7%BB%93%E6%9E%84/"}]},{"title":"C 语言入门实录 二","slug":"c-study-2","date":"2017-01-21T03:29:00.000Z","updated":"2021-03-20T16:25:01.802Z","comments":true,"path":"2017/01/21/c-study-2/","link":"","permalink":"http://blog.crazylaw.cn/2017/01/21/c-study-2/","excerpt":"C语言中的类型分为一下几种 序号 类型和描述 1 基本类型：算术类型 ，整数类型和浮点类型 2 枚举类型：也是算术类型，但是是一些离散固定的整数类型 3 void类型：表明没有可用的值 4 派生类型：指针类型，数组类型，结构类型，函数类型以及共用体类型 整数类型 类型 存储大小（1个字节(bit)占8位[bits]） 值的范围（由二进制得出） char 1字节 -128127 或者 0255 unsigned char 1字节 0~255 signed char 1字节 -128~127 int 2字节(16位系统)或4字节(32/64位系统) -32,768 到 32,767 或 -2,147,483,648 到 2,147,483,647 unsigned int 2字节(16位系统)或4字节(32/64位系统) 0 到 65,535 或 0 到 4,294,967,295 short 2字节 -32,768 到 32,767 unsigned short 2字节 0 到 65,535 long 4字节 -2,147,483,648 到 2,147,483,647 unsigned long 4字节 0 到 4,294,967,295","text":"C语言中的类型分为一下几种 序号 类型和描述 1 基本类型：算术类型 ，整数类型和浮点类型 2 枚举类型：也是算术类型，但是是一些离散固定的整数类型 3 void类型：表明没有可用的值 4 派生类型：指针类型，数组类型，结构类型，函数类型以及共用体类型 整数类型 类型 存储大小（1个字节(bit)占8位[bits]） 值的范围（由二进制得出） char 1字节 -128127 或者 0255 unsigned char 1字节 0~255 signed char 1字节 -128~127 int 2字节(16位系统)或4字节(32/64位系统) -32,768 到 32,767 或 -2,147,483,648 到 2,147,483,647 unsigned int 2字节(16位系统)或4字节(32/64位系统) 0 到 65,535 或 0 到 4,294,967,295 short 2字节 -32,768 到 32,767 unsigned short 2字节 0 到 65,535 long 4字节 -2,147,483,648 到 2,147,483,647 unsigned long 4字节 0 到 4,294,967,295 但是C语言的取值范围是如何计算的出来的呢？ 二进制数据在计算机中存储的是补码正数的补码等于原码,负数补码需要在符号位之后取反+1 如果以最高位为符号位，二进制原码最大为0111111111111111=2的15次方减1=32767最小为1111111111111111=-2的15次方减1=-32767此时0有两种表示方法，即正0和负0：0000000000000000=1000000000000000=0所以，二进制原码表示时，范围是-32767～-0和0～32767，因为有两个零的存在，所以不同的数值个数一共只有2的16次方减1个，比16位二进制能够提供的2的16次方个编码少1个。但是计算机中采用二进制补码存储数据，即正数编码不变，从0000000000000000到0111111111111111依旧表示0到32767，而负数需要把除符号位以后的部分取反加1，即-32767的补码为1000000000000001。到此，再来看原码的正0和负0：0000000000000000和1000000000000000，补码表示中，前者的补码还是0000000000000000，后者经过非符号位取反加1后，同样变成了0000000000000000，也就是正0和负0在补码系统中的编码是一样的。但是，我们知道，16位二进制数可以表示2的16次方个编码，而在补码中零的编码只有一个，也就是补码中会比原码多一个编码出来，这个编码就是1000000000000000，因为任何一个原码都不可能在转成补码时变成1000000000000000。所以，人为规定1000000000000000这个补码编码为-32768。所以，补码系统中，范围是-23768～32767。因此，实际上，二进制的最小数确实是1111111111111111，只是二进制补码的最小值才是1000000000000000，而补码的1111111111111111是二进制值的-1 当然，如果你不确定当前每个类型占的自己数的话，可以使用一下代码来打印一下 123456789#include &lt;stdio.h&gt;int main()&#123; printf(\"int 存储的字节大小为: %lu \\n\",sizeof(int)); return 0;&#125; 浮点类型 类型 存储大小（1个字节(bit)占8位[bits]） 值的范围（由二进制得出） 精度 float 4 字节 1.2E-38 到 3.4E+38 6 位小数 double 8 字节 2.3E-308 到 1.7E+308 15 位小数 long double 16 字节 3.4E-4932 到 1.1E+4932 19 位小数 想要用float的函数和宏就需要引入float相关的函数定义 123456789101112131415#include &lt;stdio.h&gt;#include &lt;float.h&gt; /** float头部 **/int main()&#123; printf(\"float 最大存储的字节为 %lu \\n\",sizeof(float)); printf(\"float 最小值为 %E\\n\",FLT_MIN); printf(\"float 最大值为 %E\\n\",FLT_MAX); printf(\"精确度 %d\\n\",FLT_DIG); return 0;&#125; void类型指定没有可用的值 序号 类型与描述 1 函数返回为空 C 中有各种函数都不返回值，或者您可以说它们返回空。不返回值的函数的返回类型为空。例如 void exit (int status); 2 函数参数为空 C 中有各种函数不接受任何参数。不带参数的函数可以接受一个 void。例如 int rand(void); 3 指针指向 void 类型为 void * 的指针代表对象的地址，而不是类型。例如，内存分配函数 void *malloc( size_t size ); 返回指向 void 的指针，可以转换为任何数据类型。","categories":[{"name":"C","slug":"C","permalink":"http://blog.crazylaw.cn/categories/C/"}],"tags":[{"name":"C","slug":"C","permalink":"http://blog.crazylaw.cn/tags/C/"},{"name":"学习记录","slug":"学习记录","permalink":"http://blog.crazylaw.cn/tags/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"}]},{"title":"C语言 入门实录 一","slug":"c-study-1","date":"2017-01-21T03:01:00.000Z","updated":"2021-03-20T16:25:01.802Z","comments":true,"path":"2017/01/21/c-study-1/","link":"","permalink":"http://blog.crazylaw.cn/2017/01/21/c-study-1/","excerpt":"开始在 linux vim 下建立一个 Hello World 程序 我们意气风发的写下： 12345678910-Hello_World.c-#include &lt;stdio.h&gt;int main()&#123; printf(\"Hello world! \\n\"); return 0;&#125;","text":"开始在 linux vim 下建立一个 Hello World 程序 我们意气风发的写下： 12345678910-Hello_World.c-#include &lt;stdio.h&gt;int main()&#123; printf(\"Hello world! \\n\"); return 0;&#125; 通过 gcc 来编译 C 代码 gcc Hello_World.c -o Hello_World 生成可执行文件，大小相差 8kb 运行可执行文件 ./Hello_World 备注 所有的 C 语言程序都需要包含 main() 函数。 代码从 main() 函数开始执行。 /_ … _/ 用于注释说明。 printf() 用于格式化输出到屏幕。printf() 函数在 “stdio.h” 头文件中声明。 stdio.h 是一个头文件 (标准输入输出头文件) and #include 是一个预处理命令，用来引入头文件。 当编译器遇到 printf() 函数时，如果没有找到 stdio.h 头文件，会发生编译错误。 return 0; 语句用于表示退出程序。","categories":[{"name":"C","slug":"C","permalink":"http://blog.crazylaw.cn/categories/C/"}],"tags":[{"name":"C","slug":"C","permalink":"http://blog.crazylaw.cn/tags/C/"},{"name":"学习记录","slug":"学习记录","permalink":"http://blog.crazylaw.cn/tags/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"}]},{"title":"深度解剖JWT认证流程","slug":"jwt-analysis","date":"2017-01-17T02:40:00.000Z","updated":"2021-03-20T16:25:01.806Z","comments":true,"path":"2017/01/17/jwt-analysis/","link":"","permalink":"http://blog.crazylaw.cn/2017/01/17/jwt-analysis/","excerpt":"说明 说明简介 传统的 cookie-session 机制可以保证的接口安全，在没有通过认证的情况下会跳转至登入界面或者调用失败。 在如今 RESTful 化的 API 接口下，cookie-session 已经不能很好发挥其余热保护好你的 API 。 更多的形式下采用的基于 Token 的验证机制，JWT 本质的也是一种 Token，但是其中又有些许不同。 什么是 JWTJWT 及时 JSON Web Token，它是基于 RFC 7519 所定义的一种在各个系统中传递紧凑和自包含的 JSON 数据形式。 紧凑（Compact） ：由于传送的数据小，JWT 可以通过 GET、POST 和 放在 HTTP 的 header 中，同时也是因为小也能传送的更快。 自包含（self-contained） : Payload 中能够包含用户的信息，避免数据库的查询。 JSON Web Token 由三部分组成使用 . 分割开： header （头部） payload （载荷） Signature（签名）","text":"说明 说明简介 传统的 cookie-session 机制可以保证的接口安全，在没有通过认证的情况下会跳转至登入界面或者调用失败。 在如今 RESTful 化的 API 接口下，cookie-session 已经不能很好发挥其余热保护好你的 API 。 更多的形式下采用的基于 Token 的验证机制，JWT 本质的也是一种 Token，但是其中又有些许不同。 什么是 JWTJWT 及时 JSON Web Token，它是基于 RFC 7519 所定义的一种在各个系统中传递紧凑和自包含的 JSON 数据形式。 紧凑（Compact） ：由于传送的数据小，JWT 可以通过 GET、POST 和 放在 HTTP 的 header 中，同时也是因为小也能传送的更快。 自包含（self-contained） : Payload 中能够包含用户的信息，避免数据库的查询。 JSON Web Token 由三部分组成使用 . 分割开： header （头部） payload （载荷） Signature（签名） 一个完整的 JWT 为以下的形式： xxxxx.yyyy.zzzz Header 一般由两个部分组成 alg tpy alg 是所采用的加密 hash 算法（HS256，MD5 等），tpy 就是 token 的类型了，在这里就肯定是”JWT”。 1234&#123; \"alg\": \"HS256\", \"tpy\": \"JWT\"&#125; 在这个基础上，我们采用 Base64 加密的算法来加密第一部分 header 结果如下： eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9 那么，目前 JWT 的形式不完整形态就为： eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.&lt;second part&gt;.&lt;third part&gt; Payload 一般分了几个部分，它是整个 JWT 的核心信息，相当于 body，其中包含了许多种的声明（claims）。 （保留声明）reserved claims ：预定义的 一些声明，并不是强制的但是推荐，它们包括 iss (issuer)，exp (expiration time)，sub (subject)，aud(audience) 等。 （公有声明）public claims : 这个部分可以随便定义，但是要注意和 IANA JSON Web Token 冲突。 Payload 一般由以下部分组成 issu （签发者） sub （面向对象） iat （签发时间） exp （过期时间） 123456&#123; \"issu\":\"Baicai\", \"sub\":\"http://usblog.crazylaw.cn\", \"iat\":1484618744 \"exp\":1484618754 (注：10s后过期)&#125; Signature 在创建该部分时候你应该已经有了 编码后的 Header 和 Payload 还需要一个一个秘钥，这个加密的算法应该 Header 中指定。 一个使用 HMAC SHA256 的例子如下: 1234HMACSHA256( base64UrlEncode(header) + &quot;.&quot; + base64UrlEncode(payload), secret) 这个 signature 是用来验证发送者的 JWT 的同时也能确保在期间不被篡改。 所以，做后你的一个完整的 JWT 应该是如下形式： 1eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiYWRtaW4iOnRydWV9.TJVA95OrM7E2cBab30RMHrHDcEfxjoYZgeFONFh7HgQ 注意\b被 . 分割开的三个部分 JSON Web Token 的工作流程 在用户使用证书或者账号密码登入的时候一个 JSON Web Token 将会返回，同时可以把这个 JWT 存储在 local storage、或者 cookie 中，用来替代传统的在服务器端创建一个 session 返回一个 cookie。 当用户想要使用受保护的路由时候，应该要在请求得时候带上 JWT ，一般的是在 header 的 Authorization 使用 Bearer 的形式，一个包含的 JWT 的请求头的 Authorization 如下： Authorization: Bearer &lt;token&gt; 这是一中无状态的认证机制，用户的状态从来不会存在服务端，在访问受保护的路由时候回校验 HTTP header 中 Authorization 的 JWT，同时 JWT 是会带上一些必要的信息，不需要多次的查询数据库。 这种无状态的操作可以充分的使用数据的 APIs，甚至是在下游服务上使用，这些 APIs 和哪服务器没有关系，因此，由于没有 cookie 的存在，所以在不存在跨域（CORS, Cross-Origin Resource Sharing）的问题。 你们以为到这里就完了？其实还没有，还有 JWT 超时的问题以及传输的位置还没有讲。JWT 一般你可以选择放在 GET 方法的 query 作为参数传递给服务器校验。你也可以选择 POST 方法的body 来传递该该参数，但是我们知道 GET、POST 方法都有自身的传递数据长度，我们要节约这些资源的话，我还是推荐放在头部。并且服务器端每次校验 jwt 的时候，可以设置一个 leeway ，我把它叫为迂回时间，这个时间的作用是在 jwt 超过了有效时间，但是可以刷新 jwt 而存在的，我们可以称之为 刷新令牌时间，只要在这个时间范围内，服务器端就会在响应性头设置新的 jwt 返回给客户端，客户端需要检测这个响应头是否存在这个属性来更新 jwt 的值，否则，如果 jwt 超过了这个 leeway，那么，你的 app 将需要重新登录来获取授权。","categories":[{"name":"协议","slug":"协议","permalink":"http://blog.crazylaw.cn/categories/%E5%8D%8F%E8%AE%AE/"}],"tags":[{"name":"协议","slug":"协议","permalink":"http://blog.crazylaw.cn/tags/%E5%8D%8F%E8%AE%AE/"},{"name":"JWT","slug":"JWT","permalink":"http://blog.crazylaw.cn/tags/JWT/"}]},{"title":"apache-benchmark","slug":"apache-benchmark","date":"2017-01-12T06:38:06.000Z","updated":"2021-03-20T16:25:01.801Z","comments":true,"path":"2017/01/12/apache-benchmark/","link":"","permalink":"http://blog.crazylaw.cn/2017/01/12/apache-benchmark/","excerpt":"简介Apache Benchmark 其实就是我们平时所熟称的abtest，即ab压力测试，它是apache自带的一个很好用的压力测试工具，当安装完apache的时候，就可以在bin下面找到ab。 但是如果我们用的http代理是nginx的时候就需要手动下载这个工具了。接下来就和大家讲讲abtest。","text":"简介Apache Benchmark 其实就是我们平时所熟称的abtest，即ab压力测试，它是apache自带的一个很好用的压力测试工具，当安装完apache的时候，就可以在bin下面找到ab。 但是如果我们用的http代理是nginx的时候就需要手动下载这个工具了。接下来就和大家讲讲abtest。 安装centos 7下安装ab，只需要一个命令就好了 yum install httpd-tools 使用ab www.baidu.com/ 注意压力测试是测试某个目录的，后尾要加上/ 这就是ab最简单的测试命令，这一命令显示了百度页面的相关数据。 1234567891011121314151617181920212223242526272829303132This is ApacheBench, Version 2.3 &lt;$Revision: 1430300 $&gt;Copyright 1996 Adam Twiss, Zeus Technology Ltd, http:&#x2F;&#x2F;www.zeustech.net&#x2F;Licensed to The Apache Software Foundation, http:&#x2F;&#x2F;www.apache.org&#x2F;Benchmarking www.baidu.com (be patient).....doneServer Software: BWS&#x2F;1.1Server Hostname: www.baidu.comServer Port: 80Document Path: &#x2F;Document Length: 102126 bytesConcurrency Level: 1Time taken for tests: 0.028 secondsComplete requests: 1Failed requests: 0Write errors: 0Total transferred: 103076 bytesHTML transferred: 102126 bytesRequests per second: 36.01 [#&#x2F;sec] (mean)Time per request: 27.773 [ms] (mean)Time per request: 27.773 [ms] (mean, across all concurrent requests)Transfer rate: 3624.39 [Kbytes&#x2F;sec] receivedConnection Times (ms) min mean[+&#x2F;-sd] median maxConnect: 8 8 0.0 8 8Processing: 20 20 0.0 20 20Waiting: 12 12 0.0 12 12Total: 28 28 0.0 28 28 上边的数据中，HTML transferred，Requests per second，Time per request是我们需要重点关注的，根据这些数据，我们能大概了解Web服务器的性能水平。 相关字段说明 字段 说明 Server Software 服务器系统 Server Hostname 服务器域名 Server Port 服务器端口 Document Path 访问的路径 Document Length 访问的文件大小 Concurrency Level 并发请求数，可以理解为同一时间的访问人数 Time taken fortests 响应时间 Complete requests 总共响应次数 Failed requests 失败的请求次数 Write errors 失败的写入次数 Total transferred 传输的总数据量 HTML transferred HTML页面大小 Requests per second 每秒支持多少人访问 Time per request 满足一个请求花费的总时间 Time per request 满足所有并发请求中的一个请求花费的总时间 Transfer rate 平均每秒收到的字节 最后的数据包括 Connect , Processing , Waiting , Total 字段。这些数据能大致说明测试过程中所需要的时间。其实我们可以只看Total字段中的min，max两列数据值，这两个值分别显示了测试过程中，花费时间最短和最长的时间。 可选参数ab命令还有很多可选参数，但常用的其实就下边几个： 参数 功能解释 -n 设置ab命令模拟请求的总次数 -c 设置ab命令模拟请求的并发数 -t 设置ab命令模拟请求的时间 -k 设置ab命令允许1个http会话响应多个请求 这三个参数的用法如下： 1）使用ab命令加上”-n”参数模拟1个用户访问百度总共5次 1ab -n 5 www.baidu.com&#x2F; 2）使用ab命令加上”-n”与”-c”参数模拟5个用户同时访问百度总共9次 1ab -c 5 -n 9 www.baidu.com&#x2F; 3）使用ab命令加上”-c”与”-t”参数模拟5个用户同时访问百度总共9秒 1ab -c 5 -t 9 www.baidu.com&#x2F; 3）使用ab命令加上”-c”与”-t”附加”-k”参数模拟5个用户同时访问百度总共9秒,百度会打开5个并发连接，从而减少web服务器创建新链接所花费的时间。 1ab -c 5 -t 9 -k www.baidu.com&#x2F; 使用ab命令的时候，有几点点要说明： 1）ab命令必须指定要访问的文件，如果没指定，那必须得在域名的结尾加上一个反斜杠，例如 1ab www.baidu.com 得改写为 1ab www.baidu.com&#x2F; 2）ab命令可能会由于目标web服务器做了相应的过滤处理，导致在某些情况下收不到任何数据，这个时候可以使用”-H”参数，来模拟成浏览器发送请求。例如： 模拟成Chrome浏览器向百度发送1个请求 1ab -H &quot;Mozilla&#x2F;5.0 (Windows; U; Windows NT 5.1; en-Us) AppleWeb Kit&#x2F;534.2 (KHTML, like Gecko) Chrome&#x2F;6.0.447.0 Safari&#x2F;534.2&quot; www.baidu.com&#x2F; 3）最后，要注意的就是，在使用ab命令测试服务器时，千万要小心，并且要限制对服务器发出的请求数量，磨途歌希望大家理性使用这些压力测试工具，我们都不希望任何一台正常的服务器陷入不必要的麻烦。 更多的可选参数如下： 更多参数 说明 -A 采用base64编码向服务器提供身份验证信息，用法: -A 用户名:密码 -C cookie信息，用法: -C mo2g=磨途歌 -d 不显示pecentiles served table -e 保存基准测试结果为csv格式的文件 -g 保存基准测试结果为gunplot或TSV格式的文件 -h 显示ab可选参数列表 -H 采用字段值的方式发送头信息和请求 -i 发送HEAD请求，默认发送GET请求 -p 通过POST发送数据，用法： -p blog=博客&amp;name=白菜 -h 显示ab可选参数列表 -q 执行多余100个请求时隐藏掉进度输出 -s 使用Https协议发送请求，默认使用Http -S 隐藏中位数和标准偏差值 -v -v 2 及以上将打印警告和信息，-v 3 打印http响应码，-v 4 打印头信息 -V 显示ab工具的版本号 -w 采用HTML表格打印结果 -x HTML标签属性，使用 -w 参数时，将放置在&lt;table&gt;标签中 -X 设置代理服务器，用法 -X 192.168.1.1:80 -y HTML标签属性，使用 -w 参数时，将放置在&lt;tr&gt;标签中 -z HTML标签属性，使用 -w 参数时，将放置在&lt;td&gt;标签中","categories":[{"name":"测试","slug":"测试","permalink":"http://blog.crazylaw.cn/categories/%E6%B5%8B%E8%AF%95/"}],"tags":[{"name":"测试工具","slug":"测试工具","permalink":"http://blog.crazylaw.cn/tags/%E6%B5%8B%E8%AF%95%E5%B7%A5%E5%85%B7/"},{"name":"Linux","slug":"Linux","permalink":"http://blog.crazylaw.cn/tags/Linux/"},{"name":"压力测试","slug":"压力测试","permalink":"http://blog.crazylaw.cn/tags/%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95/"},{"name":"性能测试","slug":"性能测试","permalink":"http://blog.crazylaw.cn/tags/%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"}]},{"title":"Hello World","slug":"hello-world","date":"2016-12-28T17:01:00.000Z","updated":"2021-03-20T16:25:01.806Z","comments":true,"path":"2016/12/29/hello-world/","link":"","permalink":"http://blog.crazylaw.cn/2016/12/29/hello-world/","excerpt":"说明新博客创建时间为：2019-03-19 17:14:25博文 2019 年之前的基本就是旧文章，虽然有一些旧文章有点技术含量不高，但是也是记录为的学习过程，所以为决定还是把一些有点用的文章记录下来，时间不太准确 欢迎大家来到我的 技术博客!，这是我 2019 年新的博客，由于旧的博客采用后端博客的形式（其实是由于之前我手误，又没有备份代码，所以导致博客代码全删了），所以现在重新弄了一个更加现代的博客，博客将采用 hexo 强力驱动，接轨现代前端博客的流行性，给大家更加的体验。（😄 也方便我自己维护），由于只是代码删除了，数据库中的文章还是有保留下来，接下来我会陆陆续续把文章都放回上去，但是有一些图片什么的，基本就没有的，我也会把文章升级一下，由于以前刚毕业，文章质量一般般，现在会取其精华，去其糟粕，浓缩更加有价值的文章出来和大家一起学习，欢迎大家来和我一起共同成长～ 🌹🌞 接下来，让我们用 C 语言写下和大家问好的代码 123456#include&lt;stdio.h&gt;int main(void)&#123; printf(\"Hello World\"); return 0; &#125;","text":"说明新博客创建时间为：2019-03-19 17:14:25博文 2019 年之前的基本就是旧文章，虽然有一些旧文章有点技术含量不高，但是也是记录为的学习过程，所以为决定还是把一些有点用的文章记录下来，时间不太准确 欢迎大家来到我的 技术博客!，这是我 2019 年新的博客，由于旧的博客采用后端博客的形式（其实是由于之前我手误，又没有备份代码，所以导致博客代码全删了），所以现在重新弄了一个更加现代的博客，博客将采用 hexo 强力驱动，接轨现代前端博客的流行性，给大家更加的体验。（😄 也方便我自己维护），由于只是代码删除了，数据库中的文章还是有保留下来，接下来我会陆陆续续把文章都放回上去，但是有一些图片什么的，基本就没有的，我也会把文章升级一下，由于以前刚毕业，文章质量一般般，现在会取其精华，去其糟粕，浓缩更加有价值的文章出来和大家一起学习，欢迎大家来和我一起共同成长～ 🌹🌞 接下来，让我们用 C 语言写下和大家问好的代码 123456#include&lt;stdio.h&gt;int main(void)&#123; printf(\"Hello World\"); return 0; &#125; 博客将会涉及以下内容 PHP C Python Erlang Shell Lua Rust Redis Memcached Openresty Mysql Tidb Docker K8s Devops 服务器 大数据 数据结构 算法 架构 等等 希望大家可以和我一起有所收获，铭记一句话：说到做不到，等于不知道 共勉。","categories":[{"name":"博客面世","slug":"博客面世","permalink":"http://blog.crazylaw.cn/categories/%E5%8D%9A%E5%AE%A2%E9%9D%A2%E4%B8%96/"}],"tags":[{"name":"博客面世","slug":"博客面世","permalink":"http://blog.crazylaw.cn/tags/%E5%8D%9A%E5%AE%A2%E9%9D%A2%E4%B8%96/"}]}],"categories":[{"name":"架构设计","slug":"架构设计","permalink":"http://blog.crazylaw.cn/categories/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/"},{"name":"智能家居","slug":"智能家居","permalink":"http://blog.crazylaw.cn/categories/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/"},{"name":"树莓派","slug":"智能家居/树莓派","permalink":"http://blog.crazylaw.cn/categories/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/%E6%A0%91%E8%8E%93%E6%B4%BE/"},{"name":"组件优化","slug":"组件优化","permalink":"http://blog.crazylaw.cn/categories/%E7%BB%84%E4%BB%B6%E4%BC%98%E5%8C%96/"},{"name":"多媒体","slug":"多媒体","permalink":"http://blog.crazylaw.cn/categories/%E5%A4%9A%E5%AA%92%E4%BD%93/"},{"name":"广告行业","slug":"广告行业","permalink":"http://blog.crazylaw.cn/categories/%E5%B9%BF%E5%91%8A%E8%A1%8C%E4%B8%9A/"},{"name":"Mac","slug":"Mac","permalink":"http://blog.crazylaw.cn/categories/Mac/"},{"name":"DevOps","slug":"DevOps","permalink":"http://blog.crazylaw.cn/categories/DevOps/"},{"name":"Go源码剖析系列","slug":"Go源码剖析系列","permalink":"http://blog.crazylaw.cn/categories/Go%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90%E7%B3%BB%E5%88%97/"},{"name":"Golang","slug":"Golang","permalink":"http://blog.crazylaw.cn/categories/Golang/"},{"name":"大数据","slug":"大数据","permalink":"http://blog.crazylaw.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"TIDB","slug":"TIDB","permalink":"http://blog.crazylaw.cn/categories/TIDB/"},{"name":"知识点","slug":"知识点","permalink":"http://blog.crazylaw.cn/categories/%E7%9F%A5%E8%AF%86%E7%82%B9/"},{"name":"年终总结","slug":"年终总结","permalink":"http://blog.crazylaw.cn/categories/%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93/"},{"name":"kubernetes","slug":"kubernetes","permalink":"http://blog.crazylaw.cn/categories/kubernetes/"},{"name":"rust","slug":"rust","permalink":"http://blog.crazylaw.cn/categories/rust/"},{"name":"数据结构","slug":"数据结构","permalink":"http://blog.crazylaw.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"数据库","slug":"数据结构/数据库","permalink":"http://blog.crazylaw.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"爬虫","slug":"爬虫","permalink":"http://blog.crazylaw.cn/categories/%E7%88%AC%E8%99%AB/"},{"name":"Docker","slug":"Docker","permalink":"http://blog.crazylaw.cn/categories/Docker/"},{"name":"Redis","slug":"Redis","permalink":"http://blog.crazylaw.cn/categories/Redis/"},{"name":"消息队列","slug":"消息队列","permalink":"http://blog.crazylaw.cn/categories/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"},{"name":"Linux","slug":"Linux","permalink":"http://blog.crazylaw.cn/categories/Linux/"},{"name":"Golang","slug":"大数据/Golang","permalink":"http://blog.crazylaw.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Golang/"},{"name":"网络协议","slug":"网络协议","permalink":"http://blog.crazylaw.cn/categories/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE/"},{"name":"C语言","slug":"C语言","permalink":"http://blog.crazylaw.cn/categories/C%E8%AF%AD%E8%A8%80/"},{"name":"LeeCode","slug":"LeeCode","permalink":"http://blog.crazylaw.cn/categories/LeeCode/"},{"name":"算法","slug":"LeeCode/算法","permalink":"http://blog.crazylaw.cn/categories/LeeCode/%E7%AE%97%E6%B3%95/"},{"name":"API网关","slug":"API网关","permalink":"http://blog.crazylaw.cn/categories/API%E7%BD%91%E5%85%B3/"},{"name":"服务注册和发现","slug":"服务注册和发现","permalink":"http://blog.crazylaw.cn/categories/%E6%9C%8D%E5%8A%A1%E6%B3%A8%E5%86%8C%E5%92%8C%E5%8F%91%E7%8E%B0/"},{"name":"nginx","slug":"nginx","permalink":"http://blog.crazylaw.cn/categories/nginx/"},{"name":"Docker","slug":"kubernetes/Docker","permalink":"http://blog.crazylaw.cn/categories/kubernetes/Docker/"},{"name":"PHP","slug":"PHP","permalink":"http://blog.crazylaw.cn/categories/PHP/"},{"name":"k8s","slug":"k8s","permalink":"http://blog.crazylaw.cn/categories/k8s/"},{"name":"Rust","slug":"Rust","permalink":"http://blog.crazylaw.cn/categories/Rust/"},{"name":"Python","slug":"Docker/Python","permalink":"http://blog.crazylaw.cn/categories/Docker/Python/"},{"name":"测试","slug":"Docker/Python/测试","permalink":"http://blog.crazylaw.cn/categories/Docker/Python/%E6%B5%8B%E8%AF%95/"},{"name":"Nginx","slug":"Nginx","permalink":"http://blog.crazylaw.cn/categories/Nginx/"},{"name":"算法","slug":"算法","permalink":"http://blog.crazylaw.cn/categories/%E7%AE%97%E6%B3%95/"},{"name":"正则","slug":"正则","permalink":"http://blog.crazylaw.cn/categories/%E6%AD%A3%E5%88%99/"},{"name":"Git","slug":"Git","permalink":"http://blog.crazylaw.cn/categories/Git/"},{"name":"Nginx","slug":"PHP/Nginx","permalink":"http://blog.crazylaw.cn/categories/PHP/Nginx/"},{"name":"网络","slug":"网络","permalink":"http://blog.crazylaw.cn/categories/%E7%BD%91%E7%BB%9C/"},{"name":"协议","slug":"网络/协议","permalink":"http://blog.crazylaw.cn/categories/%E7%BD%91%E7%BB%9C/%E5%8D%8F%E8%AE%AE/"},{"name":"MongoDb","slug":"MongoDb","permalink":"http://blog.crazylaw.cn/categories/MongoDb/"},{"name":"PHP","slug":"Linux/PHP","permalink":"http://blog.crazylaw.cn/categories/Linux/PHP/"},{"name":"协议","slug":"协议","permalink":"http://blog.crazylaw.cn/categories/%E5%8D%8F%E8%AE%AE/"},{"name":"网络","slug":"协议/网络","permalink":"http://blog.crazylaw.cn/categories/%E5%8D%8F%E8%AE%AE/%E7%BD%91%E7%BB%9C/"},{"name":"Linux","slug":"协议/网络/Linux","permalink":"http://blog.crazylaw.cn/categories/%E5%8D%8F%E8%AE%AE/%E7%BD%91%E7%BB%9C/Linux/"},{"name":"MQ","slug":"MQ","permalink":"http://blog.crazylaw.cn/categories/MQ/"},{"name":"进制","slug":"进制","permalink":"http://blog.crazylaw.cn/categories/%E8%BF%9B%E5%88%B6/"},{"name":"设计模式","slug":"设计模式","permalink":"http://blog.crazylaw.cn/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"算法","slug":"协议/网络/算法","permalink":"http://blog.crazylaw.cn/categories/%E5%8D%8F%E8%AE%AE/%E7%BD%91%E7%BB%9C/%E7%AE%97%E6%B3%95/"},{"name":"C","slug":"C","permalink":"http://blog.crazylaw.cn/categories/C/"},{"name":"测试","slug":"测试","permalink":"http://blog.crazylaw.cn/categories/%E6%B5%8B%E8%AF%95/"},{"name":"博客面世","slug":"博客面世","permalink":"http://blog.crazylaw.cn/categories/%E5%8D%9A%E5%AE%A2%E9%9D%A2%E4%B8%96/"}],"tags":[{"name":"mock","slug":"mock","permalink":"http://blog.crazylaw.cn/tags/mock/"},{"name":"机器学习","slug":"机器学习","permalink":"http://blog.crazylaw.cn/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"随机森林","slug":"随机森林","permalink":"http://blog.crazylaw.cn/tags/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/"},{"name":"iptables","slug":"iptables","permalink":"http://blog.crazylaw.cn/tags/iptables/"},{"name":"智能家居","slug":"智能家居","permalink":"http://blog.crazylaw.cn/tags/%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85/"},{"name":"Home Assistant","slug":"Home-Assistant","permalink":"http://blog.crazylaw.cn/tags/Home-Assistant/"},{"name":"HA","slug":"HA","permalink":"http://blog.crazylaw.cn/tags/HA/"},{"name":"cache","slug":"cache","permalink":"http://blog.crazylaw.cn/tags/cache/"},{"name":"aac","slug":"aac","permalink":"http://blog.crazylaw.cn/tags/aac/"},{"name":"广告行业","slug":"广告行业","permalink":"http://blog.crazylaw.cn/tags/%E5%B9%BF%E5%91%8A%E8%A1%8C%E4%B8%9A/"},{"name":"Mac","slug":"Mac","permalink":"http://blog.crazylaw.cn/tags/Mac/"},{"name":"DevOps","slug":"DevOps","permalink":"http://blog.crazylaw.cn/tags/DevOps/"},{"name":"Golang","slug":"Golang","permalink":"http://blog.crazylaw.cn/tags/Golang/"},{"name":"Go源码剖析","slug":"Go源码剖析","permalink":"http://blog.crazylaw.cn/tags/Go%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90/"},{"name":"flink","slug":"flink","permalink":"http://blog.crazylaw.cn/tags/flink/"},{"name":"TIDB","slug":"TIDB","permalink":"http://blog.crazylaw.cn/tags/TIDB/"},{"name":"2022杂乱知识点总结","slug":"2022杂乱知识点总结","permalink":"http://blog.crazylaw.cn/tags/2022%E6%9D%82%E4%B9%B1%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93/"},{"name":"年终总结","slug":"年终总结","permalink":"http://blog.crazylaw.cn/tags/%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93/"},{"name":"kubernetes","slug":"kubernetes","permalink":"http://blog.crazylaw.cn/tags/kubernetes/"},{"name":"rust","slug":"rust","permalink":"http://blog.crazylaw.cn/tags/rust/"},{"name":"Kafka","slug":"Kafka","permalink":"http://blog.crazylaw.cn/tags/Kafka/"},{"name":"TiDB","slug":"TiDB","permalink":"http://blog.crazylaw.cn/tags/TiDB/"},{"name":"RocketDB","slug":"RocketDB","permalink":"http://blog.crazylaw.cn/tags/RocketDB/"},{"name":"SQL","slug":"SQL","permalink":"http://blog.crazylaw.cn/tags/SQL/"},{"name":"大数据","slug":"大数据","permalink":"http://blog.crazylaw.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"flume","slug":"flume","permalink":"http://blog.crazylaw.cn/tags/flume/"},{"name":"kudu","slug":"kudu","permalink":"http://blog.crazylaw.cn/tags/kudu/"},{"name":"数据库开发知识","slug":"数据库开发知识","permalink":"http://blog.crazylaw.cn/tags/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%BC%80%E5%8F%91%E7%9F%A5%E8%AF%86/"},{"name":"Jenkins","slug":"Jenkins","permalink":"http://blog.crazylaw.cn/tags/Jenkins/"},{"name":"爬虫","slug":"爬虫","permalink":"http://blog.crazylaw.cn/tags/%E7%88%AC%E8%99%AB/"},{"name":"Docker","slug":"Docker","permalink":"http://blog.crazylaw.cn/tags/Docker/"},{"name":"大数据，Hadoop","slug":"大数据，Hadoop","permalink":"http://blog.crazylaw.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%EF%BC%8CHadoop/"},{"name":"Redis","slug":"Redis","permalink":"http://blog.crazylaw.cn/tags/Redis/"},{"name":"Impala","slug":"Impala","permalink":"http://blog.crazylaw.cn/tags/Impala/"},{"name":"消息队列，Rabbitmq","slug":"消息队列，Rabbitmq","permalink":"http://blog.crazylaw.cn/tags/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%EF%BC%8CRabbitmq/"},{"name":"消息队列，Kafka","slug":"消息队列，Kafka","permalink":"http://blog.crazylaw.cn/tags/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%EF%BC%8CKafka/"},{"name":"Linux","slug":"Linux","permalink":"http://blog.crazylaw.cn/tags/Linux/"},{"name":"Swoole","slug":"Swoole","permalink":"http://blog.crazylaw.cn/tags/Swoole/"},{"name":"大数据，流式计算","slug":"大数据，流式计算","permalink":"http://blog.crazylaw.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%EF%BC%8C%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"},{"name":"TCP","slug":"TCP","permalink":"http://blog.crazylaw.cn/tags/TCP/"},{"name":"Redis，源码剖析","slug":"Redis，源码剖析","permalink":"http://blog.crazylaw.cn/tags/Redis%EF%BC%8C%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90/"},{"name":"数据结构","slug":"数据结构","permalink":"http://blog.crazylaw.cn/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"C语言","slug":"C语言","permalink":"http://blog.crazylaw.cn/tags/C%E8%AF%AD%E8%A8%80/"},{"name":"算法","slug":"算法","permalink":"http://blog.crazylaw.cn/tags/%E7%AE%97%E6%B3%95/"},{"name":"LeeCode","slug":"LeeCode","permalink":"http://blog.crazylaw.cn/tags/LeeCode/"},{"name":"API网关","slug":"API网关","permalink":"http://blog.crazylaw.cn/tags/API%E7%BD%91%E5%85%B3/"},{"name":"服务注册和发现","slug":"服务注册和发现","permalink":"http://blog.crazylaw.cn/tags/%E6%9C%8D%E5%8A%A1%E6%B3%A8%E5%86%8C%E5%92%8C%E5%8F%91%E7%8E%B0/"},{"name":"nginx","slug":"nginx","permalink":"http://blog.crazylaw.cn/tags/nginx/"},{"name":"PHP","slug":"PHP","permalink":"http://blog.crazylaw.cn/tags/PHP/"},{"name":"k8s","slug":"k8s","permalink":"http://blog.crazylaw.cn/tags/k8s/"},{"name":"Rust","slug":"Rust","permalink":"http://blog.crazylaw.cn/tags/Rust/"},{"name":"Shell","slug":"Shell","permalink":"http://blog.crazylaw.cn/tags/Shell/"},{"name":"Python","slug":"Python","permalink":"http://blog.crazylaw.cn/tags/Python/"},{"name":"测试","slug":"测试","permalink":"http://blog.crazylaw.cn/tags/%E6%B5%8B%E8%AF%95/"},{"name":"jsonschema","slug":"jsonschema","permalink":"http://blog.crazylaw.cn/tags/jsonschema/"},{"name":"Laravel","slug":"Laravel","permalink":"http://blog.crazylaw.cn/tags/Laravel/"},{"name":"Nginx","slug":"Nginx","permalink":"http://blog.crazylaw.cn/tags/Nginx/"},{"name":"Openresty","slug":"Openresty","permalink":"http://blog.crazylaw.cn/tags/Openresty/"},{"name":"进制","slug":"进制","permalink":"http://blog.crazylaw.cn/tags/%E8%BF%9B%E5%88%B6/"},{"name":"ssl","slug":"ssl","permalink":"http://blog.crazylaw.cn/tags/ssl/"},{"name":"正则","slug":"正则","permalink":"http://blog.crazylaw.cn/tags/%E6%AD%A3%E5%88%99/"},{"name":"Git","slug":"Git","permalink":"http://blog.crazylaw.cn/tags/Git/"},{"name":"协议","slug":"协议","permalink":"http://blog.crazylaw.cn/tags/%E5%8D%8F%E8%AE%AE/"},{"name":"网络","slug":"网络","permalink":"http://blog.crazylaw.cn/tags/%E7%BD%91%E7%BB%9C/"},{"name":"Crontab","slug":"Crontab","permalink":"http://blog.crazylaw.cn/tags/Crontab/"},{"name":"MongoDb","slug":"MongoDb","permalink":"http://blog.crazylaw.cn/tags/MongoDb/"},{"name":"网络，异步","slug":"网络，异步","permalink":"http://blog.crazylaw.cn/tags/%E7%BD%91%E7%BB%9C%EF%BC%8C%E5%BC%82%E6%AD%A5/"},{"name":"全文搜索","slug":"全文搜索","permalink":"http://blog.crazylaw.cn/tags/%E5%85%A8%E6%96%87%E6%90%9C%E7%B4%A2/"},{"name":"树","slug":"树","permalink":"http://blog.crazylaw.cn/tags/%E6%A0%91/"},{"name":"TcpDump","slug":"TcpDump","permalink":"http://blog.crazylaw.cn/tags/TcpDump/"},{"name":"编译安装","slug":"编译安装","permalink":"http://blog.crazylaw.cn/tags/%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85/"},{"name":"Centos7","slug":"Centos7","permalink":"http://blog.crazylaw.cn/tags/Centos7/"},{"name":"Systemctl","slug":"Systemctl","permalink":"http://blog.crazylaw.cn/tags/Systemctl/"},{"name":"ascii","slug":"ascii","permalink":"http://blog.crazylaw.cn/tags/ascii/"},{"name":"Promise","slug":"Promise","permalink":"http://blog.crazylaw.cn/tags/Promise/"},{"name":"ES6","slug":"ES6","permalink":"http://blog.crazylaw.cn/tags/ES6/"},{"name":"C","slug":"C","permalink":"http://blog.crazylaw.cn/tags/C/"},{"name":"ID发号器","slug":"ID发号器","permalink":"http://blog.crazylaw.cn/tags/ID%E5%8F%91%E5%8F%B7%E5%99%A8/"},{"name":"MQ","slug":"MQ","permalink":"http://blog.crazylaw.cn/tags/MQ/"},{"name":"设计模式，PHP","slug":"设计模式，PHP","permalink":"http://blog.crazylaw.cn/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%EF%BC%8CPHP/"},{"name":"设计模式","slug":"设计模式","permalink":"http://blog.crazylaw.cn/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"动态规划","slug":"动态规划","permalink":"http://blog.crazylaw.cn/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"},{"name":"Hash函数","slug":"Hash函数","permalink":"http://blog.crazylaw.cn/tags/Hash%E5%87%BD%E6%95%B0/"},{"name":"语言包","slug":"语言包","permalink":"http://blog.crazylaw.cn/tags/%E8%AF%AD%E8%A8%80%E5%8C%85/"},{"name":"Ascii","slug":"Ascii","permalink":"http://blog.crazylaw.cn/tags/Ascii/"},{"name":"IP","slug":"IP","permalink":"http://blog.crazylaw.cn/tags/IP/"},{"name":"BST树","slug":"BST树","permalink":"http://blog.crazylaw.cn/tags/BST%E6%A0%91/"},{"name":"循环队列","slug":"循环队列","permalink":"http://blog.crazylaw.cn/tags/%E5%BE%AA%E7%8E%AF%E9%98%9F%E5%88%97/"},{"name":"PHP扩展","slug":"PHP扩展","permalink":"http://blog.crazylaw.cn/tags/PHP%E6%89%A9%E5%B1%95/"},{"name":"单链表","slug":"单链表","permalink":"http://blog.crazylaw.cn/tags/%E5%8D%95%E9%93%BE%E8%A1%A8/"},{"name":"线性存储结构","slug":"线性存储结构","permalink":"http://blog.crazylaw.cn/tags/%E7%BA%BF%E6%80%A7%E5%AD%98%E5%82%A8%E7%BB%93%E6%9E%84/"},{"name":"学习记录","slug":"学习记录","permalink":"http://blog.crazylaw.cn/tags/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"},{"name":"JWT","slug":"JWT","permalink":"http://blog.crazylaw.cn/tags/JWT/"},{"name":"测试工具","slug":"测试工具","permalink":"http://blog.crazylaw.cn/tags/%E6%B5%8B%E8%AF%95%E5%B7%A5%E5%85%B7/"},{"name":"压力测试","slug":"压力测试","permalink":"http://blog.crazylaw.cn/tags/%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95/"},{"name":"性能测试","slug":"性能测试","permalink":"http://blog.crazylaw.cn/tags/%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/"},{"name":"博客面世","slug":"博客面世","permalink":"http://blog.crazylaw.cn/tags/%E5%8D%9A%E5%AE%A2%E9%9D%A2%E4%B8%96/"}]}
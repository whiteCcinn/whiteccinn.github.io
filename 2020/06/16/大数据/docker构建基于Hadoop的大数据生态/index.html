<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>【大数据】- docker构建基于Hadoop的大数据生态 | 白菜君の技术库</title>

  
  <meta name="author" content="白菜(whiteCcinn)">
  

  
  <meta name="description" content="知道做不到，等于不知道">
  

  
  <meta name="keywords" content="白菜,文辉,技术博客,whiteCcinn">
  

  <meta id="viewport" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">

  <meta property="og:title" content="【大数据】- docker构建基于Hadoop的大数据生态"/>

  <meta property="og:site_name" content="白菜君の技术库"/>

  
  <meta property="og:image" content="/favicon.ico"/>
  

  <link href="/favicon.ico" rel="icon">
  <link rel="alternate" href="/atom.xml" title="白菜君の技术库" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
<meta name="generator" content="Hexo 4.2.1"></head>


<body>
<div class="blog">
  <div class="content">

    <header>
  <div class="site-branding">
    <h1 class="site-title">
      <a href="/">白菜君の技术库</a>
    </h1>
    <p class="site-description"></p>
  </div>
  <nav class="site-navigation">
    <ul>
      
        <li><a href="/">主页</a></li>
      
        <li><a href="/archives/">文章</a></li>
      
        <li><a href="/tags/">标签</a></li>
      
        <li><a href="/categories/">分类</a></li>
      
        <li><a href="/about/">关于我</a></li>
      
    </ul>
  </nav>
</header>

    <main class="site-main posts-loop">
    <article>

  
    
    <h3 class="article-title"><span>【大数据】- docker构建基于Hadoop的大数据生态</span></h3>
    
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2020/06/16/大数据/docker构建基于Hadoop的大数据生态/" rel="bookmark">
        <time class="entry-date published" datetime="2020-06-16T10:07:40.000Z">
          2020-06-16
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
        <h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>众所周知，hadoop的大数据生态的组件版本对依赖十分的繁杂，对此我们在如果需要用apache开源库来一点点堆起积木，有点繁琐<br>我们这个项目的目的只是为了更快更方便的构建大数据生态环境。<br>所以我们这里选择采用cdh的方式来构建大数据生态，但是由于我们希望在终端就部署好服务，而不需要客户端，所以这里就不选择用cmf</p>
<a id="more"></a>

<ul>
<li><a href="https://github.com/base-big-data" target="_blank" rel="noopener">基于docker的cdh6的大数据生态</a></li>
</ul>
<blockquote>
<p>在这里，你可以找到你需要的东西</p>
</blockquote>
<ul>
<li>Hadoop</li>
<li>Impala</li>
<li>Hive</li>
<li>Kudu</li>
</ul>
<p>接下来，我会记录一下我在这个过程中所遇到的一些问题。</p>
<h2 id="选择底层镜像"><a href="#选择底层镜像" class="headerlink" title="选择底层镜像"></a>选择底层镜像</h2><ul>
<li><a href="https://github.com/base-big-data/docker-centos-openjdk" target="_blank" rel="noopener">底层操作系统镜像</a></li>
</ul>
<figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> centos:<span class="number">7</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">LABEL</span><span class="bash"> maintainer=<span class="string">"Caiwenhui &lt;471113744@qq.com&gt;"</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">ENV</span> JAVA_HOME /usr/lib/jvm/java-openjdk</span><br><span class="line"></span><br><span class="line"><span class="comment"># 换阿里云源</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> yum install -y wget;\</span></span><br><span class="line"><span class="bash">  wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo;\</span></span><br><span class="line"><span class="bash">  <span class="comment"># 移除阿里已经不用的域名</span></span></span><br><span class="line">  sed -i <span class="string">'/mirrors.aliyuncs.com/d'</span> /etc/yum.repos.d/CentOS-Base.repo;\</span><br><span class="line">  sed -i <span class="string">'/mirrors.cloud.aliyuncs.com/d'</span> /etc/yum.repos.d/CentOS-Base.repo;\</span><br><span class="line">  yum clean all; \</span><br><span class="line">  yum makecache</span><br><span class="line"></span><br><span class="line"><span class="comment"># # 安装基础必备软件</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> yum update -y; \</span></span><br><span class="line"><span class="bash">  yum intall -y deltarpm;\</span></span><br><span class="line"><span class="bash">  yum install -y java-1.8.0-openjdk-devel unzip curl vim python-setuptools sudo; \</span></span><br><span class="line"><span class="bash">  yum clean all;\</span></span><br><span class="line"><span class="bash">  yum makecache</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> [<span class="string">"/bin/bash"</span>]</span></span><br></pre></td></tr></table></figure>

<p>在这里，我们基于的是centos7做操作系统镜像，并且在基础上，替换成了阿里云的yum的数据源，并且安装好了jdk8，方便后续部署服务</p>
<h2 id="选择基础镜像"><a href="#选择基础镜像" class="headerlink" title="选择基础镜像"></a>选择基础镜像</h2><ul>
<li><a href="https://github.com/base-big-data/docker-cdh6" target="_blank" rel="noopener">基于cdh6的基础镜像</a></li>
</ul>
<figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> ccinn/centos-openjdk:latest</span><br><span class="line"></span><br><span class="line"><span class="keyword">LABEL</span><span class="bash"> maintainer=<span class="string">"Caiwenhui &lt;471113744@qq.com&gt;"</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">USER</span> root</span><br><span class="line"></span><br><span class="line"><span class="keyword">ENV</span> CDH_VERSION <span class="number">6.3</span>.<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">ADD</span><span class="bash"> cloudera-cdh6.repo /etc/yum.repos.d/</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> rpm --import https://archive.cloudera.com/cdh6/<span class="variable">$CDH_VERSION</span>/redhat7/yum/RPM-GPG-KEY-cloudera;\</span></span><br><span class="line"><span class="bash">  yum makecache</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">WORKDIR</span><span class="bash"> /</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> [<span class="string">"/bin/bash"</span>]</span></span><br></pre></td></tr></table></figure>

<p>在这里，我们的目录很简单，就是把<code>cloudera-cdh6.repo</code>放在yum的源中，方便我们通过yum安装服务，减少不必要的依赖问题</p>
<blockquote>
<p>注意，我们这里选择的hadoop3的版本</p>
</blockquote>
<h2 id="构建hadoop镜像"><a href="#构建hadoop镜像" class="headerlink" title="构建hadoop镜像"></a>构建hadoop镜像</h2><ul>
<li><a href="https://github.com/base-big-data/docker-cdh6-hadoop" target="_blank" rel="noopener">基于基础镜像的hadoop服务</a></li>
</ul>
<figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> ccinn/cdh6:latest</span><br><span class="line"></span><br><span class="line"><span class="keyword">LABEL</span><span class="bash"> maintainer=<span class="string">"Caiwenhui &lt;471113744@qq.com&gt;"</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">USER</span> root</span><br><span class="line"></span><br><span class="line"><span class="keyword">ADD</span><span class="bash"> support.sh /support.sh</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> <span class="built_in">source</span> /support.sh;\</span></span><br><span class="line"><span class="bash">  loop_exec <span class="string">'yum install -y hadoop'</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">WORKDIR</span><span class="bash"> /</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> [<span class="string">"/bin/bash"</span>]</span></span><br></pre></td></tr></table></figure>

<h3 id="问题1"><a href="#问题1" class="headerlink" title="问题1"></a>问题1</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Install  1 Package (+52 Dependent packages)</span><br><span class="line"></span><br><span class="line">Total download size: 712 M</span><br><span class="line">Installed size: 856 M</span><br><span class="line">Downloading packages:</span><br><span class="line">https:&#x2F;&#x2F;archive.cloudera.com&#x2F;cdh6&#x2F;6.3.2&#x2F;redhat7&#x2F;yum&#x2F;RPMS&#x2F;x86_64&#x2F;hadoop-hdfs-3.0.0%2Bcdh6.3.2-1605554.el7.x86_64.rpm: [Errno 14] curl#18 - &quot;transfer closed with 23278968 bytes remaining to read&quot;</span><br><span class="line">Trying other mirror.</span><br><span class="line">https:&#x2F;&#x2F;archive.cloudera.com&#x2F;cdh6&#x2F;6.3.2&#x2F;redhat7&#x2F;yum&#x2F;RPMS&#x2F;x86_64&#x2F;hadoop-3.0.0%2Bcdh6.3.2-1605554.el7.x86_64.rpm: [Errno 14] curl#18 - &quot;transfer closed with 40971220 bytes remaining to read&quot;</span><br><span class="line">Trying other mirror.</span><br><span class="line">https:&#x2F;&#x2F;archive.cloudera.com&#x2F;cdh6&#x2F;6.3.2&#x2F;redhat7&#x2F;yum&#x2F;RPMS&#x2F;noarch&#x2F;hive-2.1.1%2Bcdh6.3.2-1605554.el7.noarch.rpm: [Errno 14] curl#18 - &quot;transfer closed with 123435828 bytes remaining to read&quot;</span><br><span class="line">Trying other mirror.</span><br></pre></td></tr></table></figure>

<p>我这里引入了一个<code>loop_exec</code>的函数，原因是因为国内的网络过慢，以及软件包“过大”，导致yum在安装包的时候，会导致传输中断，使得我们无法“一步”安装到位</p>
<p>所以这里我写了一个循环调用的函数，因为yum安装过程中是可以断点续传的，所以我利用这个函数来执行多几次这个命令即可，直到安装成功为止</p>
<blockquote>
<p>一般我是3次安装完毕</p>
</blockquote>
<h3 id="问题2"><a href="#问题2" class="headerlink" title="问题2"></a>问题2</h3><p>我发现原来<code>hadoop</code>包是基础包而已，我们还要安装<code>hadoop-namenode</code>,<code>hadoop-datanode</code>，因此。我又手动在容器中调试了起来。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/bin/hdfs --config /etc/hadoop/conf  namenode</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">[root@25b603a089cc &#x2F;]# &#x2F;usr&#x2F;bin&#x2F;hdfs --config &#x2F;etc&#x2F;hadoop&#x2F;conf  namenode</span><br><span class="line">2020-06-16 10:18:34,126 INFO namenode.NameNode: STARTUP_MSG:</span><br><span class="line">&#x2F;************************************************************</span><br><span class="line">STARTUP_MSG: Starting NameNode</span><br><span class="line">STARTUP_MSG:   host &#x3D; 25b603a089cc&#x2F;172.17.0.3</span><br><span class="line">STARTUP_MSG:   args &#x3D; []</span><br><span class="line">STARTUP_MSG:   version &#x3D; 3.0.0-cdh6.3.2</span><br><span class="line">STARTUP_MSG:   classpath &#x3D; &#x2F;etc&#x2F;hadoop&#x2F;conf:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-lang3-3.7.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-beanutils-1.9.4.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;log4j-1.2.17.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-logging-1.1.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;re2j-1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jackson-core-asl-1.9.13.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jettison-1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;guava-11.0.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;javax.activation-api-1.2.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;paranamer-2.8.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jetty-util-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;slf4j-log4j12.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jsr311-api-1.1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;azure-data-lake-store-sdk-2.2.9.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jersey-servlet-1.19.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jaxb-api-2.2.11.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;metrics-core-3.0.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;nimbus-jose-jwt-4.41.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;javax.servlet-api-3.1.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;protobuf-java-2.5.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-math3-3.1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jersey-core-1.19.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-compress-1.18.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;accessors-smart-1.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;woodstox-core-5.0.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jetty-io-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jackson-xc-1.9.13.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jetty-xml-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;curator-recipes-2.12.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jetty-servlet-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-lang-2.6.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;audience-annotations-0.5.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerby-config-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jetty-security-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jetty-server-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerb-util-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;netty-3.10.6.Final.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerb-admin-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jersey-json-1.19.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jackson-databind-2.9.9.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerb-core-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jackson-jaxrs-1.9.13.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerby-asn1-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-configuration2-2.1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerb-crypto-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;httpcore-4.4.6.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;log4j-core-2.8.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;json-smart-2.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;snappy-java-1.1.4.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jackson-mapper-asl-1.9.13-cloudera.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerb-simplekdc-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;logredactor-2.0.7.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;avro.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jackson-annotations-2.9.9.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerb-client-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jersey-server-1.19.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;curator-client-2.12.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-cli-1.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jcip-annotations-1.0-1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerb-server-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerby-xdr-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerb-common-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;htrace-core4-4.1.0-incubating.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;aws-java-sdk-bundle-1.11.271.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerby-util-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jetty-webapp-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;stax2-api-3.1.4.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;slf4j-api-1.7.25.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;log4j-api-2.8.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-io-2.6.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerby-pkix-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;curator-framework-2.12.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;gson-2.2.4.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jsp-api-2.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;xz-1.6.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;zookeeper.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jetty-http-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;wildfly-openssl-1.0.4.Final.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jackson-core-2.9.9.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerb-identity-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jaxb-impl-2.2.3-1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-net-3.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jul-to-slf4j-1.7.25.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;asm-5.0.4.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jsch-0.1.54.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jsr305-3.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-collections-3.2.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;httpclient-4.5.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-codec-1.11.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-format-javadoc.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-auth-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-jackson.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-azure-datalake.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-format.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-aws-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-azure-datalake-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-protobuf.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-annotations-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-auth.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-aws.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-azure.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-scala_2.11.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-nfs.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-annotations.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-pig.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-pig-bundle.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-common.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-hadoop-bundle.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-kms.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-kms-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-cascading.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-common-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-generator.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-azure-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-hadoop.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-common.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-common-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-avro.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-common-3.0.0-cdh6.3.2-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-format-sources.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-column.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-thrift.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-encoding.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-nfs-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-cascading3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-lang3-3.7.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-beanutils-1.9.4.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;log4j-1.2.17.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-logging-1.1.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;re2j-1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jackson-core-asl-1.9.13.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jettison-1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;guava-11.0.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;javax.activation-api-1.2.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;paranamer-2.8.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jetty-util-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jsr311-api-1.1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jersey-servlet-1.19.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jaxb-api-2.2.11.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;nimbus-jose-jwt-4.41.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;javax.servlet-api-3.1.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;protobuf-java-2.5.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;okio-1.6.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-math3-3.1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jersey-core-1.19.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-compress-1.18.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;accessors-smart-1.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;woodstox-core-5.0.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jetty-io-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jackson-xc-1.9.13.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jetty-xml-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;curator-recipes-2.12.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jetty-servlet-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-lang-2.6.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;json-simple-1.1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;avro-1.8.2-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;audience-annotations-0.5.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerby-config-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jetty-security-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jetty-server-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerb-util-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;zookeeper-3.4.5-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;netty-3.10.6.Final.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerb-admin-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jersey-json-1.19.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jackson-databind-2.9.9.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerb-core-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jackson-jaxrs-1.9.13.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerby-asn1-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-configuration2-2.1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerb-crypto-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;httpcore-4.4.6.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;json-smart-2.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;snappy-java-1.1.4.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jackson-mapper-asl-1.9.13-cloudera.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerb-simplekdc-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jackson-annotations-2.9.9.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerb-client-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jersey-server-1.19.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;curator-client-2.12.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-cli-1.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jcip-annotations-1.0-1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerb-server-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerby-xdr-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerb-common-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;htrace-core4-4.1.0-incubating.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerby-util-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jetty-webapp-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;stax2-api-3.1.4.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-io-2.6.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerby-pkix-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;curator-framework-2.12.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-daemon-1.0.13.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;gson-2.2.4.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;leveldbjni-all-1.8.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;xz-1.6.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jetty-http-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;okhttp-2.7.5.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jetty-util-ajax-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jackson-core-2.9.9.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerb-identity-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jaxb-impl-2.2.3-1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-net-3.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;asm-5.0.4.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jsch-0.1.54.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jsr305-3.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-collections-3.2.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;httpclient-4.5.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-codec-1.11.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-native-client.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-client-3.0.0-cdh6.3.2-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-nfs-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-native-client-3.0.0-cdh6.3.2-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-nfs.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-httpfs-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-client-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-3.0.0-cdh6.3.2-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-native-client-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-client-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-httpfs.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-native-client-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-client.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-examples-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-openstack.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-aliyun.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-core.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-extras-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-uploader.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-openstack-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-distcp.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-hs-plugins-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-sls.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-uploader-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-kafka.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-datajoin-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;netty-buffer-4.1.17.Final.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;netty-common-4.1.17.Final.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-jobclient-3.0.0-cdh6.3.2-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-gridmix-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;jdom-1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-jobclient-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-examples.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-kafka-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;netty-codec-http-4.1.17.Final.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-rumen-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-app.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-extras.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-streaming-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;lz4-java-1.5.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;aliyun-sdk-oss-2.8.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-app-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-datajoin.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;azure-storage-5.4.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-hs.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-nativetask-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-jobclient.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-distcp-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-hs-plugins.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-shuffle-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-aliyun-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-common.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;kafka-clients-2.2.1-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;ojalgo-43.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-shuffle.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-gridmix.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-hs-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-jobclient-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;netty-resolver-4.1.17.Final.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-resourceestimator-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-rumen.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;azure-keyvault-core-0.8.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;netty-codec-4.1.17.Final.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-nativetask.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;netty-transport-4.1.17.Final.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-sls-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-resourceestimator.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-archives.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-archives-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-common-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;netty-handler-4.1.17.Final.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-core-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-archive-logs.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;zstd-jni-1.3.8-1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-streaming.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-archive-logs-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;jackson-jaxrs-json-provider-2.9.9.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;guice-servlet-4.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;jackson-module-jaxb-annotations-2.9.9.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;jersey-client-1.19.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;objenesis-1.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;guice-4.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;jackson-jaxrs-base-2.9.9.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;metrics-core-3.0.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;java-util-1.9.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;fst-2.50.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;javax.inject-1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;geronimo-jcache_1.0_spec-1.0-alpha-1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;json-io-2.5.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;HikariCP-java7-2.4.12.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;bcprov-jdk15on-1.60.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;ehcache-3.3.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;bcpkix-jdk15on-1.60.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;aopalliance-1.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;jersey-guice-1.19.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;mssql-jdbc-6.2.1.jre7.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-applicationhistoryservice-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-registry-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-tests-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-common.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-sharedcachemanager-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-client-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-applicationhistoryservice.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-applications-unmanaged-am-launcher-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-resourcemanager.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-common.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-resourcemanager-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-timeline-pluginstorage.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-common-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-applications-distributedshell.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-router-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-client.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-common-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-web-proxy-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-sharedcachemanager.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-web-proxy.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-router.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-api-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-nodemanager-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-registry.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-applications-distributedshell-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-timeline-pluginstorage-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-api.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-applications-unmanaged-am-launcher.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-nodemanager.jar</span><br><span class="line">STARTUP_MSG:   build &#x3D; http:&#x2F;&#x2F;github.com&#x2F;cloudera&#x2F;hadoop -r 9aff20de3b5ecccf3c19d57f71b214fb4d37ee89; compiled by &#39;jenkins&#39; on 2019-11-08T13:49Z</span><br><span class="line">STARTUP_MSG:   java &#x3D; 1.8.0_252</span><br><span class="line">************************************************************&#x2F;</span><br><span class="line">2020-06-16 10:18:34,143 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]</span><br><span class="line">2020-06-16 10:18:34,296 INFO namenode.NameNode: createNameNode []</span><br><span class="line">2020-06-16 10:18:34,497 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties</span><br><span class="line">2020-06-16 10:18:34,709 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).</span><br><span class="line">2020-06-16 10:18:34,709 INFO impl.MetricsSystemImpl: NameNode metrics system started</span><br><span class="line">2020-06-16 10:18:34,764 INFO namenode.NameNode: fs.defaultFS is file:&#x2F;&#x2F;&#x2F;</span><br><span class="line">2020-06-16 10:18:34,972 ERROR namenode.NameNode: Failed to start namenode.</span><br><span class="line">java.lang.IllegalArgumentException: Invalid URI for NameNode address (check fs.defaultFS): file:&#x2F;&#x2F;&#x2F; has no authority.</span><br><span class="line">	at org.apache.hadoop.hdfs.DFSUtilClient.getNNAddress(DFSUtilClient.java:646)</span><br><span class="line">	at org.apache.hadoop.hdfs.DFSUtilClient.getNNAddressCheckLogical(DFSUtilClient.java:675)</span><br><span class="line">	at org.apache.hadoop.hdfs.DFSUtilClient.getNNAddress(DFSUtilClient.java:637)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.NameNode.getRpcServerAddress(NameNode.java:562)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.NameNode.loginAsNameNodeUser(NameNode.java:693)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:713)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.NameNode.&lt;init&gt;(NameNode.java:950)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.NameNode.&lt;init&gt;(NameNode.java:929)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1653)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1720)</span><br><span class="line">2020-06-16 10:18:34,983 INFO util.ExitUtil: Exiting with status 1: java.lang.IllegalArgumentException: Invalid URI for NameNode address (check fs.defaultFS): file:&#x2F;&#x2F;&#x2F; has no authority.</span><br><span class="line">2020-06-16 10:18:34,988 INFO namenode.NameNode: SHUTDOWN_MSG:</span><br><span class="line">&#x2F;************************************************************</span><br><span class="line">SHUTDOWN_MSG: Shutting down NameNode at 25b603a089cc&#x2F;172.17.0.3</span><br><span class="line">************************************************************&#x2F;</span><br></pre></td></tr></table></figure>

<p>我以前台的方式启动服务，发现服务并没有启动成功，他告诉我检查<code>fs.defaultFS</code>参数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">[root@25b603a089cc &#x2F;]# cat &#x2F;etc&#x2F;hadoop&#x2F;conf&#x2F;core-site.xml</span><br><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot;?&gt;</span><br><span class="line">&lt;!--</span><br><span class="line">  Licensed to the Apache Software Foundation (ASF) under one or more</span><br><span class="line">  contributor license agreements.  See the NOTICE file distributed with</span><br><span class="line">  this work for additional information regarding copyright ownership.</span><br><span class="line">  The ASF licenses this file to You under the Apache License, Version 2.0</span><br><span class="line">  (the &quot;License&quot;); you may not use this file except in compliance with</span><br><span class="line">  the License.  You may obtain a copy of the License at</span><br><span class="line"></span><br><span class="line">      http:&#x2F;&#x2F;www.apache.org&#x2F;licenses&#x2F;LICENSE-2.0</span><br><span class="line"></span><br><span class="line">  Unless required by applicable law or agreed to in writing, software</span><br><span class="line">  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span><br><span class="line">  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line">  See the License for the specific language governing permissions and</span><br><span class="line">  limitations under the License.</span><br><span class="line">--&gt;</span><br><span class="line">&lt;?xml-stylesheet type&#x3D;&quot;text&#x2F;xsl&quot; href&#x3D;&quot;configuration.xsl&quot;?&gt;</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure>

<p>于是我发现原来这个core-site文件默认是什么配置都没有的。于是我需要手动加上一些配置。</p>
<p>在这里顺带说明一下配置信息。</p>
<p>具体的参数配置请参见 &gt;&gt; <a href="https://www.stefaanlippens.net/hadoop-3-default-ports.html" target="_blank" rel="noopener">hadoop3-默认配置参数</a> &lt;&lt;&lt;</p>
<p>在hadoop集群中，需要配置的文件主要包括四个，分别是<code>core-site.xml</code>、<code>hdfs-site.xml</code>、<code>mapred-site.xml</code>和<code>yarn-site.xml</code>，这四个文件分别是对不同组件的配置参数，主要内容如下表所示：</p>
<table>
<thead>
<tr>
<th>配置文件名</th>
<th>配置对象</th>
<th>主要内容</th>
</tr>
</thead>
<tbody><tr>
<td>core-site.xml</td>
<td>集群全局参数</td>
<td>用于定义系统级别的参数，如HDFS  URL、Hadoop的临时目录等</td>
</tr>
<tr>
<td>hdfs-site.xml</td>
<td>HDFS参数</td>
<td>如名称节点和数据节点的存放位置、文件副本的个数、文件读取权限等</td>
</tr>
<tr>
<td>mapred-site.xml</td>
<td>Mapreduce参数</td>
<td>包括JobHistory Server和应用程序参数两部分，如reduce任务的默认个数、任务所能够使用内存的默认上下限等</td>
</tr>
<tr>
<td>yarn-site.xml</td>
<td>集群资源管理系统参数</td>
<td>配置 ResourceManager，NodeManager 的通信端口，web监控端口等</td>
</tr>
</tbody></table>
<p>搭建集群配置时重要参数：</p>
<p>core-site.xml</p>
<table>
<thead>
<tr>
<th>参数名</th>
<th>默认值</th>
<th>参数解释</th>
</tr>
</thead>
<tbody><tr>
<td>fs.defaultFS</td>
<td>file:///</td>
<td>文件系统主机和端口</td>
</tr>
<tr>
<td>io.file.buffer.size</td>
<td>4096</td>
<td>流文件的缓冲区大小</td>
</tr>
<tr>
<td>hadoop.tmp.dir</td>
<td>/tmp/hadoop-${user.name}</td>
<td>临时文件夹</td>
</tr>
</tbody></table>
<p>hdfs-site.xml</p>
<table>
<thead>
<tr>
<th>参数名</th>
<th>默认值</th>
<th>参数解释</th>
</tr>
</thead>
<tbody><tr>
<td>dfs.namenode.secondary.http-address</td>
<td>0.0.0.0:50090</td>
<td>定义HDFS对应的HTTP服务器地址和端口</td>
</tr>
<tr>
<td>dfs.namenode.name.dir</td>
<td>file://${hadoop.tmp.dir}/dfs/name</td>
<td>定义DFS的名称节点在本地文件系统的位置</td>
</tr>
<tr>
<td>dfs.datanode.data.dir</td>
<td>file://${hadoop.tmp.dir}/dfs/data</td>
<td>定义DFS数据节点存储数据块时存储在本地文件系统的位置</td>
</tr>
<tr>
<td>dfs.replication</td>
<td>3</td>
<td>缺省的块复制数量</td>
</tr>
<tr>
<td>dfs.webhdfs.enabled</td>
<td>true</td>
<td>是否通过http协议读取hdfs文件，如果选是，则集群安全性较差</td>
</tr>
</tbody></table>
<p>mapred-site.xml</p>
<table>
<thead>
<tr>
<th>参数名</th>
<th>默认值</th>
<th>参数解释</th>
</tr>
</thead>
<tbody><tr>
<td>mapreduce.framework.name</td>
<td>local</td>
<td>取值local、classic或yarn其中之一，如果不是yarn，则不会使用YARN集群来实现资源的分配</td>
</tr>
<tr>
<td>mapreduce.jobhistory.address</td>
<td>0.0.0.0:10020</td>
<td>定义历史服务器的地址和端口，通过历史服务器查看已经运行完的Mapreduce作业记录</td>
</tr>
<tr>
<td>mapreduce.jobhistory.webapp.address</td>
<td>0.0.0.0:19888</td>
<td>定义历史服务器web应用访问的地址和端口</td>
</tr>
</tbody></table>
<p>yarn-site.xml</p>
<table>
<thead>
<tr>
<th>参数名</th>
<th>默认值</th>
<th>参数解释</th>
</tr>
</thead>
<tbody><tr>
<td>yarn.resourcemanager.address</td>
<td>0.0.0.0:8032</td>
<td>ResourceManager 提供给客户端访问的地址。客户端通过该地址向RM提交应用程序，杀死应用程序等</td>
</tr>
<tr>
<td>yarn.resourcemanager.scheduler.address</td>
<td>0.0.0.0:8030</td>
<td>ResourceManager提供给ApplicationMaster的访问地址。ApplicationMaster通过该地址向RM申请资源、释放资源等</td>
</tr>
<tr>
<td>yarn.resourcemanager.resource-tracker.address</td>
<td>0.0.0.0:8031</td>
<td>ResourceManager 提供给NodeManager的地址。NodeManager通过该地址向RM汇报心跳，领取任务等</td>
</tr>
<tr>
<td>yarn.resourcemanager.webapp.address</td>
<td>0.0.0.0:8088</td>
<td>ResourceManager对web 服务提供地址。用户可通过该地址在浏览器中查看集群各类信息</td>
</tr>
<tr>
<td>yarn.nodemanager.aux-services</td>
<td></td>
<td>通过该配置项，用户可以自定义一些服务，例如Map-Reduce的shuffle功能就是采用这种方式实现的，这样就可以在NodeManager上扩展自己的服务。</td>
</tr>
</tbody></table>
<p>基于以上信息。</p>
<p>整理一份配置如下：</p>
<p>core-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://0.0.0.0:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.hue.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.hue.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>io.compression.codecs<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.io.compress.SnappyCodec<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>hdfs-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///var/lib/hadoop-hdfs/cache/hdfs/dfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- CLOUDERA-BUILD: CDH-64745. --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>cloudera.erasure_coding.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>接着，我们尝试启动服务。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br></pre></td><td class="code"><pre><span class="line">[root@25b603a089cc &#x2F;]# sudo &#x2F;usr&#x2F;bin&#x2F;hdfs --config &#x2F;etc&#x2F;hadoop&#x2F;conf namenode</span><br><span class="line">2020-06-16 15:55:52,251 INFO namenode.NameNode: STARTUP_MSG:</span><br><span class="line">&#x2F;************************************************************</span><br><span class="line">STARTUP_MSG: Starting NameNode</span><br><span class="line">STARTUP_MSG:   host &#x3D; 25b603a089cc&#x2F;172.17.0.3</span><br><span class="line">STARTUP_MSG:   args &#x3D; []</span><br><span class="line">STARTUP_MSG:   version &#x3D; 3.0.0-cdh6.3.2</span><br><span class="line">STARTUP_MSG:   classpath &#x3D; &#x2F;etc&#x2F;hadoop&#x2F;conf:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-lang3-3.7.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-beanutils-1.9.4.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;log4j-1.2.17.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-logging-1.1.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;re2j-1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jackson-core-asl-1.9.13.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jettison-1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;guava-11.0.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;javax.activation-api-1.2.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;paranamer-2.8.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jetty-util-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;slf4j-log4j12.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jsr311-api-1.1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;azure-data-lake-store-sdk-2.2.9.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jersey-servlet-1.19.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jaxb-api-2.2.11.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;metrics-core-3.0.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;nimbus-jose-jwt-4.41.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;javax.servlet-api-3.1.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;protobuf-java-2.5.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-math3-3.1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jersey-core-1.19.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-compress-1.18.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;accessors-smart-1.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;woodstox-core-5.0.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jetty-io-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jackson-xc-1.9.13.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jetty-xml-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;curator-recipes-2.12.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jetty-servlet-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-lang-2.6.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;audience-annotations-0.5.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerby-config-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jetty-security-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jetty-server-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerb-util-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;netty-3.10.6.Final.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerb-admin-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jersey-json-1.19.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jackson-databind-2.9.9.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerb-core-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jackson-jaxrs-1.9.13.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerby-asn1-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-configuration2-2.1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerb-crypto-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;httpcore-4.4.6.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;log4j-core-2.8.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;json-smart-2.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;snappy-java-1.1.4.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jackson-mapper-asl-1.9.13-cloudera.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerb-simplekdc-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;logredactor-2.0.7.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;avro.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jackson-annotations-2.9.9.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerb-client-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jersey-server-1.19.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;curator-client-2.12.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-cli-1.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jcip-annotations-1.0-1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerb-server-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerby-xdr-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerb-common-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;htrace-core4-4.1.0-incubating.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;aws-java-sdk-bundle-1.11.271.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerby-util-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jetty-webapp-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;stax2-api-3.1.4.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;slf4j-api-1.7.25.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;log4j-api-2.8.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-io-2.6.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerby-pkix-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;curator-framework-2.12.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;gson-2.2.4.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jsp-api-2.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;xz-1.6.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;zookeeper.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jetty-http-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;wildfly-openssl-1.0.4.Final.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jackson-core-2.9.9.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;kerb-identity-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jaxb-impl-2.2.3-1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-net-3.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jul-to-slf4j-1.7.25.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;asm-5.0.4.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jsch-0.1.54.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;jsr305-3.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-collections-3.2.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;httpclient-4.5.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;lib&#x2F;commons-codec-1.11.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-format-javadoc.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-auth-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-jackson.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-azure-datalake.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-format.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-aws-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-azure-datalake-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-protobuf.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-annotations-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-auth.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-aws.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-azure.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-scala_2.11.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-nfs.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-annotations.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-pig.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-pig-bundle.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-common.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-hadoop-bundle.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-kms.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-kms-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-cascading.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-common-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-generator.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-azure-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-hadoop.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-common.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-common-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-avro.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-common-3.0.0-cdh6.3.2-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-format-sources.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-column.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-thrift.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-encoding.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;hadoop-nfs-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop&#x2F;.&#x2F;&#x2F;parquet-cascading3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-lang3-3.7.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-beanutils-1.9.4.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;log4j-1.2.17.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-logging-1.1.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;re2j-1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jackson-core-asl-1.9.13.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jettison-1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;guava-11.0.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;javax.activation-api-1.2.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;paranamer-2.8.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jetty-util-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jsr311-api-1.1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jersey-servlet-1.19.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jaxb-api-2.2.11.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;nimbus-jose-jwt-4.41.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;javax.servlet-api-3.1.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;protobuf-java-2.5.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;okio-1.6.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-math3-3.1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jersey-core-1.19.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-compress-1.18.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;accessors-smart-1.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;woodstox-core-5.0.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jetty-io-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jackson-xc-1.9.13.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jetty-xml-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;curator-recipes-2.12.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jetty-servlet-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-lang-2.6.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;json-simple-1.1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;avro-1.8.2-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;audience-annotations-0.5.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerby-config-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jetty-security-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jetty-server-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerb-util-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;zookeeper-3.4.5-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;netty-3.10.6.Final.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerb-admin-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jersey-json-1.19.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jackson-databind-2.9.9.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerb-core-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jackson-jaxrs-1.9.13.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerby-asn1-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-configuration2-2.1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerb-crypto-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;httpcore-4.4.6.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;json-smart-2.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;snappy-java-1.1.4.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jackson-mapper-asl-1.9.13-cloudera.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerb-simplekdc-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jackson-annotations-2.9.9.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerb-client-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jersey-server-1.19.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;curator-client-2.12.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-cli-1.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jcip-annotations-1.0-1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerb-server-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerby-xdr-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerb-common-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;htrace-core4-4.1.0-incubating.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerby-util-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jetty-webapp-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;stax2-api-3.1.4.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-io-2.6.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerby-pkix-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;curator-framework-2.12.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-daemon-1.0.13.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;gson-2.2.4.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;leveldbjni-all-1.8.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;xz-1.6.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jetty-http-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;okhttp-2.7.5.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jetty-util-ajax-9.3.25.v20180904.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jackson-core-2.9.9.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;kerb-identity-1.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jaxb-impl-2.2.3-1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-net-3.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;asm-5.0.4.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jsch-0.1.54.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;jsr305-3.0.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-collections-3.2.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;httpclient-4.5.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;lib&#x2F;commons-codec-1.11.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-native-client.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-client-3.0.0-cdh6.3.2-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-nfs-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-native-client-3.0.0-cdh6.3.2-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-nfs.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-httpfs-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-client-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-3.0.0-cdh6.3.2-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-native-client-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-client-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-httpfs.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-native-client-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;.&#x2F;&#x2F;hadoop-hdfs-client.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-examples-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-openstack.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-aliyun.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-core.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-extras-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-uploader.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-openstack-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-distcp.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-hs-plugins-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-sls.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-uploader-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-kafka.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-datajoin-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;netty-buffer-4.1.17.Final.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;netty-common-4.1.17.Final.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-jobclient-3.0.0-cdh6.3.2-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-gridmix-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;jdom-1.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-jobclient-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-examples.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-kafka-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;netty-codec-http-4.1.17.Final.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-rumen-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-app.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-extras.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-streaming-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;lz4-java-1.5.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;aliyun-sdk-oss-2.8.3.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-app-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-datajoin.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;azure-storage-5.4.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-hs.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-nativetask-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-jobclient.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-distcp-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-hs-plugins.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-shuffle-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-aliyun-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-common.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;kafka-clients-2.2.1-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;ojalgo-43.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-shuffle.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-gridmix.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-hs-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-jobclient-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;netty-resolver-4.1.17.Final.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-resourceestimator-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-rumen.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;azure-keyvault-core-0.8.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;netty-codec-4.1.17.Final.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-nativetask.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;netty-transport-4.1.17.Final.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-sls-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-resourceestimator.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-archives.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-archives-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-common-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;netty-handler-4.1.17.Final.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-mapreduce-client-core-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-archive-logs.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;zstd-jni-1.3.8-1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-streaming.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-mapreduce&#x2F;.&#x2F;&#x2F;hadoop-archive-logs-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;jackson-jaxrs-json-provider-2.9.9.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;guice-servlet-4.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;jackson-module-jaxb-annotations-2.9.9.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;jersey-client-1.19.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;objenesis-1.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;guice-4.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;jackson-jaxrs-base-2.9.9.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;metrics-core-3.0.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;java-util-1.9.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;fst-2.50.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;javax.inject-1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;geronimo-jcache_1.0_spec-1.0-alpha-1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;json-io-2.5.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;HikariCP-java7-2.4.12.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;bcprov-jdk15on-1.60.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;ehcache-3.3.1.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;bcpkix-jdk15on-1.60.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;aopalliance-1.0.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;jersey-guice-1.19.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;lib&#x2F;mssql-jdbc-6.2.1.jre7.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-applicationhistoryservice-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-registry-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-tests-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-common.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-sharedcachemanager-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-client-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-applicationhistoryservice.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-applications-unmanaged-am-launcher-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-resourcemanager.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-common.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-resourcemanager-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-timeline-pluginstorage.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-common-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-applications-distributedshell.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-tests.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-router-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-client.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-common-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-web-proxy-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-sharedcachemanager.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-web-proxy.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-router.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-api-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-nodemanager-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-registry.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-applications-distributedshell-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-timeline-pluginstorage-3.0.0-cdh6.3.2.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-api.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-applications-unmanaged-am-launcher.jar:&#x2F;usr&#x2F;lib&#x2F;hadoop-yarn&#x2F;.&#x2F;&#x2F;hadoop-yarn-server-nodemanager.jar</span><br><span class="line">STARTUP_MSG:   build &#x3D; http:&#x2F;&#x2F;github.com&#x2F;cloudera&#x2F;hadoop -r 9aff20de3b5ecccf3c19d57f71b214fb4d37ee89; compiled by &#39;jenkins&#39; on 2019-11-08T13:49Z</span><br><span class="line">STARTUP_MSG:   java &#x3D; 1.8.0_252</span><br><span class="line">************************************************************&#x2F;</span><br><span class="line">2020-06-16 15:55:52,271 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]</span><br><span class="line">2020-06-16 15:55:52,438 INFO namenode.NameNode: createNameNode []</span><br><span class="line">2020-06-16 15:55:52,671 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties</span><br><span class="line">2020-06-16 15:55:52,908 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).</span><br><span class="line">2020-06-16 15:55:52,909 INFO impl.MetricsSystemImpl: NameNode metrics system started</span><br><span class="line">2020-06-16 15:55:52,970 INFO namenode.NameNode: fs.defaultFS is hdfs:&#x2F;&#x2F;0.0.0.0:8020</span><br><span class="line">2020-06-16 15:55:52,977 INFO namenode.NameNode: Clients are to use 0.0.0.0:8020 to access this namenode&#x2F;service.</span><br><span class="line">2020-06-16 15:55:53,268 INFO util.JvmPauseMonitor: Starting JVM pause monitor</span><br><span class="line">2020-06-16 15:55:53,317 INFO hdfs.DFSUtil: Starting Web-server for hdfs at: http:&#x2F;&#x2F;0.0.0.0:9870</span><br><span class="line">2020-06-16 15:55:53,363 INFO util.log: Logging initialized @2085ms</span><br><span class="line">2020-06-16 15:55:53,573 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.</span><br><span class="line">2020-06-16 15:55:53,606 INFO http.HttpRequestLog: Http request log for http.requests.namenode is not defined</span><br><span class="line">2020-06-16 15:55:53,629 INFO http.HttpServer2: Added global filter &#39;safety&#39; (class&#x3D;org.apache.hadoop.http.HttpServer2$QuotingInputFilter)</span><br><span class="line">2020-06-16 15:55:53,635 INFO http.HttpServer2: Added filter static_user_filter (class&#x3D;org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs</span><br><span class="line">2020-06-16 15:55:53,635 INFO http.HttpServer2: Added filter static_user_filter (class&#x3D;org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static</span><br><span class="line">2020-06-16 15:55:53,635 INFO http.HttpServer2: Added filter static_user_filter (class&#x3D;org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs</span><br><span class="line">2020-06-16 15:55:53,697 INFO http.HttpServer2: Added filter &#39;org.apache.hadoop.hdfs.web.AuthFilter&#39; (class&#x3D;org.apache.hadoop.hdfs.web.AuthFilter)</span><br><span class="line">2020-06-16 15:55:53,700 INFO http.HttpServer2: addJerseyResourcePackage: packageName&#x3D;org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec&#x3D;&#x2F;webhdfs&#x2F;v1&#x2F;*</span><br><span class="line">2020-06-16 15:55:53,738 INFO http.HttpServer2: Jetty bound to port 9870</span><br><span class="line">2020-06-16 15:55:53,747 INFO server.Server: jetty-9.3.25.v20180904, build timestamp: 2018-09-04T21:11:46Z, git hash: 3ce520221d0240229c862b122d2b06c12a625732</span><br><span class="line">2020-06-16 15:55:53,858 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7bba5817&#123;&#x2F;logs,file:&#x2F;&#x2F;&#x2F;var&#x2F;log&#x2F;hadoop-hdfs&#x2F;,AVAILABLE&#125;</span><br><span class="line">2020-06-16 15:55:53,859 INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@75437611&#123;&#x2F;static,file:&#x2F;&#x2F;&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;webapps&#x2F;static&#x2F;,AVAILABLE&#125;</span><br><span class="line">2020-06-16 15:55:54,026 INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@20bd8be5&#123;&#x2F;,file:&#x2F;&#x2F;&#x2F;usr&#x2F;lib&#x2F;hadoop-hdfs&#x2F;webapps&#x2F;hdfs&#x2F;,AVAILABLE&#125;&#123;&#x2F;hdfs&#125;</span><br><span class="line">2020-06-16 15:55:54,041 INFO server.AbstractConnector: Started ServerConnector@24105dc5&#123;HTTP&#x2F;1.1,[http&#x2F;1.1]&#125;&#123;0.0.0.0:9870&#125;</span><br><span class="line">2020-06-16 15:55:54,041 INFO server.Server: Started @2764ms</span><br><span class="line">2020-06-16 15:55:54,394 WARN namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!</span><br><span class="line">2020-06-16 15:55:54,395 WARN namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!</span><br><span class="line">2020-06-16 15:55:54,533 INFO namenode.FSEditLog: Edit logging is async:true</span><br><span class="line">2020-06-16 15:55:54,560 INFO namenode.FSNamesystem: KeyProvider: null</span><br><span class="line">2020-06-16 15:55:54,563 INFO namenode.FSNamesystem: fsLock is fair: true</span><br><span class="line">2020-06-16 15:55:54,564 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false</span><br><span class="line">2020-06-16 15:55:54,583 INFO namenode.FSNamesystem: fsOwner             &#x3D; root (auth:SIMPLE)</span><br><span class="line">2020-06-16 15:55:54,583 INFO namenode.FSNamesystem: supergroup          &#x3D; supergroup</span><br><span class="line">2020-06-16 15:55:54,583 INFO namenode.FSNamesystem: isPermissionEnabled &#x3D; true</span><br><span class="line">2020-06-16 15:55:54,584 INFO namenode.FSNamesystem: HA Enabled: false</span><br><span class="line">2020-06-16 15:55:54,683 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling</span><br><span class="line">2020-06-16 15:55:54,712 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured&#x3D;1000, counted&#x3D;60, effected&#x3D;1000</span><br><span class="line">2020-06-16 15:55:54,713 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check&#x3D;true</span><br><span class="line">2020-06-16 15:55:54,722 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000</span><br><span class="line">2020-06-16 15:55:54,723 INFO blockmanagement.BlockManager: The block deletion will start around 2020 Jun 16 15:55:54</span><br><span class="line">2020-06-16 15:55:54,727 INFO util.GSet: Computing capacity for map BlocksMap</span><br><span class="line">2020-06-16 15:55:54,727 INFO util.GSet: VM type       &#x3D; 64-bit</span><br><span class="line">2020-06-16 15:55:54,732 INFO util.GSet: 2.0% max memory 876.5 MB &#x3D; 17.5 MB</span><br><span class="line">2020-06-16 15:55:54,732 INFO util.GSet: capacity      &#x3D; 2^21 &#x3D; 2097152 entries</span><br><span class="line">2020-06-16 15:55:54,760 INFO blockmanagement.BlockManager: dfs.block.access.token.enable &#x3D; false</span><br><span class="line">2020-06-16 15:55:54,771 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS</span><br><span class="line">2020-06-16 15:55:54,772 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct &#x3D; 0.9990000128746033</span><br><span class="line">2020-06-16 15:55:54,772 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes &#x3D; 0</span><br><span class="line">2020-06-16 15:55:54,772 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension &#x3D; 30000</span><br><span class="line">2020-06-16 15:55:54,772 INFO blockmanagement.BlockManager: defaultReplication         &#x3D; 3</span><br><span class="line">2020-06-16 15:55:54,772 INFO blockmanagement.BlockManager: maxReplication             &#x3D; 512</span><br><span class="line">2020-06-16 15:55:54,773 INFO blockmanagement.BlockManager: minReplication             &#x3D; 1</span><br><span class="line">2020-06-16 15:55:54,773 INFO blockmanagement.BlockManager: maxReplicationStreams      &#x3D; 2</span><br><span class="line">2020-06-16 15:55:54,773 INFO blockmanagement.BlockManager: redundancyRecheckInterval  &#x3D; 3000ms</span><br><span class="line">2020-06-16 15:55:54,773 INFO blockmanagement.BlockManager: encryptDataTransfer        &#x3D; false</span><br><span class="line">2020-06-16 15:55:54,773 INFO blockmanagement.BlockManager: maxNumBlocksToLog          &#x3D; 1000</span><br><span class="line">2020-06-16 15:55:54,841 INFO namenode.FSDirectory: GLOBAL serial map: bits&#x3D;24 maxEntries&#x3D;16777215</span><br><span class="line">2020-06-16 15:55:54,878 INFO util.GSet: Computing capacity for map INodeMap</span><br><span class="line">2020-06-16 15:55:54,878 INFO util.GSet: VM type       &#x3D; 64-bit</span><br><span class="line">2020-06-16 15:55:54,879 INFO util.GSet: 1.0% max memory 876.5 MB &#x3D; 8.8 MB</span><br><span class="line">2020-06-16 15:55:54,879 INFO util.GSet: capacity      &#x3D; 2^20 &#x3D; 1048576 entries</span><br><span class="line">2020-06-16 15:55:54,880 INFO namenode.FSDirectory: ACLs enabled? false</span><br><span class="line">2020-06-16 15:55:54,880 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true</span><br><span class="line">2020-06-16 15:55:54,880 INFO namenode.FSDirectory: XAttrs enabled? true</span><br><span class="line">2020-06-16 15:55:54,881 INFO namenode.NameNode: Caching file names occurring more than 10 times</span><br><span class="line">2020-06-16 15:55:54,891 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: true, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true</span><br><span class="line">2020-06-16 15:55:54,903 INFO util.GSet: Computing capacity for map cachedBlocks</span><br><span class="line">2020-06-16 15:55:54,903 INFO util.GSet: VM type       &#x3D; 64-bit</span><br><span class="line">2020-06-16 15:55:54,905 INFO util.GSet: 0.25% max memory 876.5 MB &#x3D; 2.2 MB</span><br><span class="line">2020-06-16 15:55:54,905 INFO util.GSet: capacity      &#x3D; 2^18 &#x3D; 262144 entries</span><br><span class="line">2020-06-16 15:55:54,925 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets &#x3D; 10</span><br><span class="line">2020-06-16 15:55:54,925 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users &#x3D; 10</span><br><span class="line">2020-06-16 15:55:54,925 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes &#x3D; 1,5,25</span><br><span class="line">2020-06-16 15:55:54,938 INFO namenode.FSNamesystem: Retry cache on namenode is enabled</span><br><span class="line">2020-06-16 15:55:54,938 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis</span><br><span class="line">2020-06-16 15:55:54,943 INFO util.GSet: Computing capacity for map NameNodeRetryCache</span><br><span class="line">2020-06-16 15:55:54,944 INFO util.GSet: VM type       &#x3D; 64-bit</span><br><span class="line">2020-06-16 15:55:54,944 INFO util.GSet: 0.029999999329447746% max memory 876.5 MB &#x3D; 269.3 KB</span><br><span class="line">2020-06-16 15:55:54,944 INFO util.GSet: capacity      &#x3D; 2^15 &#x3D; 32768 entries</span><br><span class="line">2020-06-16 15:55:54,978 INFO common.Storage: Lock on &#x2F;var&#x2F;lib&#x2F;hadoop-hdfs&#x2F;cache&#x2F;hdfs&#x2F;dfs&#x2F;name&#x2F;in_use.lock acquired by nodename 2029@25b603a089cc</span><br><span class="line">2020-06-16 15:55:55,028 INFO namenode.FileJournalManager: Recovering unfinalized segments in &#x2F;var&#x2F;lib&#x2F;hadoop-hdfs&#x2F;cache&#x2F;hdfs&#x2F;dfs&#x2F;name&#x2F;current</span><br><span class="line">2020-06-16 15:55:55,084 INFO namenode.FileJournalManager: Finalizing edits file &#x2F;var&#x2F;lib&#x2F;hadoop-hdfs&#x2F;cache&#x2F;hdfs&#x2F;dfs&#x2F;name&#x2F;current&#x2F;edits_inprogress_0000000000000000001 -&gt; &#x2F;var&#x2F;lib&#x2F;hadoop-hdfs&#x2F;cache&#x2F;hdfs&#x2F;dfs&#x2F;name&#x2F;current&#x2F;edits_0000000000000000001-0000000000000000001</span><br><span class="line">2020-06-16 15:55:55,121 INFO namenode.FSImage: Planning to load image: FSImageFile(file&#x3D;&#x2F;var&#x2F;lib&#x2F;hadoop-hdfs&#x2F;cache&#x2F;hdfs&#x2F;dfs&#x2F;name&#x2F;current&#x2F;fsimage_0000000000000000000, cpktTxId&#x3D;0000000000000000000)</span><br><span class="line">2020-06-16 15:55:55,282 INFO namenode.FSImageFormatPBINode: Loading 1 INodes.</span><br><span class="line">2020-06-16 15:55:55,349 INFO namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.</span><br><span class="line">2020-06-16 15:55:55,350 INFO namenode.FSImage: Loaded image for txid 0 from &#x2F;var&#x2F;lib&#x2F;hadoop-hdfs&#x2F;cache&#x2F;hdfs&#x2F;dfs&#x2F;name&#x2F;current&#x2F;fsimage_0000000000000000000</span><br><span class="line">2020-06-16 15:55:55,350 INFO namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@5674e1f2 expecting start txid #1</span><br><span class="line">2020-06-16 15:55:55,351 INFO namenode.FSImage: Start loading edits file &#x2F;var&#x2F;lib&#x2F;hadoop-hdfs&#x2F;cache&#x2F;hdfs&#x2F;dfs&#x2F;name&#x2F;current&#x2F;edits_0000000000000000001-0000000000000000001 maxTxnsToRead &#x3D; 9223372036854775807</span><br><span class="line">2020-06-16 15:55:55,355 INFO namenode.RedundantEditLogInputStream: Fast-forwarding stream &#39;&#x2F;var&#x2F;lib&#x2F;hadoop-hdfs&#x2F;cache&#x2F;hdfs&#x2F;dfs&#x2F;name&#x2F;current&#x2F;edits_0000000000000000001-0000000000000000001&#39; to transaction ID 1</span><br><span class="line">2020-06-16 15:55:55,390 INFO namenode.FSImage: Edits file &#x2F;var&#x2F;lib&#x2F;hadoop-hdfs&#x2F;cache&#x2F;hdfs&#x2F;dfs&#x2F;name&#x2F;current&#x2F;edits_0000000000000000001-0000000000000000001 of size 1048576 edits # 1 loaded in 0 seconds</span><br><span class="line">2020-06-16 15:55:55,390 INFO namenode.FSNamesystem: Need to save fs image? false (staleImage&#x3D;false, haEnabled&#x3D;false, isRollingUpgrade&#x3D;false)</span><br><span class="line">2020-06-16 15:55:55,392 INFO namenode.FSEditLog: Starting log segment at 2</span><br><span class="line">2020-06-16 15:55:55,519 INFO namenode.NameCache: initialized with 0 entries 0 lookups</span><br><span class="line">2020-06-16 15:55:55,519 INFO namenode.FSNamesystem: Finished loading FSImage in 569 msecs</span><br><span class="line">2020-06-16 15:55:55,917 INFO namenode.NameNode: RPC server is binding to 0.0.0.0:8020</span><br><span class="line">2020-06-16 15:55:55,939 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler</span><br><span class="line">2020-06-16 15:55:55,965 INFO ipc.Server: Starting Socket Reader #1 for port 8020</span><br><span class="line">2020-06-16 15:55:56,404 INFO namenode.FSNamesystem: Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.</span><br><span class="line">2020-06-16 15:55:56,423 INFO namenode.LeaseManager: Number of blocks under construction: 0</span><br><span class="line">2020-06-16 15:55:56,448 INFO blockmanagement.BlockManager: initializing replication queues</span><br><span class="line">2020-06-16 15:55:56,449 INFO hdfs.StateChange: STATE* Leaving safe mode after 0 secs</span><br><span class="line">2020-06-16 15:55:56,449 INFO hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes</span><br><span class="line">2020-06-16 15:55:56,449 INFO hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks</span><br><span class="line">2020-06-16 15:55:56,469 INFO blockmanagement.BlockManager: Total number of blocks            &#x3D; 0</span><br><span class="line">2020-06-16 15:55:56,469 INFO blockmanagement.BlockManager: Number of invalid blocks          &#x3D; 0</span><br><span class="line">2020-06-16 15:55:56,469 INFO blockmanagement.BlockManager: Number of under-replicated blocks &#x3D; 0</span><br><span class="line">2020-06-16 15:55:56,469 INFO blockmanagement.BlockManager: Number of  over-replicated blocks &#x3D; 0</span><br><span class="line">2020-06-16 15:55:56,470 INFO blockmanagement.BlockManager: Number of blocks being written    &#x3D; 0</span><br><span class="line">2020-06-16 15:55:56,471 INFO hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 22 msec</span><br><span class="line">2020-06-16 15:55:56,536 INFO ipc.Server: IPC Server Responder: starting</span><br><span class="line">2020-06-16 15:55:56,543 INFO ipc.Server: IPC Server listener on 8020: starting</span><br><span class="line">2020-06-16 15:55:56,554 INFO namenode.NameNode: NameNode RPC up at: 0.0.0.0&#x2F;0.0.0.0:8020</span><br><span class="line">2020-06-16 15:55:56,561 INFO namenode.FSNamesystem: Starting services required for active state</span><br><span class="line">2020-06-16 15:55:56,561 INFO namenode.FSDirectory: Initializing quota with 4 thread(s)</span><br><span class="line">2020-06-16 15:55:56,581 INFO namenode.FSDirectory: Quota initialization completed in 19 milliseconds</span><br><span class="line">name space&#x3D;1</span><br><span class="line">storage space&#x3D;0</span><br><span class="line">storage types&#x3D;RAM_DISK&#x3D;0, SSD&#x3D;0, DISK&#x3D;0, ARCHIVE&#x3D;0</span><br><span class="line">2020-06-16 15:55:56,595 INFO blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds</span><br><span class="line">^C2020-06-16 15:56:01,491 ERROR namenode.NameNode: RECEIVED SIGNAL 2: SIGINT</span><br><span class="line">2020-06-16 15:56:01,496 INFO namenode.NameNode: SHUTDOWN_MSG:</span><br><span class="line">&#x2F;************************************************************</span><br><span class="line">SHUTDOWN_MSG: Shutting down NameNode at 25b603a089cc&#x2F;172.17.0.3</span><br><span class="line">************************************************************&#x2F;</span><br></pre></td></tr></table></figure>

<p>发现namenode启动成功。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@25b603a089cc &#x2F;]# netstat -anp</span><br><span class="line">Active Internet connections (servers and established)</span><br><span class="line">Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID&#x2F;Program name</span><br><span class="line">tcp        0      0 0.0.0.0:9870            0.0.0.0:*               LISTEN      1872&#x2F;java</span><br><span class="line">tcp        0      0 0.0.0.0:8020            0.0.0.0:*               LISTEN      1872&#x2F;java</span><br><span class="line">tcp        0      0 172.17.0.3:36568        151.101.108.167:443     TIME_WAIT   -</span><br><span class="line">tcp        0      0 172.17.0.3:43778        202.104.186.227:80      TIME_WAIT   -</span><br><span class="line">tcp        0      0 172.17.0.3:43774        202.104.186.227:80      TIME_WAIT   -</span><br></pre></td></tr></table></figure>

<p>并且看到了8020他的rpc端口已经启动，9870就是web的端口。</p>
<p>接着，我们就可以在浏览器看到服务启动完毕了。</p>
<p>浏览器输入：<code>http://localhost:9870/</code></p>
<p><img src="/images/%E5%A4%A7%E6%95%B0%E6%8D%AE/hadoop-namenode.png" alt="hadoop-namenode-webui"></p>
<p>至此，大数据服务namenode容器安装完毕.</p>
<p>接着，我们继续部署我们的datanode服务。为了调试方便，我在同一个容器中部署datanode</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y hadoop-hdfs-datanode</span><br></pre></td></tr></table></figure>

<p>安装完毕之后，再启动服务</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@9ba32201bb25 &#x2F;]# netstat -anp</span><br><span class="line">Active Internet connections (servers and established)</span><br><span class="line">Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID&#x2F;Program name</span><br><span class="line">tcp        0      0 0.0.0.0:8020            0.0.0.0:*               LISTEN      917&#x2F;java</span><br><span class="line">tcp        0      0 127.0.0.11:39643        0.0.0.0:*               LISTEN      -</span><br><span class="line">tcp        0      0 0.0.0.0:9864            0.0.0.0:*               LISTEN      990&#x2F;java</span><br><span class="line">tcp        0      0 0.0.0.0:9866            0.0.0.0:*               LISTEN      990&#x2F;java</span><br><span class="line">tcp        0      0 0.0.0.0:9867            0.0.0.0:*               LISTEN      990&#x2F;java</span><br><span class="line">tcp        0      0 127.0.0.1:38411         0.0.0.0:*               LISTEN      990&#x2F;java</span><br><span class="line">tcp        0      0 0.0.0.0:9870            0.0.0.0:*               LISTEN      917&#x2F;java</span><br><span class="line">tcp        0      0 172.24.0.2:42042        113.96.181.216:80       TIME_WAIT   -</span><br><span class="line">tcp        0      0 172.24.0.2:8020         172.24.0.2:58574        ESTABLISHED 917&#x2F;java</span><br><span class="line">tcp        0      0 172.24.0.2:58574        172.24.0.2:8020         ESTABLISHED 990&#x2F;java</span><br><span class="line">udp        0      0 127.0.0.11:54498        0.0.0.0:*</span><br></pre></td></tr></table></figure>

<p>这里我们可以看到除了8020/9870这2个是namenode的端口之外，其他端口都是datanode节点的端口。</p>
<p>我们在<code>hdfs-site.xml</code>中添加了如下配置</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">	 &lt;name&gt;dfs.datanode.data.dir&lt;&#x2F;name&gt;</span><br><span class="line">	 &lt;value&gt;file:&#x2F;&#x2F;&#x2F;var&#x2F;lib&#x2F;hadoop-hdfs&#x2F;cache&#x2F;hdfs&#x2F;dfs&#x2F;data&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>

<p>再调整dockerfile。最终如下。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">FROM ccinn&#x2F;cdh6:latest</span><br><span class="line"></span><br><span class="line">LABEL maintainer&#x3D;&quot;Caiwenhui &lt;471113744@qq.com&gt;&quot;</span><br><span class="line"></span><br><span class="line">USER root</span><br><span class="line"></span><br><span class="line">ADD support.sh &#x2F;support.sh</span><br><span class="line">ADD conf&#x2F;core-site.xml &#x2F;etc&#x2F;hadoop&#x2F;conf&#x2F;</span><br><span class="line">ADD conf&#x2F;hdfs-site.xml &#x2F;etc&#x2F;hadoop&#x2F;conf&#x2F;</span><br><span class="line">ADD conf&#x2F;mapred-site.xml &#x2F;etc&#x2F;hadoop&#x2F;conf&#x2F;</span><br><span class="line">ADD bin&#x2F;run.sh &#x2F;bin&#x2F;</span><br><span class="line"></span><br><span class="line">RUN source &#x2F;support.sh;\</span><br><span class="line">  loop_exec &#39;yum install -y hadoop-hdfs-namenode hadoop-hdfs-datanode&#39;</span><br><span class="line"></span><br><span class="line">RUN mkdir -p var&#x2F;lib&#x2F;hadoop-hdfs&#x2F;cache&#x2F;hdfs&#x2F;dfs&#x2F;name;\</span><br><span class="line">  mkdir -p var&#x2F;lib&#x2F;hadoop-hdfs&#x2F;cache&#x2F;hdfs&#x2F;dfs&#x2F;data;</span><br><span class="line"></span><br><span class="line">WORKDIR &#x2F;</span><br><span class="line"></span><br><span class="line"># 9870 namenode&#39;s http</span><br><span class="line"># 9864 datanode&#39;s http</span><br><span class="line"></span><br><span class="line">EXPOSE 9870 9864</span><br><span class="line"></span><br><span class="line">CMD [&quot;&#x2F;bin&#x2F;run.sh&quot;]</span><br></pre></td></tr></table></figure>

<p>浏览器输入：<code>http://localhost:9864/</code></p>
<p><img src="/images/%E5%A4%A7%E6%95%B0%E6%8D%AE/hadoop-datanode.png" alt="hadoop-datanode-webui"></p>
<h2 id="构建hive镜像"><a href="#构建hive镜像" class="headerlink" title="构建hive镜像"></a>构建hive镜像</h2><ul>
<li><a href="https://github.com/base-big-data/docker-cdh6-hive" target="_blank" rel="noopener">基于基础镜像的hive服务</a></li>
</ul>
<p>前面，我们说到hadoop的生态组件是个繁杂的依赖关系，版本搞不对，经常会出现服务起不来的问题。</p>
<p>当我们不想用cdh给我们选择好的组件的时候，记得需要自己梳理好版本关系。</p>
<p><a href="http://hive.apache.org/downloads.html" target="_blank" rel="noopener">hive下载前的版本依赖说明-查看hadoop和hive版本的关系</a></p>
<p><a href="https://archive.cloudera.com/cdh6/6.3.2/redhat7/yum/RPMS/noarch/" target="_blank" rel="noopener">由于我们用的cdh6的源，所以我直接yum安装，这里hive的版本为2.1.1</a></p>
<figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> ccinn/cdh6:latest</span><br><span class="line"></span><br><span class="line"><span class="keyword">LABEL</span><span class="bash"> maintainer=<span class="string">"Caiwenhui &lt;471113744@qq.com&gt;"</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">USER</span> root</span><br><span class="line"></span><br><span class="line"><span class="keyword">ADD</span><span class="bash"> bin/support.sh /bin/</span></span><br><span class="line"><span class="keyword">ADD</span><span class="bash"> bin/run.sh /bin/</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装元数据存储服务 postgres</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> <span class="built_in">source</span> /bin/support.sh;\</span></span><br><span class="line"><span class="bash">  loop_exec <span class="string">'yum install -y hive hive-metastore postgresql-jdbc'</span> ;\</span></span><br><span class="line"><span class="bash">  ln -s /usr/share/java/postgresql-jdbc.jar /usr/lib/hive/lib/postgresql-jdbc.jar</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">ADD</span><span class="bash"> conf/hive-site.xml /etc/hive/conf/</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">WORKDIR</span><span class="bash"> /</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> [<span class="string">"/bin/run.sh"</span>]</span></span><br></pre></td></tr></table></figure>

<p>这里我们使用<code>postgresql</code>作为我们的元数据存储服务，所以我们安装了一个<code>postgresql-jdbc</code>，并且需要把jar包放在hive服务加载的环境变量下。</p>
<h2 id="构建用于hive镜像的postgres镜像"><a href="#构建用于hive镜像的postgres镜像" class="headerlink" title="构建用于hive镜像的postgres镜像"></a>构建用于hive镜像的postgres镜像</h2><ul>
<li><a href="https://github.com/base-big-data/docker-cdh6-hive-postgresql" target="_blank" rel="noopener">构建用于hive镜像的postgres镜像</a></li>
</ul>
<figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> postgres:<span class="number">9</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">LABEL</span><span class="bash"> maintainer=<span class="string">"Caiwenhui &lt;471113744@qq.com&gt;"</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> cdh6-hive-postgres /cdh6-hive-postgres</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> init-hive-db.sh /docker-entrypoint-initdb.d/init-user-db.sh</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 因为schema内部用了相对地址加载关联的sql，所以这里的工作目录需要指定为脚本当前目录</span></span><br><span class="line"><span class="keyword">WORKDIR</span><span class="bash"> /cdh6-hive-postgres</span></span><br></pre></td></tr></table></figure>

<p>整个dockerfile很简单，我们的用户和密码都是使用了了<code>hive</code>。</p>
<h2 id="构建impala镜像"><a href="#构建impala镜像" class="headerlink" title="构建impala镜像"></a>构建impala镜像</h2><ul>
<li><a href="https://github.com/base-big-data/docker-cdh6-impala" target="_blank" rel="noopener">构建impala镜像</a></li>
</ul>
<figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> ccinn/cdh6:latest</span><br><span class="line"></span><br><span class="line"><span class="keyword">LABEL</span><span class="bash"> maintainer=<span class="string">"Caiwenhui &lt;471113744@qq.com&gt;"</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">USER</span> root</span><br><span class="line"></span><br><span class="line"><span class="keyword">ADD</span><span class="bash"> bin/support.sh /bin/</span></span><br><span class="line"><span class="keyword">ADD</span><span class="bash"> bin/run.sh /bin/</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> <span class="built_in">source</span> /bin/support.sh;\</span></span><br><span class="line"><span class="bash">  loop_exec <span class="string">'yum install -y impala impala-server impala-shell impala-catalog impala-state-store'</span> ;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">WORKDIR</span><span class="bash"> /</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> [<span class="string">"/bin/bash"</span>]</span></span><br></pre></td></tr></table></figure>

<p>在这里，需要注意的是，我们的impala服务启动是有关联顺序问题的。</p>
<ul>
<li>1⃣️ impala-state-store</li>
<li>2⃣️ impala-catalog</li>
<li>3⃣️ impala-server(impalad)</li>
</ul>
<p><img src="/images/%E5%A4%A7%E6%95%B0%E6%8D%AE/impala.png" alt="impala-shell"></p>
<p>到此，我们整套大数据<code>OLAP</code>的体系设施，算是基本完成了。其实到hive已经算ok了。但是大家其实可以看到，我这里到计算引擎并没有使用<code>hiveserver2</code>。这里到<code>hive只是用了metastore</code>。</p>
<p>也因此，这个hive到metastore，目前来说仅仅只是<code>为了服务impala</code>用到。</p>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

    
      

    <span class="post-categories">
      <i class="icon-categories"></i>
        <a href="/categories/大数据/">大数据</a>
    </span>
    

    
    

    <span class="post-tags">
      <i class="icon-tags"></i>
        <a href="/tags/大数据，Hadoop/">大数据，Hadoop</a>
    </span>
    

    </div>

    
  </div>
</article>

  






    </main>

    <footer class="site-footer">
  <p class="site-info">
    Proudly powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and
    Theme by <a href="https://github.com/CodeDaraW/Hacker" target="_blank">Hacker</a>
    </br>
    
    &copy; 2025 白菜(whiteCcinn)
    
  </p>
</footer>
    
  </div>
</div>
</body>
</html>
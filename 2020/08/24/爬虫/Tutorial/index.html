<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>【爬虫】- scrapy | 白菜君の技术库</title>

  
  <meta name="author" content="白菜(whiteCcinn)">
  

  
  <meta name="description" content="知道做不到，等于不知道">
  

  
  <meta name="keywords" content="白菜,文辉,技术博客,whiteCcinn">
  

  <meta id="viewport" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">

  <meta property="og:title" content="【爬虫】- scrapy"/>

  <meta property="og:site_name" content="白菜君の技术库"/>

  
  <meta property="og:image" content="/favicon.ico"/>
  

  <link href="/favicon.ico" rel="icon">
  <link rel="alternate" href="/atom.xml" title="白菜君の技术库" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
<meta name="generator" content="Hexo 4.2.1"></head>


<body>
<div class="blog">
  <div class="content">

    <header>
  <div class="site-branding">
    <h1 class="site-title">
      <a href="/">白菜君の技术库</a>
    </h1>
    <p class="site-description"></p>
  </div>
  <nav class="site-navigation">
    <ul>
      
        <li><a href="/">主页</a></li>
      
        <li><a href="/archives/">文章</a></li>
      
        <li><a href="/tags/">标签</a></li>
      
        <li><a href="/categories/">分类</a></li>
      
        <li><a href="/about/">关于我</a></li>
      
    </ul>
  </nav>
</header>

    <main class="site-main posts-loop">
    <article>

  
    
    <h3 class="article-title"><span>【爬虫】- scrapy</span></h3>
    
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2020/08/24/爬虫/Tutorial/" rel="bookmark">
        <time class="entry-date published" datetime="2020-08-24T01:46:51.000Z">
          2020-08-24
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
        <h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近在和朋友写一个二手房的项目，需要用到爬虫，这里借助了scrapy来写。顺便整理了scrapy的用法.</p>
<p><a href="https://github.com/four-seas/source" target="_blank" rel="noopener">fous_seas/sources</a></p>
<a id="more"></a>

<h2 id="第一个-Spider"><a href="#第一个-Spider" class="headerlink" title="第一个 Spider"></a>第一个 Spider</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QuotesSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"quotes"</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        urls = [</span><br><span class="line">            <span class="string">'http://quotes.toscrape.com/page/1/'</span>,</span><br><span class="line">            <span class="string">'http://quotes.toscrape.com/page/2/'</span>,</span><br><span class="line">        ]</span><br><span class="line">        <span class="keyword">for</span> url <span class="keyword">in</span> urls:</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(url=url, callback=self.parse)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        page = response.url.split(<span class="string">"/"</span>)[<span class="number">-2</span>]</span><br><span class="line">        filename = <span class="string">'quotes-%s.html'</span> % page</span><br><span class="line">        <span class="keyword">with</span> open(filename, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(response.body)</span><br><span class="line">        self.log(<span class="string">'Saved file %s'</span> % filename)</span><br></pre></td></tr></table></figure>

<p>可以看到，我们新建的 QuotesSpider 类是继承自 scrapy.Spider 类的；下面看看其属性和方法的意义.</p>
<ul>
<li>name</li>
</ul>
<p>是 Spider 的标识符，用于唯一标识该 Spider；它必须在整个项目中是全局唯一的.</p>
<ul>
<li>start_requests()</li>
</ul>
<p>必须定义并返回一组可以被 Spider 爬取的 Requests，Request 对象由一个 URL 和一个回调函数构成.</p>
<ul>
<li>parse()</li>
</ul>
<p>就是 Request 对象中的回调方法，用来解析每一个 Request 之后的 Response；所以，parse() 方法就是用来解析返回的内容，通过解析得到的 URL 同样可以创建对应的 Requests 进而继续爬取.</p>
<p>再来看看具体的实现。</p>
<ul>
<li>start_request(self)</li>
</ul>
<p>方法分别针对 <a href="http://quotes.toscrape.com/page/1/" target="_blank" rel="noopener">http://quotes.toscrape.com/page/1/</a> 和 <a href="http://quotes.toscrape.com/page/2/" target="_blank" rel="noopener">http://quotes.toscrape.com/page/2/</a> 创建了两个需要被爬取的 Requests 对象；并通过 yield 进行迭代返回；备注，yield 是迭代生成器，是一个 Generator；</p>
<ul>
<li>parse(self, response)</li>
</ul>
<p>对 Request 的反馈的内容 Response 进行解析，这里的解析的逻辑很简单，就是分别创建两个本地文件，然后将 response.body 的内容放入这两个文件当中。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl quotes</span><br></pre></td></tr></table></figure>

<p>大致会输出如下内容</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">2016-12-16 21:24:05 [scrapy.core.engine] INFO: Spider opened</span><br><span class="line">2016-12-16 21:24:05 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)</span><br><span class="line">2016-12-16 21:24:05 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023</span><br><span class="line">2016-12-16 21:24:05 [scrapy.core.engine] DEBUG: Crawled (404) &lt;GET http://quotes.toscrape.com/robots.txt&gt; (referer: None)</span><br><span class="line">2016-12-16 21:24:05 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://quotes.toscrape.com/page/1/&gt; (referer: None)</span><br><span class="line">2016-12-16 21:24:05 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://quotes.toscrape.com/page/2/&gt; (referer: None)</span><br><span class="line">2016-12-16 21:24:05 [quotes] DEBUG: Saved file quotes-1.html</span><br><span class="line">2016-12-16 21:24:05 [quotes] DEBUG: Saved file quotes-2.html</span><br><span class="line">2016-12-16 21:24:05 [scrapy.core.engine] INFO: Closing spider (finished)</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>可以看到，通过爬取，我们在本地生成了两个 html 文件 quotes-1.html 和 quotes-2.html</p>
<h2 id="如何提取"><a href="#如何提取" class="headerlink" title="如何提取"></a>如何提取</h2><h3 id="通过命令行的方式提取"><a href="#通过命令行的方式提取" class="headerlink" title="通过命令行的方式提取"></a>通过命令行的方式提取</h3><p>Scrapy 提供了命令行的方式可以对需要被爬取的内容进行高效的<code>调试</code>，通过使用<code>Scrapy shell</code>进入命令行，然后在命令行中可以快速的对要爬取的内容进行提取；</p>
<blockquote>
<p>一定要学会调试！！！这是不能跳过的步骤</p>
</blockquote>
<p>我们试着通过 Scrapy shell 来提取下 “<a href="http://quotes.toscrape.com/page/1/&quot;" target="_blank" rel="noopener">http://quotes.toscrape.com/page/1/&quot;</a> 中的数据，通过执行如下命令，进入 shell</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scrapy shell &quot;http:&#x2F;&#x2F;quotes.toscrape.com&#x2F;page&#x2F;1&#x2F;&quot;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 或者</span><br><span class="line"></span><br><span class="line">scrapy shll</span><br><span class="line">fetch(&#39;http:&#x2F;&#x2F;quotes.toscrape.com&#x2F;page&#x2F;1&#x2F;&#39;)</span><br></pre></td></tr></table></figure>

<p>输出</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[ ... Scrapy log here ... ]</span><br><span class="line">2016-09-19 12:09:27 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http:&#x2F;&#x2F;quotes.toscrape.com&#x2F;page&#x2F;1&#x2F;&gt; (referer: None)</span><br><span class="line">[s] Available Scrapy objects:</span><br><span class="line">[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)</span><br><span class="line">[s]   crawler    &lt;scrapy.crawler.Crawler object at 0x7fa91d888c90&gt;</span><br><span class="line">[s]   item       &#123;&#125;</span><br><span class="line">[s]   request    &lt;GET http:&#x2F;&#x2F;quotes.toscrape.com&#x2F;page&#x2F;1&#x2F;&gt;</span><br><span class="line">[s]   response   &lt;200 http:&#x2F;&#x2F;quotes.toscrape.com&#x2F;page&#x2F;1&#x2F;&gt;</span><br><span class="line">[s]   settings   &lt;scrapy.settings.Settings object at 0x7fa91d888c10&gt;</span><br><span class="line">[s]   spider     &lt;DefaultSpider &#39;default&#39; at 0x7fa91c8af990&gt;</span><br><span class="line">[s] Useful shortcuts:</span><br><span class="line">[s]   shelp()           Shell help (print this help)</span><br><span class="line">[s]   fetch(req_or_url) Fetch request (or URL) and update local objects</span><br><span class="line">[s]   view(response)    View response in a browser</span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure>

<p>这样，我们就进入了 Scrapy shell 的环境，上面显示了连接请求和返回的相关信息，response 返回 status code 200 表示成功返回；</p>
<h3 id="通过-CSS-标准进行提取"><a href="#通过-CSS-标准进行提取" class="headerlink" title="通过 CSS 标准进行提取"></a>通过 CSS 标准进行提取</h3><p>这里主要是遵循 CSS 标准 <a href="https://www.w3.org/TR/selectors/" target="_blank" rel="noopener">https://www.w3.org/TR/selectors/</a> 来对网页的元素进行提取.</p>
<h4 id="通过使用-css-选择我们要提取的元素"><a href="#通过使用-css-选择我们要提取的元素" class="headerlink" title="通过使用 css() 选择我们要提取的元素"></a>通过使用 css() 选择我们要提取的元素</h4><p>下面演示一下如何提取元素.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; response.css(&#39;title&#39;)</span><br><span class="line">[&lt;Selector xpath&#x3D;u&#39;descendant-or-self::title&#39; data&#x3D;u&#39;&lt;title&gt;Quotes to Scrape&lt;&#x2F;title&gt;&#39;&gt;]</span><br></pre></td></tr></table></figure>

<p>可以看到，它通过返回一个类似 SelectorList 的对象成功的获取到了 <a href="http://quotes.toscrape.com/page/1/" target="_blank" rel="noopener">http://quotes.toscrape.com/page/1/</a> 页面中的 <title/> 的信息，该信息是封装在Selector对象中的 data 属性中的.</p>
<h4 id="提取Selector元素的文本内容，一般有两种方式用来提取"><a href="#提取Selector元素的文本内容，一般有两种方式用来提取" class="headerlink" title="提取Selector元素的文本内容，一般有两种方式用来提取"></a>提取Selector元素的文本内容，一般有两种方式用来提取</h4><ul>
<li>通过使用 extract() 或者 extract_first() 方法来提取元素的内容；下面演示如何提取 #1 返回的元素 <title/> 中的文本内容 text；</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; response.css(&#39;title::text&#39;).extract_first()</span><br><span class="line">&#39;Quotes to Scrape&#39;</span><br></pre></td></tr></table></figure>

<ul>
<li>extract_first() 表示提取返回队列中的第一个 Selector 对象；同样也可以使用如下的方式.</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; response.css(&#39;title::text&#39;)[0].extract()</span><br><span class="line">&#39;Quotes to Scrape&#39;</span><br></pre></td></tr></table></figure>

<p>不过 extract_first() 方法可以在当页面没有找到的情况下，避免出现<code>IndexError</code>的错误；</p>
<ul>
<li>通过 re() 方法来使用正则表达式的方式来进行提取元素的文本内容</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; response.css(&#39;title::text&#39;).re(r&#39;Quotes.*&#39;)</span><br><span class="line">[&#39;Quotes to Scrape&#39;]</span><br><span class="line">&gt;&gt;&gt; response.css(&#39;title::text&#39;).re(r&#39;Q\w+&#39;)</span><br><span class="line">[&#39;Quotes&#39;]</span><br><span class="line">&gt;&gt;&gt; response.css(&#39;title::text&#39;).re(r&#39;(\w+) to (\w+)&#39;)</span><br><span class="line">[&#39;Quotes&#39;, &#39;Scrape&#39;]</span><br></pre></td></tr></table></figure>

<h3 id="使用-XPath"><a href="#使用-XPath" class="headerlink" title="使用 XPath"></a>使用 XPath</h3><p>除了使用 <code>CSS 标准</code> 来提取元素意外，我们还可以使用 <code>XPath 标准</code>来提取元素，比如:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; response.xpath(&#39;&#x2F;&#x2F;title&#39;)</span><br><span class="line">[&lt;Selector xpath&#x3D;&#39;&#x2F;&#x2F;title&#39; data&#x3D;&#39;&lt;title&gt;Quotes to Scrape&lt;&#x2F;title&gt;&#39;&gt;]</span><br><span class="line">&gt;&gt;&gt; response.xpath(&#39;&#x2F;&#x2F;title&#x2F;text()&#39;).extract_first()</span><br><span class="line">&#39;Quotes to Scrape&#39;</span><br></pre></td></tr></table></figure>

<p>XPath 比 CSS 的爬取方式更为强大，因为它不仅仅是根据 HTML 的结构元素去进行检索(Navigating)，并且它可以顺带的对文本(text)进行检索；所以它可以支持 CSS 标准不能做到的场景，比如，检索一个 包含文本内容”Next Page”的 link 元素；这就使得通过 XPath 去构建爬虫更为简单.</p>
<h2 id="数据流设计图"><a href="#数据流设计图" class="headerlink" title="数据流设计图"></a>数据流设计图</h2><p><img src="/images/%E7%88%AC%E8%99%AB/scrapy_architecture.png" alt="架构图"></p>
<h2 id="Items"><a href="#Items" class="headerlink" title="Items"></a>Items</h2><p>Scrapy 的核心目的就是从非结构化的网页中提取出结构化的数据；默认的，Scrapy 爬虫以 dicts 的形式返回格式化的数据；但是，这里有一个问题，就是 dicts 并不能很好的表示这种结构化数据的结构，而且经常容易出错，转换也麻烦。</p>
<p>因此，Item 诞生了，它提供了这样一个简单的容器来收集爬取到的数据，并提供非常简便的 API 来声明它的 fields。</p>
<h3 id="声明-Items"><a href="#声明-Items" class="headerlink" title="声明 Items"></a>声明 Items</h3><p>通过一个简单的 class 和多个 Field 对象来声明 Items 对象；看一个 Product Item 的例子。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line"></span><br><span class="line">class Product(scrapy.Item):</span><br><span class="line">    name &#x3D; scrapy.Field()</span><br><span class="line">    price &#x3D; scrapy.Field()</span><br><span class="line">    stock &#x3D; scrapy.Field()</span><br><span class="line">    last_updated &#x3D; scrapy.Field(serializer&#x3D;str)</span><br></pre></td></tr></table></figure>

<p>需要注意的是，Product 继承自 scrapy.Item 父类.</p>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

    
      

    <span class="post-categories">
      <i class="icon-categories"></i>
        <a href="/categories/爬虫/">爬虫</a>
    </span>
    

    
    

    <span class="post-tags">
      <i class="icon-tags"></i>
        <a href="/tags/爬虫/">爬虫</a>
    </span>
    

    </div>

    
  </div>
</article>

  






    </main>

    <footer class="site-footer">
  <p class="site-info">
    Proudly powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and
    Theme by <a href="https://github.com/CodeDaraW/Hacker" target="_blank">Hacker</a>
    </br>
    
    &copy; 2025 白菜(whiteCcinn)
    
  </p>
</footer>
    
  </div>
</div>
</body>
</html>